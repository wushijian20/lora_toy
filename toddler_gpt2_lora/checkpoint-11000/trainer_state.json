{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.064066852367688,
  "eval_steps": 500,
  "global_step": 11000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002785515320334262,
      "grad_norm": 1.2840194702148438,
      "learning_rate": 0.00099974930362117,
      "loss": 4.3861,
      "step": 10
    },
    {
      "epoch": 0.005571030640668524,
      "grad_norm": 2.154874086380005,
      "learning_rate": 0.0009994707520891365,
      "loss": 3.4879,
      "step": 20
    },
    {
      "epoch": 0.008356545961002786,
      "grad_norm": 1.0960689783096313,
      "learning_rate": 0.0009991922005571031,
      "loss": 3.1786,
      "step": 30
    },
    {
      "epoch": 0.011142061281337047,
      "grad_norm": 1.6222344636917114,
      "learning_rate": 0.0009989136490250697,
      "loss": 3.4405,
      "step": 40
    },
    {
      "epoch": 0.013927576601671309,
      "grad_norm": 1.4486805200576782,
      "learning_rate": 0.0009986350974930363,
      "loss": 3.2406,
      "step": 50
    },
    {
      "epoch": 0.016713091922005572,
      "grad_norm": 1.4545409679412842,
      "learning_rate": 0.0009983565459610027,
      "loss": 2.9184,
      "step": 60
    },
    {
      "epoch": 0.019498607242339833,
      "grad_norm": 1.2460061311721802,
      "learning_rate": 0.0009980779944289695,
      "loss": 3.2825,
      "step": 70
    },
    {
      "epoch": 0.022284122562674095,
      "grad_norm": 1.1035219430923462,
      "learning_rate": 0.0009977994428969359,
      "loss": 2.9467,
      "step": 80
    },
    {
      "epoch": 0.025069637883008356,
      "grad_norm": 1.084650993347168,
      "learning_rate": 0.0009975208913649025,
      "loss": 2.8749,
      "step": 90
    },
    {
      "epoch": 0.027855153203342618,
      "grad_norm": 1.3431271314620972,
      "learning_rate": 0.000997242339832869,
      "loss": 2.7405,
      "step": 100
    },
    {
      "epoch": 0.03064066852367688,
      "grad_norm": 1.5075708627700806,
      "learning_rate": 0.0009969637883008356,
      "loss": 3.0164,
      "step": 110
    },
    {
      "epoch": 0.033426183844011144,
      "grad_norm": 1.139190673828125,
      "learning_rate": 0.0009966852367688022,
      "loss": 3.1564,
      "step": 120
    },
    {
      "epoch": 0.036211699164345405,
      "grad_norm": 1.4167214632034302,
      "learning_rate": 0.0009964066852367688,
      "loss": 2.9413,
      "step": 130
    },
    {
      "epoch": 0.03899721448467967,
      "grad_norm": 1.5525418519973755,
      "learning_rate": 0.0009961281337047354,
      "loss": 3.1507,
      "step": 140
    },
    {
      "epoch": 0.04178272980501393,
      "grad_norm": 1.2979793548583984,
      "learning_rate": 0.000995849582172702,
      "loss": 2.8883,
      "step": 150
    },
    {
      "epoch": 0.04456824512534819,
      "grad_norm": 1.815700650215149,
      "learning_rate": 0.0009955710306406686,
      "loss": 2.7748,
      "step": 160
    },
    {
      "epoch": 0.04735376044568245,
      "grad_norm": 1.3322603702545166,
      "learning_rate": 0.000995292479108635,
      "loss": 2.8884,
      "step": 170
    },
    {
      "epoch": 0.05013927576601671,
      "grad_norm": 1.5178040266036987,
      "learning_rate": 0.0009950139275766018,
      "loss": 2.7766,
      "step": 180
    },
    {
      "epoch": 0.052924791086350974,
      "grad_norm": 1.1066555976867676,
      "learning_rate": 0.0009947353760445684,
      "loss": 3.1326,
      "step": 190
    },
    {
      "epoch": 0.055710306406685235,
      "grad_norm": 1.3780025243759155,
      "learning_rate": 0.0009944568245125348,
      "loss": 2.837,
      "step": 200
    },
    {
      "epoch": 0.0584958217270195,
      "grad_norm": 1.104160189628601,
      "learning_rate": 0.0009941782729805016,
      "loss": 3.0843,
      "step": 210
    },
    {
      "epoch": 0.06128133704735376,
      "grad_norm": 1.1225481033325195,
      "learning_rate": 0.000993899721448468,
      "loss": 2.8273,
      "step": 220
    },
    {
      "epoch": 0.06406685236768803,
      "grad_norm": 1.7669438123703003,
      "learning_rate": 0.0009936211699164345,
      "loss": 2.9873,
      "step": 230
    },
    {
      "epoch": 0.06685236768802229,
      "grad_norm": 1.3010444641113281,
      "learning_rate": 0.0009933426183844011,
      "loss": 2.8877,
      "step": 240
    },
    {
      "epoch": 0.06963788300835655,
      "grad_norm": 1.6041792631149292,
      "learning_rate": 0.0009930640668523677,
      "loss": 2.8832,
      "step": 250
    },
    {
      "epoch": 0.07242339832869081,
      "grad_norm": 3.745532989501953,
      "learning_rate": 0.0009927855153203343,
      "loss": 3.1192,
      "step": 260
    },
    {
      "epoch": 0.07520891364902507,
      "grad_norm": 1.5474622249603271,
      "learning_rate": 0.0009925069637883009,
      "loss": 3.131,
      "step": 270
    },
    {
      "epoch": 0.07799442896935933,
      "grad_norm": 1.5285918712615967,
      "learning_rate": 0.0009922284122562675,
      "loss": 2.7621,
      "step": 280
    },
    {
      "epoch": 0.0807799442896936,
      "grad_norm": 1.1603755950927734,
      "learning_rate": 0.000991949860724234,
      "loss": 2.7839,
      "step": 290
    },
    {
      "epoch": 0.08356545961002786,
      "grad_norm": 1.2766029834747314,
      "learning_rate": 0.0009916713091922007,
      "loss": 2.9389,
      "step": 300
    },
    {
      "epoch": 0.08635097493036212,
      "grad_norm": 1.3101592063903809,
      "learning_rate": 0.000991392757660167,
      "loss": 2.9411,
      "step": 310
    },
    {
      "epoch": 0.08913649025069638,
      "grad_norm": 1.0913796424865723,
      "learning_rate": 0.0009911142061281338,
      "loss": 2.746,
      "step": 320
    },
    {
      "epoch": 0.09192200557103064,
      "grad_norm": 1.9779218435287476,
      "learning_rate": 0.0009908356545961002,
      "loss": 2.8228,
      "step": 330
    },
    {
      "epoch": 0.0947075208913649,
      "grad_norm": 1.625001072883606,
      "learning_rate": 0.0009905571030640668,
      "loss": 2.9564,
      "step": 340
    },
    {
      "epoch": 0.09749303621169916,
      "grad_norm": 1.529102087020874,
      "learning_rate": 0.0009902785515320334,
      "loss": 2.628,
      "step": 350
    },
    {
      "epoch": 0.10027855153203342,
      "grad_norm": 1.090011715888977,
      "learning_rate": 0.00099,
      "loss": 2.8333,
      "step": 360
    },
    {
      "epoch": 0.10306406685236769,
      "grad_norm": 1.157884120941162,
      "learning_rate": 0.0009897214484679666,
      "loss": 2.8647,
      "step": 370
    },
    {
      "epoch": 0.10584958217270195,
      "grad_norm": 1.417336106300354,
      "learning_rate": 0.0009894428969359332,
      "loss": 2.8201,
      "step": 380
    },
    {
      "epoch": 0.10863509749303621,
      "grad_norm": 1.907312273979187,
      "learning_rate": 0.0009891643454038998,
      "loss": 2.8994,
      "step": 390
    },
    {
      "epoch": 0.11142061281337047,
      "grad_norm": 1.195726752281189,
      "learning_rate": 0.0009888857938718664,
      "loss": 2.8248,
      "step": 400
    },
    {
      "epoch": 0.11420612813370473,
      "grad_norm": 1.2463825941085815,
      "learning_rate": 0.000988607242339833,
      "loss": 2.6675,
      "step": 410
    },
    {
      "epoch": 0.116991643454039,
      "grad_norm": 1.379415512084961,
      "learning_rate": 0.0009883286908077993,
      "loss": 2.7798,
      "step": 420
    },
    {
      "epoch": 0.11977715877437325,
      "grad_norm": 1.220304012298584,
      "learning_rate": 0.0009880501392757661,
      "loss": 2.5676,
      "step": 430
    },
    {
      "epoch": 0.12256267409470752,
      "grad_norm": 1.1389501094818115,
      "learning_rate": 0.0009877715877437327,
      "loss": 2.7481,
      "step": 440
    },
    {
      "epoch": 0.12534818941504178,
      "grad_norm": 1.3198240995407104,
      "learning_rate": 0.000987493036211699,
      "loss": 2.6279,
      "step": 450
    },
    {
      "epoch": 0.12813370473537605,
      "grad_norm": 1.134945034980774,
      "learning_rate": 0.0009872144846796657,
      "loss": 2.6653,
      "step": 460
    },
    {
      "epoch": 0.1309192200557103,
      "grad_norm": 1.6973408460617065,
      "learning_rate": 0.0009869359331476323,
      "loss": 2.6176,
      "step": 470
    },
    {
      "epoch": 0.13370473537604458,
      "grad_norm": 1.4935957193374634,
      "learning_rate": 0.0009866573816155989,
      "loss": 2.7296,
      "step": 480
    },
    {
      "epoch": 0.13649025069637882,
      "grad_norm": 1.3750529289245605,
      "learning_rate": 0.0009863788300835655,
      "loss": 2.5529,
      "step": 490
    },
    {
      "epoch": 0.1392757660167131,
      "grad_norm": 1.1334233283996582,
      "learning_rate": 0.000986100278551532,
      "loss": 2.6938,
      "step": 500
    },
    {
      "epoch": 0.14206128133704735,
      "grad_norm": 1.2611706256866455,
      "learning_rate": 0.0009858217270194986,
      "loss": 2.6381,
      "step": 510
    },
    {
      "epoch": 0.14484679665738162,
      "grad_norm": 1.4956426620483398,
      "learning_rate": 0.0009855431754874652,
      "loss": 2.8845,
      "step": 520
    },
    {
      "epoch": 0.14763231197771587,
      "grad_norm": 1.519185185432434,
      "learning_rate": 0.0009852646239554318,
      "loss": 2.8678,
      "step": 530
    },
    {
      "epoch": 0.15041782729805014,
      "grad_norm": 1.400498628616333,
      "learning_rate": 0.0009849860724233984,
      "loss": 2.5209,
      "step": 540
    },
    {
      "epoch": 0.1532033426183844,
      "grad_norm": 1.174527883529663,
      "learning_rate": 0.000984707520891365,
      "loss": 2.8298,
      "step": 550
    },
    {
      "epoch": 0.15598885793871867,
      "grad_norm": 1.046203851699829,
      "learning_rate": 0.0009844289693593314,
      "loss": 2.9215,
      "step": 560
    },
    {
      "epoch": 0.15877437325905291,
      "grad_norm": 1.573647141456604,
      "learning_rate": 0.000984150417827298,
      "loss": 2.7208,
      "step": 570
    },
    {
      "epoch": 0.1615598885793872,
      "grad_norm": 1.3784769773483276,
      "learning_rate": 0.0009838718662952646,
      "loss": 2.7379,
      "step": 580
    },
    {
      "epoch": 0.16434540389972144,
      "grad_norm": 1.1505237817764282,
      "learning_rate": 0.0009835933147632312,
      "loss": 2.7575,
      "step": 590
    },
    {
      "epoch": 0.1671309192200557,
      "grad_norm": 1.7309839725494385,
      "learning_rate": 0.0009833147632311977,
      "loss": 2.9615,
      "step": 600
    },
    {
      "epoch": 0.16991643454038996,
      "grad_norm": 1.3678911924362183,
      "learning_rate": 0.0009830362116991643,
      "loss": 2.6519,
      "step": 610
    },
    {
      "epoch": 0.17270194986072424,
      "grad_norm": 1.6271007061004639,
      "learning_rate": 0.000982757660167131,
      "loss": 2.8411,
      "step": 620
    },
    {
      "epoch": 0.17548746518105848,
      "grad_norm": 1.1443215608596802,
      "learning_rate": 0.0009824791086350975,
      "loss": 2.7633,
      "step": 630
    },
    {
      "epoch": 0.17827298050139276,
      "grad_norm": 1.3229976892471313,
      "learning_rate": 0.0009822005571030641,
      "loss": 2.5299,
      "step": 640
    },
    {
      "epoch": 0.181058495821727,
      "grad_norm": 1.2686564922332764,
      "learning_rate": 0.0009819220055710307,
      "loss": 2.8143,
      "step": 650
    },
    {
      "epoch": 0.18384401114206128,
      "grad_norm": 1.7103956937789917,
      "learning_rate": 0.0009816434540389973,
      "loss": 2.6153,
      "step": 660
    },
    {
      "epoch": 0.18662952646239556,
      "grad_norm": 1.1940762996673584,
      "learning_rate": 0.0009813649025069639,
      "loss": 2.7772,
      "step": 670
    },
    {
      "epoch": 0.1894150417827298,
      "grad_norm": 1.1911855936050415,
      "learning_rate": 0.0009810863509749305,
      "loss": 2.6748,
      "step": 680
    },
    {
      "epoch": 0.19220055710306408,
      "grad_norm": 1.1384587287902832,
      "learning_rate": 0.000980807799442897,
      "loss": 2.7359,
      "step": 690
    },
    {
      "epoch": 0.19498607242339833,
      "grad_norm": 1.5987355709075928,
      "learning_rate": 0.0009805292479108634,
      "loss": 2.7105,
      "step": 700
    },
    {
      "epoch": 0.1977715877437326,
      "grad_norm": 1.310819387435913,
      "learning_rate": 0.00098025069637883,
      "loss": 2.6184,
      "step": 710
    },
    {
      "epoch": 0.20055710306406685,
      "grad_norm": 1.2290271520614624,
      "learning_rate": 0.0009799721448467966,
      "loss": 2.7928,
      "step": 720
    },
    {
      "epoch": 0.20334261838440112,
      "grad_norm": 1.4798976182937622,
      "learning_rate": 0.0009796935933147632,
      "loss": 2.5798,
      "step": 730
    },
    {
      "epoch": 0.20612813370473537,
      "grad_norm": 10.657740592956543,
      "learning_rate": 0.0009794150417827298,
      "loss": 2.8772,
      "step": 740
    },
    {
      "epoch": 0.20891364902506965,
      "grad_norm": 1.506508231163025,
      "learning_rate": 0.0009791364902506964,
      "loss": 2.7455,
      "step": 750
    },
    {
      "epoch": 0.2116991643454039,
      "grad_norm": 1.8453625440597534,
      "learning_rate": 0.000978857938718663,
      "loss": 2.641,
      "step": 760
    },
    {
      "epoch": 0.21448467966573817,
      "grad_norm": 1.25679349899292,
      "learning_rate": 0.0009785793871866296,
      "loss": 2.6111,
      "step": 770
    },
    {
      "epoch": 0.21727019498607242,
      "grad_norm": 1.1964632272720337,
      "learning_rate": 0.0009783008356545962,
      "loss": 2.4162,
      "step": 780
    },
    {
      "epoch": 0.2200557103064067,
      "grad_norm": 1.8626738786697388,
      "learning_rate": 0.0009780222841225628,
      "loss": 2.8126,
      "step": 790
    },
    {
      "epoch": 0.22284122562674094,
      "grad_norm": 1.968948483467102,
      "learning_rate": 0.0009777437325905294,
      "loss": 2.6974,
      "step": 800
    },
    {
      "epoch": 0.22562674094707522,
      "grad_norm": 1.3703383207321167,
      "learning_rate": 0.0009774651810584957,
      "loss": 2.6127,
      "step": 810
    },
    {
      "epoch": 0.22841225626740946,
      "grad_norm": 1.504210114479065,
      "learning_rate": 0.0009771866295264623,
      "loss": 2.6646,
      "step": 820
    },
    {
      "epoch": 0.23119777158774374,
      "grad_norm": 1.2684354782104492,
      "learning_rate": 0.000976908077994429,
      "loss": 2.8587,
      "step": 830
    },
    {
      "epoch": 0.233983286908078,
      "grad_norm": 1.0541630983352661,
      "learning_rate": 0.0009766295264623955,
      "loss": 2.5336,
      "step": 840
    },
    {
      "epoch": 0.23676880222841226,
      "grad_norm": 1.1500130891799927,
      "learning_rate": 0.0009763509749303622,
      "loss": 2.5435,
      "step": 850
    },
    {
      "epoch": 0.2395543175487465,
      "grad_norm": 1.2419148683547974,
      "learning_rate": 0.0009760724233983287,
      "loss": 2.7044,
      "step": 860
    },
    {
      "epoch": 0.24233983286908078,
      "grad_norm": 1.7566492557525635,
      "learning_rate": 0.0009757938718662953,
      "loss": 2.593,
      "step": 870
    },
    {
      "epoch": 0.24512534818941503,
      "grad_norm": 1.3943686485290527,
      "learning_rate": 0.0009755153203342619,
      "loss": 2.528,
      "step": 880
    },
    {
      "epoch": 0.2479108635097493,
      "grad_norm": 1.9906026124954224,
      "learning_rate": 0.0009752367688022285,
      "loss": 2.5273,
      "step": 890
    },
    {
      "epoch": 0.25069637883008355,
      "grad_norm": 2.0821707248687744,
      "learning_rate": 0.0009749582172701949,
      "loss": 2.5782,
      "step": 900
    },
    {
      "epoch": 0.25348189415041783,
      "grad_norm": 1.6509175300598145,
      "learning_rate": 0.0009746796657381615,
      "loss": 2.4893,
      "step": 910
    },
    {
      "epoch": 0.2562674094707521,
      "grad_norm": 0.9984078407287598,
      "learning_rate": 0.0009744011142061282,
      "loss": 2.8026,
      "step": 920
    },
    {
      "epoch": 0.2590529247910863,
      "grad_norm": 1.4023449420928955,
      "learning_rate": 0.0009741225626740947,
      "loss": 2.7435,
      "step": 930
    },
    {
      "epoch": 0.2618384401114206,
      "grad_norm": 1.3604308366775513,
      "learning_rate": 0.0009738440111420613,
      "loss": 2.6713,
      "step": 940
    },
    {
      "epoch": 0.2646239554317549,
      "grad_norm": 1.4668251276016235,
      "learning_rate": 0.0009735654596100279,
      "loss": 2.8515,
      "step": 950
    },
    {
      "epoch": 0.26740947075208915,
      "grad_norm": 1.1580578088760376,
      "learning_rate": 0.0009732869080779945,
      "loss": 2.7169,
      "step": 960
    },
    {
      "epoch": 0.27019498607242337,
      "grad_norm": 1.3737342357635498,
      "learning_rate": 0.000973008356545961,
      "loss": 2.5633,
      "step": 970
    },
    {
      "epoch": 0.27298050139275765,
      "grad_norm": 1.1599451303482056,
      "learning_rate": 0.0009727298050139276,
      "loss": 2.6884,
      "step": 980
    },
    {
      "epoch": 0.2757660167130919,
      "grad_norm": 1.05137038230896,
      "learning_rate": 0.0009724512534818942,
      "loss": 2.4235,
      "step": 990
    },
    {
      "epoch": 0.2785515320334262,
      "grad_norm": 1.0877658128738403,
      "learning_rate": 0.0009721727019498607,
      "loss": 2.6942,
      "step": 1000
    },
    {
      "epoch": 0.28133704735376047,
      "grad_norm": 2.194101333618164,
      "learning_rate": 0.0009718941504178273,
      "loss": 2.475,
      "step": 1010
    },
    {
      "epoch": 0.2841225626740947,
      "grad_norm": 1.2096844911575317,
      "learning_rate": 0.0009716155988857938,
      "loss": 2.6578,
      "step": 1020
    },
    {
      "epoch": 0.28690807799442897,
      "grad_norm": 1.0541399717330933,
      "learning_rate": 0.0009713370473537605,
      "loss": 2.83,
      "step": 1030
    },
    {
      "epoch": 0.28969359331476324,
      "grad_norm": 1.3901503086090088,
      "learning_rate": 0.000971058495821727,
      "loss": 2.5087,
      "step": 1040
    },
    {
      "epoch": 0.2924791086350975,
      "grad_norm": 1.2318532466888428,
      "learning_rate": 0.0009707799442896936,
      "loss": 2.5932,
      "step": 1050
    },
    {
      "epoch": 0.29526462395543174,
      "grad_norm": 1.2542519569396973,
      "learning_rate": 0.0009705013927576602,
      "loss": 2.5852,
      "step": 1060
    },
    {
      "epoch": 0.298050139275766,
      "grad_norm": 1.1860110759735107,
      "learning_rate": 0.0009702228412256268,
      "loss": 2.5883,
      "step": 1070
    },
    {
      "epoch": 0.3008356545961003,
      "grad_norm": 1.6900781393051147,
      "learning_rate": 0.0009699442896935934,
      "loss": 2.7473,
      "step": 1080
    },
    {
      "epoch": 0.30362116991643456,
      "grad_norm": 1.2406134605407715,
      "learning_rate": 0.0009696657381615599,
      "loss": 2.6102,
      "step": 1090
    },
    {
      "epoch": 0.3064066852367688,
      "grad_norm": 1.1991857290267944,
      "learning_rate": 0.0009693871866295266,
      "loss": 2.6797,
      "step": 1100
    },
    {
      "epoch": 0.30919220055710306,
      "grad_norm": 1.3589164018630981,
      "learning_rate": 0.000969108635097493,
      "loss": 2.7528,
      "step": 1110
    },
    {
      "epoch": 0.31197771587743733,
      "grad_norm": 1.2236847877502441,
      "learning_rate": 0.0009688300835654596,
      "loss": 2.6546,
      "step": 1120
    },
    {
      "epoch": 0.3147632311977716,
      "grad_norm": 1.274904489517212,
      "learning_rate": 0.0009685515320334262,
      "loss": 2.5034,
      "step": 1130
    },
    {
      "epoch": 0.31754874651810583,
      "grad_norm": 1.236973524093628,
      "learning_rate": 0.0009682729805013928,
      "loss": 2.6597,
      "step": 1140
    },
    {
      "epoch": 0.3203342618384401,
      "grad_norm": 1.1496905088424683,
      "learning_rate": 0.0009679944289693593,
      "loss": 2.6729,
      "step": 1150
    },
    {
      "epoch": 0.3231197771587744,
      "grad_norm": 1.8131681680679321,
      "learning_rate": 0.0009677158774373259,
      "loss": 2.5096,
      "step": 1160
    },
    {
      "epoch": 0.32590529247910865,
      "grad_norm": 1.1523754596710205,
      "learning_rate": 0.0009674373259052926,
      "loss": 2.674,
      "step": 1170
    },
    {
      "epoch": 0.3286908077994429,
      "grad_norm": 1.370139241218567,
      "learning_rate": 0.0009671587743732591,
      "loss": 2.6944,
      "step": 1180
    },
    {
      "epoch": 0.33147632311977715,
      "grad_norm": 1.2893624305725098,
      "learning_rate": 0.0009668802228412257,
      "loss": 2.8343,
      "step": 1190
    },
    {
      "epoch": 0.3342618384401114,
      "grad_norm": 1.529947280883789,
      "learning_rate": 0.0009666016713091921,
      "loss": 2.5806,
      "step": 1200
    },
    {
      "epoch": 0.3370473537604457,
      "grad_norm": 1.4873576164245605,
      "learning_rate": 0.0009663231197771588,
      "loss": 2.4799,
      "step": 1210
    },
    {
      "epoch": 0.3398328690807799,
      "grad_norm": 1.774054765701294,
      "learning_rate": 0.0009660445682451253,
      "loss": 2.5528,
      "step": 1220
    },
    {
      "epoch": 0.3426183844011142,
      "grad_norm": 1.0401158332824707,
      "learning_rate": 0.0009657660167130919,
      "loss": 2.5364,
      "step": 1230
    },
    {
      "epoch": 0.34540389972144847,
      "grad_norm": 1.3592889308929443,
      "learning_rate": 0.0009654874651810586,
      "loss": 2.5348,
      "step": 1240
    },
    {
      "epoch": 0.34818941504178275,
      "grad_norm": 1.1431103944778442,
      "learning_rate": 0.0009652089136490251,
      "loss": 2.62,
      "step": 1250
    },
    {
      "epoch": 0.35097493036211697,
      "grad_norm": 1.5082741975784302,
      "learning_rate": 0.0009649303621169917,
      "loss": 2.6316,
      "step": 1260
    },
    {
      "epoch": 0.35376044568245124,
      "grad_norm": 1.2143549919128418,
      "learning_rate": 0.0009646518105849582,
      "loss": 2.8123,
      "step": 1270
    },
    {
      "epoch": 0.3565459610027855,
      "grad_norm": 1.06922447681427,
      "learning_rate": 0.0009643732590529249,
      "loss": 2.6701,
      "step": 1280
    },
    {
      "epoch": 0.3593314763231198,
      "grad_norm": 1.0305991172790527,
      "learning_rate": 0.0009640947075208914,
      "loss": 2.5861,
      "step": 1290
    },
    {
      "epoch": 0.362116991643454,
      "grad_norm": 1.1585032939910889,
      "learning_rate": 0.0009638161559888579,
      "loss": 2.6447,
      "step": 1300
    },
    {
      "epoch": 0.3649025069637883,
      "grad_norm": 1.4382073879241943,
      "learning_rate": 0.0009635376044568245,
      "loss": 2.5628,
      "step": 1310
    },
    {
      "epoch": 0.36768802228412256,
      "grad_norm": 1.4349507093429565,
      "learning_rate": 0.0009632590529247911,
      "loss": 2.7178,
      "step": 1320
    },
    {
      "epoch": 0.37047353760445684,
      "grad_norm": 1.4539517164230347,
      "learning_rate": 0.0009629805013927577,
      "loss": 2.6386,
      "step": 1330
    },
    {
      "epoch": 0.3732590529247911,
      "grad_norm": 1.190468668937683,
      "learning_rate": 0.0009627019498607242,
      "loss": 2.4744,
      "step": 1340
    },
    {
      "epoch": 0.37604456824512533,
      "grad_norm": 1.119303822517395,
      "learning_rate": 0.0009624233983286909,
      "loss": 2.6525,
      "step": 1350
    },
    {
      "epoch": 0.3788300835654596,
      "grad_norm": 1.793784499168396,
      "learning_rate": 0.0009621448467966574,
      "loss": 2.7358,
      "step": 1360
    },
    {
      "epoch": 0.3816155988857939,
      "grad_norm": 1.217052698135376,
      "learning_rate": 0.000961866295264624,
      "loss": 2.6905,
      "step": 1370
    },
    {
      "epoch": 0.38440111420612816,
      "grad_norm": 1.2579952478408813,
      "learning_rate": 0.0009615877437325905,
      "loss": 2.5621,
      "step": 1380
    },
    {
      "epoch": 0.3871866295264624,
      "grad_norm": 1.7327309846878052,
      "learning_rate": 0.0009613091922005572,
      "loss": 2.6191,
      "step": 1390
    },
    {
      "epoch": 0.38997214484679665,
      "grad_norm": 1.1122167110443115,
      "learning_rate": 0.0009610306406685237,
      "loss": 2.4747,
      "step": 1400
    },
    {
      "epoch": 0.39275766016713093,
      "grad_norm": 1.5513404607772827,
      "learning_rate": 0.0009607520891364902,
      "loss": 2.7159,
      "step": 1410
    },
    {
      "epoch": 0.3955431754874652,
      "grad_norm": 1.1327400207519531,
      "learning_rate": 0.0009604735376044569,
      "loss": 2.6202,
      "step": 1420
    },
    {
      "epoch": 0.3983286908077994,
      "grad_norm": 1.3590971231460571,
      "learning_rate": 0.0009601949860724234,
      "loss": 2.5425,
      "step": 1430
    },
    {
      "epoch": 0.4011142061281337,
      "grad_norm": 1.1408050060272217,
      "learning_rate": 0.00095991643454039,
      "loss": 2.411,
      "step": 1440
    },
    {
      "epoch": 0.403899721448468,
      "grad_norm": 1.1366922855377197,
      "learning_rate": 0.0009596378830083565,
      "loss": 2.3751,
      "step": 1450
    },
    {
      "epoch": 0.40668523676880225,
      "grad_norm": 1.4062976837158203,
      "learning_rate": 0.0009593593314763232,
      "loss": 2.5531,
      "step": 1460
    },
    {
      "epoch": 0.40947075208913647,
      "grad_norm": 1.335235834121704,
      "learning_rate": 0.0009590807799442897,
      "loss": 2.5434,
      "step": 1470
    },
    {
      "epoch": 0.41225626740947074,
      "grad_norm": 3.3134031295776367,
      "learning_rate": 0.0009588022284122563,
      "loss": 2.7846,
      "step": 1480
    },
    {
      "epoch": 0.415041782729805,
      "grad_norm": 1.0454902648925781,
      "learning_rate": 0.000958523676880223,
      "loss": 2.5277,
      "step": 1490
    },
    {
      "epoch": 0.4178272980501393,
      "grad_norm": 1.2939006090164185,
      "learning_rate": 0.0009582451253481894,
      "loss": 2.6115,
      "step": 1500
    },
    {
      "epoch": 0.4206128133704735,
      "grad_norm": 1.9945476055145264,
      "learning_rate": 0.000957966573816156,
      "loss": 2.6617,
      "step": 1510
    },
    {
      "epoch": 0.4233983286908078,
      "grad_norm": 1.3265721797943115,
      "learning_rate": 0.0009576880222841225,
      "loss": 2.686,
      "step": 1520
    },
    {
      "epoch": 0.42618384401114207,
      "grad_norm": 0.9371128082275391,
      "learning_rate": 0.0009574094707520892,
      "loss": 2.5515,
      "step": 1530
    },
    {
      "epoch": 0.42896935933147634,
      "grad_norm": 1.3978512287139893,
      "learning_rate": 0.0009571309192200557,
      "loss": 2.6325,
      "step": 1540
    },
    {
      "epoch": 0.43175487465181056,
      "grad_norm": 1.2752435207366943,
      "learning_rate": 0.0009568523676880223,
      "loss": 2.636,
      "step": 1550
    },
    {
      "epoch": 0.43454038997214484,
      "grad_norm": 1.6711556911468506,
      "learning_rate": 0.0009565738161559889,
      "loss": 2.567,
      "step": 1560
    },
    {
      "epoch": 0.4373259052924791,
      "grad_norm": 1.335307240486145,
      "learning_rate": 0.0009562952646239555,
      "loss": 2.6164,
      "step": 1570
    },
    {
      "epoch": 0.4401114206128134,
      "grad_norm": 1.1553456783294678,
      "learning_rate": 0.0009560167130919221,
      "loss": 2.6844,
      "step": 1580
    },
    {
      "epoch": 0.4428969359331476,
      "grad_norm": 1.7144556045532227,
      "learning_rate": 0.0009557381615598885,
      "loss": 2.6237,
      "step": 1590
    },
    {
      "epoch": 0.4456824512534819,
      "grad_norm": 1.1878187656402588,
      "learning_rate": 0.0009554596100278552,
      "loss": 2.5384,
      "step": 1600
    },
    {
      "epoch": 0.44846796657381616,
      "grad_norm": 1.1038094758987427,
      "learning_rate": 0.0009551810584958217,
      "loss": 2.5169,
      "step": 1610
    },
    {
      "epoch": 0.45125348189415043,
      "grad_norm": 1.7697409391403198,
      "learning_rate": 0.0009549025069637883,
      "loss": 2.5856,
      "step": 1620
    },
    {
      "epoch": 0.45403899721448465,
      "grad_norm": 1.2317006587982178,
      "learning_rate": 0.0009546239554317548,
      "loss": 2.6675,
      "step": 1630
    },
    {
      "epoch": 0.4568245125348189,
      "grad_norm": 1.1886661052703857,
      "learning_rate": 0.0009543454038997215,
      "loss": 2.4544,
      "step": 1640
    },
    {
      "epoch": 0.4596100278551532,
      "grad_norm": 1.1133674383163452,
      "learning_rate": 0.0009540668523676881,
      "loss": 2.5635,
      "step": 1650
    },
    {
      "epoch": 0.4623955431754875,
      "grad_norm": 1.9716854095458984,
      "learning_rate": 0.0009537883008356546,
      "loss": 2.8094,
      "step": 1660
    },
    {
      "epoch": 0.46518105849582175,
      "grad_norm": 1.365518569946289,
      "learning_rate": 0.0009535097493036213,
      "loss": 2.6355,
      "step": 1670
    },
    {
      "epoch": 0.467966573816156,
      "grad_norm": 1.402441382408142,
      "learning_rate": 0.0009532311977715878,
      "loss": 2.3999,
      "step": 1680
    },
    {
      "epoch": 0.47075208913649025,
      "grad_norm": 1.7524722814559937,
      "learning_rate": 0.0009529526462395543,
      "loss": 2.7777,
      "step": 1690
    },
    {
      "epoch": 0.4735376044568245,
      "grad_norm": 1.1459492444992065,
      "learning_rate": 0.0009526740947075208,
      "loss": 2.564,
      "step": 1700
    },
    {
      "epoch": 0.4763231197771588,
      "grad_norm": 7.510284423828125,
      "learning_rate": 0.0009523955431754875,
      "loss": 2.6053,
      "step": 1710
    },
    {
      "epoch": 0.479108635097493,
      "grad_norm": 1.5252152681350708,
      "learning_rate": 0.0009521169916434541,
      "loss": 2.4871,
      "step": 1720
    },
    {
      "epoch": 0.4818941504178273,
      "grad_norm": 1.2871932983398438,
      "learning_rate": 0.0009518384401114206,
      "loss": 2.5053,
      "step": 1730
    },
    {
      "epoch": 0.48467966573816157,
      "grad_norm": 1.7470824718475342,
      "learning_rate": 0.0009515598885793872,
      "loss": 2.7847,
      "step": 1740
    },
    {
      "epoch": 0.48746518105849584,
      "grad_norm": 0.9987716674804688,
      "learning_rate": 0.0009512813370473538,
      "loss": 2.7062,
      "step": 1750
    },
    {
      "epoch": 0.49025069637883006,
      "grad_norm": 1.338960886001587,
      "learning_rate": 0.0009510027855153204,
      "loss": 2.5096,
      "step": 1760
    },
    {
      "epoch": 0.49303621169916434,
      "grad_norm": 1.1796753406524658,
      "learning_rate": 0.0009507242339832869,
      "loss": 2.536,
      "step": 1770
    },
    {
      "epoch": 0.4958217270194986,
      "grad_norm": 1.287132740020752,
      "learning_rate": 0.0009504456824512536,
      "loss": 2.5299,
      "step": 1780
    },
    {
      "epoch": 0.4986072423398329,
      "grad_norm": 1.1568644046783447,
      "learning_rate": 0.00095016713091922,
      "loss": 2.5204,
      "step": 1790
    },
    {
      "epoch": 0.5013927576601671,
      "grad_norm": 1.163244605064392,
      "learning_rate": 0.0009498885793871866,
      "loss": 2.5523,
      "step": 1800
    },
    {
      "epoch": 0.5041782729805014,
      "grad_norm": 1.148862600326538,
      "learning_rate": 0.0009496100278551532,
      "loss": 2.1509,
      "step": 1810
    },
    {
      "epoch": 0.5069637883008357,
      "grad_norm": 1.3004590272903442,
      "learning_rate": 0.0009493314763231198,
      "loss": 2.5613,
      "step": 1820
    },
    {
      "epoch": 0.5097493036211699,
      "grad_norm": 1.424458622932434,
      "learning_rate": 0.0009490529247910864,
      "loss": 2.6177,
      "step": 1830
    },
    {
      "epoch": 0.5125348189415042,
      "grad_norm": 1.2391772270202637,
      "learning_rate": 0.0009487743732590529,
      "loss": 2.2701,
      "step": 1840
    },
    {
      "epoch": 0.5153203342618384,
      "grad_norm": 1.0167710781097412,
      "learning_rate": 0.0009484958217270196,
      "loss": 2.561,
      "step": 1850
    },
    {
      "epoch": 0.5181058495821727,
      "grad_norm": 1.1867314577102661,
      "learning_rate": 0.0009482172701949861,
      "loss": 2.6917,
      "step": 1860
    },
    {
      "epoch": 0.520891364902507,
      "grad_norm": 1.193684458732605,
      "learning_rate": 0.0009479387186629527,
      "loss": 2.7426,
      "step": 1870
    },
    {
      "epoch": 0.5236768802228412,
      "grad_norm": 1.7442216873168945,
      "learning_rate": 0.0009476601671309193,
      "loss": 2.6804,
      "step": 1880
    },
    {
      "epoch": 0.5264623955431755,
      "grad_norm": 1.3376418352127075,
      "learning_rate": 0.0009473816155988858,
      "loss": 2.6356,
      "step": 1890
    },
    {
      "epoch": 0.5292479108635098,
      "grad_norm": 1.4306271076202393,
      "learning_rate": 0.0009471030640668524,
      "loss": 2.3171,
      "step": 1900
    },
    {
      "epoch": 0.532033426183844,
      "grad_norm": 1.5463263988494873,
      "learning_rate": 0.0009468245125348189,
      "loss": 2.6849,
      "step": 1910
    },
    {
      "epoch": 0.5348189415041783,
      "grad_norm": 1.184936761856079,
      "learning_rate": 0.0009465459610027855,
      "loss": 2.5248,
      "step": 1920
    },
    {
      "epoch": 0.5376044568245125,
      "grad_norm": 1.2416313886642456,
      "learning_rate": 0.0009462674094707521,
      "loss": 2.7187,
      "step": 1930
    },
    {
      "epoch": 0.5403899721448467,
      "grad_norm": 1.124880313873291,
      "learning_rate": 0.0009459888579387187,
      "loss": 2.3432,
      "step": 1940
    },
    {
      "epoch": 0.5431754874651811,
      "grad_norm": 1.4465066194534302,
      "learning_rate": 0.0009457103064066852,
      "loss": 2.7065,
      "step": 1950
    },
    {
      "epoch": 0.5459610027855153,
      "grad_norm": 1.2358649969100952,
      "learning_rate": 0.0009454317548746519,
      "loss": 2.6932,
      "step": 1960
    },
    {
      "epoch": 0.5487465181058496,
      "grad_norm": 1.2498185634613037,
      "learning_rate": 0.0009451532033426185,
      "loss": 2.556,
      "step": 1970
    },
    {
      "epoch": 0.5515320334261838,
      "grad_norm": 1.4456546306610107,
      "learning_rate": 0.000944874651810585,
      "loss": 2.5335,
      "step": 1980
    },
    {
      "epoch": 0.5543175487465181,
      "grad_norm": 1.2559243440628052,
      "learning_rate": 0.0009445961002785515,
      "loss": 2.3494,
      "step": 1990
    },
    {
      "epoch": 0.5571030640668524,
      "grad_norm": 1.206810712814331,
      "learning_rate": 0.0009443175487465181,
      "loss": 2.5171,
      "step": 2000
    },
    {
      "epoch": 0.5598885793871866,
      "grad_norm": 1.7402015924453735,
      "learning_rate": 0.0009440389972144847,
      "loss": 2.528,
      "step": 2010
    },
    {
      "epoch": 0.5626740947075209,
      "grad_norm": 1.1082751750946045,
      "learning_rate": 0.0009437604456824512,
      "loss": 2.5047,
      "step": 2020
    },
    {
      "epoch": 0.5654596100278552,
      "grad_norm": 1.6649291515350342,
      "learning_rate": 0.0009434818941504178,
      "loss": 2.6178,
      "step": 2030
    },
    {
      "epoch": 0.5682451253481894,
      "grad_norm": 1.6362732648849487,
      "learning_rate": 0.0009432033426183845,
      "loss": 2.5183,
      "step": 2040
    },
    {
      "epoch": 0.5710306406685237,
      "grad_norm": 1.33102285861969,
      "learning_rate": 0.000942924791086351,
      "loss": 2.5807,
      "step": 2050
    },
    {
      "epoch": 0.5738161559888579,
      "grad_norm": 1.0900118350982666,
      "learning_rate": 0.0009426462395543176,
      "loss": 2.4656,
      "step": 2060
    },
    {
      "epoch": 0.5766016713091922,
      "grad_norm": 1.745469570159912,
      "learning_rate": 0.0009423676880222842,
      "loss": 2.4451,
      "step": 2070
    },
    {
      "epoch": 0.5793871866295265,
      "grad_norm": 1.1367565393447876,
      "learning_rate": 0.0009420891364902508,
      "loss": 2.5539,
      "step": 2080
    },
    {
      "epoch": 0.5821727019498607,
      "grad_norm": 1.6965676546096802,
      "learning_rate": 0.0009418105849582172,
      "loss": 2.6322,
      "step": 2090
    },
    {
      "epoch": 0.584958217270195,
      "grad_norm": 1.3401778936386108,
      "learning_rate": 0.0009415320334261838,
      "loss": 2.534,
      "step": 2100
    },
    {
      "epoch": 0.5877437325905293,
      "grad_norm": 1.1287577152252197,
      "learning_rate": 0.0009412534818941504,
      "loss": 2.6249,
      "step": 2110
    },
    {
      "epoch": 0.5905292479108635,
      "grad_norm": 1.3366490602493286,
      "learning_rate": 0.000940974930362117,
      "loss": 2.5586,
      "step": 2120
    },
    {
      "epoch": 0.5933147632311978,
      "grad_norm": 1.8460195064544678,
      "learning_rate": 0.0009406963788300836,
      "loss": 2.6206,
      "step": 2130
    },
    {
      "epoch": 0.596100278551532,
      "grad_norm": 1.9260218143463135,
      "learning_rate": 0.0009404178272980502,
      "loss": 2.7042,
      "step": 2140
    },
    {
      "epoch": 0.5988857938718662,
      "grad_norm": 1.5443943738937378,
      "learning_rate": 0.0009401392757660168,
      "loss": 2.4039,
      "step": 2150
    },
    {
      "epoch": 0.6016713091922006,
      "grad_norm": 0.9831684231758118,
      "learning_rate": 0.0009398607242339833,
      "loss": 2.4131,
      "step": 2160
    },
    {
      "epoch": 0.6044568245125348,
      "grad_norm": 1.2676228284835815,
      "learning_rate": 0.0009395821727019499,
      "loss": 2.5306,
      "step": 2170
    },
    {
      "epoch": 0.6072423398328691,
      "grad_norm": 1.2655693292617798,
      "learning_rate": 0.0009393036211699164,
      "loss": 2.4938,
      "step": 2180
    },
    {
      "epoch": 0.6100278551532033,
      "grad_norm": 0.9881119728088379,
      "learning_rate": 0.000939025069637883,
      "loss": 2.6313,
      "step": 2190
    },
    {
      "epoch": 0.6128133704735376,
      "grad_norm": 1.1482727527618408,
      "learning_rate": 0.0009387465181058496,
      "loss": 2.7808,
      "step": 2200
    },
    {
      "epoch": 0.6155988857938719,
      "grad_norm": 1.144348382949829,
      "learning_rate": 0.0009384679665738161,
      "loss": 2.5364,
      "step": 2210
    },
    {
      "epoch": 0.6183844011142061,
      "grad_norm": 1.1865334510803223,
      "learning_rate": 0.0009381894150417828,
      "loss": 2.3514,
      "step": 2220
    },
    {
      "epoch": 0.6211699164345403,
      "grad_norm": 1.1391347646713257,
      "learning_rate": 0.0009379108635097493,
      "loss": 2.5811,
      "step": 2230
    },
    {
      "epoch": 0.6239554317548747,
      "grad_norm": 1.3156872987747192,
      "learning_rate": 0.0009376323119777159,
      "loss": 2.4006,
      "step": 2240
    },
    {
      "epoch": 0.6267409470752089,
      "grad_norm": 1.3639733791351318,
      "learning_rate": 0.0009373537604456825,
      "loss": 2.5577,
      "step": 2250
    },
    {
      "epoch": 0.6295264623955432,
      "grad_norm": 1.174679160118103,
      "learning_rate": 0.0009370752089136491,
      "loss": 2.7211,
      "step": 2260
    },
    {
      "epoch": 0.6323119777158774,
      "grad_norm": 1.532568335533142,
      "learning_rate": 0.0009367966573816156,
      "loss": 2.7403,
      "step": 2270
    },
    {
      "epoch": 0.6350974930362117,
      "grad_norm": 1.4306436777114868,
      "learning_rate": 0.0009365181058495821,
      "loss": 2.5863,
      "step": 2280
    },
    {
      "epoch": 0.637883008356546,
      "grad_norm": 1.4753378629684448,
      "learning_rate": 0.0009362395543175488,
      "loss": 2.5551,
      "step": 2290
    },
    {
      "epoch": 0.6406685236768802,
      "grad_norm": 1.4872273206710815,
      "learning_rate": 0.0009359610027855153,
      "loss": 2.7422,
      "step": 2300
    },
    {
      "epoch": 0.6434540389972145,
      "grad_norm": 1.1254687309265137,
      "learning_rate": 0.0009356824512534819,
      "loss": 2.5091,
      "step": 2310
    },
    {
      "epoch": 0.6462395543175488,
      "grad_norm": 1.2862012386322021,
      "learning_rate": 0.0009354038997214485,
      "loss": 2.5518,
      "step": 2320
    },
    {
      "epoch": 0.649025069637883,
      "grad_norm": 1.3595821857452393,
      "learning_rate": 0.0009351253481894151,
      "loss": 2.4441,
      "step": 2330
    },
    {
      "epoch": 0.6518105849582173,
      "grad_norm": 1.102573275566101,
      "learning_rate": 0.0009348467966573816,
      "loss": 2.3341,
      "step": 2340
    },
    {
      "epoch": 0.6545961002785515,
      "grad_norm": 1.533652901649475,
      "learning_rate": 0.0009345682451253482,
      "loss": 2.4908,
      "step": 2350
    },
    {
      "epoch": 0.6573816155988857,
      "grad_norm": 1.5299750566482544,
      "learning_rate": 0.0009342896935933149,
      "loss": 2.6004,
      "step": 2360
    },
    {
      "epoch": 0.6601671309192201,
      "grad_norm": 1.0821197032928467,
      "learning_rate": 0.0009340111420612814,
      "loss": 2.6225,
      "step": 2370
    },
    {
      "epoch": 0.6629526462395543,
      "grad_norm": 1.1534849405288696,
      "learning_rate": 0.000933732590529248,
      "loss": 2.4417,
      "step": 2380
    },
    {
      "epoch": 0.6657381615598886,
      "grad_norm": 1.0310934782028198,
      "learning_rate": 0.0009334540389972144,
      "loss": 2.5188,
      "step": 2390
    },
    {
      "epoch": 0.6685236768802229,
      "grad_norm": 1.2364609241485596,
      "learning_rate": 0.0009331754874651811,
      "loss": 2.2814,
      "step": 2400
    },
    {
      "epoch": 0.6713091922005571,
      "grad_norm": 1.021937370300293,
      "learning_rate": 0.0009328969359331476,
      "loss": 2.525,
      "step": 2410
    },
    {
      "epoch": 0.6740947075208914,
      "grad_norm": 1.1188453435897827,
      "learning_rate": 0.0009326183844011142,
      "loss": 2.637,
      "step": 2420
    },
    {
      "epoch": 0.6768802228412256,
      "grad_norm": 1.3072495460510254,
      "learning_rate": 0.0009323398328690808,
      "loss": 2.4784,
      "step": 2430
    },
    {
      "epoch": 0.6796657381615598,
      "grad_norm": 1.2688318490982056,
      "learning_rate": 0.0009320612813370474,
      "loss": 2.5733,
      "step": 2440
    },
    {
      "epoch": 0.6824512534818942,
      "grad_norm": 1.2329766750335693,
      "learning_rate": 0.000931782729805014,
      "loss": 2.5463,
      "step": 2450
    },
    {
      "epoch": 0.6852367688022284,
      "grad_norm": 1.4346177577972412,
      "learning_rate": 0.0009315041782729805,
      "loss": 2.5082,
      "step": 2460
    },
    {
      "epoch": 0.6880222841225627,
      "grad_norm": 2.4677248001098633,
      "learning_rate": 0.0009312256267409472,
      "loss": 2.5901,
      "step": 2470
    },
    {
      "epoch": 0.6908077994428969,
      "grad_norm": 1.1987907886505127,
      "learning_rate": 0.0009309470752089136,
      "loss": 2.5878,
      "step": 2480
    },
    {
      "epoch": 0.6935933147632312,
      "grad_norm": 1.2632218599319458,
      "learning_rate": 0.0009306685236768802,
      "loss": 2.5329,
      "step": 2490
    },
    {
      "epoch": 0.6963788300835655,
      "grad_norm": 1.2860089540481567,
      "learning_rate": 0.0009303899721448468,
      "loss": 2.4634,
      "step": 2500
    },
    {
      "epoch": 0.6991643454038997,
      "grad_norm": 1.378875970840454,
      "learning_rate": 0.0009301114206128134,
      "loss": 2.6547,
      "step": 2510
    },
    {
      "epoch": 0.7019498607242339,
      "grad_norm": 1.2580184936523438,
      "learning_rate": 0.0009298328690807799,
      "loss": 2.7712,
      "step": 2520
    },
    {
      "epoch": 0.7047353760445683,
      "grad_norm": 1.05880868434906,
      "learning_rate": 0.0009295543175487465,
      "loss": 2.2454,
      "step": 2530
    },
    {
      "epoch": 0.7075208913649025,
      "grad_norm": 1.0613309144973755,
      "learning_rate": 0.0009292757660167132,
      "loss": 2.5936,
      "step": 2540
    },
    {
      "epoch": 0.7103064066852368,
      "grad_norm": 1.392040729522705,
      "learning_rate": 0.0009289972144846797,
      "loss": 2.6245,
      "step": 2550
    },
    {
      "epoch": 0.713091922005571,
      "grad_norm": 0.9483211636543274,
      "learning_rate": 0.0009287186629526463,
      "loss": 2.5132,
      "step": 2560
    },
    {
      "epoch": 0.7158774373259053,
      "grad_norm": 1.5085972547531128,
      "learning_rate": 0.0009284401114206127,
      "loss": 2.5447,
      "step": 2570
    },
    {
      "epoch": 0.7186629526462396,
      "grad_norm": 1.1602801084518433,
      "learning_rate": 0.0009281615598885794,
      "loss": 2.4016,
      "step": 2580
    },
    {
      "epoch": 0.7214484679665738,
      "grad_norm": 1.3816580772399902,
      "learning_rate": 0.0009278830083565459,
      "loss": 2.6206,
      "step": 2590
    },
    {
      "epoch": 0.724233983286908,
      "grad_norm": 1.1086705923080444,
      "learning_rate": 0.0009276044568245125,
      "loss": 2.5344,
      "step": 2600
    },
    {
      "epoch": 0.7270194986072424,
      "grad_norm": 1.0299288034439087,
      "learning_rate": 0.0009273259052924792,
      "loss": 2.4897,
      "step": 2610
    },
    {
      "epoch": 0.7298050139275766,
      "grad_norm": 1.7090506553649902,
      "learning_rate": 0.0009270473537604457,
      "loss": 2.5758,
      "step": 2620
    },
    {
      "epoch": 0.7325905292479109,
      "grad_norm": 1.1588163375854492,
      "learning_rate": 0.0009267688022284123,
      "loss": 2.4412,
      "step": 2630
    },
    {
      "epoch": 0.7353760445682451,
      "grad_norm": 1.1340761184692383,
      "learning_rate": 0.0009264902506963788,
      "loss": 2.474,
      "step": 2640
    },
    {
      "epoch": 0.7381615598885793,
      "grad_norm": 2.122035264968872,
      "learning_rate": 0.0009262116991643455,
      "loss": 2.6817,
      "step": 2650
    },
    {
      "epoch": 0.7409470752089137,
      "grad_norm": 1.1821680068969727,
      "learning_rate": 0.000925933147632312,
      "loss": 2.4353,
      "step": 2660
    },
    {
      "epoch": 0.7437325905292479,
      "grad_norm": 1.3817375898361206,
      "learning_rate": 0.0009256545961002786,
      "loss": 2.5588,
      "step": 2670
    },
    {
      "epoch": 0.7465181058495822,
      "grad_norm": 1.2401751279830933,
      "learning_rate": 0.0009253760445682451,
      "loss": 2.2828,
      "step": 2680
    },
    {
      "epoch": 0.7493036211699164,
      "grad_norm": 1.2571775913238525,
      "learning_rate": 0.0009250974930362117,
      "loss": 2.4972,
      "step": 2690
    },
    {
      "epoch": 0.7520891364902507,
      "grad_norm": 1.6825190782546997,
      "learning_rate": 0.0009248189415041783,
      "loss": 2.4345,
      "step": 2700
    },
    {
      "epoch": 0.754874651810585,
      "grad_norm": 1.2267513275146484,
      "learning_rate": 0.0009245403899721448,
      "loss": 2.5957,
      "step": 2710
    },
    {
      "epoch": 0.7576601671309192,
      "grad_norm": 1.5243191719055176,
      "learning_rate": 0.0009242618384401115,
      "loss": 2.4952,
      "step": 2720
    },
    {
      "epoch": 0.7604456824512534,
      "grad_norm": 1.1846650838851929,
      "learning_rate": 0.000923983286908078,
      "loss": 2.5663,
      "step": 2730
    },
    {
      "epoch": 0.7632311977715878,
      "grad_norm": 1.1917568445205688,
      "learning_rate": 0.0009237047353760446,
      "loss": 2.4529,
      "step": 2740
    },
    {
      "epoch": 0.766016713091922,
      "grad_norm": 1.352648377418518,
      "learning_rate": 0.0009234261838440111,
      "loss": 2.6413,
      "step": 2750
    },
    {
      "epoch": 0.7688022284122563,
      "grad_norm": 1.727471947669983,
      "learning_rate": 0.0009231476323119778,
      "loss": 2.3129,
      "step": 2760
    },
    {
      "epoch": 0.7715877437325905,
      "grad_norm": 1.5598509311676025,
      "learning_rate": 0.0009228690807799444,
      "loss": 2.5441,
      "step": 2770
    },
    {
      "epoch": 0.7743732590529248,
      "grad_norm": 1.1470754146575928,
      "learning_rate": 0.0009225905292479108,
      "loss": 2.3836,
      "step": 2780
    },
    {
      "epoch": 0.7771587743732591,
      "grad_norm": 1.4743454456329346,
      "learning_rate": 0.0009223119777158775,
      "loss": 2.7022,
      "step": 2790
    },
    {
      "epoch": 0.7799442896935933,
      "grad_norm": 1.1145402193069458,
      "learning_rate": 0.000922033426183844,
      "loss": 2.3814,
      "step": 2800
    },
    {
      "epoch": 0.7827298050139275,
      "grad_norm": 1.4922983646392822,
      "learning_rate": 0.0009217548746518106,
      "loss": 2.5209,
      "step": 2810
    },
    {
      "epoch": 0.7855153203342619,
      "grad_norm": 1.3594104051589966,
      "learning_rate": 0.0009214763231197771,
      "loss": 2.6745,
      "step": 2820
    },
    {
      "epoch": 0.7883008356545961,
      "grad_norm": 1.466610074043274,
      "learning_rate": 0.0009211977715877438,
      "loss": 2.7521,
      "step": 2830
    },
    {
      "epoch": 0.7910863509749304,
      "grad_norm": 1.4076071977615356,
      "learning_rate": 0.0009209192200557103,
      "loss": 2.511,
      "step": 2840
    },
    {
      "epoch": 0.7938718662952646,
      "grad_norm": 1.1187593936920166,
      "learning_rate": 0.0009206406685236769,
      "loss": 2.399,
      "step": 2850
    },
    {
      "epoch": 0.7966573816155988,
      "grad_norm": 1.06912362575531,
      "learning_rate": 0.0009203621169916436,
      "loss": 2.4725,
      "step": 2860
    },
    {
      "epoch": 0.7994428969359332,
      "grad_norm": 1.273851990699768,
      "learning_rate": 0.00092008356545961,
      "loss": 2.2137,
      "step": 2870
    },
    {
      "epoch": 0.8022284122562674,
      "grad_norm": 1.1153093576431274,
      "learning_rate": 0.0009198050139275766,
      "loss": 2.4395,
      "step": 2880
    },
    {
      "epoch": 0.8050139275766016,
      "grad_norm": 1.678955316543579,
      "learning_rate": 0.0009195264623955431,
      "loss": 2.468,
      "step": 2890
    },
    {
      "epoch": 0.807799442896936,
      "grad_norm": 0.9879109859466553,
      "learning_rate": 0.0009192479108635098,
      "loss": 2.4845,
      "step": 2900
    },
    {
      "epoch": 0.8105849582172702,
      "grad_norm": 1.1129482984542847,
      "learning_rate": 0.0009189693593314763,
      "loss": 2.5903,
      "step": 2910
    },
    {
      "epoch": 0.8133704735376045,
      "grad_norm": 1.3030588626861572,
      "learning_rate": 0.0009186908077994429,
      "loss": 2.5479,
      "step": 2920
    },
    {
      "epoch": 0.8161559888579387,
      "grad_norm": 1.4459642171859741,
      "learning_rate": 0.0009184122562674095,
      "loss": 2.4442,
      "step": 2930
    },
    {
      "epoch": 0.8189415041782729,
      "grad_norm": 1.5102137327194214,
      "learning_rate": 0.0009181337047353761,
      "loss": 2.6095,
      "step": 2940
    },
    {
      "epoch": 0.8217270194986073,
      "grad_norm": 1.2964024543762207,
      "learning_rate": 0.0009178551532033427,
      "loss": 2.6698,
      "step": 2950
    },
    {
      "epoch": 0.8245125348189415,
      "grad_norm": 1.744264841079712,
      "learning_rate": 0.0009175766016713092,
      "loss": 2.7047,
      "step": 2960
    },
    {
      "epoch": 0.8272980501392758,
      "grad_norm": 1.418626308441162,
      "learning_rate": 0.0009172980501392759,
      "loss": 2.4725,
      "step": 2970
    },
    {
      "epoch": 0.83008356545961,
      "grad_norm": 1.689955472946167,
      "learning_rate": 0.0009170194986072423,
      "loss": 2.4886,
      "step": 2980
    },
    {
      "epoch": 0.8328690807799443,
      "grad_norm": 1.708001971244812,
      "learning_rate": 0.0009167409470752089,
      "loss": 2.4697,
      "step": 2990
    },
    {
      "epoch": 0.8356545961002786,
      "grad_norm": 1.0075879096984863,
      "learning_rate": 0.0009164623955431754,
      "loss": 2.5168,
      "step": 3000
    },
    {
      "epoch": 0.8384401114206128,
      "grad_norm": 1.1822031736373901,
      "learning_rate": 0.0009161838440111421,
      "loss": 2.4955,
      "step": 3010
    },
    {
      "epoch": 0.841225626740947,
      "grad_norm": 1.7763776779174805,
      "learning_rate": 0.0009159052924791087,
      "loss": 2.4372,
      "step": 3020
    },
    {
      "epoch": 0.8440111420612814,
      "grad_norm": 1.2953461408615112,
      "learning_rate": 0.0009156267409470752,
      "loss": 2.4257,
      "step": 3030
    },
    {
      "epoch": 0.8467966573816156,
      "grad_norm": 1.6734503507614136,
      "learning_rate": 0.0009153481894150419,
      "loss": 2.5669,
      "step": 3040
    },
    {
      "epoch": 0.8495821727019499,
      "grad_norm": 1.2400703430175781,
      "learning_rate": 0.0009150696378830084,
      "loss": 2.6963,
      "step": 3050
    },
    {
      "epoch": 0.8523676880222841,
      "grad_norm": 1.1290812492370605,
      "learning_rate": 0.000914791086350975,
      "loss": 2.6802,
      "step": 3060
    },
    {
      "epoch": 0.8551532033426184,
      "grad_norm": 1.19234037399292,
      "learning_rate": 0.0009145125348189414,
      "loss": 2.608,
      "step": 3070
    },
    {
      "epoch": 0.8579387186629527,
      "grad_norm": 1.2364051342010498,
      "learning_rate": 0.0009142339832869081,
      "loss": 2.4903,
      "step": 3080
    },
    {
      "epoch": 0.8607242339832869,
      "grad_norm": 1.6729354858398438,
      "learning_rate": 0.0009139554317548747,
      "loss": 2.4627,
      "step": 3090
    },
    {
      "epoch": 0.8635097493036211,
      "grad_norm": 1.8998438119888306,
      "learning_rate": 0.0009136768802228412,
      "loss": 2.4398,
      "step": 3100
    },
    {
      "epoch": 0.8662952646239555,
      "grad_norm": 1.394219160079956,
      "learning_rate": 0.0009133983286908078,
      "loss": 2.5279,
      "step": 3110
    },
    {
      "epoch": 0.8690807799442897,
      "grad_norm": 1.389923095703125,
      "learning_rate": 0.0009131197771587744,
      "loss": 2.4721,
      "step": 3120
    },
    {
      "epoch": 0.871866295264624,
      "grad_norm": 1.3986363410949707,
      "learning_rate": 0.000912841225626741,
      "loss": 2.6389,
      "step": 3130
    },
    {
      "epoch": 0.8746518105849582,
      "grad_norm": 1.2839164733886719,
      "learning_rate": 0.0009125626740947075,
      "loss": 2.5392,
      "step": 3140
    },
    {
      "epoch": 0.8774373259052924,
      "grad_norm": 1.5510696172714233,
      "learning_rate": 0.0009122841225626742,
      "loss": 2.5194,
      "step": 3150
    },
    {
      "epoch": 0.8802228412256268,
      "grad_norm": 1.5165585279464722,
      "learning_rate": 0.0009120055710306407,
      "loss": 2.5704,
      "step": 3160
    },
    {
      "epoch": 0.883008356545961,
      "grad_norm": 1.580206274986267,
      "learning_rate": 0.0009117270194986072,
      "loss": 2.5532,
      "step": 3170
    },
    {
      "epoch": 0.8857938718662952,
      "grad_norm": 1.6651115417480469,
      "learning_rate": 0.0009114484679665738,
      "loss": 2.2884,
      "step": 3180
    },
    {
      "epoch": 0.8885793871866295,
      "grad_norm": 1.1568703651428223,
      "learning_rate": 0.0009111699164345404,
      "loss": 2.6279,
      "step": 3190
    },
    {
      "epoch": 0.8913649025069638,
      "grad_norm": 1.2758097648620605,
      "learning_rate": 0.000910891364902507,
      "loss": 2.2472,
      "step": 3200
    },
    {
      "epoch": 0.8941504178272981,
      "grad_norm": 1.550922155380249,
      "learning_rate": 0.0009106128133704735,
      "loss": 2.6759,
      "step": 3210
    },
    {
      "epoch": 0.8969359331476323,
      "grad_norm": 1.0590204000473022,
      "learning_rate": 0.0009103342618384402,
      "loss": 2.5679,
      "step": 3220
    },
    {
      "epoch": 0.8997214484679665,
      "grad_norm": 1.579368233680725,
      "learning_rate": 0.0009100557103064067,
      "loss": 2.4722,
      "step": 3230
    },
    {
      "epoch": 0.9025069637883009,
      "grad_norm": 1.5835844278335571,
      "learning_rate": 0.0009097771587743733,
      "loss": 2.4494,
      "step": 3240
    },
    {
      "epoch": 0.9052924791086351,
      "grad_norm": 1.220284104347229,
      "learning_rate": 0.0009094986072423399,
      "loss": 2.6299,
      "step": 3250
    },
    {
      "epoch": 0.9080779944289693,
      "grad_norm": 1.1127558946609497,
      "learning_rate": 0.0009092200557103065,
      "loss": 2.4793,
      "step": 3260
    },
    {
      "epoch": 0.9108635097493036,
      "grad_norm": 1.304783582687378,
      "learning_rate": 0.000908941504178273,
      "loss": 2.5737,
      "step": 3270
    },
    {
      "epoch": 0.9136490250696379,
      "grad_norm": 1.5065256357192993,
      "learning_rate": 0.0009086629526462395,
      "loss": 2.3658,
      "step": 3280
    },
    {
      "epoch": 0.9164345403899722,
      "grad_norm": 1.7429677248001099,
      "learning_rate": 0.0009083844011142061,
      "loss": 2.5065,
      "step": 3290
    },
    {
      "epoch": 0.9192200557103064,
      "grad_norm": 1.2562928199768066,
      "learning_rate": 0.0009081058495821727,
      "loss": 2.6153,
      "step": 3300
    },
    {
      "epoch": 0.9220055710306406,
      "grad_norm": 1.0412288904190063,
      "learning_rate": 0.0009078272980501393,
      "loss": 2.3057,
      "step": 3310
    },
    {
      "epoch": 0.924791086350975,
      "grad_norm": 1.628635287284851,
      "learning_rate": 0.0009075487465181058,
      "loss": 2.608,
      "step": 3320
    },
    {
      "epoch": 0.9275766016713092,
      "grad_norm": 1.3479400873184204,
      "learning_rate": 0.0009072701949860725,
      "loss": 2.5453,
      "step": 3330
    },
    {
      "epoch": 0.9303621169916435,
      "grad_norm": 1.532528042793274,
      "learning_rate": 0.0009069916434540391,
      "loss": 2.6629,
      "step": 3340
    },
    {
      "epoch": 0.9331476323119777,
      "grad_norm": 1.1401934623718262,
      "learning_rate": 0.0009067130919220056,
      "loss": 2.4911,
      "step": 3350
    },
    {
      "epoch": 0.935933147632312,
      "grad_norm": 1.4988995790481567,
      "learning_rate": 0.0009064345403899722,
      "loss": 2.4127,
      "step": 3360
    },
    {
      "epoch": 0.9387186629526463,
      "grad_norm": 1.8984172344207764,
      "learning_rate": 0.0009061559888579387,
      "loss": 2.5183,
      "step": 3370
    },
    {
      "epoch": 0.9415041782729805,
      "grad_norm": 2.886582136154175,
      "learning_rate": 0.0009058774373259053,
      "loss": 2.434,
      "step": 3380
    },
    {
      "epoch": 0.9442896935933147,
      "grad_norm": 1.317057728767395,
      "learning_rate": 0.0009055988857938718,
      "loss": 2.5229,
      "step": 3390
    },
    {
      "epoch": 0.947075208913649,
      "grad_norm": 0.9273808002471924,
      "learning_rate": 0.0009053203342618385,
      "loss": 2.3869,
      "step": 3400
    },
    {
      "epoch": 0.9498607242339833,
      "grad_norm": 1.631961703300476,
      "learning_rate": 0.0009050417827298051,
      "loss": 2.5104,
      "step": 3410
    },
    {
      "epoch": 0.9526462395543176,
      "grad_norm": 1.120706558227539,
      "learning_rate": 0.0009047632311977716,
      "loss": 2.6222,
      "step": 3420
    },
    {
      "epoch": 0.9554317548746518,
      "grad_norm": 1.6801543235778809,
      "learning_rate": 0.0009044846796657382,
      "loss": 2.658,
      "step": 3430
    },
    {
      "epoch": 0.958217270194986,
      "grad_norm": 1.4296263456344604,
      "learning_rate": 0.0009042061281337048,
      "loss": 2.6003,
      "step": 3440
    },
    {
      "epoch": 0.9610027855153204,
      "grad_norm": 1.5682204961776733,
      "learning_rate": 0.0009039275766016714,
      "loss": 2.4021,
      "step": 3450
    },
    {
      "epoch": 0.9637883008356546,
      "grad_norm": 1.559866189956665,
      "learning_rate": 0.0009036490250696378,
      "loss": 2.5678,
      "step": 3460
    },
    {
      "epoch": 0.9665738161559888,
      "grad_norm": 1.3528755903244019,
      "learning_rate": 0.0009033704735376044,
      "loss": 2.8601,
      "step": 3470
    },
    {
      "epoch": 0.9693593314763231,
      "grad_norm": 1.7843915224075317,
      "learning_rate": 0.000903091922005571,
      "loss": 2.5616,
      "step": 3480
    },
    {
      "epoch": 0.9721448467966574,
      "grad_norm": 1.2097856998443604,
      "learning_rate": 0.0009028133704735376,
      "loss": 2.5941,
      "step": 3490
    },
    {
      "epoch": 0.9749303621169917,
      "grad_norm": 1.5091629028320312,
      "learning_rate": 0.0009025348189415042,
      "loss": 2.4545,
      "step": 3500
    },
    {
      "epoch": 0.9777158774373259,
      "grad_norm": 1.9660820960998535,
      "learning_rate": 0.0009022562674094708,
      "loss": 2.2924,
      "step": 3510
    },
    {
      "epoch": 0.9805013927576601,
      "grad_norm": 1.1597849130630493,
      "learning_rate": 0.0009019777158774374,
      "loss": 2.541,
      "step": 3520
    },
    {
      "epoch": 0.9832869080779945,
      "grad_norm": 1.4375479221343994,
      "learning_rate": 0.0009016991643454039,
      "loss": 2.4329,
      "step": 3530
    },
    {
      "epoch": 0.9860724233983287,
      "grad_norm": 1.5755987167358398,
      "learning_rate": 0.0009014206128133705,
      "loss": 2.4869,
      "step": 3540
    },
    {
      "epoch": 0.9888579387186629,
      "grad_norm": 1.2432537078857422,
      "learning_rate": 0.0009011420612813371,
      "loss": 2.5331,
      "step": 3550
    },
    {
      "epoch": 0.9916434540389972,
      "grad_norm": 1.5774426460266113,
      "learning_rate": 0.0009008635097493037,
      "loss": 2.2686,
      "step": 3560
    },
    {
      "epoch": 0.9944289693593314,
      "grad_norm": 1.8172963857650757,
      "learning_rate": 0.0009005849582172702,
      "loss": 2.5244,
      "step": 3570
    },
    {
      "epoch": 0.9972144846796658,
      "grad_norm": 1.5224865674972534,
      "learning_rate": 0.0009003064066852367,
      "loss": 2.7115,
      "step": 3580
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.491431713104248,
      "learning_rate": 0.0009000278551532034,
      "loss": 2.3646,
      "step": 3590
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.397735357284546,
      "eval_runtime": 7.3795,
      "eval_samples_per_second": 432.55,
      "eval_steps_per_second": 54.069,
      "step": 3590
    },
    {
      "epoch": 1.0027855153203342,
      "grad_norm": 1.5593105554580688,
      "learning_rate": 0.0008997493036211699,
      "loss": 2.4212,
      "step": 3600
    },
    {
      "epoch": 1.0055710306406684,
      "grad_norm": 1.2981131076812744,
      "learning_rate": 0.0008994707520891365,
      "loss": 2.4372,
      "step": 3610
    },
    {
      "epoch": 1.0083565459610029,
      "grad_norm": 1.6116950511932373,
      "learning_rate": 0.0008991922005571031,
      "loss": 2.7428,
      "step": 3620
    },
    {
      "epoch": 1.011142061281337,
      "grad_norm": 1.1859445571899414,
      "learning_rate": 0.0008989136490250697,
      "loss": 2.612,
      "step": 3630
    },
    {
      "epoch": 1.0139275766016713,
      "grad_norm": 1.517853856086731,
      "learning_rate": 0.0008986350974930362,
      "loss": 2.4953,
      "step": 3640
    },
    {
      "epoch": 1.0167130919220055,
      "grad_norm": 1.2989213466644287,
      "learning_rate": 0.0008983565459610028,
      "loss": 2.414,
      "step": 3650
    },
    {
      "epoch": 1.0194986072423398,
      "grad_norm": 1.2483879327774048,
      "learning_rate": 0.0008980779944289695,
      "loss": 2.5336,
      "step": 3660
    },
    {
      "epoch": 1.0222841225626742,
      "grad_norm": 1.141396403312683,
      "learning_rate": 0.0008977994428969359,
      "loss": 2.4453,
      "step": 3670
    },
    {
      "epoch": 1.0250696378830084,
      "grad_norm": 1.195465087890625,
      "learning_rate": 0.0008975208913649025,
      "loss": 2.2789,
      "step": 3680
    },
    {
      "epoch": 1.0278551532033426,
      "grad_norm": 1.664591908454895,
      "learning_rate": 0.0008972423398328691,
      "loss": 2.5636,
      "step": 3690
    },
    {
      "epoch": 1.0306406685236769,
      "grad_norm": 1.4461817741394043,
      "learning_rate": 0.0008969637883008357,
      "loss": 2.5362,
      "step": 3700
    },
    {
      "epoch": 1.033426183844011,
      "grad_norm": 1.161739706993103,
      "learning_rate": 0.0008966852367688022,
      "loss": 2.5035,
      "step": 3710
    },
    {
      "epoch": 1.0362116991643453,
      "grad_norm": 1.7722245454788208,
      "learning_rate": 0.0008964066852367688,
      "loss": 2.4122,
      "step": 3720
    },
    {
      "epoch": 1.0389972144846797,
      "grad_norm": 1.8447202444076538,
      "learning_rate": 0.0008961281337047355,
      "loss": 2.5141,
      "step": 3730
    },
    {
      "epoch": 1.041782729805014,
      "grad_norm": 1.2036467790603638,
      "learning_rate": 0.000895849582172702,
      "loss": 2.4546,
      "step": 3740
    },
    {
      "epoch": 1.0445682451253482,
      "grad_norm": 5.819509029388428,
      "learning_rate": 0.0008955710306406686,
      "loss": 2.3339,
      "step": 3750
    },
    {
      "epoch": 1.0473537604456824,
      "grad_norm": 1.3619776964187622,
      "learning_rate": 0.000895292479108635,
      "loss": 2.4826,
      "step": 3760
    },
    {
      "epoch": 1.0501392757660166,
      "grad_norm": 1.0850274562835693,
      "learning_rate": 0.0008950139275766017,
      "loss": 2.4749,
      "step": 3770
    },
    {
      "epoch": 1.052924791086351,
      "grad_norm": 1.817582368850708,
      "learning_rate": 0.0008947353760445682,
      "loss": 2.414,
      "step": 3780
    },
    {
      "epoch": 1.0557103064066853,
      "grad_norm": 1.478825330734253,
      "learning_rate": 0.0008944568245125348,
      "loss": 2.6212,
      "step": 3790
    },
    {
      "epoch": 1.0584958217270195,
      "grad_norm": 1.1996793746948242,
      "learning_rate": 0.0008941782729805014,
      "loss": 2.3864,
      "step": 3800
    },
    {
      "epoch": 1.0612813370473537,
      "grad_norm": 1.2405664920806885,
      "learning_rate": 0.000893899721448468,
      "loss": 2.3628,
      "step": 3810
    },
    {
      "epoch": 1.064066852367688,
      "grad_norm": 1.5485687255859375,
      "learning_rate": 0.0008936211699164346,
      "loss": 2.4992,
      "step": 3820
    },
    {
      "epoch": 1.0668523676880224,
      "grad_norm": 1.026391863822937,
      "learning_rate": 0.0008933426183844011,
      "loss": 2.565,
      "step": 3830
    },
    {
      "epoch": 1.0696378830083566,
      "grad_norm": 1.5376060009002686,
      "learning_rate": 0.0008930640668523678,
      "loss": 2.5188,
      "step": 3840
    },
    {
      "epoch": 1.0724233983286908,
      "grad_norm": 1.0413479804992676,
      "learning_rate": 0.0008927855153203343,
      "loss": 2.3866,
      "step": 3850
    },
    {
      "epoch": 1.075208913649025,
      "grad_norm": 1.6323193311691284,
      "learning_rate": 0.0008925069637883008,
      "loss": 2.608,
      "step": 3860
    },
    {
      "epoch": 1.0779944289693593,
      "grad_norm": 1.8664036989212036,
      "learning_rate": 0.0008922284122562674,
      "loss": 2.4298,
      "step": 3870
    },
    {
      "epoch": 1.0807799442896937,
      "grad_norm": 1.2878358364105225,
      "learning_rate": 0.000891949860724234,
      "loss": 2.3395,
      "step": 3880
    },
    {
      "epoch": 1.083565459610028,
      "grad_norm": 1.3740932941436768,
      "learning_rate": 0.0008916713091922006,
      "loss": 2.512,
      "step": 3890
    },
    {
      "epoch": 1.0863509749303621,
      "grad_norm": 1.2444067001342773,
      "learning_rate": 0.0008913927576601671,
      "loss": 2.5302,
      "step": 3900
    },
    {
      "epoch": 1.0891364902506964,
      "grad_norm": 2.0942788124084473,
      "learning_rate": 0.0008911142061281338,
      "loss": 2.4217,
      "step": 3910
    },
    {
      "epoch": 1.0919220055710306,
      "grad_norm": 1.718898892402649,
      "learning_rate": 0.0008908356545961003,
      "loss": 2.567,
      "step": 3920
    },
    {
      "epoch": 1.0947075208913648,
      "grad_norm": 1.667136788368225,
      "learning_rate": 0.0008905571030640669,
      "loss": 2.3522,
      "step": 3930
    },
    {
      "epoch": 1.0974930362116992,
      "grad_norm": 1.2751134634017944,
      "learning_rate": 0.0008902785515320334,
      "loss": 2.5642,
      "step": 3940
    },
    {
      "epoch": 1.1002785515320335,
      "grad_norm": 1.4631513357162476,
      "learning_rate": 0.0008900000000000001,
      "loss": 2.351,
      "step": 3950
    },
    {
      "epoch": 1.1030640668523677,
      "grad_norm": 1.0128222703933716,
      "learning_rate": 0.0008897214484679665,
      "loss": 2.3303,
      "step": 3960
    },
    {
      "epoch": 1.105849582172702,
      "grad_norm": 1.1810134649276733,
      "learning_rate": 0.0008894428969359331,
      "loss": 2.3579,
      "step": 3970
    },
    {
      "epoch": 1.1086350974930361,
      "grad_norm": 1.1461336612701416,
      "learning_rate": 0.0008891643454038998,
      "loss": 2.4838,
      "step": 3980
    },
    {
      "epoch": 1.1114206128133706,
      "grad_norm": 1.5287007093429565,
      "learning_rate": 0.0008888857938718663,
      "loss": 2.3714,
      "step": 3990
    },
    {
      "epoch": 1.1142061281337048,
      "grad_norm": 1.8095390796661377,
      "learning_rate": 0.0008886072423398329,
      "loss": 2.253,
      "step": 4000
    },
    {
      "epoch": 1.116991643454039,
      "grad_norm": 1.675278663635254,
      "learning_rate": 0.0008883286908077994,
      "loss": 2.6471,
      "step": 4010
    },
    {
      "epoch": 1.1197771587743732,
      "grad_norm": 1.2028225660324097,
      "learning_rate": 0.0008880501392757661,
      "loss": 2.6135,
      "step": 4020
    },
    {
      "epoch": 1.1225626740947074,
      "grad_norm": 1.3847585916519165,
      "learning_rate": 0.0008877715877437326,
      "loss": 2.4791,
      "step": 4030
    },
    {
      "epoch": 1.1253481894150417,
      "grad_norm": 1.3021461963653564,
      "learning_rate": 0.0008874930362116992,
      "loss": 2.4751,
      "step": 4040
    },
    {
      "epoch": 1.128133704735376,
      "grad_norm": 1.6454262733459473,
      "learning_rate": 0.0008872144846796659,
      "loss": 2.4707,
      "step": 4050
    },
    {
      "epoch": 1.1309192200557103,
      "grad_norm": 1.5796153545379639,
      "learning_rate": 0.0008869359331476323,
      "loss": 2.5181,
      "step": 4060
    },
    {
      "epoch": 1.1337047353760445,
      "grad_norm": 1.3428486585617065,
      "learning_rate": 0.0008866573816155989,
      "loss": 2.4414,
      "step": 4070
    },
    {
      "epoch": 1.1364902506963788,
      "grad_norm": 1.3642890453338623,
      "learning_rate": 0.0008863788300835654,
      "loss": 2.5464,
      "step": 4080
    },
    {
      "epoch": 1.1392757660167132,
      "grad_norm": 1.6575239896774292,
      "learning_rate": 0.0008861002785515321,
      "loss": 2.4584,
      "step": 4090
    },
    {
      "epoch": 1.1420612813370474,
      "grad_norm": 1.1528486013412476,
      "learning_rate": 0.0008858217270194986,
      "loss": 2.5487,
      "step": 4100
    },
    {
      "epoch": 1.1448467966573816,
      "grad_norm": 1.6422556638717651,
      "learning_rate": 0.0008855431754874652,
      "loss": 2.6407,
      "step": 4110
    },
    {
      "epoch": 1.1476323119777159,
      "grad_norm": 1.1659678220748901,
      "learning_rate": 0.0008852646239554317,
      "loss": 2.4035,
      "step": 4120
    },
    {
      "epoch": 1.15041782729805,
      "grad_norm": 1.1729705333709717,
      "learning_rate": 0.0008849860724233984,
      "loss": 2.3238,
      "step": 4130
    },
    {
      "epoch": 1.1532033426183843,
      "grad_norm": 2.2853970527648926,
      "learning_rate": 0.000884707520891365,
      "loss": 2.4917,
      "step": 4140
    },
    {
      "epoch": 1.1559888579387188,
      "grad_norm": 1.5353424549102783,
      "learning_rate": 0.0008844289693593314,
      "loss": 2.4376,
      "step": 4150
    },
    {
      "epoch": 1.158774373259053,
      "grad_norm": 1.4450420141220093,
      "learning_rate": 0.0008841504178272981,
      "loss": 2.3024,
      "step": 4160
    },
    {
      "epoch": 1.1615598885793872,
      "grad_norm": 1.5458306074142456,
      "learning_rate": 0.0008838718662952646,
      "loss": 2.3987,
      "step": 4170
    },
    {
      "epoch": 1.1643454038997214,
      "grad_norm": 1.548515796661377,
      "learning_rate": 0.0008835933147632312,
      "loss": 2.5626,
      "step": 4180
    },
    {
      "epoch": 1.1671309192200556,
      "grad_norm": 1.2337225675582886,
      "learning_rate": 0.0008833147632311977,
      "loss": 2.3231,
      "step": 4190
    },
    {
      "epoch": 1.16991643454039,
      "grad_norm": 1.0686241388320923,
      "learning_rate": 0.0008830362116991644,
      "loss": 2.7435,
      "step": 4200
    },
    {
      "epoch": 1.1727019498607243,
      "grad_norm": 1.1496175527572632,
      "learning_rate": 0.0008827576601671309,
      "loss": 2.4415,
      "step": 4210
    },
    {
      "epoch": 1.1754874651810585,
      "grad_norm": 1.0511130094528198,
      "learning_rate": 0.0008824791086350975,
      "loss": 2.5049,
      "step": 4220
    },
    {
      "epoch": 1.1782729805013927,
      "grad_norm": 1.43406081199646,
      "learning_rate": 0.0008822005571030642,
      "loss": 2.3448,
      "step": 4230
    },
    {
      "epoch": 1.181058495821727,
      "grad_norm": 1.6437138319015503,
      "learning_rate": 0.0008819220055710307,
      "loss": 2.4873,
      "step": 4240
    },
    {
      "epoch": 1.1838440111420612,
      "grad_norm": 1.9596487283706665,
      "learning_rate": 0.0008816434540389973,
      "loss": 2.4873,
      "step": 4250
    },
    {
      "epoch": 1.1866295264623956,
      "grad_norm": 1.175866961479187,
      "learning_rate": 0.0008813649025069637,
      "loss": 2.4503,
      "step": 4260
    },
    {
      "epoch": 1.1894150417827298,
      "grad_norm": 1.5261820554733276,
      "learning_rate": 0.0008810863509749304,
      "loss": 2.4934,
      "step": 4270
    },
    {
      "epoch": 1.192200557103064,
      "grad_norm": 1.706544280052185,
      "learning_rate": 0.0008808077994428969,
      "loss": 2.5813,
      "step": 4280
    },
    {
      "epoch": 1.1949860724233983,
      "grad_norm": 1.4929132461547852,
      "learning_rate": 0.0008805292479108635,
      "loss": 2.4576,
      "step": 4290
    },
    {
      "epoch": 1.1977715877437327,
      "grad_norm": 2.1815779209136963,
      "learning_rate": 0.0008802506963788301,
      "loss": 2.3798,
      "step": 4300
    },
    {
      "epoch": 1.200557103064067,
      "grad_norm": 1.246375560760498,
      "learning_rate": 0.0008799721448467967,
      "loss": 2.4346,
      "step": 4310
    },
    {
      "epoch": 1.2033426183844012,
      "grad_norm": 1.5373544692993164,
      "learning_rate": 0.0008796935933147633,
      "loss": 2.4975,
      "step": 4320
    },
    {
      "epoch": 1.2061281337047354,
      "grad_norm": 1.7021143436431885,
      "learning_rate": 0.0008794150417827298,
      "loss": 2.4443,
      "step": 4330
    },
    {
      "epoch": 1.2089136490250696,
      "grad_norm": 1.1893073320388794,
      "learning_rate": 0.0008791364902506965,
      "loss": 2.6007,
      "step": 4340
    },
    {
      "epoch": 1.2116991643454038,
      "grad_norm": 1.6880290508270264,
      "learning_rate": 0.000878857938718663,
      "loss": 2.359,
      "step": 4350
    },
    {
      "epoch": 1.2144846796657383,
      "grad_norm": 1.7805156707763672,
      "learning_rate": 0.0008785793871866295,
      "loss": 2.4987,
      "step": 4360
    },
    {
      "epoch": 1.2172701949860725,
      "grad_norm": 1.1447371244430542,
      "learning_rate": 0.000878300835654596,
      "loss": 2.6267,
      "step": 4370
    },
    {
      "epoch": 1.2200557103064067,
      "grad_norm": 1.6702297925949097,
      "learning_rate": 0.0008780222841225627,
      "loss": 2.5946,
      "step": 4380
    },
    {
      "epoch": 1.222841225626741,
      "grad_norm": 1.3178101778030396,
      "learning_rate": 0.0008777437325905293,
      "loss": 2.4778,
      "step": 4390
    },
    {
      "epoch": 1.2256267409470751,
      "grad_norm": 1.4557198286056519,
      "learning_rate": 0.0008774651810584958,
      "loss": 2.3415,
      "step": 4400
    },
    {
      "epoch": 1.2284122562674096,
      "grad_norm": 1.3501002788543701,
      "learning_rate": 0.0008771866295264625,
      "loss": 2.3077,
      "step": 4410
    },
    {
      "epoch": 1.2311977715877438,
      "grad_norm": 1.1925262212753296,
      "learning_rate": 0.000876908077994429,
      "loss": 2.1927,
      "step": 4420
    },
    {
      "epoch": 1.233983286908078,
      "grad_norm": 1.5833343267440796,
      "learning_rate": 0.0008766295264623956,
      "loss": 2.3244,
      "step": 4430
    },
    {
      "epoch": 1.2367688022284122,
      "grad_norm": 1.054939866065979,
      "learning_rate": 0.000876350974930362,
      "loss": 2.4705,
      "step": 4440
    },
    {
      "epoch": 1.2395543175487465,
      "grad_norm": 2.019254684448242,
      "learning_rate": 0.0008760724233983288,
      "loss": 2.4434,
      "step": 4450
    },
    {
      "epoch": 1.2423398328690807,
      "grad_norm": 1.4074331521987915,
      "learning_rate": 0.0008757938718662953,
      "loss": 2.2297,
      "step": 4460
    },
    {
      "epoch": 1.2451253481894151,
      "grad_norm": 1.5462722778320312,
      "learning_rate": 0.0008755153203342618,
      "loss": 2.6025,
      "step": 4470
    },
    {
      "epoch": 1.2479108635097493,
      "grad_norm": 1.7716648578643799,
      "learning_rate": 0.0008752367688022284,
      "loss": 2.3434,
      "step": 4480
    },
    {
      "epoch": 1.2506963788300836,
      "grad_norm": 1.5014852285385132,
      "learning_rate": 0.000874958217270195,
      "loss": 2.4651,
      "step": 4490
    },
    {
      "epoch": 1.2534818941504178,
      "grad_norm": 1.4961960315704346,
      "learning_rate": 0.0008746796657381616,
      "loss": 2.5279,
      "step": 4500
    },
    {
      "epoch": 1.2562674094707522,
      "grad_norm": 1.5342555046081543,
      "learning_rate": 0.0008744011142061281,
      "loss": 2.3328,
      "step": 4510
    },
    {
      "epoch": 1.2590529247910864,
      "grad_norm": 1.0711063146591187,
      "learning_rate": 0.0008741225626740948,
      "loss": 2.4366,
      "step": 4520
    },
    {
      "epoch": 1.2618384401114207,
      "grad_norm": 1.3145135641098022,
      "learning_rate": 0.0008738440111420613,
      "loss": 2.6598,
      "step": 4530
    },
    {
      "epoch": 1.2646239554317549,
      "grad_norm": 1.653624176979065,
      "learning_rate": 0.0008735654596100279,
      "loss": 2.3518,
      "step": 4540
    },
    {
      "epoch": 1.267409470752089,
      "grad_norm": 1.1552844047546387,
      "learning_rate": 0.0008732869080779944,
      "loss": 2.3562,
      "step": 4550
    },
    {
      "epoch": 1.2701949860724233,
      "grad_norm": 1.4657578468322754,
      "learning_rate": 0.000873008356545961,
      "loss": 2.5733,
      "step": 4560
    },
    {
      "epoch": 1.2729805013927575,
      "grad_norm": 1.375367522239685,
      "learning_rate": 0.0008727298050139276,
      "loss": 2.5587,
      "step": 4570
    },
    {
      "epoch": 1.275766016713092,
      "grad_norm": 1.324321985244751,
      "learning_rate": 0.0008724512534818941,
      "loss": 2.5803,
      "step": 4580
    },
    {
      "epoch": 1.2785515320334262,
      "grad_norm": 1.6006171703338623,
      "learning_rate": 0.0008721727019498608,
      "loss": 2.4641,
      "step": 4590
    },
    {
      "epoch": 1.2813370473537604,
      "grad_norm": 1.363006353378296,
      "learning_rate": 0.0008718941504178273,
      "loss": 2.2643,
      "step": 4600
    },
    {
      "epoch": 1.2841225626740946,
      "grad_norm": 1.2838157415390015,
      "learning_rate": 0.0008716155988857939,
      "loss": 2.2577,
      "step": 4610
    },
    {
      "epoch": 1.286908077994429,
      "grad_norm": 1.1273750066757202,
      "learning_rate": 0.0008713370473537605,
      "loss": 2.3721,
      "step": 4620
    },
    {
      "epoch": 1.2896935933147633,
      "grad_norm": 1.1856783628463745,
      "learning_rate": 0.0008710584958217271,
      "loss": 2.4563,
      "step": 4630
    },
    {
      "epoch": 1.2924791086350975,
      "grad_norm": 1.4169940948486328,
      "learning_rate": 0.0008707799442896937,
      "loss": 2.4886,
      "step": 4640
    },
    {
      "epoch": 1.2952646239554317,
      "grad_norm": 2.4220340251922607,
      "learning_rate": 0.0008705013927576601,
      "loss": 2.4522,
      "step": 4650
    },
    {
      "epoch": 1.298050139275766,
      "grad_norm": 1.4113950729370117,
      "learning_rate": 0.0008702228412256267,
      "loss": 2.3123,
      "step": 4660
    },
    {
      "epoch": 1.3008356545961002,
      "grad_norm": 1.3271572589874268,
      "learning_rate": 0.0008699442896935933,
      "loss": 2.5054,
      "step": 4670
    },
    {
      "epoch": 1.3036211699164346,
      "grad_norm": 1.4193942546844482,
      "learning_rate": 0.0008696657381615599,
      "loss": 2.5467,
      "step": 4680
    },
    {
      "epoch": 1.3064066852367688,
      "grad_norm": 1.3567800521850586,
      "learning_rate": 0.0008693871866295264,
      "loss": 2.6821,
      "step": 4690
    },
    {
      "epoch": 1.309192200557103,
      "grad_norm": 1.1462795734405518,
      "learning_rate": 0.0008691086350974931,
      "loss": 2.5264,
      "step": 4700
    },
    {
      "epoch": 1.3119777158774373,
      "grad_norm": 1.2633705139160156,
      "learning_rate": 0.0008688300835654597,
      "loss": 2.2729,
      "step": 4710
    },
    {
      "epoch": 1.3147632311977717,
      "grad_norm": 1.4208569526672363,
      "learning_rate": 0.0008685515320334262,
      "loss": 2.3515,
      "step": 4720
    },
    {
      "epoch": 1.317548746518106,
      "grad_norm": 1.3327795267105103,
      "learning_rate": 0.0008682729805013928,
      "loss": 2.6065,
      "step": 4730
    },
    {
      "epoch": 1.3203342618384402,
      "grad_norm": 2.0600273609161377,
      "learning_rate": 0.0008679944289693594,
      "loss": 2.4239,
      "step": 4740
    },
    {
      "epoch": 1.3231197771587744,
      "grad_norm": 1.701816201210022,
      "learning_rate": 0.0008677158774373259,
      "loss": 2.7741,
      "step": 4750
    },
    {
      "epoch": 1.3259052924791086,
      "grad_norm": 3.457326650619507,
      "learning_rate": 0.0008674373259052924,
      "loss": 2.5082,
      "step": 4760
    },
    {
      "epoch": 1.3286908077994428,
      "grad_norm": 1.3398488759994507,
      "learning_rate": 0.0008671587743732591,
      "loss": 2.5385,
      "step": 4770
    },
    {
      "epoch": 1.331476323119777,
      "grad_norm": 1.8948661088943481,
      "learning_rate": 0.0008668802228412257,
      "loss": 2.6584,
      "step": 4780
    },
    {
      "epoch": 1.3342618384401115,
      "grad_norm": 1.550597071647644,
      "learning_rate": 0.0008666016713091922,
      "loss": 2.4351,
      "step": 4790
    },
    {
      "epoch": 1.3370473537604457,
      "grad_norm": 1.7041109800338745,
      "learning_rate": 0.0008663231197771588,
      "loss": 2.5625,
      "step": 4800
    },
    {
      "epoch": 1.33983286908078,
      "grad_norm": 2.128828525543213,
      "learning_rate": 0.0008660445682451254,
      "loss": 2.2395,
      "step": 4810
    },
    {
      "epoch": 1.3426183844011141,
      "grad_norm": 1.0591498613357544,
      "learning_rate": 0.000865766016713092,
      "loss": 2.1993,
      "step": 4820
    },
    {
      "epoch": 1.3454038997214486,
      "grad_norm": 1.1929551362991333,
      "learning_rate": 0.0008654874651810585,
      "loss": 2.2711,
      "step": 4830
    },
    {
      "epoch": 1.3481894150417828,
      "grad_norm": 1.0719852447509766,
      "learning_rate": 0.000865208913649025,
      "loss": 2.4562,
      "step": 4840
    },
    {
      "epoch": 1.350974930362117,
      "grad_norm": 1.3695597648620605,
      "learning_rate": 0.0008649303621169916,
      "loss": 2.7366,
      "step": 4850
    },
    {
      "epoch": 1.3537604456824512,
      "grad_norm": 1.420884370803833,
      "learning_rate": 0.0008646518105849582,
      "loss": 2.4791,
      "step": 4860
    },
    {
      "epoch": 1.3565459610027855,
      "grad_norm": 1.2282696962356567,
      "learning_rate": 0.0008643732590529248,
      "loss": 2.3377,
      "step": 4870
    },
    {
      "epoch": 1.3593314763231197,
      "grad_norm": 1.4054524898529053,
      "learning_rate": 0.0008640947075208914,
      "loss": 2.3451,
      "step": 4880
    },
    {
      "epoch": 1.362116991643454,
      "grad_norm": 1.4363030195236206,
      "learning_rate": 0.000863816155988858,
      "loss": 2.2022,
      "step": 4890
    },
    {
      "epoch": 1.3649025069637883,
      "grad_norm": 1.5929983854293823,
      "learning_rate": 0.0008635376044568245,
      "loss": 2.5155,
      "step": 4900
    },
    {
      "epoch": 1.3676880222841226,
      "grad_norm": 1.6249438524246216,
      "learning_rate": 0.0008632590529247911,
      "loss": 2.4254,
      "step": 4910
    },
    {
      "epoch": 1.3704735376044568,
      "grad_norm": 1.217421054840088,
      "learning_rate": 0.0008629805013927577,
      "loss": 2.5397,
      "step": 4920
    },
    {
      "epoch": 1.3732590529247912,
      "grad_norm": 1.4538894891738892,
      "learning_rate": 0.0008627019498607243,
      "loss": 2.4001,
      "step": 4930
    },
    {
      "epoch": 1.3760445682451254,
      "grad_norm": 1.0621610879898071,
      "learning_rate": 0.0008624233983286909,
      "loss": 2.5301,
      "step": 4940
    },
    {
      "epoch": 1.3788300835654597,
      "grad_norm": 1.2887012958526611,
      "learning_rate": 0.0008621448467966574,
      "loss": 2.4069,
      "step": 4950
    },
    {
      "epoch": 1.3816155988857939,
      "grad_norm": 1.4405642747879028,
      "learning_rate": 0.000861866295264624,
      "loss": 2.4643,
      "step": 4960
    },
    {
      "epoch": 1.384401114206128,
      "grad_norm": 1.308788776397705,
      "learning_rate": 0.0008615877437325905,
      "loss": 2.4766,
      "step": 4970
    },
    {
      "epoch": 1.3871866295264623,
      "grad_norm": 2.0563783645629883,
      "learning_rate": 0.0008613091922005571,
      "loss": 2.4513,
      "step": 4980
    },
    {
      "epoch": 1.3899721448467965,
      "grad_norm": 1.694658875465393,
      "learning_rate": 0.0008610306406685237,
      "loss": 2.5412,
      "step": 4990
    },
    {
      "epoch": 1.392757660167131,
      "grad_norm": 1.9649903774261475,
      "learning_rate": 0.0008607520891364903,
      "loss": 2.3865,
      "step": 5000
    },
    {
      "epoch": 1.3955431754874652,
      "grad_norm": 1.6681616306304932,
      "learning_rate": 0.0008604735376044568,
      "loss": 2.5954,
      "step": 5010
    },
    {
      "epoch": 1.3983286908077994,
      "grad_norm": 1.5635480880737305,
      "learning_rate": 0.0008601949860724234,
      "loss": 2.4157,
      "step": 5020
    },
    {
      "epoch": 1.4011142061281336,
      "grad_norm": 1.8090097904205322,
      "learning_rate": 0.0008599164345403901,
      "loss": 2.5142,
      "step": 5030
    },
    {
      "epoch": 1.403899721448468,
      "grad_norm": 1.1813710927963257,
      "learning_rate": 0.0008596378830083565,
      "loss": 2.3769,
      "step": 5040
    },
    {
      "epoch": 1.4066852367688023,
      "grad_norm": 1.4610099792480469,
      "learning_rate": 0.0008593593314763231,
      "loss": 2.5689,
      "step": 5050
    },
    {
      "epoch": 1.4094707520891365,
      "grad_norm": 0.9706261157989502,
      "learning_rate": 0.0008590807799442897,
      "loss": 2.4351,
      "step": 5060
    },
    {
      "epoch": 1.4122562674094707,
      "grad_norm": 1.2633148431777954,
      "learning_rate": 0.0008588022284122563,
      "loss": 2.4133,
      "step": 5070
    },
    {
      "epoch": 1.415041782729805,
      "grad_norm": 1.4761557579040527,
      "learning_rate": 0.0008585236768802228,
      "loss": 2.2429,
      "step": 5080
    },
    {
      "epoch": 1.4178272980501392,
      "grad_norm": 1.4464341402053833,
      "learning_rate": 0.0008582451253481894,
      "loss": 2.5102,
      "step": 5090
    },
    {
      "epoch": 1.4206128133704734,
      "grad_norm": 1.7756080627441406,
      "learning_rate": 0.0008579665738161561,
      "loss": 2.4295,
      "step": 5100
    },
    {
      "epoch": 1.4233983286908078,
      "grad_norm": 1.5289620161056519,
      "learning_rate": 0.0008576880222841226,
      "loss": 2.3946,
      "step": 5110
    },
    {
      "epoch": 1.426183844011142,
      "grad_norm": 1.3602383136749268,
      "learning_rate": 0.0008574094707520892,
      "loss": 2.5487,
      "step": 5120
    },
    {
      "epoch": 1.4289693593314763,
      "grad_norm": 1.3451564311981201,
      "learning_rate": 0.0008571309192200557,
      "loss": 2.4228,
      "step": 5130
    },
    {
      "epoch": 1.4317548746518105,
      "grad_norm": 7.392559051513672,
      "learning_rate": 0.0008568523676880224,
      "loss": 2.3032,
      "step": 5140
    },
    {
      "epoch": 1.434540389972145,
      "grad_norm": 1.8583354949951172,
      "learning_rate": 0.0008565738161559888,
      "loss": 2.5398,
      "step": 5150
    },
    {
      "epoch": 1.4373259052924792,
      "grad_norm": 1.2296764850616455,
      "learning_rate": 0.0008562952646239554,
      "loss": 2.3776,
      "step": 5160
    },
    {
      "epoch": 1.4401114206128134,
      "grad_norm": 0.903922438621521,
      "learning_rate": 0.000856016713091922,
      "loss": 2.4346,
      "step": 5170
    },
    {
      "epoch": 1.4428969359331476,
      "grad_norm": 1.3784345388412476,
      "learning_rate": 0.0008557381615598886,
      "loss": 2.7413,
      "step": 5180
    },
    {
      "epoch": 1.4456824512534818,
      "grad_norm": 1.1668848991394043,
      "learning_rate": 0.0008554596100278552,
      "loss": 2.4202,
      "step": 5190
    },
    {
      "epoch": 1.448467966573816,
      "grad_norm": 1.8008323907852173,
      "learning_rate": 0.0008551810584958217,
      "loss": 2.4284,
      "step": 5200
    },
    {
      "epoch": 1.4512534818941505,
      "grad_norm": 1.46597158908844,
      "learning_rate": 0.0008549025069637884,
      "loss": 2.4117,
      "step": 5210
    },
    {
      "epoch": 1.4540389972144847,
      "grad_norm": 2.519930362701416,
      "learning_rate": 0.0008546239554317549,
      "loss": 2.6526,
      "step": 5220
    },
    {
      "epoch": 1.456824512534819,
      "grad_norm": 2.253481388092041,
      "learning_rate": 0.0008543454038997215,
      "loss": 2.4843,
      "step": 5230
    },
    {
      "epoch": 1.4596100278551531,
      "grad_norm": 1.5676368474960327,
      "learning_rate": 0.000854066852367688,
      "loss": 2.3869,
      "step": 5240
    },
    {
      "epoch": 1.4623955431754876,
      "grad_norm": 1.8594762086868286,
      "learning_rate": 0.0008537883008356546,
      "loss": 2.4005,
      "step": 5250
    },
    {
      "epoch": 1.4651810584958218,
      "grad_norm": 1.6145737171173096,
      "learning_rate": 0.0008535097493036212,
      "loss": 2.4026,
      "step": 5260
    },
    {
      "epoch": 1.467966573816156,
      "grad_norm": 1.6718175411224365,
      "learning_rate": 0.0008532311977715877,
      "loss": 2.4363,
      "step": 5270
    },
    {
      "epoch": 1.4707520891364902,
      "grad_norm": 1.4791057109832764,
      "learning_rate": 0.0008529526462395544,
      "loss": 2.5444,
      "step": 5280
    },
    {
      "epoch": 1.4735376044568245,
      "grad_norm": 1.2138651609420776,
      "learning_rate": 0.0008526740947075209,
      "loss": 2.4697,
      "step": 5290
    },
    {
      "epoch": 1.4763231197771587,
      "grad_norm": 1.4286870956420898,
      "learning_rate": 0.0008523955431754875,
      "loss": 2.4652,
      "step": 5300
    },
    {
      "epoch": 1.479108635097493,
      "grad_norm": 1.4191044569015503,
      "learning_rate": 0.000852116991643454,
      "loss": 2.4577,
      "step": 5310
    },
    {
      "epoch": 1.4818941504178273,
      "grad_norm": 1.5416271686553955,
      "learning_rate": 0.0008518384401114207,
      "loss": 2.2441,
      "step": 5320
    },
    {
      "epoch": 1.4846796657381616,
      "grad_norm": 1.4051612615585327,
      "learning_rate": 0.0008515598885793872,
      "loss": 2.3758,
      "step": 5330
    },
    {
      "epoch": 1.4874651810584958,
      "grad_norm": 1.5542724132537842,
      "learning_rate": 0.0008512813370473537,
      "loss": 2.6092,
      "step": 5340
    },
    {
      "epoch": 1.49025069637883,
      "grad_norm": 1.5594180822372437,
      "learning_rate": 0.0008510027855153204,
      "loss": 2.5189,
      "step": 5350
    },
    {
      "epoch": 1.4930362116991645,
      "grad_norm": 1.7091022729873657,
      "learning_rate": 0.0008507242339832869,
      "loss": 2.4598,
      "step": 5360
    },
    {
      "epoch": 1.4958217270194987,
      "grad_norm": 1.6574478149414062,
      "learning_rate": 0.0008504456824512535,
      "loss": 2.6081,
      "step": 5370
    },
    {
      "epoch": 1.498607242339833,
      "grad_norm": 1.1679922342300415,
      "learning_rate": 0.00085016713091922,
      "loss": 2.5776,
      "step": 5380
    },
    {
      "epoch": 1.501392757660167,
      "grad_norm": 1.5107452869415283,
      "learning_rate": 0.0008498885793871867,
      "loss": 2.5952,
      "step": 5390
    },
    {
      "epoch": 1.5041782729805013,
      "grad_norm": 1.5589832067489624,
      "learning_rate": 0.0008496100278551532,
      "loss": 2.41,
      "step": 5400
    },
    {
      "epoch": 1.5069637883008355,
      "grad_norm": 1.6834379434585571,
      "learning_rate": 0.0008493314763231198,
      "loss": 2.6359,
      "step": 5410
    },
    {
      "epoch": 1.5097493036211698,
      "grad_norm": 1.9696227312088013,
      "learning_rate": 0.0008490529247910865,
      "loss": 2.4841,
      "step": 5420
    },
    {
      "epoch": 1.5125348189415042,
      "grad_norm": 1.769350290298462,
      "learning_rate": 0.000848774373259053,
      "loss": 2.3464,
      "step": 5430
    },
    {
      "epoch": 1.5153203342618384,
      "grad_norm": 1.275773286819458,
      "learning_rate": 0.0008484958217270195,
      "loss": 2.5737,
      "step": 5440
    },
    {
      "epoch": 1.5181058495821727,
      "grad_norm": 1.4373281002044678,
      "learning_rate": 0.000848217270194986,
      "loss": 2.4938,
      "step": 5450
    },
    {
      "epoch": 1.520891364902507,
      "grad_norm": 1.992476224899292,
      "learning_rate": 0.0008479387186629527,
      "loss": 2.489,
      "step": 5460
    },
    {
      "epoch": 1.5236768802228413,
      "grad_norm": 1.1473641395568848,
      "learning_rate": 0.0008476601671309192,
      "loss": 2.4771,
      "step": 5470
    },
    {
      "epoch": 1.5264623955431755,
      "grad_norm": 1.6520682573318481,
      "learning_rate": 0.0008473816155988858,
      "loss": 2.5858,
      "step": 5480
    },
    {
      "epoch": 1.5292479108635098,
      "grad_norm": 1.9758909940719604,
      "learning_rate": 0.0008471030640668523,
      "loss": 2.4318,
      "step": 5490
    },
    {
      "epoch": 1.532033426183844,
      "grad_norm": 2.206946849822998,
      "learning_rate": 0.000846824512534819,
      "loss": 2.2885,
      "step": 5500
    },
    {
      "epoch": 1.5348189415041782,
      "grad_norm": 1.7625101804733276,
      "learning_rate": 0.0008465459610027856,
      "loss": 2.3655,
      "step": 5510
    },
    {
      "epoch": 1.5376044568245124,
      "grad_norm": 1.3464412689208984,
      "learning_rate": 0.0008462674094707521,
      "loss": 2.3203,
      "step": 5520
    },
    {
      "epoch": 1.5403899721448466,
      "grad_norm": 1.1816092729568481,
      "learning_rate": 0.0008459888579387188,
      "loss": 2.4914,
      "step": 5530
    },
    {
      "epoch": 1.543175487465181,
      "grad_norm": 1.2567238807678223,
      "learning_rate": 0.0008457103064066852,
      "loss": 2.4264,
      "step": 5540
    },
    {
      "epoch": 1.5459610027855153,
      "grad_norm": 1.1811022758483887,
      "learning_rate": 0.0008454317548746518,
      "loss": 2.5307,
      "step": 5550
    },
    {
      "epoch": 1.5487465181058497,
      "grad_norm": 1.19041907787323,
      "learning_rate": 0.0008451532033426183,
      "loss": 2.4017,
      "step": 5560
    },
    {
      "epoch": 1.551532033426184,
      "grad_norm": 1.6910886764526367,
      "learning_rate": 0.000844874651810585,
      "loss": 2.1203,
      "step": 5570
    },
    {
      "epoch": 1.5543175487465182,
      "grad_norm": 1.240424394607544,
      "learning_rate": 0.0008445961002785516,
      "loss": 2.6313,
      "step": 5580
    },
    {
      "epoch": 1.5571030640668524,
      "grad_norm": 1.3129178285598755,
      "learning_rate": 0.0008443175487465181,
      "loss": 2.3698,
      "step": 5590
    },
    {
      "epoch": 1.5598885793871866,
      "grad_norm": 1.2767444849014282,
      "learning_rate": 0.0008440389972144848,
      "loss": 2.4276,
      "step": 5600
    },
    {
      "epoch": 1.5626740947075208,
      "grad_norm": 1.4741268157958984,
      "learning_rate": 0.0008437604456824513,
      "loss": 2.3371,
      "step": 5610
    },
    {
      "epoch": 1.565459610027855,
      "grad_norm": 1.5070691108703613,
      "learning_rate": 0.0008434818941504179,
      "loss": 2.5228,
      "step": 5620
    },
    {
      "epoch": 1.5682451253481893,
      "grad_norm": 1.2148206233978271,
      "learning_rate": 0.0008432033426183843,
      "loss": 2.3798,
      "step": 5630
    },
    {
      "epoch": 1.5710306406685237,
      "grad_norm": 1.7434210777282715,
      "learning_rate": 0.000842924791086351,
      "loss": 2.3247,
      "step": 5640
    },
    {
      "epoch": 1.573816155988858,
      "grad_norm": 1.2053792476654053,
      "learning_rate": 0.0008426462395543175,
      "loss": 2.3449,
      "step": 5650
    },
    {
      "epoch": 1.5766016713091922,
      "grad_norm": 1.6404305696487427,
      "learning_rate": 0.0008423676880222841,
      "loss": 2.5402,
      "step": 5660
    },
    {
      "epoch": 1.5793871866295266,
      "grad_norm": 1.2774022817611694,
      "learning_rate": 0.0008420891364902507,
      "loss": 2.7066,
      "step": 5670
    },
    {
      "epoch": 1.5821727019498608,
      "grad_norm": 1.2883810997009277,
      "learning_rate": 0.0008418105849582173,
      "loss": 2.644,
      "step": 5680
    },
    {
      "epoch": 1.584958217270195,
      "grad_norm": 1.5368412733078003,
      "learning_rate": 0.0008415320334261839,
      "loss": 2.3864,
      "step": 5690
    },
    {
      "epoch": 1.5877437325905293,
      "grad_norm": 1.420724868774414,
      "learning_rate": 0.0008412534818941504,
      "loss": 2.4816,
      "step": 5700
    },
    {
      "epoch": 1.5905292479108635,
      "grad_norm": 1.2914570569992065,
      "learning_rate": 0.0008409749303621171,
      "loss": 2.3547,
      "step": 5710
    },
    {
      "epoch": 1.5933147632311977,
      "grad_norm": 1.5089613199234009,
      "learning_rate": 0.0008406963788300836,
      "loss": 2.5343,
      "step": 5720
    },
    {
      "epoch": 1.596100278551532,
      "grad_norm": 1.232037901878357,
      "learning_rate": 0.0008404178272980501,
      "loss": 2.5181,
      "step": 5730
    },
    {
      "epoch": 1.5988857938718661,
      "grad_norm": 1.7325568199157715,
      "learning_rate": 0.0008401392757660166,
      "loss": 2.4605,
      "step": 5740
    },
    {
      "epoch": 1.6016713091922006,
      "grad_norm": 0.9720683693885803,
      "learning_rate": 0.0008398607242339833,
      "loss": 2.4944,
      "step": 5750
    },
    {
      "epoch": 1.6044568245125348,
      "grad_norm": 1.3852565288543701,
      "learning_rate": 0.0008395821727019499,
      "loss": 2.3824,
      "step": 5760
    },
    {
      "epoch": 1.6072423398328692,
      "grad_norm": 1.5953519344329834,
      "learning_rate": 0.0008393036211699164,
      "loss": 2.295,
      "step": 5770
    },
    {
      "epoch": 1.6100278551532035,
      "grad_norm": 1.3479864597320557,
      "learning_rate": 0.0008390250696378831,
      "loss": 2.2609,
      "step": 5780
    },
    {
      "epoch": 1.6128133704735377,
      "grad_norm": 1.178468108177185,
      "learning_rate": 0.0008387465181058496,
      "loss": 2.3795,
      "step": 5790
    },
    {
      "epoch": 1.615598885793872,
      "grad_norm": 1.442689061164856,
      "learning_rate": 0.0008384679665738162,
      "loss": 2.3874,
      "step": 5800
    },
    {
      "epoch": 1.6183844011142061,
      "grad_norm": 1.595273494720459,
      "learning_rate": 0.0008381894150417827,
      "loss": 2.5958,
      "step": 5810
    },
    {
      "epoch": 1.6211699164345403,
      "grad_norm": 1.4463319778442383,
      "learning_rate": 0.0008379108635097494,
      "loss": 2.6028,
      "step": 5820
    },
    {
      "epoch": 1.6239554317548746,
      "grad_norm": 1.3557837009429932,
      "learning_rate": 0.000837632311977716,
      "loss": 2.5656,
      "step": 5830
    },
    {
      "epoch": 1.6267409470752088,
      "grad_norm": 1.6465238332748413,
      "learning_rate": 0.0008373537604456824,
      "loss": 2.5667,
      "step": 5840
    },
    {
      "epoch": 1.6295264623955432,
      "grad_norm": 1.2677605152130127,
      "learning_rate": 0.000837075208913649,
      "loss": 2.4448,
      "step": 5850
    },
    {
      "epoch": 1.6323119777158774,
      "grad_norm": 1.6326340436935425,
      "learning_rate": 0.0008367966573816156,
      "loss": 2.3935,
      "step": 5860
    },
    {
      "epoch": 1.6350974930362117,
      "grad_norm": 1.4095051288604736,
      "learning_rate": 0.0008365181058495822,
      "loss": 2.3898,
      "step": 5870
    },
    {
      "epoch": 1.637883008356546,
      "grad_norm": 1.6227326393127441,
      "learning_rate": 0.0008362395543175487,
      "loss": 2.3638,
      "step": 5880
    },
    {
      "epoch": 1.6406685236768803,
      "grad_norm": 1.3752682209014893,
      "learning_rate": 0.0008359610027855154,
      "loss": 2.3383,
      "step": 5890
    },
    {
      "epoch": 1.6434540389972145,
      "grad_norm": 1.873189926147461,
      "learning_rate": 0.0008356824512534819,
      "loss": 2.4945,
      "step": 5900
    },
    {
      "epoch": 1.6462395543175488,
      "grad_norm": 1.743126630783081,
      "learning_rate": 0.0008354038997214485,
      "loss": 2.2817,
      "step": 5910
    },
    {
      "epoch": 1.649025069637883,
      "grad_norm": 1.4640896320343018,
      "learning_rate": 0.0008351253481894151,
      "loss": 2.4601,
      "step": 5920
    },
    {
      "epoch": 1.6518105849582172,
      "grad_norm": 1.3137675523757935,
      "learning_rate": 0.0008348467966573816,
      "loss": 2.5638,
      "step": 5930
    },
    {
      "epoch": 1.6545961002785514,
      "grad_norm": 1.2046961784362793,
      "learning_rate": 0.0008345682451253482,
      "loss": 2.3084,
      "step": 5940
    },
    {
      "epoch": 1.6573816155988856,
      "grad_norm": 1.3312538862228394,
      "learning_rate": 0.0008342896935933147,
      "loss": 2.4162,
      "step": 5950
    },
    {
      "epoch": 1.66016713091922,
      "grad_norm": 1.4303053617477417,
      "learning_rate": 0.0008340111420612814,
      "loss": 2.4541,
      "step": 5960
    },
    {
      "epoch": 1.6629526462395543,
      "grad_norm": 1.2691106796264648,
      "learning_rate": 0.0008337325905292479,
      "loss": 2.3322,
      "step": 5970
    },
    {
      "epoch": 1.6657381615598887,
      "grad_norm": 1.1390435695648193,
      "learning_rate": 0.0008334540389972145,
      "loss": 2.2717,
      "step": 5980
    },
    {
      "epoch": 1.668523676880223,
      "grad_norm": 1.8570890426635742,
      "learning_rate": 0.0008331754874651811,
      "loss": 2.4742,
      "step": 5990
    },
    {
      "epoch": 1.6713091922005572,
      "grad_norm": 1.1296697854995728,
      "learning_rate": 0.0008328969359331477,
      "loss": 2.2977,
      "step": 6000
    },
    {
      "epoch": 1.6740947075208914,
      "grad_norm": 1.2643065452575684,
      "learning_rate": 0.0008326183844011143,
      "loss": 2.3597,
      "step": 6010
    },
    {
      "epoch": 1.6768802228412256,
      "grad_norm": 1.3764960765838623,
      "learning_rate": 0.0008323398328690808,
      "loss": 2.4691,
      "step": 6020
    },
    {
      "epoch": 1.6796657381615598,
      "grad_norm": 1.5745573043823242,
      "learning_rate": 0.0008320612813370473,
      "loss": 2.7364,
      "step": 6030
    },
    {
      "epoch": 1.682451253481894,
      "grad_norm": 1.6773014068603516,
      "learning_rate": 0.0008317827298050139,
      "loss": 2.332,
      "step": 6040
    },
    {
      "epoch": 1.6852367688022283,
      "grad_norm": 1.3421497344970703,
      "learning_rate": 0.0008315041782729805,
      "loss": 2.4333,
      "step": 6050
    },
    {
      "epoch": 1.6880222841225627,
      "grad_norm": 1.1064913272857666,
      "learning_rate": 0.000831225626740947,
      "loss": 2.4062,
      "step": 6060
    },
    {
      "epoch": 1.690807799442897,
      "grad_norm": 1.3415062427520752,
      "learning_rate": 0.0008309470752089137,
      "loss": 2.5547,
      "step": 6070
    },
    {
      "epoch": 1.6935933147632312,
      "grad_norm": 1.562626600265503,
      "learning_rate": 0.0008306685236768803,
      "loss": 2.3707,
      "step": 6080
    },
    {
      "epoch": 1.6963788300835656,
      "grad_norm": 0.95414799451828,
      "learning_rate": 0.0008303899721448468,
      "loss": 2.4927,
      "step": 6090
    },
    {
      "epoch": 1.6991643454038998,
      "grad_norm": 1.349945306777954,
      "learning_rate": 0.0008301114206128134,
      "loss": 2.6923,
      "step": 6100
    },
    {
      "epoch": 1.701949860724234,
      "grad_norm": 1.99566650390625,
      "learning_rate": 0.00082983286908078,
      "loss": 2.2841,
      "step": 6110
    },
    {
      "epoch": 1.7047353760445683,
      "grad_norm": 1.2558735609054565,
      "learning_rate": 0.0008295543175487466,
      "loss": 2.4111,
      "step": 6120
    },
    {
      "epoch": 1.7075208913649025,
      "grad_norm": 1.0987590551376343,
      "learning_rate": 0.000829275766016713,
      "loss": 2.4022,
      "step": 6130
    },
    {
      "epoch": 1.7103064066852367,
      "grad_norm": 2.866203546524048,
      "learning_rate": 0.0008289972144846797,
      "loss": 2.2819,
      "step": 6140
    },
    {
      "epoch": 1.713091922005571,
      "grad_norm": 0.9099667072296143,
      "learning_rate": 0.0008287186629526463,
      "loss": 2.4293,
      "step": 6150
    },
    {
      "epoch": 1.7158774373259051,
      "grad_norm": 1.8381832838058472,
      "learning_rate": 0.0008284401114206128,
      "loss": 2.5236,
      "step": 6160
    },
    {
      "epoch": 1.7186629526462396,
      "grad_norm": 1.462134838104248,
      "learning_rate": 0.0008281615598885794,
      "loss": 2.3681,
      "step": 6170
    },
    {
      "epoch": 1.7214484679665738,
      "grad_norm": 1.1203333139419556,
      "learning_rate": 0.000827883008356546,
      "loss": 2.3484,
      "step": 6180
    },
    {
      "epoch": 1.724233983286908,
      "grad_norm": 1.2680754661560059,
      "learning_rate": 0.0008276044568245126,
      "loss": 2.4148,
      "step": 6190
    },
    {
      "epoch": 1.7270194986072425,
      "grad_norm": 1.7480159997940063,
      "learning_rate": 0.0008273259052924791,
      "loss": 2.6661,
      "step": 6200
    },
    {
      "epoch": 1.7298050139275767,
      "grad_norm": 1.4057271480560303,
      "learning_rate": 0.0008270473537604457,
      "loss": 2.4239,
      "step": 6210
    },
    {
      "epoch": 1.732590529247911,
      "grad_norm": 1.6399661302566528,
      "learning_rate": 0.0008267688022284122,
      "loss": 2.2966,
      "step": 6220
    },
    {
      "epoch": 1.7353760445682451,
      "grad_norm": 1.4471371173858643,
      "learning_rate": 0.0008264902506963788,
      "loss": 2.5211,
      "step": 6230
    },
    {
      "epoch": 1.7381615598885793,
      "grad_norm": 1.5068148374557495,
      "learning_rate": 0.0008262116991643454,
      "loss": 2.5038,
      "step": 6240
    },
    {
      "epoch": 1.7409470752089136,
      "grad_norm": 1.339417815208435,
      "learning_rate": 0.000825933147632312,
      "loss": 2.4747,
      "step": 6250
    },
    {
      "epoch": 1.7437325905292478,
      "grad_norm": 1.1985443830490112,
      "learning_rate": 0.0008256545961002786,
      "loss": 2.4816,
      "step": 6260
    },
    {
      "epoch": 1.7465181058495822,
      "grad_norm": 1.2834937572479248,
      "learning_rate": 0.0008253760445682451,
      "loss": 2.4074,
      "step": 6270
    },
    {
      "epoch": 1.7493036211699164,
      "grad_norm": 1.5863772630691528,
      "learning_rate": 0.0008250974930362117,
      "loss": 2.3436,
      "step": 6280
    },
    {
      "epoch": 1.7520891364902507,
      "grad_norm": 1.4046013355255127,
      "learning_rate": 0.0008248189415041783,
      "loss": 2.2898,
      "step": 6290
    },
    {
      "epoch": 1.754874651810585,
      "grad_norm": 1.2047582864761353,
      "learning_rate": 0.0008245403899721449,
      "loss": 2.4782,
      "step": 6300
    },
    {
      "epoch": 1.7576601671309193,
      "grad_norm": 1.4348340034484863,
      "learning_rate": 0.0008242618384401115,
      "loss": 2.2834,
      "step": 6310
    },
    {
      "epoch": 1.7604456824512535,
      "grad_norm": 1.4646048545837402,
      "learning_rate": 0.000823983286908078,
      "loss": 2.5312,
      "step": 6320
    },
    {
      "epoch": 1.7632311977715878,
      "grad_norm": 1.1535780429840088,
      "learning_rate": 0.0008237047353760446,
      "loss": 2.2959,
      "step": 6330
    },
    {
      "epoch": 1.766016713091922,
      "grad_norm": 1.7098382711410522,
      "learning_rate": 0.0008234261838440111,
      "loss": 2.4776,
      "step": 6340
    },
    {
      "epoch": 1.7688022284122562,
      "grad_norm": 1.151153802871704,
      "learning_rate": 0.0008231476323119777,
      "loss": 2.3554,
      "step": 6350
    },
    {
      "epoch": 1.7715877437325904,
      "grad_norm": 1.1105693578720093,
      "learning_rate": 0.0008228690807799443,
      "loss": 2.476,
      "step": 6360
    },
    {
      "epoch": 1.7743732590529246,
      "grad_norm": 1.3276675939559937,
      "learning_rate": 0.0008225905292479109,
      "loss": 2.3553,
      "step": 6370
    },
    {
      "epoch": 1.777158774373259,
      "grad_norm": 1.181079387664795,
      "learning_rate": 0.0008223119777158774,
      "loss": 2.3659,
      "step": 6380
    },
    {
      "epoch": 1.7799442896935933,
      "grad_norm": 1.257873773574829,
      "learning_rate": 0.000822033426183844,
      "loss": 2.4904,
      "step": 6390
    },
    {
      "epoch": 1.7827298050139275,
      "grad_norm": 1.0463005304336548,
      "learning_rate": 0.0008217548746518107,
      "loss": 2.2976,
      "step": 6400
    },
    {
      "epoch": 1.785515320334262,
      "grad_norm": 1.3912805318832397,
      "learning_rate": 0.0008214763231197772,
      "loss": 2.416,
      "step": 6410
    },
    {
      "epoch": 1.7883008356545962,
      "grad_norm": 1.1875910758972168,
      "learning_rate": 0.0008211977715877437,
      "loss": 2.3713,
      "step": 6420
    },
    {
      "epoch": 1.7910863509749304,
      "grad_norm": 0.9566900134086609,
      "learning_rate": 0.0008209192200557103,
      "loss": 2.3868,
      "step": 6430
    },
    {
      "epoch": 1.7938718662952646,
      "grad_norm": 1.284903645515442,
      "learning_rate": 0.0008206406685236769,
      "loss": 2.4495,
      "step": 6440
    },
    {
      "epoch": 1.7966573816155988,
      "grad_norm": 1.9530730247497559,
      "learning_rate": 0.0008203621169916434,
      "loss": 2.37,
      "step": 6450
    },
    {
      "epoch": 1.799442896935933,
      "grad_norm": 1.6074453592300415,
      "learning_rate": 0.00082008356545961,
      "loss": 2.3302,
      "step": 6460
    },
    {
      "epoch": 1.8022284122562673,
      "grad_norm": 0.9746838808059692,
      "learning_rate": 0.0008198050139275767,
      "loss": 2.34,
      "step": 6470
    },
    {
      "epoch": 1.8050139275766015,
      "grad_norm": 1.722078800201416,
      "learning_rate": 0.0008195264623955432,
      "loss": 2.5026,
      "step": 6480
    },
    {
      "epoch": 1.807799442896936,
      "grad_norm": 1.0056898593902588,
      "learning_rate": 0.0008192479108635098,
      "loss": 2.5026,
      "step": 6490
    },
    {
      "epoch": 1.8105849582172702,
      "grad_norm": 1.74678635597229,
      "learning_rate": 0.0008189693593314764,
      "loss": 2.4645,
      "step": 6500
    },
    {
      "epoch": 1.8133704735376046,
      "grad_norm": 1.3847345113754272,
      "learning_rate": 0.000818690807799443,
      "loss": 2.4499,
      "step": 6510
    },
    {
      "epoch": 1.8161559888579388,
      "grad_norm": 1.717911958694458,
      "learning_rate": 0.0008184122562674094,
      "loss": 2.2657,
      "step": 6520
    },
    {
      "epoch": 1.818941504178273,
      "grad_norm": 2.138252019882202,
      "learning_rate": 0.000818133704735376,
      "loss": 2.519,
      "step": 6530
    },
    {
      "epoch": 1.8217270194986073,
      "grad_norm": 1.569498062133789,
      "learning_rate": 0.0008178551532033426,
      "loss": 2.4544,
      "step": 6540
    },
    {
      "epoch": 1.8245125348189415,
      "grad_norm": 1.2019338607788086,
      "learning_rate": 0.0008175766016713092,
      "loss": 2.1874,
      "step": 6550
    },
    {
      "epoch": 1.8272980501392757,
      "grad_norm": 1.0763612985610962,
      "learning_rate": 0.0008172980501392758,
      "loss": 2.2884,
      "step": 6560
    },
    {
      "epoch": 1.83008356545961,
      "grad_norm": 2.008368968963623,
      "learning_rate": 0.0008170194986072423,
      "loss": 2.3894,
      "step": 6570
    },
    {
      "epoch": 1.8328690807799441,
      "grad_norm": 1.4757318496704102,
      "learning_rate": 0.000816740947075209,
      "loss": 2.4282,
      "step": 6580
    },
    {
      "epoch": 1.8356545961002786,
      "grad_norm": 1.405335545539856,
      "learning_rate": 0.0008164623955431755,
      "loss": 2.5322,
      "step": 6590
    },
    {
      "epoch": 1.8384401114206128,
      "grad_norm": 1.394051194190979,
      "learning_rate": 0.0008161838440111421,
      "loss": 2.4235,
      "step": 6600
    },
    {
      "epoch": 1.841225626740947,
      "grad_norm": 1.2842164039611816,
      "learning_rate": 0.0008159052924791087,
      "loss": 2.2559,
      "step": 6610
    },
    {
      "epoch": 1.8440111420612815,
      "grad_norm": 1.3123998641967773,
      "learning_rate": 0.0008156267409470752,
      "loss": 2.4682,
      "step": 6620
    },
    {
      "epoch": 1.8467966573816157,
      "grad_norm": 1.2043389081954956,
      "learning_rate": 0.0008153481894150418,
      "loss": 2.3725,
      "step": 6630
    },
    {
      "epoch": 1.84958217270195,
      "grad_norm": 2.5434577465057373,
      "learning_rate": 0.0008150696378830083,
      "loss": 2.3973,
      "step": 6640
    },
    {
      "epoch": 1.8523676880222841,
      "grad_norm": 1.0565294027328491,
      "learning_rate": 0.000814791086350975,
      "loss": 2.4653,
      "step": 6650
    },
    {
      "epoch": 1.8551532033426184,
      "grad_norm": 1.2518049478530884,
      "learning_rate": 0.0008145125348189415,
      "loss": 2.431,
      "step": 6660
    },
    {
      "epoch": 1.8579387186629526,
      "grad_norm": 1.0736119747161865,
      "learning_rate": 0.0008142339832869081,
      "loss": 2.2969,
      "step": 6670
    },
    {
      "epoch": 1.8607242339832868,
      "grad_norm": 1.3754123449325562,
      "learning_rate": 0.0008139554317548746,
      "loss": 2.4353,
      "step": 6680
    },
    {
      "epoch": 1.863509749303621,
      "grad_norm": 1.5035947561264038,
      "learning_rate": 0.0008136768802228413,
      "loss": 2.3644,
      "step": 6690
    },
    {
      "epoch": 1.8662952646239555,
      "grad_norm": 1.263003945350647,
      "learning_rate": 0.0008133983286908078,
      "loss": 2.3284,
      "step": 6700
    },
    {
      "epoch": 1.8690807799442897,
      "grad_norm": 1.631212592124939,
      "learning_rate": 0.0008131197771587744,
      "loss": 2.3684,
      "step": 6710
    },
    {
      "epoch": 1.8718662952646241,
      "grad_norm": 1.3502775430679321,
      "learning_rate": 0.000812841225626741,
      "loss": 2.5679,
      "step": 6720
    },
    {
      "epoch": 1.8746518105849583,
      "grad_norm": 1.5666013956069946,
      "learning_rate": 0.0008125626740947075,
      "loss": 2.2837,
      "step": 6730
    },
    {
      "epoch": 1.8774373259052926,
      "grad_norm": 1.164220929145813,
      "learning_rate": 0.0008122841225626741,
      "loss": 2.3594,
      "step": 6740
    },
    {
      "epoch": 1.8802228412256268,
      "grad_norm": 1.5533705949783325,
      "learning_rate": 0.0008120055710306406,
      "loss": 2.315,
      "step": 6750
    },
    {
      "epoch": 1.883008356545961,
      "grad_norm": 1.748683214187622,
      "learning_rate": 0.0008117270194986073,
      "loss": 2.5152,
      "step": 6760
    },
    {
      "epoch": 1.8857938718662952,
      "grad_norm": 1.5651408433914185,
      "learning_rate": 0.0008114484679665738,
      "loss": 2.2927,
      "step": 6770
    },
    {
      "epoch": 1.8885793871866294,
      "grad_norm": 2.3147897720336914,
      "learning_rate": 0.0008111699164345404,
      "loss": 2.4833,
      "step": 6780
    },
    {
      "epoch": 1.8913649025069637,
      "grad_norm": 1.5875558853149414,
      "learning_rate": 0.0008108913649025071,
      "loss": 2.3467,
      "step": 6790
    },
    {
      "epoch": 1.894150417827298,
      "grad_norm": 1.2705727815628052,
      "learning_rate": 0.0008106128133704736,
      "loss": 2.3996,
      "step": 6800
    },
    {
      "epoch": 1.8969359331476323,
      "grad_norm": 2.1113150119781494,
      "learning_rate": 0.0008103342618384402,
      "loss": 2.4107,
      "step": 6810
    },
    {
      "epoch": 1.8997214484679665,
      "grad_norm": 1.813565969467163,
      "learning_rate": 0.0008100557103064066,
      "loss": 2.3264,
      "step": 6820
    },
    {
      "epoch": 1.902506963788301,
      "grad_norm": 1.2330787181854248,
      "learning_rate": 0.0008097771587743733,
      "loss": 2.336,
      "step": 6830
    },
    {
      "epoch": 1.9052924791086352,
      "grad_norm": 1.2896074056625366,
      "learning_rate": 0.0008094986072423398,
      "loss": 2.6178,
      "step": 6840
    },
    {
      "epoch": 1.9080779944289694,
      "grad_norm": 1.1541211605072021,
      "learning_rate": 0.0008092200557103064,
      "loss": 2.4323,
      "step": 6850
    },
    {
      "epoch": 1.9108635097493036,
      "grad_norm": 1.5347723960876465,
      "learning_rate": 0.0008089415041782729,
      "loss": 2.4192,
      "step": 6860
    },
    {
      "epoch": 1.9136490250696379,
      "grad_norm": 1.897680401802063,
      "learning_rate": 0.0008086629526462396,
      "loss": 2.4667,
      "step": 6870
    },
    {
      "epoch": 1.916434540389972,
      "grad_norm": 1.3518460988998413,
      "learning_rate": 0.0008083844011142062,
      "loss": 2.4204,
      "step": 6880
    },
    {
      "epoch": 1.9192200557103063,
      "grad_norm": 1.832296371459961,
      "learning_rate": 0.0008081058495821727,
      "loss": 2.2927,
      "step": 6890
    },
    {
      "epoch": 1.9220055710306405,
      "grad_norm": 1.322979211807251,
      "learning_rate": 0.0008078272980501394,
      "loss": 2.3523,
      "step": 6900
    },
    {
      "epoch": 1.924791086350975,
      "grad_norm": 1.7961318492889404,
      "learning_rate": 0.0008075487465181059,
      "loss": 2.3673,
      "step": 6910
    },
    {
      "epoch": 1.9275766016713092,
      "grad_norm": 1.6131647825241089,
      "learning_rate": 0.0008072701949860724,
      "loss": 2.2188,
      "step": 6920
    },
    {
      "epoch": 1.9303621169916436,
      "grad_norm": 1.4047913551330566,
      "learning_rate": 0.0008069916434540389,
      "loss": 2.4701,
      "step": 6930
    },
    {
      "epoch": 1.9331476323119778,
      "grad_norm": 1.3033145666122437,
      "learning_rate": 0.0008067130919220056,
      "loss": 2.3598,
      "step": 6940
    },
    {
      "epoch": 1.935933147632312,
      "grad_norm": 1.4304487705230713,
      "learning_rate": 0.0008064345403899722,
      "loss": 2.4718,
      "step": 6950
    },
    {
      "epoch": 1.9387186629526463,
      "grad_norm": 1.432222843170166,
      "learning_rate": 0.0008061559888579387,
      "loss": 2.4223,
      "step": 6960
    },
    {
      "epoch": 1.9415041782729805,
      "grad_norm": 1.2511779069900513,
      "learning_rate": 0.0008058774373259054,
      "loss": 2.5141,
      "step": 6970
    },
    {
      "epoch": 1.9442896935933147,
      "grad_norm": 1.1041837930679321,
      "learning_rate": 0.0008055988857938719,
      "loss": 2.3896,
      "step": 6980
    },
    {
      "epoch": 1.947075208913649,
      "grad_norm": 1.4512009620666504,
      "learning_rate": 0.0008053203342618385,
      "loss": 2.4866,
      "step": 6990
    },
    {
      "epoch": 1.9498607242339832,
      "grad_norm": 1.1579104661941528,
      "learning_rate": 0.000805041782729805,
      "loss": 2.4275,
      "step": 7000
    },
    {
      "epoch": 1.9526462395543176,
      "grad_norm": 1.4708575010299683,
      "learning_rate": 0.0008047632311977717,
      "loss": 2.5642,
      "step": 7010
    },
    {
      "epoch": 1.9554317548746518,
      "grad_norm": 1.613075613975525,
      "learning_rate": 0.0008044846796657381,
      "loss": 2.3841,
      "step": 7020
    },
    {
      "epoch": 1.958217270194986,
      "grad_norm": 1.5922809839248657,
      "learning_rate": 0.0008042061281337047,
      "loss": 2.2237,
      "step": 7030
    },
    {
      "epoch": 1.9610027855153205,
      "grad_norm": 1.1342005729675293,
      "learning_rate": 0.0008039275766016713,
      "loss": 2.37,
      "step": 7040
    },
    {
      "epoch": 1.9637883008356547,
      "grad_norm": 1.2186145782470703,
      "learning_rate": 0.0008036490250696379,
      "loss": 2.5176,
      "step": 7050
    },
    {
      "epoch": 1.966573816155989,
      "grad_norm": 1.9192311763763428,
      "learning_rate": 0.0008033704735376045,
      "loss": 2.441,
      "step": 7060
    },
    {
      "epoch": 1.9693593314763231,
      "grad_norm": 1.4646897315979004,
      "learning_rate": 0.000803091922005571,
      "loss": 2.5179,
      "step": 7070
    },
    {
      "epoch": 1.9721448467966574,
      "grad_norm": 1.456370234489441,
      "learning_rate": 0.0008028133704735377,
      "loss": 2.4548,
      "step": 7080
    },
    {
      "epoch": 1.9749303621169916,
      "grad_norm": 1.2759555578231812,
      "learning_rate": 0.0008025348189415042,
      "loss": 2.3693,
      "step": 7090
    },
    {
      "epoch": 1.9777158774373258,
      "grad_norm": 1.764106273651123,
      "learning_rate": 0.0008022562674094708,
      "loss": 2.2319,
      "step": 7100
    },
    {
      "epoch": 1.98050139275766,
      "grad_norm": 1.392533779144287,
      "learning_rate": 0.0008019777158774373,
      "loss": 2.4095,
      "step": 7110
    },
    {
      "epoch": 1.9832869080779945,
      "grad_norm": 1.366507649421692,
      "learning_rate": 0.0008016991643454039,
      "loss": 2.4997,
      "step": 7120
    },
    {
      "epoch": 1.9860724233983287,
      "grad_norm": 1.8847013711929321,
      "learning_rate": 0.0008014206128133705,
      "loss": 2.5308,
      "step": 7130
    },
    {
      "epoch": 1.988857938718663,
      "grad_norm": 1.1940280199050903,
      "learning_rate": 0.000801142061281337,
      "loss": 2.6878,
      "step": 7140
    },
    {
      "epoch": 1.9916434540389973,
      "grad_norm": 1.3204154968261719,
      "learning_rate": 0.0008008635097493037,
      "loss": 2.568,
      "step": 7150
    },
    {
      "epoch": 1.9944289693593316,
      "grad_norm": 1.7207311391830444,
      "learning_rate": 0.0008005849582172702,
      "loss": 2.5302,
      "step": 7160
    },
    {
      "epoch": 1.9972144846796658,
      "grad_norm": 1.5227282047271729,
      "learning_rate": 0.0008003064066852368,
      "loss": 2.3751,
      "step": 7170
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.179631233215332,
      "learning_rate": 0.0008000278551532033,
      "loss": 2.4653,
      "step": 7180
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.3556978702545166,
      "eval_runtime": 6.6216,
      "eval_samples_per_second": 482.057,
      "eval_steps_per_second": 60.257,
      "step": 7180
    },
    {
      "epoch": 2.002785515320334,
      "grad_norm": 1.3453212976455688,
      "learning_rate": 0.00079974930362117,
      "loss": 2.6607,
      "step": 7190
    },
    {
      "epoch": 2.0055710306406684,
      "grad_norm": 1.1035836935043335,
      "learning_rate": 0.0007994707520891366,
      "loss": 2.2951,
      "step": 7200
    },
    {
      "epoch": 2.0083565459610027,
      "grad_norm": 1.6278761625289917,
      "learning_rate": 0.000799192200557103,
      "loss": 2.2508,
      "step": 7210
    },
    {
      "epoch": 2.011142061281337,
      "grad_norm": 1.5138803720474243,
      "learning_rate": 0.0007989136490250696,
      "loss": 2.3115,
      "step": 7220
    },
    {
      "epoch": 2.013927576601671,
      "grad_norm": 1.0917413234710693,
      "learning_rate": 0.0007986350974930362,
      "loss": 2.3969,
      "step": 7230
    },
    {
      "epoch": 2.0167130919220058,
      "grad_norm": 1.4884893894195557,
      "learning_rate": 0.0007983565459610028,
      "loss": 2.3464,
      "step": 7240
    },
    {
      "epoch": 2.01949860724234,
      "grad_norm": 1.6122921705245972,
      "learning_rate": 0.0007980779944289693,
      "loss": 2.5179,
      "step": 7250
    },
    {
      "epoch": 2.022284122562674,
      "grad_norm": 1.5920754671096802,
      "learning_rate": 0.000797799442896936,
      "loss": 2.7088,
      "step": 7260
    },
    {
      "epoch": 2.0250696378830084,
      "grad_norm": 1.494230031967163,
      "learning_rate": 0.0007975208913649026,
      "loss": 2.4697,
      "step": 7270
    },
    {
      "epoch": 2.0278551532033426,
      "grad_norm": 1.403394103050232,
      "learning_rate": 0.0007972423398328691,
      "loss": 2.3259,
      "step": 7280
    },
    {
      "epoch": 2.030640668523677,
      "grad_norm": 1.4464670419692993,
      "learning_rate": 0.0007969637883008357,
      "loss": 2.3063,
      "step": 7290
    },
    {
      "epoch": 2.033426183844011,
      "grad_norm": 1.6362191438674927,
      "learning_rate": 0.0007966852367688023,
      "loss": 2.3642,
      "step": 7300
    },
    {
      "epoch": 2.0362116991643453,
      "grad_norm": 1.5637233257293701,
      "learning_rate": 0.0007964066852367688,
      "loss": 2.3926,
      "step": 7310
    },
    {
      "epoch": 2.0389972144846795,
      "grad_norm": 1.2208242416381836,
      "learning_rate": 0.0007961281337047353,
      "loss": 2.1584,
      "step": 7320
    },
    {
      "epoch": 2.0417827298050137,
      "grad_norm": 1.15878427028656,
      "learning_rate": 0.000795849582172702,
      "loss": 2.3832,
      "step": 7330
    },
    {
      "epoch": 2.0445682451253484,
      "grad_norm": 1.0647684335708618,
      "learning_rate": 0.0007955710306406685,
      "loss": 2.2874,
      "step": 7340
    },
    {
      "epoch": 2.0473537604456826,
      "grad_norm": 1.2820518016815186,
      "learning_rate": 0.0007952924791086351,
      "loss": 2.491,
      "step": 7350
    },
    {
      "epoch": 2.050139275766017,
      "grad_norm": 1.66392183303833,
      "learning_rate": 0.0007950139275766017,
      "loss": 2.2995,
      "step": 7360
    },
    {
      "epoch": 2.052924791086351,
      "grad_norm": 1.0753968954086304,
      "learning_rate": 0.0007947353760445683,
      "loss": 2.3087,
      "step": 7370
    },
    {
      "epoch": 2.0557103064066853,
      "grad_norm": 1.361895203590393,
      "learning_rate": 0.0007944568245125349,
      "loss": 2.4484,
      "step": 7380
    },
    {
      "epoch": 2.0584958217270195,
      "grad_norm": 2.264072895050049,
      "learning_rate": 0.0007941782729805014,
      "loss": 2.4481,
      "step": 7390
    },
    {
      "epoch": 2.0612813370473537,
      "grad_norm": 1.54152250289917,
      "learning_rate": 0.000793899721448468,
      "loss": 2.4818,
      "step": 7400
    },
    {
      "epoch": 2.064066852367688,
      "grad_norm": 1.2707403898239136,
      "learning_rate": 0.0007936211699164345,
      "loss": 2.2113,
      "step": 7410
    },
    {
      "epoch": 2.066852367688022,
      "grad_norm": 1.2559406757354736,
      "learning_rate": 0.0007933426183844011,
      "loss": 2.2546,
      "step": 7420
    },
    {
      "epoch": 2.0696378830083564,
      "grad_norm": 1.6063097715377808,
      "learning_rate": 0.0007930640668523676,
      "loss": 2.4105,
      "step": 7430
    },
    {
      "epoch": 2.0724233983286906,
      "grad_norm": 1.6733657121658325,
      "learning_rate": 0.0007927855153203343,
      "loss": 2.2669,
      "step": 7440
    },
    {
      "epoch": 2.0752089136490253,
      "grad_norm": 1.5204365253448486,
      "learning_rate": 0.0007925069637883009,
      "loss": 2.4219,
      "step": 7450
    },
    {
      "epoch": 2.0779944289693595,
      "grad_norm": 1.2422515153884888,
      "learning_rate": 0.0007922284122562674,
      "loss": 2.3651,
      "step": 7460
    },
    {
      "epoch": 2.0807799442896937,
      "grad_norm": 1.684012532234192,
      "learning_rate": 0.000791949860724234,
      "loss": 2.3811,
      "step": 7470
    },
    {
      "epoch": 2.083565459610028,
      "grad_norm": 1.5672991275787354,
      "learning_rate": 0.0007916713091922006,
      "loss": 2.5062,
      "step": 7480
    },
    {
      "epoch": 2.086350974930362,
      "grad_norm": 1.1453132629394531,
      "learning_rate": 0.0007913927576601672,
      "loss": 2.3013,
      "step": 7490
    },
    {
      "epoch": 2.0891364902506964,
      "grad_norm": 1.6245533227920532,
      "learning_rate": 0.0007911142061281336,
      "loss": 2.3354,
      "step": 7500
    },
    {
      "epoch": 2.0919220055710306,
      "grad_norm": 2.298712968826294,
      "learning_rate": 0.0007908356545961003,
      "loss": 2.4695,
      "step": 7510
    },
    {
      "epoch": 2.094707520891365,
      "grad_norm": 1.492261528968811,
      "learning_rate": 0.0007905571030640669,
      "loss": 2.2391,
      "step": 7520
    },
    {
      "epoch": 2.097493036211699,
      "grad_norm": 1.0585415363311768,
      "learning_rate": 0.0007902785515320334,
      "loss": 2.3627,
      "step": 7530
    },
    {
      "epoch": 2.1002785515320332,
      "grad_norm": 1.8437236547470093,
      "learning_rate": 0.00079,
      "loss": 2.6862,
      "step": 7540
    },
    {
      "epoch": 2.103064066852368,
      "grad_norm": 1.4239869117736816,
      "learning_rate": 0.0007897214484679666,
      "loss": 2.3403,
      "step": 7550
    },
    {
      "epoch": 2.105849582172702,
      "grad_norm": 1.514460802078247,
      "learning_rate": 0.0007894428969359332,
      "loss": 2.5961,
      "step": 7560
    },
    {
      "epoch": 2.1086350974930363,
      "grad_norm": 1.1364526748657227,
      "learning_rate": 0.0007891643454038997,
      "loss": 2.2607,
      "step": 7570
    },
    {
      "epoch": 2.1114206128133706,
      "grad_norm": 2.752535104751587,
      "learning_rate": 0.0007888857938718663,
      "loss": 2.2706,
      "step": 7580
    },
    {
      "epoch": 2.114206128133705,
      "grad_norm": 1.140853762626648,
      "learning_rate": 0.0007886072423398329,
      "loss": 2.2845,
      "step": 7590
    },
    {
      "epoch": 2.116991643454039,
      "grad_norm": 1.1988213062286377,
      "learning_rate": 0.0007883286908077995,
      "loss": 2.4645,
      "step": 7600
    },
    {
      "epoch": 2.1197771587743732,
      "grad_norm": 1.634780764579773,
      "learning_rate": 0.000788050139275766,
      "loss": 2.4848,
      "step": 7610
    },
    {
      "epoch": 2.1225626740947074,
      "grad_norm": 1.3125979900360107,
      "learning_rate": 0.0007877715877437326,
      "loss": 2.4643,
      "step": 7620
    },
    {
      "epoch": 2.1253481894150417,
      "grad_norm": 1.117775797843933,
      "learning_rate": 0.0007874930362116992,
      "loss": 2.3072,
      "step": 7630
    },
    {
      "epoch": 2.128133704735376,
      "grad_norm": 1.8878843784332275,
      "learning_rate": 0.0007872144846796657,
      "loss": 2.3745,
      "step": 7640
    },
    {
      "epoch": 2.13091922005571,
      "grad_norm": 1.3035587072372437,
      "learning_rate": 0.0007869359331476323,
      "loss": 2.2285,
      "step": 7650
    },
    {
      "epoch": 2.1337047353760448,
      "grad_norm": 1.223335862159729,
      "learning_rate": 0.0007866573816155989,
      "loss": 2.3944,
      "step": 7660
    },
    {
      "epoch": 2.136490250696379,
      "grad_norm": 1.5938912630081177,
      "learning_rate": 0.0007863788300835655,
      "loss": 2.4739,
      "step": 7670
    },
    {
      "epoch": 2.139275766016713,
      "grad_norm": 1.002762794494629,
      "learning_rate": 0.0007861002785515321,
      "loss": 2.3459,
      "step": 7680
    },
    {
      "epoch": 2.1420612813370474,
      "grad_norm": 1.4070748090744019,
      "learning_rate": 0.0007858217270194987,
      "loss": 2.2527,
      "step": 7690
    },
    {
      "epoch": 2.1448467966573816,
      "grad_norm": 1.5111035108566284,
      "learning_rate": 0.0007855431754874653,
      "loss": 2.3875,
      "step": 7700
    },
    {
      "epoch": 2.147632311977716,
      "grad_norm": 2.0895113945007324,
      "learning_rate": 0.0007852646239554317,
      "loss": 2.5695,
      "step": 7710
    },
    {
      "epoch": 2.15041782729805,
      "grad_norm": 1.6621885299682617,
      "learning_rate": 0.0007849860724233983,
      "loss": 2.4544,
      "step": 7720
    },
    {
      "epoch": 2.1532033426183843,
      "grad_norm": 1.3012093305587769,
      "learning_rate": 0.0007847075208913649,
      "loss": 2.3507,
      "step": 7730
    },
    {
      "epoch": 2.1559888579387185,
      "grad_norm": 1.664260983467102,
      "learning_rate": 0.0007844289693593315,
      "loss": 2.2632,
      "step": 7740
    },
    {
      "epoch": 2.1587743732590527,
      "grad_norm": 1.6347448825836182,
      "learning_rate": 0.000784150417827298,
      "loss": 2.2126,
      "step": 7750
    },
    {
      "epoch": 2.1615598885793874,
      "grad_norm": 2.5198276042938232,
      "learning_rate": 0.0007838718662952646,
      "loss": 2.5193,
      "step": 7760
    },
    {
      "epoch": 2.1643454038997216,
      "grad_norm": 1.2400208711624146,
      "learning_rate": 0.0007835933147632313,
      "loss": 2.3442,
      "step": 7770
    },
    {
      "epoch": 2.167130919220056,
      "grad_norm": 1.1855803728103638,
      "learning_rate": 0.0007833147632311978,
      "loss": 2.3466,
      "step": 7780
    },
    {
      "epoch": 2.16991643454039,
      "grad_norm": 1.3222978115081787,
      "learning_rate": 0.0007830362116991644,
      "loss": 2.2835,
      "step": 7790
    },
    {
      "epoch": 2.1727019498607243,
      "grad_norm": 1.5952283143997192,
      "learning_rate": 0.000782757660167131,
      "loss": 2.3275,
      "step": 7800
    },
    {
      "epoch": 2.1754874651810585,
      "grad_norm": 1.8580290079116821,
      "learning_rate": 0.0007824791086350975,
      "loss": 2.2523,
      "step": 7810
    },
    {
      "epoch": 2.1782729805013927,
      "grad_norm": 1.588371753692627,
      "learning_rate": 0.000782200557103064,
      "loss": 2.3901,
      "step": 7820
    },
    {
      "epoch": 2.181058495821727,
      "grad_norm": 1.5030392408370972,
      "learning_rate": 0.0007819220055710306,
      "loss": 2.3686,
      "step": 7830
    },
    {
      "epoch": 2.183844011142061,
      "grad_norm": 1.4872692823410034,
      "learning_rate": 0.0007816434540389973,
      "loss": 2.5144,
      "step": 7840
    },
    {
      "epoch": 2.1866295264623954,
      "grad_norm": 2.1316940784454346,
      "learning_rate": 0.0007813649025069638,
      "loss": 2.316,
      "step": 7850
    },
    {
      "epoch": 2.1894150417827296,
      "grad_norm": 2.1245522499084473,
      "learning_rate": 0.0007810863509749304,
      "loss": 2.3966,
      "step": 7860
    },
    {
      "epoch": 2.1922005571030643,
      "grad_norm": 1.1319622993469238,
      "learning_rate": 0.000780807799442897,
      "loss": 2.4584,
      "step": 7870
    },
    {
      "epoch": 2.1949860724233985,
      "grad_norm": 1.1304526329040527,
      "learning_rate": 0.0007805292479108636,
      "loss": 2.224,
      "step": 7880
    },
    {
      "epoch": 2.1977715877437327,
      "grad_norm": 1.376865029335022,
      "learning_rate": 0.00078025069637883,
      "loss": 2.4138,
      "step": 7890
    },
    {
      "epoch": 2.200557103064067,
      "grad_norm": 1.2918295860290527,
      "learning_rate": 0.0007799721448467966,
      "loss": 2.28,
      "step": 7900
    },
    {
      "epoch": 2.203342618384401,
      "grad_norm": 1.2234834432601929,
      "learning_rate": 0.0007796935933147632,
      "loss": 2.2875,
      "step": 7910
    },
    {
      "epoch": 2.2061281337047354,
      "grad_norm": 1.5959925651550293,
      "learning_rate": 0.0007794150417827298,
      "loss": 2.5564,
      "step": 7920
    },
    {
      "epoch": 2.2089136490250696,
      "grad_norm": 1.2370902299880981,
      "learning_rate": 0.0007791364902506964,
      "loss": 2.3204,
      "step": 7930
    },
    {
      "epoch": 2.211699164345404,
      "grad_norm": 1.7273848056793213,
      "learning_rate": 0.0007788579387186629,
      "loss": 2.4606,
      "step": 7940
    },
    {
      "epoch": 2.214484679665738,
      "grad_norm": 1.978658676147461,
      "learning_rate": 0.0007785793871866296,
      "loss": 2.4427,
      "step": 7950
    },
    {
      "epoch": 2.2172701949860723,
      "grad_norm": 1.5949851274490356,
      "learning_rate": 0.0007783008356545961,
      "loss": 2.5003,
      "step": 7960
    },
    {
      "epoch": 2.220055710306407,
      "grad_norm": 1.5568246841430664,
      "learning_rate": 0.0007780222841225627,
      "loss": 2.3162,
      "step": 7970
    },
    {
      "epoch": 2.222841225626741,
      "grad_norm": 1.5978367328643799,
      "learning_rate": 0.0007777437325905293,
      "loss": 2.5276,
      "step": 7980
    },
    {
      "epoch": 2.2256267409470754,
      "grad_norm": 1.2224229574203491,
      "learning_rate": 0.0007774651810584959,
      "loss": 2.3944,
      "step": 7990
    },
    {
      "epoch": 2.2284122562674096,
      "grad_norm": 1.4874629974365234,
      "learning_rate": 0.0007771866295264624,
      "loss": 2.1786,
      "step": 8000
    },
    {
      "epoch": 2.231197771587744,
      "grad_norm": 1.878524661064148,
      "learning_rate": 0.0007769080779944289,
      "loss": 2.5292,
      "step": 8010
    },
    {
      "epoch": 2.233983286908078,
      "grad_norm": 1.5469121932983398,
      "learning_rate": 0.0007766295264623956,
      "loss": 2.3234,
      "step": 8020
    },
    {
      "epoch": 2.2367688022284122,
      "grad_norm": 1.2823669910430908,
      "learning_rate": 0.0007763509749303621,
      "loss": 2.2994,
      "step": 8030
    },
    {
      "epoch": 2.2395543175487465,
      "grad_norm": 1.575114369392395,
      "learning_rate": 0.0007760724233983287,
      "loss": 2.3443,
      "step": 8040
    },
    {
      "epoch": 2.2423398328690807,
      "grad_norm": 1.4382259845733643,
      "learning_rate": 0.0007757938718662953,
      "loss": 2.3211,
      "step": 8050
    },
    {
      "epoch": 2.245125348189415,
      "grad_norm": 1.607909083366394,
      "learning_rate": 0.0007755153203342619,
      "loss": 2.5369,
      "step": 8060
    },
    {
      "epoch": 2.247910863509749,
      "grad_norm": 1.328190565109253,
      "learning_rate": 0.0007752367688022284,
      "loss": 2.3097,
      "step": 8070
    },
    {
      "epoch": 2.2506963788300833,
      "grad_norm": 1.4126689434051514,
      "learning_rate": 0.000774958217270195,
      "loss": 2.2133,
      "step": 8080
    },
    {
      "epoch": 2.253481894150418,
      "grad_norm": 1.572833776473999,
      "learning_rate": 0.0007746796657381617,
      "loss": 2.1742,
      "step": 8090
    },
    {
      "epoch": 2.256267409470752,
      "grad_norm": 1.2270212173461914,
      "learning_rate": 0.0007744011142061281,
      "loss": 2.4275,
      "step": 8100
    },
    {
      "epoch": 2.2590529247910864,
      "grad_norm": 1.3822630643844604,
      "learning_rate": 0.0007741225626740947,
      "loss": 2.339,
      "step": 8110
    },
    {
      "epoch": 2.2618384401114207,
      "grad_norm": 1.2307243347167969,
      "learning_rate": 0.0007738440111420612,
      "loss": 2.2225,
      "step": 8120
    },
    {
      "epoch": 2.264623955431755,
      "grad_norm": 1.310272455215454,
      "learning_rate": 0.0007735654596100279,
      "loss": 2.229,
      "step": 8130
    },
    {
      "epoch": 2.267409470752089,
      "grad_norm": 1.6313135623931885,
      "learning_rate": 0.0007732869080779944,
      "loss": 2.4181,
      "step": 8140
    },
    {
      "epoch": 2.2701949860724233,
      "grad_norm": 1.45780348777771,
      "learning_rate": 0.000773008356545961,
      "loss": 2.3781,
      "step": 8150
    },
    {
      "epoch": 2.2729805013927575,
      "grad_norm": 1.229520559310913,
      "learning_rate": 0.0007727298050139277,
      "loss": 2.1492,
      "step": 8160
    },
    {
      "epoch": 2.2757660167130918,
      "grad_norm": 1.680285930633545,
      "learning_rate": 0.0007724512534818942,
      "loss": 2.3884,
      "step": 8170
    },
    {
      "epoch": 2.2785515320334264,
      "grad_norm": 1.8521662950515747,
      "learning_rate": 0.0007721727019498608,
      "loss": 2.5741,
      "step": 8180
    },
    {
      "epoch": 2.2813370473537606,
      "grad_norm": 1.3929111957550049,
      "learning_rate": 0.0007718941504178272,
      "loss": 2.2813,
      "step": 8190
    },
    {
      "epoch": 2.284122562674095,
      "grad_norm": 1.418949842453003,
      "learning_rate": 0.000771615598885794,
      "loss": 2.4946,
      "step": 8200
    },
    {
      "epoch": 2.286908077994429,
      "grad_norm": 1.507152795791626,
      "learning_rate": 0.0007713370473537604,
      "loss": 2.3587,
      "step": 8210
    },
    {
      "epoch": 2.2896935933147633,
      "grad_norm": 1.510338544845581,
      "learning_rate": 0.000771058495821727,
      "loss": 2.3262,
      "step": 8220
    },
    {
      "epoch": 2.2924791086350975,
      "grad_norm": 1.293959140777588,
      "learning_rate": 0.0007707799442896935,
      "loss": 2.2061,
      "step": 8230
    },
    {
      "epoch": 2.2952646239554317,
      "grad_norm": 1.589704155921936,
      "learning_rate": 0.0007705013927576602,
      "loss": 2.5749,
      "step": 8240
    },
    {
      "epoch": 2.298050139275766,
      "grad_norm": 1.512417197227478,
      "learning_rate": 0.0007702228412256268,
      "loss": 2.1943,
      "step": 8250
    },
    {
      "epoch": 2.3008356545961,
      "grad_norm": 1.3761014938354492,
      "learning_rate": 0.0007699442896935933,
      "loss": 2.4421,
      "step": 8260
    },
    {
      "epoch": 2.3036211699164344,
      "grad_norm": 1.771228551864624,
      "learning_rate": 0.00076966573816156,
      "loss": 2.2709,
      "step": 8270
    },
    {
      "epoch": 2.3064066852367686,
      "grad_norm": 1.1901108026504517,
      "learning_rate": 0.0007693871866295265,
      "loss": 2.522,
      "step": 8280
    },
    {
      "epoch": 2.309192200557103,
      "grad_norm": 1.4808087348937988,
      "learning_rate": 0.000769108635097493,
      "loss": 2.3769,
      "step": 8290
    },
    {
      "epoch": 2.3119777158774375,
      "grad_norm": 1.254188060760498,
      "learning_rate": 0.0007688300835654595,
      "loss": 2.2286,
      "step": 8300
    },
    {
      "epoch": 2.3147632311977717,
      "grad_norm": 1.6605315208435059,
      "learning_rate": 0.0007685515320334262,
      "loss": 2.6407,
      "step": 8310
    },
    {
      "epoch": 2.317548746518106,
      "grad_norm": 1.839382529258728,
      "learning_rate": 0.0007682729805013928,
      "loss": 2.3126,
      "step": 8320
    },
    {
      "epoch": 2.32033426183844,
      "grad_norm": 1.3233692646026611,
      "learning_rate": 0.0007679944289693593,
      "loss": 2.5444,
      "step": 8330
    },
    {
      "epoch": 2.3231197771587744,
      "grad_norm": 1.8215296268463135,
      "learning_rate": 0.000767715877437326,
      "loss": 2.2432,
      "step": 8340
    },
    {
      "epoch": 2.3259052924791086,
      "grad_norm": 1.310067892074585,
      "learning_rate": 0.0007674373259052925,
      "loss": 2.3925,
      "step": 8350
    },
    {
      "epoch": 2.328690807799443,
      "grad_norm": 1.6789500713348389,
      "learning_rate": 0.0007671587743732591,
      "loss": 2.5775,
      "step": 8360
    },
    {
      "epoch": 2.331476323119777,
      "grad_norm": 1.404484748840332,
      "learning_rate": 0.0007668802228412256,
      "loss": 2.2987,
      "step": 8370
    },
    {
      "epoch": 2.3342618384401113,
      "grad_norm": 1.730255365371704,
      "learning_rate": 0.0007666016713091923,
      "loss": 2.3102,
      "step": 8380
    },
    {
      "epoch": 2.337047353760446,
      "grad_norm": 1.3826320171356201,
      "learning_rate": 0.0007663231197771587,
      "loss": 2.5032,
      "step": 8390
    },
    {
      "epoch": 2.33983286908078,
      "grad_norm": 1.275206446647644,
      "learning_rate": 0.0007660445682451253,
      "loss": 2.2844,
      "step": 8400
    },
    {
      "epoch": 2.3426183844011144,
      "grad_norm": 1.6573362350463867,
      "learning_rate": 0.000765766016713092,
      "loss": 2.395,
      "step": 8410
    },
    {
      "epoch": 2.3454038997214486,
      "grad_norm": 1.268288254737854,
      "learning_rate": 0.0007654874651810585,
      "loss": 2.1946,
      "step": 8420
    },
    {
      "epoch": 2.348189415041783,
      "grad_norm": 1.3500971794128418,
      "learning_rate": 0.0007652089136490251,
      "loss": 2.3701,
      "step": 8430
    },
    {
      "epoch": 2.350974930362117,
      "grad_norm": 1.5014503002166748,
      "learning_rate": 0.0007649303621169916,
      "loss": 2.5947,
      "step": 8440
    },
    {
      "epoch": 2.3537604456824512,
      "grad_norm": 1.6004843711853027,
      "learning_rate": 0.0007646518105849583,
      "loss": 2.4138,
      "step": 8450
    },
    {
      "epoch": 2.3565459610027855,
      "grad_norm": 1.521604299545288,
      "learning_rate": 0.0007643732590529248,
      "loss": 2.5395,
      "step": 8460
    },
    {
      "epoch": 2.3593314763231197,
      "grad_norm": 1.46548330783844,
      "learning_rate": 0.0007640947075208914,
      "loss": 2.3222,
      "step": 8470
    },
    {
      "epoch": 2.362116991643454,
      "grad_norm": 1.5567688941955566,
      "learning_rate": 0.000763816155988858,
      "loss": 2.5098,
      "step": 8480
    },
    {
      "epoch": 2.364902506963788,
      "grad_norm": 1.2229642868041992,
      "learning_rate": 0.0007635376044568246,
      "loss": 2.3272,
      "step": 8490
    },
    {
      "epoch": 2.3676880222841223,
      "grad_norm": 1.6049498319625854,
      "learning_rate": 0.0007632590529247911,
      "loss": 2.4465,
      "step": 8500
    },
    {
      "epoch": 2.370473537604457,
      "grad_norm": 1.5083626508712769,
      "learning_rate": 0.0007629805013927576,
      "loss": 2.2483,
      "step": 8510
    },
    {
      "epoch": 2.3732590529247912,
      "grad_norm": 1.663628101348877,
      "learning_rate": 0.0007627019498607243,
      "loss": 2.5807,
      "step": 8520
    },
    {
      "epoch": 2.3760445682451254,
      "grad_norm": 1.3287395238876343,
      "learning_rate": 0.0007624233983286908,
      "loss": 2.3119,
      "step": 8530
    },
    {
      "epoch": 2.3788300835654597,
      "grad_norm": 1.2165738344192505,
      "learning_rate": 0.0007621448467966574,
      "loss": 2.4187,
      "step": 8540
    },
    {
      "epoch": 2.381615598885794,
      "grad_norm": 1.328127384185791,
      "learning_rate": 0.0007618662952646239,
      "loss": 2.2401,
      "step": 8550
    },
    {
      "epoch": 2.384401114206128,
      "grad_norm": 1.3685859441757202,
      "learning_rate": 0.0007615877437325906,
      "loss": 2.2992,
      "step": 8560
    },
    {
      "epoch": 2.3871866295264623,
      "grad_norm": 1.7586675882339478,
      "learning_rate": 0.0007613091922005572,
      "loss": 2.4935,
      "step": 8570
    },
    {
      "epoch": 2.3899721448467965,
      "grad_norm": 1.77312171459198,
      "learning_rate": 0.0007610306406685237,
      "loss": 2.3331,
      "step": 8580
    },
    {
      "epoch": 2.3927576601671308,
      "grad_norm": 1.745690941810608,
      "learning_rate": 0.0007607520891364902,
      "loss": 2.4201,
      "step": 8590
    },
    {
      "epoch": 2.3955431754874654,
      "grad_norm": 1.9310635328292847,
      "learning_rate": 0.0007604735376044568,
      "loss": 2.4073,
      "step": 8600
    },
    {
      "epoch": 2.3983286908077996,
      "grad_norm": 1.5861876010894775,
      "learning_rate": 0.0007601949860724234,
      "loss": 2.4509,
      "step": 8610
    },
    {
      "epoch": 2.401114206128134,
      "grad_norm": 1.7632277011871338,
      "learning_rate": 0.0007599164345403899,
      "loss": 2.4396,
      "step": 8620
    },
    {
      "epoch": 2.403899721448468,
      "grad_norm": 2.0569941997528076,
      "learning_rate": 0.0007596378830083566,
      "loss": 2.3138,
      "step": 8630
    },
    {
      "epoch": 2.4066852367688023,
      "grad_norm": 1.3228648900985718,
      "learning_rate": 0.0007593593314763232,
      "loss": 2.4611,
      "step": 8640
    },
    {
      "epoch": 2.4094707520891365,
      "grad_norm": 1.9975626468658447,
      "learning_rate": 0.0007590807799442897,
      "loss": 2.4817,
      "step": 8650
    },
    {
      "epoch": 2.4122562674094707,
      "grad_norm": 1.6008007526397705,
      "learning_rate": 0.0007588022284122563,
      "loss": 2.4914,
      "step": 8660
    },
    {
      "epoch": 2.415041782729805,
      "grad_norm": 1.741327166557312,
      "learning_rate": 0.0007585236768802229,
      "loss": 2.4711,
      "step": 8670
    },
    {
      "epoch": 2.417827298050139,
      "grad_norm": 1.9052764177322388,
      "learning_rate": 0.0007582451253481895,
      "loss": 2.3198,
      "step": 8680
    },
    {
      "epoch": 2.4206128133704734,
      "grad_norm": 1.7543716430664062,
      "learning_rate": 0.0007579665738161559,
      "loss": 2.3139,
      "step": 8690
    },
    {
      "epoch": 2.4233983286908076,
      "grad_norm": 1.892686367034912,
      "learning_rate": 0.0007576880222841226,
      "loss": 2.2322,
      "step": 8700
    },
    {
      "epoch": 2.426183844011142,
      "grad_norm": 1.0162283182144165,
      "learning_rate": 0.0007574094707520891,
      "loss": 2.2933,
      "step": 8710
    },
    {
      "epoch": 2.4289693593314765,
      "grad_norm": 1.5183748006820679,
      "learning_rate": 0.0007571309192200557,
      "loss": 2.4243,
      "step": 8720
    },
    {
      "epoch": 2.4317548746518107,
      "grad_norm": 1.5056129693984985,
      "learning_rate": 0.0007568523676880223,
      "loss": 2.5281,
      "step": 8730
    },
    {
      "epoch": 2.434540389972145,
      "grad_norm": 1.4076117277145386,
      "learning_rate": 0.0007565738161559889,
      "loss": 2.3356,
      "step": 8740
    },
    {
      "epoch": 2.437325905292479,
      "grad_norm": 1.3695964813232422,
      "learning_rate": 0.0007562952646239555,
      "loss": 2.3236,
      "step": 8750
    },
    {
      "epoch": 2.4401114206128134,
      "grad_norm": 1.3994873762130737,
      "learning_rate": 0.000756016713091922,
      "loss": 2.3551,
      "step": 8760
    },
    {
      "epoch": 2.4428969359331476,
      "grad_norm": 1.4615823030471802,
      "learning_rate": 0.0007557381615598886,
      "loss": 2.3718,
      "step": 8770
    },
    {
      "epoch": 2.445682451253482,
      "grad_norm": 1.7600350379943848,
      "learning_rate": 0.0007554596100278552,
      "loss": 2.3096,
      "step": 8780
    },
    {
      "epoch": 2.448467966573816,
      "grad_norm": 1.1486798524856567,
      "learning_rate": 0.0007551810584958217,
      "loss": 2.4262,
      "step": 8790
    },
    {
      "epoch": 2.4512534818941503,
      "grad_norm": 1.6435338258743286,
      "learning_rate": 0.0007549025069637883,
      "loss": 2.5572,
      "step": 8800
    },
    {
      "epoch": 2.4540389972144845,
      "grad_norm": 1.302655816078186,
      "learning_rate": 0.0007546239554317549,
      "loss": 2.3475,
      "step": 8810
    },
    {
      "epoch": 2.456824512534819,
      "grad_norm": 1.8435481786727905,
      "learning_rate": 0.0007543454038997215,
      "loss": 2.5081,
      "step": 8820
    },
    {
      "epoch": 2.4596100278551534,
      "grad_norm": 1.5797711610794067,
      "learning_rate": 0.000754066852367688,
      "loss": 2.4678,
      "step": 8830
    },
    {
      "epoch": 2.4623955431754876,
      "grad_norm": 1.4038817882537842,
      "learning_rate": 0.0007537883008356546,
      "loss": 2.3829,
      "step": 8840
    },
    {
      "epoch": 2.465181058495822,
      "grad_norm": 1.6948305368423462,
      "learning_rate": 0.0007535097493036212,
      "loss": 2.4319,
      "step": 8850
    },
    {
      "epoch": 2.467966573816156,
      "grad_norm": 1.236025333404541,
      "learning_rate": 0.0007532311977715878,
      "loss": 2.4236,
      "step": 8860
    },
    {
      "epoch": 2.4707520891364902,
      "grad_norm": 1.272585391998291,
      "learning_rate": 0.0007529526462395543,
      "loss": 2.4497,
      "step": 8870
    },
    {
      "epoch": 2.4735376044568245,
      "grad_norm": 1.453408122062683,
      "learning_rate": 0.000752674094707521,
      "loss": 2.6332,
      "step": 8880
    },
    {
      "epoch": 2.4763231197771587,
      "grad_norm": 1.4913842678070068,
      "learning_rate": 0.0007523955431754875,
      "loss": 2.3321,
      "step": 8890
    },
    {
      "epoch": 2.479108635097493,
      "grad_norm": 1.6544443368911743,
      "learning_rate": 0.000752116991643454,
      "loss": 2.2986,
      "step": 8900
    },
    {
      "epoch": 2.481894150417827,
      "grad_norm": 1.5955597162246704,
      "learning_rate": 0.0007518384401114206,
      "loss": 2.277,
      "step": 8910
    },
    {
      "epoch": 2.4846796657381613,
      "grad_norm": 2.104308843612671,
      "learning_rate": 0.0007515598885793872,
      "loss": 2.3734,
      "step": 8920
    },
    {
      "epoch": 2.487465181058496,
      "grad_norm": 1.42264986038208,
      "learning_rate": 0.0007512813370473538,
      "loss": 2.302,
      "step": 8930
    },
    {
      "epoch": 2.4902506963788302,
      "grad_norm": 1.3063336610794067,
      "learning_rate": 0.0007510027855153203,
      "loss": 2.3423,
      "step": 8940
    },
    {
      "epoch": 2.4930362116991645,
      "grad_norm": 1.7656183242797852,
      "learning_rate": 0.0007507242339832869,
      "loss": 2.3422,
      "step": 8950
    },
    {
      "epoch": 2.4958217270194987,
      "grad_norm": 1.280914306640625,
      "learning_rate": 0.0007504456824512536,
      "loss": 2.1851,
      "step": 8960
    },
    {
      "epoch": 2.498607242339833,
      "grad_norm": 1.2560691833496094,
      "learning_rate": 0.0007501671309192201,
      "loss": 2.3981,
      "step": 8970
    },
    {
      "epoch": 2.501392757660167,
      "grad_norm": 1.422998309135437,
      "learning_rate": 0.0007498885793871867,
      "loss": 2.2511,
      "step": 8980
    },
    {
      "epoch": 2.5041782729805013,
      "grad_norm": 1.6117914915084839,
      "learning_rate": 0.0007496100278551532,
      "loss": 2.3284,
      "step": 8990
    },
    {
      "epoch": 2.5069637883008355,
      "grad_norm": 1.8260858058929443,
      "learning_rate": 0.0007493314763231198,
      "loss": 2.3727,
      "step": 9000
    },
    {
      "epoch": 2.5097493036211698,
      "grad_norm": 1.4425089359283447,
      "learning_rate": 0.0007490529247910863,
      "loss": 2.3887,
      "step": 9010
    },
    {
      "epoch": 2.5125348189415044,
      "grad_norm": 1.754693627357483,
      "learning_rate": 0.0007487743732590529,
      "loss": 2.3063,
      "step": 9020
    },
    {
      "epoch": 2.5153203342618387,
      "grad_norm": 1.510303258895874,
      "learning_rate": 0.0007484958217270195,
      "loss": 2.4057,
      "step": 9030
    },
    {
      "epoch": 2.518105849582173,
      "grad_norm": 1.0992568731307983,
      "learning_rate": 0.0007482172701949861,
      "loss": 2.3752,
      "step": 9040
    },
    {
      "epoch": 2.520891364902507,
      "grad_norm": 1.567515254020691,
      "learning_rate": 0.0007479387186629527,
      "loss": 2.3108,
      "step": 9050
    },
    {
      "epoch": 2.5236768802228413,
      "grad_norm": 1.0926311016082764,
      "learning_rate": 0.0007476601671309193,
      "loss": 2.3984,
      "step": 9060
    },
    {
      "epoch": 2.5264623955431755,
      "grad_norm": 1.2758294343948364,
      "learning_rate": 0.0007473816155988859,
      "loss": 2.1368,
      "step": 9070
    },
    {
      "epoch": 2.5292479108635098,
      "grad_norm": 1.9625256061553955,
      "learning_rate": 0.0007471030640668523,
      "loss": 2.3809,
      "step": 9080
    },
    {
      "epoch": 2.532033426183844,
      "grad_norm": 1.6947535276412964,
      "learning_rate": 0.0007468245125348189,
      "loss": 2.4348,
      "step": 9090
    },
    {
      "epoch": 2.534818941504178,
      "grad_norm": 1.000897765159607,
      "learning_rate": 0.0007465459610027855,
      "loss": 2.204,
      "step": 9100
    },
    {
      "epoch": 2.5376044568245124,
      "grad_norm": 2.2054758071899414,
      "learning_rate": 0.0007462674094707521,
      "loss": 2.2544,
      "step": 9110
    },
    {
      "epoch": 2.5403899721448466,
      "grad_norm": 1.5077131986618042,
      "learning_rate": 0.0007459888579387186,
      "loss": 2.2347,
      "step": 9120
    },
    {
      "epoch": 2.543175487465181,
      "grad_norm": 1.2393014430999756,
      "learning_rate": 0.0007457103064066852,
      "loss": 2.3421,
      "step": 9130
    },
    {
      "epoch": 2.545961002785515,
      "grad_norm": 1.5157214403152466,
      "learning_rate": 0.0007454317548746519,
      "loss": 2.4486,
      "step": 9140
    },
    {
      "epoch": 2.5487465181058497,
      "grad_norm": 1.2835371494293213,
      "learning_rate": 0.0007451532033426184,
      "loss": 2.3516,
      "step": 9150
    },
    {
      "epoch": 2.551532033426184,
      "grad_norm": 1.6130692958831787,
      "learning_rate": 0.000744874651810585,
      "loss": 2.0606,
      "step": 9160
    },
    {
      "epoch": 2.554317548746518,
      "grad_norm": 1.13271963596344,
      "learning_rate": 0.0007445961002785516,
      "loss": 2.4617,
      "step": 9170
    },
    {
      "epoch": 2.5571030640668524,
      "grad_norm": 2.118086099624634,
      "learning_rate": 0.0007443175487465182,
      "loss": 2.3977,
      "step": 9180
    },
    {
      "epoch": 2.5598885793871866,
      "grad_norm": 1.9061678647994995,
      "learning_rate": 0.0007440389972144846,
      "loss": 2.4022,
      "step": 9190
    },
    {
      "epoch": 2.562674094707521,
      "grad_norm": 1.1023249626159668,
      "learning_rate": 0.0007437604456824512,
      "loss": 2.3464,
      "step": 9200
    },
    {
      "epoch": 2.565459610027855,
      "grad_norm": 1.4218262434005737,
      "learning_rate": 0.0007434818941504179,
      "loss": 2.2384,
      "step": 9210
    },
    {
      "epoch": 2.5682451253481893,
      "grad_norm": 1.786250352859497,
      "learning_rate": 0.0007432033426183844,
      "loss": 2.2878,
      "step": 9220
    },
    {
      "epoch": 2.571030640668524,
      "grad_norm": 1.835086703300476,
      "learning_rate": 0.000742924791086351,
      "loss": 2.4375,
      "step": 9230
    },
    {
      "epoch": 2.573816155988858,
      "grad_norm": 1.5413856506347656,
      "learning_rate": 0.0007426462395543176,
      "loss": 2.1829,
      "step": 9240
    },
    {
      "epoch": 2.5766016713091924,
      "grad_norm": 1.4222835302352905,
      "learning_rate": 0.0007423676880222842,
      "loss": 2.302,
      "step": 9250
    },
    {
      "epoch": 2.5793871866295266,
      "grad_norm": 1.404261589050293,
      "learning_rate": 0.0007420891364902507,
      "loss": 2.3664,
      "step": 9260
    },
    {
      "epoch": 2.582172701949861,
      "grad_norm": 2.2602107524871826,
      "learning_rate": 0.0007418105849582173,
      "loss": 2.2242,
      "step": 9270
    },
    {
      "epoch": 2.584958217270195,
      "grad_norm": 1.499386191368103,
      "learning_rate": 0.0007415320334261838,
      "loss": 2.5243,
      "step": 9280
    },
    {
      "epoch": 2.5877437325905293,
      "grad_norm": 1.4477266073226929,
      "learning_rate": 0.0007412534818941504,
      "loss": 2.5919,
      "step": 9290
    },
    {
      "epoch": 2.5905292479108635,
      "grad_norm": 1.2188270092010498,
      "learning_rate": 0.000740974930362117,
      "loss": 2.3399,
      "step": 9300
    },
    {
      "epoch": 2.5933147632311977,
      "grad_norm": 1.7967960834503174,
      "learning_rate": 0.0007406963788300835,
      "loss": 2.4233,
      "step": 9310
    },
    {
      "epoch": 2.596100278551532,
      "grad_norm": 1.4883873462677002,
      "learning_rate": 0.0007404178272980502,
      "loss": 2.4066,
      "step": 9320
    },
    {
      "epoch": 2.598885793871866,
      "grad_norm": 1.2778873443603516,
      "learning_rate": 0.0007401392757660167,
      "loss": 2.1362,
      "step": 9330
    },
    {
      "epoch": 2.6016713091922004,
      "grad_norm": 1.5886642932891846,
      "learning_rate": 0.0007398607242339833,
      "loss": 2.4824,
      "step": 9340
    },
    {
      "epoch": 2.6044568245125346,
      "grad_norm": 1.7356152534484863,
      "learning_rate": 0.0007395821727019499,
      "loss": 2.3923,
      "step": 9350
    },
    {
      "epoch": 2.6072423398328692,
      "grad_norm": 1.5503350496292114,
      "learning_rate": 0.0007393036211699165,
      "loss": 2.1582,
      "step": 9360
    },
    {
      "epoch": 2.6100278551532035,
      "grad_norm": 1.5227775573730469,
      "learning_rate": 0.0007390250696378831,
      "loss": 2.5439,
      "step": 9370
    },
    {
      "epoch": 2.6128133704735377,
      "grad_norm": 1.8742660284042358,
      "learning_rate": 0.0007387465181058495,
      "loss": 2.3678,
      "step": 9380
    },
    {
      "epoch": 2.615598885793872,
      "grad_norm": 1.3258079290390015,
      "learning_rate": 0.0007384679665738162,
      "loss": 2.3986,
      "step": 9390
    },
    {
      "epoch": 2.618384401114206,
      "grad_norm": 1.4580235481262207,
      "learning_rate": 0.0007381894150417827,
      "loss": 2.2085,
      "step": 9400
    },
    {
      "epoch": 2.6211699164345403,
      "grad_norm": 1.4571881294250488,
      "learning_rate": 0.0007379108635097493,
      "loss": 2.571,
      "step": 9410
    },
    {
      "epoch": 2.6239554317548746,
      "grad_norm": 1.6174637079238892,
      "learning_rate": 0.0007376323119777159,
      "loss": 2.4898,
      "step": 9420
    },
    {
      "epoch": 2.6267409470752088,
      "grad_norm": 1.4749284982681274,
      "learning_rate": 0.0007373537604456825,
      "loss": 2.4214,
      "step": 9430
    },
    {
      "epoch": 2.6295264623955434,
      "grad_norm": 1.2535912990570068,
      "learning_rate": 0.000737075208913649,
      "loss": 2.4901,
      "step": 9440
    },
    {
      "epoch": 2.6323119777158777,
      "grad_norm": 1.965847373008728,
      "learning_rate": 0.0007367966573816156,
      "loss": 2.2778,
      "step": 9450
    },
    {
      "epoch": 2.635097493036212,
      "grad_norm": 1.5091028213500977,
      "learning_rate": 0.0007365181058495823,
      "loss": 2.3805,
      "step": 9460
    },
    {
      "epoch": 2.637883008356546,
      "grad_norm": 1.281112551689148,
      "learning_rate": 0.0007362395543175488,
      "loss": 2.1511,
      "step": 9470
    },
    {
      "epoch": 2.6406685236768803,
      "grad_norm": 4.8245134353637695,
      "learning_rate": 0.0007359610027855153,
      "loss": 2.493,
      "step": 9480
    },
    {
      "epoch": 2.6434540389972145,
      "grad_norm": 1.7839995622634888,
      "learning_rate": 0.0007356824512534818,
      "loss": 2.4324,
      "step": 9490
    },
    {
      "epoch": 2.6462395543175488,
      "grad_norm": 1.2064627408981323,
      "learning_rate": 0.0007354038997214485,
      "loss": 2.5242,
      "step": 9500
    },
    {
      "epoch": 2.649025069637883,
      "grad_norm": 1.0563980340957642,
      "learning_rate": 0.000735125348189415,
      "loss": 2.2859,
      "step": 9510
    },
    {
      "epoch": 2.651810584958217,
      "grad_norm": 1.7674334049224854,
      "learning_rate": 0.0007348467966573816,
      "loss": 2.268,
      "step": 9520
    },
    {
      "epoch": 2.6545961002785514,
      "grad_norm": 1.260494589805603,
      "learning_rate": 0.0007345682451253483,
      "loss": 2.4301,
      "step": 9530
    },
    {
      "epoch": 2.6573816155988856,
      "grad_norm": 1.3600847721099854,
      "learning_rate": 0.0007342896935933148,
      "loss": 2.3854,
      "step": 9540
    },
    {
      "epoch": 2.66016713091922,
      "grad_norm": 1.426769495010376,
      "learning_rate": 0.0007340111420612814,
      "loss": 2.3697,
      "step": 9550
    },
    {
      "epoch": 2.662952646239554,
      "grad_norm": 1.6216932535171509,
      "learning_rate": 0.0007337325905292479,
      "loss": 2.3451,
      "step": 9560
    },
    {
      "epoch": 2.6657381615598887,
      "grad_norm": 1.2822633981704712,
      "learning_rate": 0.0007334540389972146,
      "loss": 2.3167,
      "step": 9570
    },
    {
      "epoch": 2.668523676880223,
      "grad_norm": 1.8307582139968872,
      "learning_rate": 0.000733175487465181,
      "loss": 2.3264,
      "step": 9580
    },
    {
      "epoch": 2.671309192200557,
      "grad_norm": 1.3375016450881958,
      "learning_rate": 0.0007328969359331476,
      "loss": 2.4371,
      "step": 9590
    },
    {
      "epoch": 2.6740947075208914,
      "grad_norm": 1.2711867094039917,
      "learning_rate": 0.0007326183844011142,
      "loss": 2.2205,
      "step": 9600
    },
    {
      "epoch": 2.6768802228412256,
      "grad_norm": 1.2141865491867065,
      "learning_rate": 0.0007323398328690808,
      "loss": 2.2001,
      "step": 9610
    },
    {
      "epoch": 2.67966573816156,
      "grad_norm": 1.5122284889221191,
      "learning_rate": 0.0007320612813370474,
      "loss": 2.2969,
      "step": 9620
    },
    {
      "epoch": 2.682451253481894,
      "grad_norm": 2.6401610374450684,
      "learning_rate": 0.0007317827298050139,
      "loss": 2.4661,
      "step": 9630
    },
    {
      "epoch": 2.6852367688022283,
      "grad_norm": 1.8751939535140991,
      "learning_rate": 0.0007315041782729806,
      "loss": 2.3316,
      "step": 9640
    },
    {
      "epoch": 2.688022284122563,
      "grad_norm": 1.4967130422592163,
      "learning_rate": 0.0007312256267409471,
      "loss": 2.4941,
      "step": 9650
    },
    {
      "epoch": 2.690807799442897,
      "grad_norm": 1.335225224494934,
      "learning_rate": 0.0007309470752089137,
      "loss": 2.4261,
      "step": 9660
    },
    {
      "epoch": 2.6935933147632314,
      "grad_norm": 1.2485532760620117,
      "learning_rate": 0.0007306685236768801,
      "loss": 2.4442,
      "step": 9670
    },
    {
      "epoch": 2.6963788300835656,
      "grad_norm": 1.6497128009796143,
      "learning_rate": 0.0007303899721448468,
      "loss": 2.4129,
      "step": 9680
    },
    {
      "epoch": 2.6991643454039,
      "grad_norm": 1.7631028890609741,
      "learning_rate": 0.0007301114206128134,
      "loss": 2.3865,
      "step": 9690
    },
    {
      "epoch": 2.701949860724234,
      "grad_norm": 1.644140601158142,
      "learning_rate": 0.0007298328690807799,
      "loss": 2.544,
      "step": 9700
    },
    {
      "epoch": 2.7047353760445683,
      "grad_norm": 2.6560447216033936,
      "learning_rate": 0.0007295543175487466,
      "loss": 2.3979,
      "step": 9710
    },
    {
      "epoch": 2.7075208913649025,
      "grad_norm": 1.7017195224761963,
      "learning_rate": 0.0007292757660167131,
      "loss": 2.3842,
      "step": 9720
    },
    {
      "epoch": 2.7103064066852367,
      "grad_norm": 1.372047781944275,
      "learning_rate": 0.0007289972144846797,
      "loss": 2.4092,
      "step": 9730
    },
    {
      "epoch": 2.713091922005571,
      "grad_norm": 1.5355594158172607,
      "learning_rate": 0.0007287186629526462,
      "loss": 2.2904,
      "step": 9740
    },
    {
      "epoch": 2.715877437325905,
      "grad_norm": 1.3777923583984375,
      "learning_rate": 0.0007284401114206129,
      "loss": 2.2285,
      "step": 9750
    },
    {
      "epoch": 2.7186629526462394,
      "grad_norm": 1.4521377086639404,
      "learning_rate": 0.0007281615598885794,
      "loss": 2.161,
      "step": 9760
    },
    {
      "epoch": 2.7214484679665736,
      "grad_norm": 2.3214869499206543,
      "learning_rate": 0.000727883008356546,
      "loss": 2.3869,
      "step": 9770
    },
    {
      "epoch": 2.724233983286908,
      "grad_norm": 1.4482883214950562,
      "learning_rate": 0.0007276044568245126,
      "loss": 2.4867,
      "step": 9780
    },
    {
      "epoch": 2.7270194986072425,
      "grad_norm": 1.8453086614608765,
      "learning_rate": 0.0007273259052924791,
      "loss": 2.34,
      "step": 9790
    },
    {
      "epoch": 2.7298050139275767,
      "grad_norm": 1.4487839937210083,
      "learning_rate": 0.0007270473537604457,
      "loss": 2.4843,
      "step": 9800
    },
    {
      "epoch": 2.732590529247911,
      "grad_norm": 2.318772315979004,
      "learning_rate": 0.0007267688022284122,
      "loss": 2.3207,
      "step": 9810
    },
    {
      "epoch": 2.735376044568245,
      "grad_norm": 1.5893248319625854,
      "learning_rate": 0.0007264902506963789,
      "loss": 2.2341,
      "step": 9820
    },
    {
      "epoch": 2.7381615598885793,
      "grad_norm": 1.5030919313430786,
      "learning_rate": 0.0007262116991643454,
      "loss": 2.3592,
      "step": 9830
    },
    {
      "epoch": 2.7409470752089136,
      "grad_norm": 1.4353735446929932,
      "learning_rate": 0.000725933147632312,
      "loss": 2.1607,
      "step": 9840
    },
    {
      "epoch": 2.743732590529248,
      "grad_norm": 1.2753115892410278,
      "learning_rate": 0.0007256545961002786,
      "loss": 2.3831,
      "step": 9850
    },
    {
      "epoch": 2.7465181058495824,
      "grad_norm": 1.2040315866470337,
      "learning_rate": 0.0007253760445682452,
      "loss": 2.3342,
      "step": 9860
    },
    {
      "epoch": 2.7493036211699167,
      "grad_norm": 1.4856972694396973,
      "learning_rate": 0.0007250974930362118,
      "loss": 2.3431,
      "step": 9870
    },
    {
      "epoch": 2.752089136490251,
      "grad_norm": 1.5927624702453613,
      "learning_rate": 0.0007248189415041782,
      "loss": 2.4213,
      "step": 9880
    },
    {
      "epoch": 2.754874651810585,
      "grad_norm": 1.7821850776672363,
      "learning_rate": 0.0007245403899721449,
      "loss": 2.2002,
      "step": 9890
    },
    {
      "epoch": 2.7576601671309193,
      "grad_norm": 1.652445673942566,
      "learning_rate": 0.0007242618384401114,
      "loss": 2.3359,
      "step": 9900
    },
    {
      "epoch": 2.7604456824512535,
      "grad_norm": 1.1972167491912842,
      "learning_rate": 0.000723983286908078,
      "loss": 2.436,
      "step": 9910
    },
    {
      "epoch": 2.7632311977715878,
      "grad_norm": 2.3586790561676025,
      "learning_rate": 0.0007237047353760445,
      "loss": 2.5092,
      "step": 9920
    },
    {
      "epoch": 2.766016713091922,
      "grad_norm": 1.4827237129211426,
      "learning_rate": 0.0007234261838440112,
      "loss": 2.4478,
      "step": 9930
    },
    {
      "epoch": 2.768802228412256,
      "grad_norm": 1.2122726440429688,
      "learning_rate": 0.0007231476323119778,
      "loss": 2.3148,
      "step": 9940
    },
    {
      "epoch": 2.7715877437325904,
      "grad_norm": 2.0165340900421143,
      "learning_rate": 0.0007228690807799443,
      "loss": 2.3287,
      "step": 9950
    },
    {
      "epoch": 2.7743732590529246,
      "grad_norm": 1.5344327688217163,
      "learning_rate": 0.000722590529247911,
      "loss": 2.0243,
      "step": 9960
    },
    {
      "epoch": 2.777158774373259,
      "grad_norm": 1.5325839519500732,
      "learning_rate": 0.0007223119777158774,
      "loss": 2.3506,
      "step": 9970
    },
    {
      "epoch": 2.779944289693593,
      "grad_norm": 1.8846380710601807,
      "learning_rate": 0.000722033426183844,
      "loss": 2.4461,
      "step": 9980
    },
    {
      "epoch": 2.7827298050139273,
      "grad_norm": 1.3128776550292969,
      "learning_rate": 0.0007217548746518105,
      "loss": 2.4715,
      "step": 9990
    },
    {
      "epoch": 2.785515320334262,
      "grad_norm": 1.2555480003356934,
      "learning_rate": 0.0007214763231197772,
      "loss": 2.3854,
      "step": 10000
    },
    {
      "epoch": 2.788300835654596,
      "grad_norm": 1.5164058208465576,
      "learning_rate": 0.0007211977715877438,
      "loss": 2.5619,
      "step": 10010
    },
    {
      "epoch": 2.7910863509749304,
      "grad_norm": 1.3890964984893799,
      "learning_rate": 0.0007209192200557103,
      "loss": 2.4312,
      "step": 10020
    },
    {
      "epoch": 2.7938718662952646,
      "grad_norm": 1.2611199617385864,
      "learning_rate": 0.0007206406685236769,
      "loss": 2.5014,
      "step": 10030
    },
    {
      "epoch": 2.796657381615599,
      "grad_norm": 1.7456505298614502,
      "learning_rate": 0.0007203621169916435,
      "loss": 2.5124,
      "step": 10040
    },
    {
      "epoch": 2.799442896935933,
      "grad_norm": 1.5387988090515137,
      "learning_rate": 0.0007200835654596101,
      "loss": 2.4236,
      "step": 10050
    },
    {
      "epoch": 2.8022284122562673,
      "grad_norm": 1.4669042825698853,
      "learning_rate": 0.0007198050139275766,
      "loss": 2.5018,
      "step": 10060
    },
    {
      "epoch": 2.8050139275766015,
      "grad_norm": 1.7409650087356567,
      "learning_rate": 0.0007195264623955433,
      "loss": 2.2623,
      "step": 10070
    },
    {
      "epoch": 2.807799442896936,
      "grad_norm": 1.6924257278442383,
      "learning_rate": 0.0007192479108635097,
      "loss": 2.2161,
      "step": 10080
    },
    {
      "epoch": 2.8105849582172704,
      "grad_norm": 2.2600462436676025,
      "learning_rate": 0.0007189693593314763,
      "loss": 2.3922,
      "step": 10090
    },
    {
      "epoch": 2.8133704735376046,
      "grad_norm": 1.65397310256958,
      "learning_rate": 0.0007186908077994429,
      "loss": 2.5399,
      "step": 10100
    },
    {
      "epoch": 2.816155988857939,
      "grad_norm": 1.3951748609542847,
      "learning_rate": 0.0007184122562674095,
      "loss": 2.3733,
      "step": 10110
    },
    {
      "epoch": 2.818941504178273,
      "grad_norm": 1.5687103271484375,
      "learning_rate": 0.0007181337047353761,
      "loss": 2.2302,
      "step": 10120
    },
    {
      "epoch": 2.8217270194986073,
      "grad_norm": 1.528421401977539,
      "learning_rate": 0.0007178551532033426,
      "loss": 2.373,
      "step": 10130
    },
    {
      "epoch": 2.8245125348189415,
      "grad_norm": 2.35581374168396,
      "learning_rate": 0.0007175766016713092,
      "loss": 2.3902,
      "step": 10140
    },
    {
      "epoch": 2.8272980501392757,
      "grad_norm": 1.4649320840835571,
      "learning_rate": 0.0007172980501392758,
      "loss": 2.3377,
      "step": 10150
    },
    {
      "epoch": 2.83008356545961,
      "grad_norm": 1.5532139539718628,
      "learning_rate": 0.0007170194986072424,
      "loss": 2.5221,
      "step": 10160
    },
    {
      "epoch": 2.832869080779944,
      "grad_norm": 1.4698774814605713,
      "learning_rate": 0.0007167409470752089,
      "loss": 2.4526,
      "step": 10170
    },
    {
      "epoch": 2.8356545961002784,
      "grad_norm": 1.1876850128173828,
      "learning_rate": 0.0007164623955431755,
      "loss": 2.3712,
      "step": 10180
    },
    {
      "epoch": 2.8384401114206126,
      "grad_norm": 1.5860419273376465,
      "learning_rate": 0.0007161838440111421,
      "loss": 2.3783,
      "step": 10190
    },
    {
      "epoch": 2.841225626740947,
      "grad_norm": 1.5265393257141113,
      "learning_rate": 0.0007159052924791086,
      "loss": 2.442,
      "step": 10200
    },
    {
      "epoch": 2.8440111420612815,
      "grad_norm": 1.4003463983535767,
      "learning_rate": 0.0007156267409470752,
      "loss": 2.2678,
      "step": 10210
    },
    {
      "epoch": 2.8467966573816157,
      "grad_norm": 1.0564321279525757,
      "learning_rate": 0.0007153481894150418,
      "loss": 2.2999,
      "step": 10220
    },
    {
      "epoch": 2.84958217270195,
      "grad_norm": 1.0369230508804321,
      "learning_rate": 0.0007150696378830084,
      "loss": 2.1763,
      "step": 10230
    },
    {
      "epoch": 2.852367688022284,
      "grad_norm": 1.128893494606018,
      "learning_rate": 0.0007147910863509749,
      "loss": 2.3968,
      "step": 10240
    },
    {
      "epoch": 2.8551532033426184,
      "grad_norm": 1.3596998453140259,
      "learning_rate": 0.0007145125348189416,
      "loss": 2.5362,
      "step": 10250
    },
    {
      "epoch": 2.8579387186629526,
      "grad_norm": 1.2808290719985962,
      "learning_rate": 0.0007142339832869082,
      "loss": 2.4822,
      "step": 10260
    },
    {
      "epoch": 2.860724233983287,
      "grad_norm": 1.39628267288208,
      "learning_rate": 0.0007139554317548746,
      "loss": 2.455,
      "step": 10270
    },
    {
      "epoch": 2.863509749303621,
      "grad_norm": 1.1907012462615967,
      "learning_rate": 0.0007136768802228412,
      "loss": 2.3396,
      "step": 10280
    },
    {
      "epoch": 2.8662952646239557,
      "grad_norm": 1.4971489906311035,
      "learning_rate": 0.0007133983286908078,
      "loss": 2.4274,
      "step": 10290
    },
    {
      "epoch": 2.86908077994429,
      "grad_norm": 1.6533138751983643,
      "learning_rate": 0.0007131197771587744,
      "loss": 2.3369,
      "step": 10300
    },
    {
      "epoch": 2.871866295264624,
      "grad_norm": 1.5584869384765625,
      "learning_rate": 0.0007128412256267409,
      "loss": 2.564,
      "step": 10310
    },
    {
      "epoch": 2.8746518105849583,
      "grad_norm": 1.7869932651519775,
      "learning_rate": 0.0007125626740947075,
      "loss": 2.2799,
      "step": 10320
    },
    {
      "epoch": 2.8774373259052926,
      "grad_norm": 1.8050440549850464,
      "learning_rate": 0.0007122841225626742,
      "loss": 2.3077,
      "step": 10330
    },
    {
      "epoch": 2.8802228412256268,
      "grad_norm": 2.210038661956787,
      "learning_rate": 0.0007120055710306407,
      "loss": 2.2064,
      "step": 10340
    },
    {
      "epoch": 2.883008356545961,
      "grad_norm": 1.2588002681732178,
      "learning_rate": 0.0007117270194986073,
      "loss": 2.3325,
      "step": 10350
    },
    {
      "epoch": 2.885793871866295,
      "grad_norm": 1.5224748849868774,
      "learning_rate": 0.0007114484679665739,
      "loss": 2.3571,
      "step": 10360
    },
    {
      "epoch": 2.8885793871866294,
      "grad_norm": 1.6230480670928955,
      "learning_rate": 0.0007111699164345404,
      "loss": 2.4175,
      "step": 10370
    },
    {
      "epoch": 2.8913649025069637,
      "grad_norm": 1.398559331893921,
      "learning_rate": 0.0007108913649025069,
      "loss": 2.5467,
      "step": 10380
    },
    {
      "epoch": 2.894150417827298,
      "grad_norm": 1.5157849788665771,
      "learning_rate": 0.0007106128133704735,
      "loss": 2.4073,
      "step": 10390
    },
    {
      "epoch": 2.896935933147632,
      "grad_norm": 1.2005220651626587,
      "learning_rate": 0.0007103342618384401,
      "loss": 2.333,
      "step": 10400
    },
    {
      "epoch": 2.8997214484679663,
      "grad_norm": 1.5346934795379639,
      "learning_rate": 0.0007100557103064067,
      "loss": 2.6289,
      "step": 10410
    },
    {
      "epoch": 2.902506963788301,
      "grad_norm": 1.1573868989944458,
      "learning_rate": 0.0007097771587743733,
      "loss": 2.404,
      "step": 10420
    },
    {
      "epoch": 2.905292479108635,
      "grad_norm": 1.3441507816314697,
      "learning_rate": 0.0007094986072423399,
      "loss": 2.33,
      "step": 10430
    },
    {
      "epoch": 2.9080779944289694,
      "grad_norm": 1.6114044189453125,
      "learning_rate": 0.0007092200557103065,
      "loss": 2.4485,
      "step": 10440
    },
    {
      "epoch": 2.9108635097493036,
      "grad_norm": 1.222044825553894,
      "learning_rate": 0.000708941504178273,
      "loss": 2.2341,
      "step": 10450
    },
    {
      "epoch": 2.913649025069638,
      "grad_norm": 1.6689094305038452,
      "learning_rate": 0.0007086629526462395,
      "loss": 2.3241,
      "step": 10460
    },
    {
      "epoch": 2.916434540389972,
      "grad_norm": 1.7025141716003418,
      "learning_rate": 0.0007083844011142061,
      "loss": 2.2802,
      "step": 10470
    },
    {
      "epoch": 2.9192200557103063,
      "grad_norm": 1.2215194702148438,
      "learning_rate": 0.0007081058495821727,
      "loss": 2.3559,
      "step": 10480
    },
    {
      "epoch": 2.9220055710306405,
      "grad_norm": 1.4949733018875122,
      "learning_rate": 0.0007078272980501393,
      "loss": 2.1593,
      "step": 10490
    },
    {
      "epoch": 2.924791086350975,
      "grad_norm": 1.7429159879684448,
      "learning_rate": 0.0007075487465181058,
      "loss": 2.4311,
      "step": 10500
    },
    {
      "epoch": 2.9275766016713094,
      "grad_norm": 1.5084911584854126,
      "learning_rate": 0.0007072701949860725,
      "loss": 2.2895,
      "step": 10510
    },
    {
      "epoch": 2.9303621169916436,
      "grad_norm": 1.1829726696014404,
      "learning_rate": 0.000706991643454039,
      "loss": 2.4126,
      "step": 10520
    },
    {
      "epoch": 2.933147632311978,
      "grad_norm": 1.4109693765640259,
      "learning_rate": 0.0007067130919220056,
      "loss": 2.3621,
      "step": 10530
    },
    {
      "epoch": 2.935933147632312,
      "grad_norm": 1.179656982421875,
      "learning_rate": 0.0007064345403899722,
      "loss": 2.2006,
      "step": 10540
    },
    {
      "epoch": 2.9387186629526463,
      "grad_norm": 1.4152681827545166,
      "learning_rate": 0.0007061559888579388,
      "loss": 2.3634,
      "step": 10550
    },
    {
      "epoch": 2.9415041782729805,
      "grad_norm": 1.6609090566635132,
      "learning_rate": 0.0007058774373259052,
      "loss": 2.3809,
      "step": 10560
    },
    {
      "epoch": 2.9442896935933147,
      "grad_norm": 1.340968132019043,
      "learning_rate": 0.0007055988857938718,
      "loss": 2.2845,
      "step": 10570
    },
    {
      "epoch": 2.947075208913649,
      "grad_norm": 1.4176462888717651,
      "learning_rate": 0.0007053203342618385,
      "loss": 2.3365,
      "step": 10580
    },
    {
      "epoch": 2.949860724233983,
      "grad_norm": 1.4390552043914795,
      "learning_rate": 0.000705041782729805,
      "loss": 2.609,
      "step": 10590
    },
    {
      "epoch": 2.9526462395543174,
      "grad_norm": 1.77232027053833,
      "learning_rate": 0.0007047632311977716,
      "loss": 2.5674,
      "step": 10600
    },
    {
      "epoch": 2.9554317548746516,
      "grad_norm": 1.4358066320419312,
      "learning_rate": 0.0007044846796657382,
      "loss": 2.4449,
      "step": 10610
    },
    {
      "epoch": 2.958217270194986,
      "grad_norm": 1.2844557762145996,
      "learning_rate": 0.0007042061281337048,
      "loss": 2.3366,
      "step": 10620
    },
    {
      "epoch": 2.9610027855153205,
      "grad_norm": 1.2705165147781372,
      "learning_rate": 0.0007039275766016713,
      "loss": 2.4443,
      "step": 10630
    },
    {
      "epoch": 2.9637883008356547,
      "grad_norm": 2.136559247970581,
      "learning_rate": 0.0007036490250696379,
      "loss": 2.4516,
      "step": 10640
    },
    {
      "epoch": 2.966573816155989,
      "grad_norm": 1.29666268825531,
      "learning_rate": 0.0007033704735376045,
      "loss": 2.3224,
      "step": 10650
    },
    {
      "epoch": 2.969359331476323,
      "grad_norm": 1.3926939964294434,
      "learning_rate": 0.000703091922005571,
      "loss": 2.2861,
      "step": 10660
    },
    {
      "epoch": 2.9721448467966574,
      "grad_norm": 1.200011134147644,
      "learning_rate": 0.0007028133704735376,
      "loss": 2.2743,
      "step": 10670
    },
    {
      "epoch": 2.9749303621169916,
      "grad_norm": 1.6265289783477783,
      "learning_rate": 0.0007025348189415041,
      "loss": 2.1831,
      "step": 10680
    },
    {
      "epoch": 2.977715877437326,
      "grad_norm": 1.797888994216919,
      "learning_rate": 0.0007022562674094708,
      "loss": 2.5314,
      "step": 10690
    },
    {
      "epoch": 2.98050139275766,
      "grad_norm": 1.6562474966049194,
      "learning_rate": 0.0007019777158774373,
      "loss": 2.6171,
      "step": 10700
    },
    {
      "epoch": 2.9832869080779947,
      "grad_norm": 1.5822592973709106,
      "learning_rate": 0.0007016991643454039,
      "loss": 2.5577,
      "step": 10710
    },
    {
      "epoch": 2.986072423398329,
      "grad_norm": 2.1591668128967285,
      "learning_rate": 0.0007014206128133705,
      "loss": 2.3048,
      "step": 10720
    },
    {
      "epoch": 2.988857938718663,
      "grad_norm": 1.6046888828277588,
      "learning_rate": 0.0007011420612813371,
      "loss": 2.3905,
      "step": 10730
    },
    {
      "epoch": 2.9916434540389973,
      "grad_norm": 1.1690574884414673,
      "learning_rate": 0.0007008635097493037,
      "loss": 2.3925,
      "step": 10740
    },
    {
      "epoch": 2.9944289693593316,
      "grad_norm": 1.4467213153839111,
      "learning_rate": 0.0007005849582172702,
      "loss": 2.2171,
      "step": 10750
    },
    {
      "epoch": 2.997214484679666,
      "grad_norm": 1.5261850357055664,
      "learning_rate": 0.0007003064066852369,
      "loss": 2.3825,
      "step": 10760
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.6494224071502686,
      "learning_rate": 0.0007000278551532033,
      "loss": 2.346,
      "step": 10770
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.3105931282043457,
      "eval_runtime": 5.4738,
      "eval_samples_per_second": 583.139,
      "eval_steps_per_second": 72.892,
      "step": 10770
    },
    {
      "epoch": 3.002785515320334,
      "grad_norm": 1.2688031196594238,
      "learning_rate": 0.0006997493036211699,
      "loss": 2.2926,
      "step": 10780
    },
    {
      "epoch": 3.0055710306406684,
      "grad_norm": 1.4000431299209595,
      "learning_rate": 0.0006994707520891365,
      "loss": 2.2249,
      "step": 10790
    },
    {
      "epoch": 3.0083565459610027,
      "grad_norm": 1.4947805404663086,
      "learning_rate": 0.0006991922005571031,
      "loss": 2.2822,
      "step": 10800
    },
    {
      "epoch": 3.011142061281337,
      "grad_norm": 2.1221606731414795,
      "learning_rate": 0.0006989136490250696,
      "loss": 2.1356,
      "step": 10810
    },
    {
      "epoch": 3.013927576601671,
      "grad_norm": 1.3203879594802856,
      "learning_rate": 0.0006986350974930362,
      "loss": 2.3743,
      "step": 10820
    },
    {
      "epoch": 3.0167130919220058,
      "grad_norm": 1.450022578239441,
      "learning_rate": 0.0006983565459610029,
      "loss": 2.166,
      "step": 10830
    },
    {
      "epoch": 3.01949860724234,
      "grad_norm": 1.0834397077560425,
      "learning_rate": 0.0006980779944289694,
      "loss": 2.3391,
      "step": 10840
    },
    {
      "epoch": 3.022284122562674,
      "grad_norm": 1.8745936155319214,
      "learning_rate": 0.000697799442896936,
      "loss": 2.396,
      "step": 10850
    },
    {
      "epoch": 3.0250696378830084,
      "grad_norm": 1.6809451580047607,
      "learning_rate": 0.0006975208913649024,
      "loss": 2.2417,
      "step": 10860
    },
    {
      "epoch": 3.0278551532033426,
      "grad_norm": 1.4717133045196533,
      "learning_rate": 0.0006972423398328691,
      "loss": 2.3599,
      "step": 10870
    },
    {
      "epoch": 3.030640668523677,
      "grad_norm": 1.7581121921539307,
      "learning_rate": 0.0006969637883008356,
      "loss": 2.5094,
      "step": 10880
    },
    {
      "epoch": 3.033426183844011,
      "grad_norm": 1.9116239547729492,
      "learning_rate": 0.0006966852367688022,
      "loss": 2.3104,
      "step": 10890
    },
    {
      "epoch": 3.0362116991643453,
      "grad_norm": 1.6163233518600464,
      "learning_rate": 0.0006964066852367689,
      "loss": 2.2237,
      "step": 10900
    },
    {
      "epoch": 3.0389972144846795,
      "grad_norm": 1.7363035678863525,
      "learning_rate": 0.0006961281337047354,
      "loss": 2.1966,
      "step": 10910
    },
    {
      "epoch": 3.0417827298050137,
      "grad_norm": 1.5645662546157837,
      "learning_rate": 0.000695849582172702,
      "loss": 2.4803,
      "step": 10920
    },
    {
      "epoch": 3.0445682451253484,
      "grad_norm": 1.3129032850265503,
      "learning_rate": 0.0006955710306406685,
      "loss": 2.1947,
      "step": 10930
    },
    {
      "epoch": 3.0473537604456826,
      "grad_norm": 1.3775691986083984,
      "learning_rate": 0.0006952924791086352,
      "loss": 2.4495,
      "step": 10940
    },
    {
      "epoch": 3.050139275766017,
      "grad_norm": 1.3135610818862915,
      "learning_rate": 0.0006950139275766017,
      "loss": 2.2356,
      "step": 10950
    },
    {
      "epoch": 3.052924791086351,
      "grad_norm": 2.2335195541381836,
      "learning_rate": 0.0006947353760445682,
      "loss": 2.4622,
      "step": 10960
    },
    {
      "epoch": 3.0557103064066853,
      "grad_norm": 1.6862084865570068,
      "learning_rate": 0.0006944568245125348,
      "loss": 2.3204,
      "step": 10970
    },
    {
      "epoch": 3.0584958217270195,
      "grad_norm": 1.1621084213256836,
      "learning_rate": 0.0006941782729805014,
      "loss": 2.3997,
      "step": 10980
    },
    {
      "epoch": 3.0612813370473537,
      "grad_norm": 1.442674994468689,
      "learning_rate": 0.000693899721448468,
      "loss": 2.4078,
      "step": 10990
    },
    {
      "epoch": 3.064066852367688,
      "grad_norm": 1.1660877466201782,
      "learning_rate": 0.0006936211699164345,
      "loss": 2.2281,
      "step": 11000
    }
  ],
  "logging_steps": 10,
  "max_steps": 35900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1456987836874752.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
