{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.988857938718663,
  "eval_steps": 500,
  "global_step": 21500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002785515320334262,
      "grad_norm": 1.2777702808380127,
      "learning_rate": 0.00099974930362117,
      "loss": 4.3969,
      "step": 10
    },
    {
      "epoch": 0.005571030640668524,
      "grad_norm": 1.7650771141052246,
      "learning_rate": 0.0009994707520891365,
      "loss": 3.4993,
      "step": 20
    },
    {
      "epoch": 0.008356545961002786,
      "grad_norm": 1.121536374092102,
      "learning_rate": 0.0009991922005571031,
      "loss": 3.182,
      "step": 30
    },
    {
      "epoch": 0.011142061281337047,
      "grad_norm": 1.5466363430023193,
      "learning_rate": 0.0009989136490250697,
      "loss": 3.4433,
      "step": 40
    },
    {
      "epoch": 0.013927576601671309,
      "grad_norm": 1.3392200469970703,
      "learning_rate": 0.0009986350974930363,
      "loss": 3.2526,
      "step": 50
    },
    {
      "epoch": 0.016713091922005572,
      "grad_norm": 1.4003591537475586,
      "learning_rate": 0.0009983565459610027,
      "loss": 2.9297,
      "step": 60
    },
    {
      "epoch": 0.019498607242339833,
      "grad_norm": 1.1948171854019165,
      "learning_rate": 0.0009980779944289695,
      "loss": 3.2728,
      "step": 70
    },
    {
      "epoch": 0.022284122562674095,
      "grad_norm": 1.0925664901733398,
      "learning_rate": 0.0009977994428969359,
      "loss": 2.9489,
      "step": 80
    },
    {
      "epoch": 0.025069637883008356,
      "grad_norm": 1.0867161750793457,
      "learning_rate": 0.0009975208913649025,
      "loss": 2.882,
      "step": 90
    },
    {
      "epoch": 0.027855153203342618,
      "grad_norm": 1.4580721855163574,
      "learning_rate": 0.000997242339832869,
      "loss": 2.7389,
      "step": 100
    },
    {
      "epoch": 0.03064066852367688,
      "grad_norm": 1.4019684791564941,
      "learning_rate": 0.0009969637883008356,
      "loss": 2.9995,
      "step": 110
    },
    {
      "epoch": 0.033426183844011144,
      "grad_norm": 1.0926460027694702,
      "learning_rate": 0.0009966852367688022,
      "loss": 3.1596,
      "step": 120
    },
    {
      "epoch": 0.036211699164345405,
      "grad_norm": 1.4548900127410889,
      "learning_rate": 0.0009964066852367688,
      "loss": 2.9555,
      "step": 130
    },
    {
      "epoch": 0.03899721448467967,
      "grad_norm": 1.5399762392044067,
      "learning_rate": 0.0009961281337047354,
      "loss": 3.1365,
      "step": 140
    },
    {
      "epoch": 0.04178272980501393,
      "grad_norm": 1.2968024015426636,
      "learning_rate": 0.000995849582172702,
      "loss": 2.8939,
      "step": 150
    },
    {
      "epoch": 0.04456824512534819,
      "grad_norm": 1.696082353591919,
      "learning_rate": 0.0009955710306406686,
      "loss": 2.7374,
      "step": 160
    },
    {
      "epoch": 0.04735376044568245,
      "grad_norm": 1.30885648727417,
      "learning_rate": 0.000995292479108635,
      "loss": 2.8485,
      "step": 170
    },
    {
      "epoch": 0.05013927576601671,
      "grad_norm": 1.320600152015686,
      "learning_rate": 0.0009950139275766018,
      "loss": 2.7569,
      "step": 180
    },
    {
      "epoch": 0.052924791086350974,
      "grad_norm": 1.1281540393829346,
      "learning_rate": 0.0009947353760445684,
      "loss": 3.1554,
      "step": 190
    },
    {
      "epoch": 0.055710306406685235,
      "grad_norm": 1.2007540464401245,
      "learning_rate": 0.0009944568245125348,
      "loss": 2.8286,
      "step": 200
    },
    {
      "epoch": 0.0584958217270195,
      "grad_norm": 1.0410748720169067,
      "learning_rate": 0.0009941782729805016,
      "loss": 3.0681,
      "step": 210
    },
    {
      "epoch": 0.06128133704735376,
      "grad_norm": 1.1308516263961792,
      "learning_rate": 0.000993899721448468,
      "loss": 2.823,
      "step": 220
    },
    {
      "epoch": 0.06406685236768803,
      "grad_norm": 1.7718303203582764,
      "learning_rate": 0.0009936211699164345,
      "loss": 2.9794,
      "step": 230
    },
    {
      "epoch": 0.06685236768802229,
      "grad_norm": 1.3107385635375977,
      "learning_rate": 0.0009933426183844011,
      "loss": 2.86,
      "step": 240
    },
    {
      "epoch": 0.06963788300835655,
      "grad_norm": 1.2536839246749878,
      "learning_rate": 0.0009930640668523677,
      "loss": 2.9039,
      "step": 250
    },
    {
      "epoch": 0.07242339832869081,
      "grad_norm": 1.420477032661438,
      "learning_rate": 0.0009927855153203343,
      "loss": 3.126,
      "step": 260
    },
    {
      "epoch": 0.07520891364902507,
      "grad_norm": 1.6711881160736084,
      "learning_rate": 0.0009925069637883009,
      "loss": 2.8972,
      "step": 270
    },
    {
      "epoch": 0.07799442896935933,
      "grad_norm": 1.694988489151001,
      "learning_rate": 0.0009922284122562675,
      "loss": 2.7816,
      "step": 280
    },
    {
      "epoch": 0.0807799442896936,
      "grad_norm": 1.0983426570892334,
      "learning_rate": 0.000991949860724234,
      "loss": 2.8463,
      "step": 290
    },
    {
      "epoch": 0.08356545961002786,
      "grad_norm": 1.1392773389816284,
      "learning_rate": 0.0009916713091922007,
      "loss": 2.9405,
      "step": 300
    },
    {
      "epoch": 0.08635097493036212,
      "grad_norm": 1.1083364486694336,
      "learning_rate": 0.000991392757660167,
      "loss": 2.9489,
      "step": 310
    },
    {
      "epoch": 0.08913649025069638,
      "grad_norm": 1.0049762725830078,
      "learning_rate": 0.0009911142061281338,
      "loss": 2.7545,
      "step": 320
    },
    {
      "epoch": 0.09192200557103064,
      "grad_norm": 1.4989159107208252,
      "learning_rate": 0.0009908356545961002,
      "loss": 2.827,
      "step": 330
    },
    {
      "epoch": 0.0947075208913649,
      "grad_norm": 1.4460601806640625,
      "learning_rate": 0.0009905571030640668,
      "loss": 2.922,
      "step": 340
    },
    {
      "epoch": 0.09749303621169916,
      "grad_norm": 1.2647724151611328,
      "learning_rate": 0.0009902785515320334,
      "loss": 2.6402,
      "step": 350
    },
    {
      "epoch": 0.10027855153203342,
      "grad_norm": 1.137195348739624,
      "learning_rate": 0.00099,
      "loss": 2.8358,
      "step": 360
    },
    {
      "epoch": 0.10306406685236769,
      "grad_norm": 1.1965945959091187,
      "learning_rate": 0.0009897214484679666,
      "loss": 2.871,
      "step": 370
    },
    {
      "epoch": 0.10584958217270195,
      "grad_norm": 1.1259201765060425,
      "learning_rate": 0.0009894428969359332,
      "loss": 2.8081,
      "step": 380
    },
    {
      "epoch": 0.10863509749303621,
      "grad_norm": 2.0894253253936768,
      "learning_rate": 0.0009891643454038998,
      "loss": 2.8969,
      "step": 390
    },
    {
      "epoch": 0.11142061281337047,
      "grad_norm": 1.2286434173583984,
      "learning_rate": 0.0009888857938718664,
      "loss": 2.8072,
      "step": 400
    },
    {
      "epoch": 0.11420612813370473,
      "grad_norm": 1.0757712125778198,
      "learning_rate": 0.000988607242339833,
      "loss": 2.7003,
      "step": 410
    },
    {
      "epoch": 0.116991643454039,
      "grad_norm": 1.7179458141326904,
      "learning_rate": 0.0009883286908077993,
      "loss": 2.7788,
      "step": 420
    },
    {
      "epoch": 0.11977715877437325,
      "grad_norm": 1.3726903200149536,
      "learning_rate": 0.0009880501392757661,
      "loss": 2.5696,
      "step": 430
    },
    {
      "epoch": 0.12256267409470752,
      "grad_norm": 1.14989173412323,
      "learning_rate": 0.0009877715877437327,
      "loss": 2.7331,
      "step": 440
    },
    {
      "epoch": 0.12534818941504178,
      "grad_norm": 1.2878109216690063,
      "learning_rate": 0.000987493036211699,
      "loss": 2.624,
      "step": 450
    },
    {
      "epoch": 0.12813370473537605,
      "grad_norm": 1.1452126502990723,
      "learning_rate": 0.0009872144846796657,
      "loss": 2.6156,
      "step": 460
    },
    {
      "epoch": 0.1309192200557103,
      "grad_norm": 1.4346585273742676,
      "learning_rate": 0.0009869359331476323,
      "loss": 2.6586,
      "step": 470
    },
    {
      "epoch": 0.13370473537604458,
      "grad_norm": 1.4669582843780518,
      "learning_rate": 0.0009866573816155989,
      "loss": 2.7476,
      "step": 480
    },
    {
      "epoch": 0.13649025069637882,
      "grad_norm": 1.610460638999939,
      "learning_rate": 0.0009863788300835655,
      "loss": 2.5211,
      "step": 490
    },
    {
      "epoch": 0.1392757660167131,
      "grad_norm": 1.2787543535232544,
      "learning_rate": 0.000986100278551532,
      "loss": 2.6962,
      "step": 500
    },
    {
      "epoch": 0.14206128133704735,
      "grad_norm": 1.3625420331954956,
      "learning_rate": 0.0009858217270194986,
      "loss": 2.6626,
      "step": 510
    },
    {
      "epoch": 0.14484679665738162,
      "grad_norm": 1.6599339246749878,
      "learning_rate": 0.0009855431754874652,
      "loss": 2.9541,
      "step": 520
    },
    {
      "epoch": 0.14763231197771587,
      "grad_norm": 1.3805036544799805,
      "learning_rate": 0.0009852646239554318,
      "loss": 2.9058,
      "step": 530
    },
    {
      "epoch": 0.15041782729805014,
      "grad_norm": 1.1459840536117554,
      "learning_rate": 0.0009849860724233984,
      "loss": 2.4951,
      "step": 540
    },
    {
      "epoch": 0.1532033426183844,
      "grad_norm": 1.278351068496704,
      "learning_rate": 0.000984707520891365,
      "loss": 2.7882,
      "step": 550
    },
    {
      "epoch": 0.15598885793871867,
      "grad_norm": 1.062248945236206,
      "learning_rate": 0.0009844289693593314,
      "loss": 2.9183,
      "step": 560
    },
    {
      "epoch": 0.15877437325905291,
      "grad_norm": 1.8217440843582153,
      "learning_rate": 0.000984150417827298,
      "loss": 2.7318,
      "step": 570
    },
    {
      "epoch": 0.1615598885793872,
      "grad_norm": 1.2369275093078613,
      "learning_rate": 0.0009838718662952646,
      "loss": 2.7745,
      "step": 580
    },
    {
      "epoch": 0.16434540389972144,
      "grad_norm": 1.242400050163269,
      "learning_rate": 0.0009835933147632312,
      "loss": 2.7332,
      "step": 590
    },
    {
      "epoch": 0.1671309192200557,
      "grad_norm": 1.8199281692504883,
      "learning_rate": 0.0009833147632311977,
      "loss": 2.9746,
      "step": 600
    },
    {
      "epoch": 0.16991643454038996,
      "grad_norm": 1.4614086151123047,
      "learning_rate": 0.0009830362116991643,
      "loss": 2.6833,
      "step": 610
    },
    {
      "epoch": 0.17270194986072424,
      "grad_norm": 1.246872067451477,
      "learning_rate": 0.000982757660167131,
      "loss": 2.8365,
      "step": 620
    },
    {
      "epoch": 0.17548746518105848,
      "grad_norm": 1.0792508125305176,
      "learning_rate": 0.0009824791086350975,
      "loss": 2.795,
      "step": 630
    },
    {
      "epoch": 0.17827298050139276,
      "grad_norm": 1.401904821395874,
      "learning_rate": 0.0009822005571030641,
      "loss": 2.4939,
      "step": 640
    },
    {
      "epoch": 0.181058495821727,
      "grad_norm": 1.273613452911377,
      "learning_rate": 0.0009819220055710307,
      "loss": 2.7808,
      "step": 650
    },
    {
      "epoch": 0.18384401114206128,
      "grad_norm": 1.510461449623108,
      "learning_rate": 0.0009816434540389973,
      "loss": 2.606,
      "step": 660
    },
    {
      "epoch": 0.18662952646239556,
      "grad_norm": 1.4030647277832031,
      "learning_rate": 0.0009813649025069639,
      "loss": 2.7183,
      "step": 670
    },
    {
      "epoch": 0.1894150417827298,
      "grad_norm": 1.3902904987335205,
      "learning_rate": 0.0009810863509749305,
      "loss": 2.6436,
      "step": 680
    },
    {
      "epoch": 0.19220055710306408,
      "grad_norm": 1.282233715057373,
      "learning_rate": 0.000980807799442897,
      "loss": 2.7614,
      "step": 690
    },
    {
      "epoch": 0.19498607242339833,
      "grad_norm": 1.247700572013855,
      "learning_rate": 0.0009805292479108634,
      "loss": 2.7213,
      "step": 700
    },
    {
      "epoch": 0.1977715877437326,
      "grad_norm": 1.9423047304153442,
      "learning_rate": 0.00098025069637883,
      "loss": 2.6238,
      "step": 710
    },
    {
      "epoch": 0.20055710306406685,
      "grad_norm": 1.189522385597229,
      "learning_rate": 0.0009799721448467966,
      "loss": 2.7811,
      "step": 720
    },
    {
      "epoch": 0.20334261838440112,
      "grad_norm": 1.268925428390503,
      "learning_rate": 0.0009796935933147632,
      "loss": 2.5515,
      "step": 730
    },
    {
      "epoch": 0.20612813370473537,
      "grad_norm": 1.566765308380127,
      "learning_rate": 0.0009794150417827298,
      "loss": 2.8544,
      "step": 740
    },
    {
      "epoch": 0.20891364902506965,
      "grad_norm": 1.4222410917282104,
      "learning_rate": 0.0009791364902506964,
      "loss": 2.7452,
      "step": 750
    },
    {
      "epoch": 0.2116991643454039,
      "grad_norm": 1.7793817520141602,
      "learning_rate": 0.000978857938718663,
      "loss": 2.6258,
      "step": 760
    },
    {
      "epoch": 0.21448467966573817,
      "grad_norm": 1.2203317880630493,
      "learning_rate": 0.0009785793871866296,
      "loss": 2.6129,
      "step": 770
    },
    {
      "epoch": 0.21727019498607242,
      "grad_norm": 1.2296749353408813,
      "learning_rate": 0.0009783008356545962,
      "loss": 2.4147,
      "step": 780
    },
    {
      "epoch": 0.2200557103064067,
      "grad_norm": 2.012601137161255,
      "learning_rate": 0.0009780222841225628,
      "loss": 2.8097,
      "step": 790
    },
    {
      "epoch": 0.22284122562674094,
      "grad_norm": 1.5866401195526123,
      "learning_rate": 0.0009777437325905294,
      "loss": 2.7063,
      "step": 800
    },
    {
      "epoch": 0.22562674094707522,
      "grad_norm": 1.1381161212921143,
      "learning_rate": 0.0009774651810584957,
      "loss": 2.6254,
      "step": 810
    },
    {
      "epoch": 0.22841225626740946,
      "grad_norm": 1.323906660079956,
      "learning_rate": 0.0009771866295264623,
      "loss": 2.6625,
      "step": 820
    },
    {
      "epoch": 0.23119777158774374,
      "grad_norm": 1.2379060983657837,
      "learning_rate": 0.000976908077994429,
      "loss": 2.829,
      "step": 830
    },
    {
      "epoch": 0.233983286908078,
      "grad_norm": 0.9392759799957275,
      "learning_rate": 0.0009766295264623955,
      "loss": 2.5653,
      "step": 840
    },
    {
      "epoch": 0.23676880222841226,
      "grad_norm": 1.0966740846633911,
      "learning_rate": 0.0009763509749303622,
      "loss": 2.542,
      "step": 850
    },
    {
      "epoch": 0.2395543175487465,
      "grad_norm": 1.1827309131622314,
      "learning_rate": 0.0009760724233983287,
      "loss": 2.6657,
      "step": 860
    },
    {
      "epoch": 0.24233983286908078,
      "grad_norm": 2.1995315551757812,
      "learning_rate": 0.0009757938718662953,
      "loss": 2.5712,
      "step": 870
    },
    {
      "epoch": 0.24512534818941503,
      "grad_norm": 1.3757152557373047,
      "learning_rate": 0.0009755153203342619,
      "loss": 2.5045,
      "step": 880
    },
    {
      "epoch": 0.2479108635097493,
      "grad_norm": 1.300731897354126,
      "learning_rate": 0.0009752367688022285,
      "loss": 2.5062,
      "step": 890
    },
    {
      "epoch": 0.25069637883008355,
      "grad_norm": 1.7121989727020264,
      "learning_rate": 0.0009749582172701949,
      "loss": 2.5394,
      "step": 900
    },
    {
      "epoch": 0.25348189415041783,
      "grad_norm": 1.4732779264450073,
      "learning_rate": 0.0009746796657381615,
      "loss": 2.467,
      "step": 910
    },
    {
      "epoch": 0.2562674094707521,
      "grad_norm": 1.0222309827804565,
      "learning_rate": 0.0009744011142061282,
      "loss": 2.7814,
      "step": 920
    },
    {
      "epoch": 0.2590529247910863,
      "grad_norm": 1.4269797801971436,
      "learning_rate": 0.0009741225626740947,
      "loss": 2.7467,
      "step": 930
    },
    {
      "epoch": 0.2618384401114206,
      "grad_norm": 1.3660566806793213,
      "learning_rate": 0.0009738440111420613,
      "loss": 2.68,
      "step": 940
    },
    {
      "epoch": 0.2646239554317549,
      "grad_norm": 1.7619974613189697,
      "learning_rate": 0.0009735654596100279,
      "loss": 2.9038,
      "step": 950
    },
    {
      "epoch": 0.26740947075208915,
      "grad_norm": 1.4360049962997437,
      "learning_rate": 0.0009732869080779945,
      "loss": 2.7045,
      "step": 960
    },
    {
      "epoch": 0.27019498607242337,
      "grad_norm": 1.2218104600906372,
      "learning_rate": 0.000973008356545961,
      "loss": 2.5466,
      "step": 970
    },
    {
      "epoch": 0.27298050139275765,
      "grad_norm": 1.2115695476531982,
      "learning_rate": 0.0009727298050139276,
      "loss": 2.6825,
      "step": 980
    },
    {
      "epoch": 0.2757660167130919,
      "grad_norm": 1.121020793914795,
      "learning_rate": 0.0009724512534818942,
      "loss": 2.4503,
      "step": 990
    },
    {
      "epoch": 0.2785515320334262,
      "grad_norm": 1.2568773031234741,
      "learning_rate": 0.0009721727019498607,
      "loss": 2.6551,
      "step": 1000
    },
    {
      "epoch": 0.28133704735376047,
      "grad_norm": 2.096107244491577,
      "learning_rate": 0.0009718941504178273,
      "loss": 2.4537,
      "step": 1010
    },
    {
      "epoch": 0.2841225626740947,
      "grad_norm": 1.1138832569122314,
      "learning_rate": 0.0009716155988857938,
      "loss": 2.6241,
      "step": 1020
    },
    {
      "epoch": 0.28690807799442897,
      "grad_norm": 1.082237720489502,
      "learning_rate": 0.0009713370473537605,
      "loss": 2.8076,
      "step": 1030
    },
    {
      "epoch": 0.28969359331476324,
      "grad_norm": 1.759205937385559,
      "learning_rate": 0.000971058495821727,
      "loss": 2.4967,
      "step": 1040
    },
    {
      "epoch": 0.2924791086350975,
      "grad_norm": 1.4228224754333496,
      "learning_rate": 0.0009707799442896936,
      "loss": 2.5495,
      "step": 1050
    },
    {
      "epoch": 0.29526462395543174,
      "grad_norm": 1.2179782390594482,
      "learning_rate": 0.0009705013927576602,
      "loss": 2.5726,
      "step": 1060
    },
    {
      "epoch": 0.298050139275766,
      "grad_norm": 1.0589041709899902,
      "learning_rate": 0.0009702228412256268,
      "loss": 2.5539,
      "step": 1070
    },
    {
      "epoch": 0.3008356545961003,
      "grad_norm": 1.321881651878357,
      "learning_rate": 0.0009699442896935934,
      "loss": 2.7221,
      "step": 1080
    },
    {
      "epoch": 0.30362116991643456,
      "grad_norm": 1.3781636953353882,
      "learning_rate": 0.0009696657381615599,
      "loss": 2.6005,
      "step": 1090
    },
    {
      "epoch": 0.3064066852367688,
      "grad_norm": 1.297652006149292,
      "learning_rate": 0.0009693871866295266,
      "loss": 2.6716,
      "step": 1100
    },
    {
      "epoch": 0.30919220055710306,
      "grad_norm": 1.3703551292419434,
      "learning_rate": 0.000969108635097493,
      "loss": 2.7372,
      "step": 1110
    },
    {
      "epoch": 0.31197771587743733,
      "grad_norm": 1.2973988056182861,
      "learning_rate": 0.0009688300835654596,
      "loss": 2.6567,
      "step": 1120
    },
    {
      "epoch": 0.3147632311977716,
      "grad_norm": 1.3623809814453125,
      "learning_rate": 0.0009685515320334262,
      "loss": 2.508,
      "step": 1130
    },
    {
      "epoch": 0.31754874651810583,
      "grad_norm": 1.2565559148788452,
      "learning_rate": 0.0009682729805013928,
      "loss": 2.6754,
      "step": 1140
    },
    {
      "epoch": 0.3203342618384401,
      "grad_norm": 1.1795793771743774,
      "learning_rate": 0.0009679944289693593,
      "loss": 2.6742,
      "step": 1150
    },
    {
      "epoch": 0.3231197771587744,
      "grad_norm": 1.4879752397537231,
      "learning_rate": 0.0009677158774373259,
      "loss": 2.4956,
      "step": 1160
    },
    {
      "epoch": 0.32590529247910865,
      "grad_norm": 1.2180157899856567,
      "learning_rate": 0.0009674373259052926,
      "loss": 2.6508,
      "step": 1170
    },
    {
      "epoch": 0.3286908077994429,
      "grad_norm": 1.2524197101593018,
      "learning_rate": 0.0009671587743732591,
      "loss": 2.6679,
      "step": 1180
    },
    {
      "epoch": 0.33147632311977715,
      "grad_norm": 1.35256826877594,
      "learning_rate": 0.0009668802228412257,
      "loss": 2.8273,
      "step": 1190
    },
    {
      "epoch": 0.3342618384401114,
      "grad_norm": 1.5260603427886963,
      "learning_rate": 0.0009666016713091921,
      "loss": 2.6268,
      "step": 1200
    },
    {
      "epoch": 0.3370473537604457,
      "grad_norm": 1.7540775537490845,
      "learning_rate": 0.0009663231197771588,
      "loss": 2.519,
      "step": 1210
    },
    {
      "epoch": 0.3398328690807799,
      "grad_norm": 1.5949130058288574,
      "learning_rate": 0.0009660445682451253,
      "loss": 2.5143,
      "step": 1220
    },
    {
      "epoch": 0.3426183844011142,
      "grad_norm": 1.0860681533813477,
      "learning_rate": 0.0009657660167130919,
      "loss": 2.5586,
      "step": 1230
    },
    {
      "epoch": 0.34540389972144847,
      "grad_norm": 1.1188979148864746,
      "learning_rate": 0.0009654874651810586,
      "loss": 2.5164,
      "step": 1240
    },
    {
      "epoch": 0.34818941504178275,
      "grad_norm": 1.1286648511886597,
      "learning_rate": 0.0009652089136490251,
      "loss": 2.5772,
      "step": 1250
    },
    {
      "epoch": 0.35097493036211697,
      "grad_norm": 1.5116323232650757,
      "learning_rate": 0.0009649303621169917,
      "loss": 2.6543,
      "step": 1260
    },
    {
      "epoch": 0.35376044568245124,
      "grad_norm": 1.458951711654663,
      "learning_rate": 0.0009646518105849582,
      "loss": 2.8363,
      "step": 1270
    },
    {
      "epoch": 0.3565459610027855,
      "grad_norm": 1.2095354795455933,
      "learning_rate": 0.0009643732590529249,
      "loss": 2.6857,
      "step": 1280
    },
    {
      "epoch": 0.3593314763231198,
      "grad_norm": 1.0491148233413696,
      "learning_rate": 0.0009640947075208914,
      "loss": 2.5778,
      "step": 1290
    },
    {
      "epoch": 0.362116991643454,
      "grad_norm": 1.2475234270095825,
      "learning_rate": 0.0009638161559888579,
      "loss": 2.6415,
      "step": 1300
    },
    {
      "epoch": 0.3649025069637883,
      "grad_norm": 1.9963531494140625,
      "learning_rate": 0.0009635376044568245,
      "loss": 2.5831,
      "step": 1310
    },
    {
      "epoch": 0.36768802228412256,
      "grad_norm": 1.413554310798645,
      "learning_rate": 0.0009632590529247911,
      "loss": 2.7378,
      "step": 1320
    },
    {
      "epoch": 0.37047353760445684,
      "grad_norm": 1.80016028881073,
      "learning_rate": 0.0009629805013927577,
      "loss": 2.6622,
      "step": 1330
    },
    {
      "epoch": 0.3732590529247911,
      "grad_norm": 1.4611876010894775,
      "learning_rate": 0.0009627019498607242,
      "loss": 2.503,
      "step": 1340
    },
    {
      "epoch": 0.37604456824512533,
      "grad_norm": 1.1218934059143066,
      "learning_rate": 0.0009624233983286909,
      "loss": 2.6757,
      "step": 1350
    },
    {
      "epoch": 0.3788300835654596,
      "grad_norm": 1.5722981691360474,
      "learning_rate": 0.0009621448467966574,
      "loss": 2.7089,
      "step": 1360
    },
    {
      "epoch": 0.3816155988857939,
      "grad_norm": 1.2269376516342163,
      "learning_rate": 0.000961866295264624,
      "loss": 2.7037,
      "step": 1370
    },
    {
      "epoch": 0.38440111420612816,
      "grad_norm": 1.387017011642456,
      "learning_rate": 0.0009615877437325905,
      "loss": 2.586,
      "step": 1380
    },
    {
      "epoch": 0.3871866295264624,
      "grad_norm": 1.515525221824646,
      "learning_rate": 0.0009613091922005572,
      "loss": 2.6129,
      "step": 1390
    },
    {
      "epoch": 0.38997214484679665,
      "grad_norm": 1.1997807025909424,
      "learning_rate": 0.0009610306406685237,
      "loss": 2.488,
      "step": 1400
    },
    {
      "epoch": 0.39275766016713093,
      "grad_norm": 1.5051579475402832,
      "learning_rate": 0.0009607520891364902,
      "loss": 2.7434,
      "step": 1410
    },
    {
      "epoch": 0.3955431754874652,
      "grad_norm": 1.159185767173767,
      "learning_rate": 0.0009604735376044569,
      "loss": 2.6266,
      "step": 1420
    },
    {
      "epoch": 0.3983286908077994,
      "grad_norm": 1.232654094696045,
      "learning_rate": 0.0009601949860724234,
      "loss": 2.5591,
      "step": 1430
    },
    {
      "epoch": 0.4011142061281337,
      "grad_norm": 1.1168028116226196,
      "learning_rate": 0.00095991643454039,
      "loss": 2.397,
      "step": 1440
    },
    {
      "epoch": 0.403899721448468,
      "grad_norm": 1.3815401792526245,
      "learning_rate": 0.0009596378830083565,
      "loss": 2.3866,
      "step": 1450
    },
    {
      "epoch": 0.40668523676880225,
      "grad_norm": 1.520772933959961,
      "learning_rate": 0.0009593593314763232,
      "loss": 2.5404,
      "step": 1460
    },
    {
      "epoch": 0.40947075208913647,
      "grad_norm": 1.3483422994613647,
      "learning_rate": 0.0009590807799442897,
      "loss": 2.5735,
      "step": 1470
    },
    {
      "epoch": 0.41225626740947074,
      "grad_norm": 1.2592273950576782,
      "learning_rate": 0.0009588022284122563,
      "loss": 2.6864,
      "step": 1480
    },
    {
      "epoch": 0.415041782729805,
      "grad_norm": 1.1354230642318726,
      "learning_rate": 0.000958523676880223,
      "loss": 2.4747,
      "step": 1490
    },
    {
      "epoch": 0.4178272980501393,
      "grad_norm": 1.1164798736572266,
      "learning_rate": 0.0009582451253481894,
      "loss": 2.5945,
      "step": 1500
    },
    {
      "epoch": 0.4206128133704735,
      "grad_norm": 1.7407081127166748,
      "learning_rate": 0.000957966573816156,
      "loss": 2.6287,
      "step": 1510
    },
    {
      "epoch": 0.4233983286908078,
      "grad_norm": 1.4179469347000122,
      "learning_rate": 0.0009576880222841225,
      "loss": 2.6931,
      "step": 1520
    },
    {
      "epoch": 0.42618384401114207,
      "grad_norm": 0.9913387894630432,
      "learning_rate": 0.0009574094707520892,
      "loss": 2.5683,
      "step": 1530
    },
    {
      "epoch": 0.42896935933147634,
      "grad_norm": 1.288426160812378,
      "learning_rate": 0.0009571309192200557,
      "loss": 2.6026,
      "step": 1540
    },
    {
      "epoch": 0.43175487465181056,
      "grad_norm": 1.3299269676208496,
      "learning_rate": 0.0009568523676880223,
      "loss": 2.588,
      "step": 1550
    },
    {
      "epoch": 0.43454038997214484,
      "grad_norm": 1.922222375869751,
      "learning_rate": 0.0009565738161559889,
      "loss": 2.5401,
      "step": 1560
    },
    {
      "epoch": 0.4373259052924791,
      "grad_norm": 1.565882921218872,
      "learning_rate": 0.0009562952646239555,
      "loss": 2.6152,
      "step": 1570
    },
    {
      "epoch": 0.4401114206128134,
      "grad_norm": 1.2204829454421997,
      "learning_rate": 0.0009560167130919221,
      "loss": 2.6914,
      "step": 1580
    },
    {
      "epoch": 0.4428969359331476,
      "grad_norm": 1.1876715421676636,
      "learning_rate": 0.0009557381615598885,
      "loss": 2.6338,
      "step": 1590
    },
    {
      "epoch": 0.4456824512534819,
      "grad_norm": 1.4208650588989258,
      "learning_rate": 0.0009554596100278552,
      "loss": 2.5563,
      "step": 1600
    },
    {
      "epoch": 0.44846796657381616,
      "grad_norm": 1.0509493350982666,
      "learning_rate": 0.0009551810584958217,
      "loss": 2.4876,
      "step": 1610
    },
    {
      "epoch": 0.45125348189415043,
      "grad_norm": 1.0826983451843262,
      "learning_rate": 0.0009549025069637883,
      "loss": 2.591,
      "step": 1620
    },
    {
      "epoch": 0.45403899721448465,
      "grad_norm": 1.4745317697525024,
      "learning_rate": 0.0009546239554317548,
      "loss": 2.6145,
      "step": 1630
    },
    {
      "epoch": 0.4568245125348189,
      "grad_norm": 1.3137785196304321,
      "learning_rate": 0.0009543454038997215,
      "loss": 2.5078,
      "step": 1640
    },
    {
      "epoch": 0.4596100278551532,
      "grad_norm": 1.4456435441970825,
      "learning_rate": 0.0009540668523676881,
      "loss": 2.576,
      "step": 1650
    },
    {
      "epoch": 0.4623955431754875,
      "grad_norm": 2.9058103561401367,
      "learning_rate": 0.0009537883008356546,
      "loss": 2.8123,
      "step": 1660
    },
    {
      "epoch": 0.46518105849582175,
      "grad_norm": 1.210128903388977,
      "learning_rate": 0.0009535097493036213,
      "loss": 2.6477,
      "step": 1670
    },
    {
      "epoch": 0.467966573816156,
      "grad_norm": 1.3550188541412354,
      "learning_rate": 0.0009532311977715878,
      "loss": 2.3989,
      "step": 1680
    },
    {
      "epoch": 0.47075208913649025,
      "grad_norm": 1.533486247062683,
      "learning_rate": 0.0009529526462395543,
      "loss": 2.7835,
      "step": 1690
    },
    {
      "epoch": 0.4735376044568245,
      "grad_norm": 1.1093838214874268,
      "learning_rate": 0.0009526740947075208,
      "loss": 2.6051,
      "step": 1700
    },
    {
      "epoch": 0.4763231197771588,
      "grad_norm": 1.8435648679733276,
      "learning_rate": 0.0009523955431754875,
      "loss": 2.5926,
      "step": 1710
    },
    {
      "epoch": 0.479108635097493,
      "grad_norm": 1.6711620092391968,
      "learning_rate": 0.0009521169916434541,
      "loss": 2.4779,
      "step": 1720
    },
    {
      "epoch": 0.4818941504178273,
      "grad_norm": 1.3479443788528442,
      "learning_rate": 0.0009518384401114206,
      "loss": 2.5155,
      "step": 1730
    },
    {
      "epoch": 0.48467966573816157,
      "grad_norm": 1.5922962427139282,
      "learning_rate": 0.0009515598885793872,
      "loss": 2.795,
      "step": 1740
    },
    {
      "epoch": 0.48746518105849584,
      "grad_norm": 1.2577641010284424,
      "learning_rate": 0.0009512813370473538,
      "loss": 2.6942,
      "step": 1750
    },
    {
      "epoch": 0.49025069637883006,
      "grad_norm": 1.1868338584899902,
      "learning_rate": 0.0009510027855153204,
      "loss": 2.4699,
      "step": 1760
    },
    {
      "epoch": 0.49303621169916434,
      "grad_norm": 1.3853003978729248,
      "learning_rate": 0.0009507242339832869,
      "loss": 2.5375,
      "step": 1770
    },
    {
      "epoch": 0.4958217270194986,
      "grad_norm": 1.1781913042068481,
      "learning_rate": 0.0009504456824512536,
      "loss": 2.5025,
      "step": 1780
    },
    {
      "epoch": 0.4986072423398329,
      "grad_norm": 1.262628197669983,
      "learning_rate": 0.00095016713091922,
      "loss": 2.5345,
      "step": 1790
    },
    {
      "epoch": 0.5013927576601671,
      "grad_norm": 1.1075992584228516,
      "learning_rate": 0.0009498885793871866,
      "loss": 2.6029,
      "step": 1800
    },
    {
      "epoch": 0.5041782729805014,
      "grad_norm": 1.3424242734909058,
      "learning_rate": 0.0009496100278551532,
      "loss": 2.137,
      "step": 1810
    },
    {
      "epoch": 0.5069637883008357,
      "grad_norm": 1.1349455118179321,
      "learning_rate": 0.0009493314763231198,
      "loss": 2.5881,
      "step": 1820
    },
    {
      "epoch": 0.5097493036211699,
      "grad_norm": 1.6739836931228638,
      "learning_rate": 0.0009490529247910864,
      "loss": 2.6388,
      "step": 1830
    },
    {
      "epoch": 0.5125348189415042,
      "grad_norm": 1.2096190452575684,
      "learning_rate": 0.0009487743732590529,
      "loss": 2.2502,
      "step": 1840
    },
    {
      "epoch": 0.5153203342618384,
      "grad_norm": 1.1820263862609863,
      "learning_rate": 0.0009484958217270196,
      "loss": 2.5372,
      "step": 1850
    },
    {
      "epoch": 0.5181058495821727,
      "grad_norm": 1.23209810256958,
      "learning_rate": 0.0009482172701949861,
      "loss": 2.6936,
      "step": 1860
    },
    {
      "epoch": 0.520891364902507,
      "grad_norm": 1.1804924011230469,
      "learning_rate": 0.0009479387186629527,
      "loss": 2.7291,
      "step": 1870
    },
    {
      "epoch": 0.5236768802228412,
      "grad_norm": 1.5289980173110962,
      "learning_rate": 0.0009476601671309193,
      "loss": 2.6525,
      "step": 1880
    },
    {
      "epoch": 0.5264623955431755,
      "grad_norm": 1.6307711601257324,
      "learning_rate": 0.0009473816155988858,
      "loss": 2.6374,
      "step": 1890
    },
    {
      "epoch": 0.5292479108635098,
      "grad_norm": 2.069382429122925,
      "learning_rate": 0.0009471030640668524,
      "loss": 2.2964,
      "step": 1900
    },
    {
      "epoch": 0.532033426183844,
      "grad_norm": 7.963252067565918,
      "learning_rate": 0.0009468245125348189,
      "loss": 2.6508,
      "step": 1910
    },
    {
      "epoch": 0.5348189415041783,
      "grad_norm": 1.3602561950683594,
      "learning_rate": 0.0009465459610027855,
      "loss": 2.516,
      "step": 1920
    },
    {
      "epoch": 0.5376044568245125,
      "grad_norm": 1.2182590961456299,
      "learning_rate": 0.0009462674094707521,
      "loss": 2.6688,
      "step": 1930
    },
    {
      "epoch": 0.5403899721448467,
      "grad_norm": 1.422895908355713,
      "learning_rate": 0.0009459888579387187,
      "loss": 2.3212,
      "step": 1940
    },
    {
      "epoch": 0.5431754874651811,
      "grad_norm": 1.41317880153656,
      "learning_rate": 0.0009457103064066852,
      "loss": 2.7405,
      "step": 1950
    },
    {
      "epoch": 0.5459610027855153,
      "grad_norm": 1.0755990743637085,
      "learning_rate": 0.0009454317548746519,
      "loss": 2.6927,
      "step": 1960
    },
    {
      "epoch": 0.5487465181058496,
      "grad_norm": 1.3661142587661743,
      "learning_rate": 0.0009451532033426185,
      "loss": 2.5271,
      "step": 1970
    },
    {
      "epoch": 0.5515320334261838,
      "grad_norm": 1.6025452613830566,
      "learning_rate": 0.000944874651810585,
      "loss": 2.5678,
      "step": 1980
    },
    {
      "epoch": 0.5543175487465181,
      "grad_norm": 1.1793705224990845,
      "learning_rate": 0.0009445961002785515,
      "loss": 2.3317,
      "step": 1990
    },
    {
      "epoch": 0.5571030640668524,
      "grad_norm": 1.1052885055541992,
      "learning_rate": 0.0009443175487465181,
      "loss": 2.5183,
      "step": 2000
    },
    {
      "epoch": 0.5598885793871866,
      "grad_norm": 1.877814531326294,
      "learning_rate": 0.0009440389972144847,
      "loss": 2.5533,
      "step": 2010
    },
    {
      "epoch": 0.5626740947075209,
      "grad_norm": 0.9979457259178162,
      "learning_rate": 0.0009437604456824512,
      "loss": 2.5141,
      "step": 2020
    },
    {
      "epoch": 0.5654596100278552,
      "grad_norm": 1.664000391960144,
      "learning_rate": 0.0009434818941504178,
      "loss": 2.5995,
      "step": 2030
    },
    {
      "epoch": 0.5682451253481894,
      "grad_norm": 1.327587366104126,
      "learning_rate": 0.0009432033426183845,
      "loss": 2.5348,
      "step": 2040
    },
    {
      "epoch": 0.5710306406685237,
      "grad_norm": 1.4465824365615845,
      "learning_rate": 0.000942924791086351,
      "loss": 2.5717,
      "step": 2050
    },
    {
      "epoch": 0.5738161559888579,
      "grad_norm": 1.3093647956848145,
      "learning_rate": 0.0009426462395543176,
      "loss": 2.4578,
      "step": 2060
    },
    {
      "epoch": 0.5766016713091922,
      "grad_norm": 1.901842713356018,
      "learning_rate": 0.0009423676880222842,
      "loss": 2.4301,
      "step": 2070
    },
    {
      "epoch": 0.5793871866295265,
      "grad_norm": 1.1070830821990967,
      "learning_rate": 0.0009420891364902508,
      "loss": 2.5675,
      "step": 2080
    },
    {
      "epoch": 0.5821727019498607,
      "grad_norm": 1.551803469657898,
      "learning_rate": 0.0009418105849582172,
      "loss": 2.6159,
      "step": 2090
    },
    {
      "epoch": 0.584958217270195,
      "grad_norm": 1.1841830015182495,
      "learning_rate": 0.0009415320334261838,
      "loss": 2.5513,
      "step": 2100
    },
    {
      "epoch": 0.5877437325905293,
      "grad_norm": 1.2188178300857544,
      "learning_rate": 0.0009412534818941504,
      "loss": 2.6357,
      "step": 2110
    },
    {
      "epoch": 0.5905292479108635,
      "grad_norm": 1.1983903646469116,
      "learning_rate": 0.000940974930362117,
      "loss": 2.6012,
      "step": 2120
    },
    {
      "epoch": 0.5933147632311978,
      "grad_norm": 1.570056438446045,
      "learning_rate": 0.0009406963788300836,
      "loss": 2.6183,
      "step": 2130
    },
    {
      "epoch": 0.596100278551532,
      "grad_norm": 1.232574701309204,
      "learning_rate": 0.0009404178272980502,
      "loss": 2.665,
      "step": 2140
    },
    {
      "epoch": 0.5988857938718662,
      "grad_norm": 1.6553981304168701,
      "learning_rate": 0.0009401392757660168,
      "loss": 2.4081,
      "step": 2150
    },
    {
      "epoch": 0.6016713091922006,
      "grad_norm": 1.0133997201919556,
      "learning_rate": 0.0009398607242339833,
      "loss": 2.4542,
      "step": 2160
    },
    {
      "epoch": 0.6044568245125348,
      "grad_norm": 1.5961579084396362,
      "learning_rate": 0.0009395821727019499,
      "loss": 2.5394,
      "step": 2170
    },
    {
      "epoch": 0.6072423398328691,
      "grad_norm": 0.9842129349708557,
      "learning_rate": 0.0009393036211699164,
      "loss": 2.485,
      "step": 2180
    },
    {
      "epoch": 0.6100278551532033,
      "grad_norm": 0.976337194442749,
      "learning_rate": 0.000939025069637883,
      "loss": 2.6303,
      "step": 2190
    },
    {
      "epoch": 0.6128133704735376,
      "grad_norm": 1.148184061050415,
      "learning_rate": 0.0009387465181058496,
      "loss": 2.7469,
      "step": 2200
    },
    {
      "epoch": 0.6155988857938719,
      "grad_norm": 1.2292206287384033,
      "learning_rate": 0.0009384679665738161,
      "loss": 2.5788,
      "step": 2210
    },
    {
      "epoch": 0.6183844011142061,
      "grad_norm": 1.4672414064407349,
      "learning_rate": 0.0009381894150417828,
      "loss": 2.3803,
      "step": 2220
    },
    {
      "epoch": 0.6211699164345403,
      "grad_norm": 1.0753039121627808,
      "learning_rate": 0.0009379108635097493,
      "loss": 2.5741,
      "step": 2230
    },
    {
      "epoch": 0.6239554317548747,
      "grad_norm": 1.4006630182266235,
      "learning_rate": 0.0009376323119777159,
      "loss": 2.3706,
      "step": 2240
    },
    {
      "epoch": 0.6267409470752089,
      "grad_norm": 1.2724519968032837,
      "learning_rate": 0.0009373537604456825,
      "loss": 2.539,
      "step": 2250
    },
    {
      "epoch": 0.6295264623955432,
      "grad_norm": 1.2491227388381958,
      "learning_rate": 0.0009370752089136491,
      "loss": 2.7182,
      "step": 2260
    },
    {
      "epoch": 0.6323119777158774,
      "grad_norm": 1.4925132989883423,
      "learning_rate": 0.0009367966573816156,
      "loss": 2.7096,
      "step": 2270
    },
    {
      "epoch": 0.6350974930362117,
      "grad_norm": 1.388200044631958,
      "learning_rate": 0.0009365181058495821,
      "loss": 2.5473,
      "step": 2280
    },
    {
      "epoch": 0.637883008356546,
      "grad_norm": 1.309496521949768,
      "learning_rate": 0.0009362395543175488,
      "loss": 2.5842,
      "step": 2290
    },
    {
      "epoch": 0.6406685236768802,
      "grad_norm": 1.8393137454986572,
      "learning_rate": 0.0009359610027855153,
      "loss": 2.7737,
      "step": 2300
    },
    {
      "epoch": 0.6434540389972145,
      "grad_norm": 4.742587566375732,
      "learning_rate": 0.0009356824512534819,
      "loss": 2.7859,
      "step": 2310
    },
    {
      "epoch": 0.6462395543175488,
      "grad_norm": 1.2533297538757324,
      "learning_rate": 0.0009354038997214485,
      "loss": 2.5762,
      "step": 2320
    },
    {
      "epoch": 0.649025069637883,
      "grad_norm": 1.3019458055496216,
      "learning_rate": 0.0009351253481894151,
      "loss": 2.4436,
      "step": 2330
    },
    {
      "epoch": 0.6518105849582173,
      "grad_norm": 1.2244460582733154,
      "learning_rate": 0.0009348467966573816,
      "loss": 2.3524,
      "step": 2340
    },
    {
      "epoch": 0.6545961002785515,
      "grad_norm": 1.6538617610931396,
      "learning_rate": 0.0009345682451253482,
      "loss": 2.4802,
      "step": 2350
    },
    {
      "epoch": 0.6573816155988857,
      "grad_norm": 3.2586660385131836,
      "learning_rate": 0.0009342896935933149,
      "loss": 2.6909,
      "step": 2360
    },
    {
      "epoch": 0.6601671309192201,
      "grad_norm": 1.18228280544281,
      "learning_rate": 0.0009340111420612814,
      "loss": 2.6456,
      "step": 2370
    },
    {
      "epoch": 0.6629526462395543,
      "grad_norm": 1.20558500289917,
      "learning_rate": 0.000933732590529248,
      "loss": 2.4466,
      "step": 2380
    },
    {
      "epoch": 0.6657381615598886,
      "grad_norm": 1.121671199798584,
      "learning_rate": 0.0009334540389972144,
      "loss": 2.481,
      "step": 2390
    },
    {
      "epoch": 0.6685236768802229,
      "grad_norm": 1.2893065214157104,
      "learning_rate": 0.0009331754874651811,
      "loss": 2.3008,
      "step": 2400
    },
    {
      "epoch": 0.6713091922005571,
      "grad_norm": 3.232966423034668,
      "learning_rate": 0.0009328969359331476,
      "loss": 2.5298,
      "step": 2410
    },
    {
      "epoch": 0.6740947075208914,
      "grad_norm": 1.07889986038208,
      "learning_rate": 0.0009326183844011142,
      "loss": 2.602,
      "step": 2420
    },
    {
      "epoch": 0.6768802228412256,
      "grad_norm": 1.2069727182388306,
      "learning_rate": 0.0009323398328690808,
      "loss": 2.5107,
      "step": 2430
    },
    {
      "epoch": 0.6796657381615598,
      "grad_norm": 1.1511386632919312,
      "learning_rate": 0.0009320612813370474,
      "loss": 2.5646,
      "step": 2440
    },
    {
      "epoch": 0.6824512534818942,
      "grad_norm": 1.3445967435836792,
      "learning_rate": 0.000931782729805014,
      "loss": 2.5636,
      "step": 2450
    },
    {
      "epoch": 0.6852367688022284,
      "grad_norm": 2.175687074661255,
      "learning_rate": 0.0009315041782729805,
      "loss": 2.5484,
      "step": 2460
    },
    {
      "epoch": 0.6880222841225627,
      "grad_norm": 1.77033269405365,
      "learning_rate": 0.0009312256267409472,
      "loss": 2.6023,
      "step": 2470
    },
    {
      "epoch": 0.6908077994428969,
      "grad_norm": 1.3223330974578857,
      "learning_rate": 0.0009309470752089136,
      "loss": 2.5597,
      "step": 2480
    },
    {
      "epoch": 0.6935933147632312,
      "grad_norm": 0.973379909992218,
      "learning_rate": 0.0009306685236768802,
      "loss": 2.5106,
      "step": 2490
    },
    {
      "epoch": 0.6963788300835655,
      "grad_norm": 1.3978427648544312,
      "learning_rate": 0.0009303899721448468,
      "loss": 2.475,
      "step": 2500
    },
    {
      "epoch": 0.6991643454038997,
      "grad_norm": 1.1118197441101074,
      "learning_rate": 0.0009301114206128134,
      "loss": 2.6572,
      "step": 2510
    },
    {
      "epoch": 0.7019498607242339,
      "grad_norm": 1.159158706665039,
      "learning_rate": 0.0009298328690807799,
      "loss": 2.744,
      "step": 2520
    },
    {
      "epoch": 0.7047353760445683,
      "grad_norm": 1.4264073371887207,
      "learning_rate": 0.0009295543175487465,
      "loss": 2.2988,
      "step": 2530
    },
    {
      "epoch": 0.7075208913649025,
      "grad_norm": 1.0038737058639526,
      "learning_rate": 0.0009292757660167132,
      "loss": 2.6336,
      "step": 2540
    },
    {
      "epoch": 0.7103064066852368,
      "grad_norm": 1.3323882818222046,
      "learning_rate": 0.0009289972144846797,
      "loss": 2.5943,
      "step": 2550
    },
    {
      "epoch": 0.713091922005571,
      "grad_norm": 1.0147569179534912,
      "learning_rate": 0.0009287186629526463,
      "loss": 2.5134,
      "step": 2560
    },
    {
      "epoch": 0.7158774373259053,
      "grad_norm": 1.4271459579467773,
      "learning_rate": 0.0009284401114206127,
      "loss": 2.5186,
      "step": 2570
    },
    {
      "epoch": 0.7186629526462396,
      "grad_norm": 1.2987439632415771,
      "learning_rate": 0.0009281615598885794,
      "loss": 2.3949,
      "step": 2580
    },
    {
      "epoch": 0.7214484679665738,
      "grad_norm": 1.4954429864883423,
      "learning_rate": 0.0009278830083565459,
      "loss": 2.6412,
      "step": 2590
    },
    {
      "epoch": 0.724233983286908,
      "grad_norm": 1.090782880783081,
      "learning_rate": 0.0009276044568245125,
      "loss": 2.5332,
      "step": 2600
    },
    {
      "epoch": 0.7270194986072424,
      "grad_norm": 1.0385299921035767,
      "learning_rate": 0.0009273259052924792,
      "loss": 2.5205,
      "step": 2610
    },
    {
      "epoch": 0.7298050139275766,
      "grad_norm": 1.2220218181610107,
      "learning_rate": 0.0009270473537604457,
      "loss": 2.549,
      "step": 2620
    },
    {
      "epoch": 0.7325905292479109,
      "grad_norm": 1.1563817262649536,
      "learning_rate": 0.0009267688022284123,
      "loss": 2.4725,
      "step": 2630
    },
    {
      "epoch": 0.7353760445682451,
      "grad_norm": 1.2204904556274414,
      "learning_rate": 0.0009264902506963788,
      "loss": 2.4911,
      "step": 2640
    },
    {
      "epoch": 0.7381615598885793,
      "grad_norm": 1.47650146484375,
      "learning_rate": 0.0009262116991643455,
      "loss": 2.7053,
      "step": 2650
    },
    {
      "epoch": 0.7409470752089137,
      "grad_norm": 1.2485514879226685,
      "learning_rate": 0.000925933147632312,
      "loss": 2.4443,
      "step": 2660
    },
    {
      "epoch": 0.7437325905292479,
      "grad_norm": 1.210825800895691,
      "learning_rate": 0.0009256545961002786,
      "loss": 2.5861,
      "step": 2670
    },
    {
      "epoch": 0.7465181058495822,
      "grad_norm": 1.133064866065979,
      "learning_rate": 0.0009253760445682451,
      "loss": 2.2651,
      "step": 2680
    },
    {
      "epoch": 0.7493036211699164,
      "grad_norm": 1.1768527030944824,
      "learning_rate": 0.0009250974930362117,
      "loss": 2.5199,
      "step": 2690
    },
    {
      "epoch": 0.7520891364902507,
      "grad_norm": 1.5120288133621216,
      "learning_rate": 0.0009248189415041783,
      "loss": 2.4315,
      "step": 2700
    },
    {
      "epoch": 0.754874651810585,
      "grad_norm": 1.159740924835205,
      "learning_rate": 0.0009245403899721448,
      "loss": 2.5868,
      "step": 2710
    },
    {
      "epoch": 0.7576601671309192,
      "grad_norm": 1.1845282316207886,
      "learning_rate": 0.0009242618384401115,
      "loss": 2.5479,
      "step": 2720
    },
    {
      "epoch": 0.7604456824512534,
      "grad_norm": 1.0858104228973389,
      "learning_rate": 0.000923983286908078,
      "loss": 2.5822,
      "step": 2730
    },
    {
      "epoch": 0.7632311977715878,
      "grad_norm": 1.2327394485473633,
      "learning_rate": 0.0009237047353760446,
      "loss": 2.4777,
      "step": 2740
    },
    {
      "epoch": 0.766016713091922,
      "grad_norm": 1.2286115884780884,
      "learning_rate": 0.0009234261838440111,
      "loss": 2.6194,
      "step": 2750
    },
    {
      "epoch": 0.7688022284122563,
      "grad_norm": 1.4070664644241333,
      "learning_rate": 0.0009231476323119778,
      "loss": 2.309,
      "step": 2760
    },
    {
      "epoch": 0.7715877437325905,
      "grad_norm": 1.4126564264297485,
      "learning_rate": 0.0009228690807799444,
      "loss": 2.5326,
      "step": 2770
    },
    {
      "epoch": 0.7743732590529248,
      "grad_norm": 1.49712336063385,
      "learning_rate": 0.0009225905292479108,
      "loss": 2.3481,
      "step": 2780
    },
    {
      "epoch": 0.7771587743732591,
      "grad_norm": 1.2510416507720947,
      "learning_rate": 0.0009223119777158775,
      "loss": 2.7475,
      "step": 2790
    },
    {
      "epoch": 0.7799442896935933,
      "grad_norm": 1.1437839269638062,
      "learning_rate": 0.000922033426183844,
      "loss": 2.3777,
      "step": 2800
    },
    {
      "epoch": 0.7827298050139275,
      "grad_norm": 1.3473021984100342,
      "learning_rate": 0.0009217548746518106,
      "loss": 2.516,
      "step": 2810
    },
    {
      "epoch": 0.7855153203342619,
      "grad_norm": 1.4354236125946045,
      "learning_rate": 0.0009214763231197771,
      "loss": 2.648,
      "step": 2820
    },
    {
      "epoch": 0.7883008356545961,
      "grad_norm": 1.357447862625122,
      "learning_rate": 0.0009211977715877438,
      "loss": 2.7018,
      "step": 2830
    },
    {
      "epoch": 0.7910863509749304,
      "grad_norm": 1.419813871383667,
      "learning_rate": 0.0009209192200557103,
      "loss": 2.5181,
      "step": 2840
    },
    {
      "epoch": 0.7938718662952646,
      "grad_norm": 1.271546483039856,
      "learning_rate": 0.0009206406685236769,
      "loss": 2.3738,
      "step": 2850
    },
    {
      "epoch": 0.7966573816155988,
      "grad_norm": 1.5175025463104248,
      "learning_rate": 0.0009203621169916436,
      "loss": 2.4937,
      "step": 2860
    },
    {
      "epoch": 0.7994428969359332,
      "grad_norm": 1.38618004322052,
      "learning_rate": 0.00092008356545961,
      "loss": 2.2005,
      "step": 2870
    },
    {
      "epoch": 0.8022284122562674,
      "grad_norm": 1.6078341007232666,
      "learning_rate": 0.0009198050139275766,
      "loss": 2.4386,
      "step": 2880
    },
    {
      "epoch": 0.8050139275766016,
      "grad_norm": 1.3587883710861206,
      "learning_rate": 0.0009195264623955431,
      "loss": 2.4942,
      "step": 2890
    },
    {
      "epoch": 0.807799442896936,
      "grad_norm": 0.9999790787696838,
      "learning_rate": 0.0009192479108635098,
      "loss": 2.4569,
      "step": 2900
    },
    {
      "epoch": 0.8105849582172702,
      "grad_norm": 1.2193816900253296,
      "learning_rate": 0.0009189693593314763,
      "loss": 2.5595,
      "step": 2910
    },
    {
      "epoch": 0.8133704735376045,
      "grad_norm": 1.1606136560440063,
      "learning_rate": 0.0009186908077994429,
      "loss": 2.5728,
      "step": 2920
    },
    {
      "epoch": 0.8161559888579387,
      "grad_norm": 1.3408945798873901,
      "learning_rate": 0.0009184122562674095,
      "loss": 2.4564,
      "step": 2930
    },
    {
      "epoch": 0.8189415041782729,
      "grad_norm": 1.4720326662063599,
      "learning_rate": 0.0009181337047353761,
      "loss": 2.6006,
      "step": 2940
    },
    {
      "epoch": 0.8217270194986073,
      "grad_norm": 1.1836766004562378,
      "learning_rate": 0.0009178551532033427,
      "loss": 2.6311,
      "step": 2950
    },
    {
      "epoch": 0.8245125348189415,
      "grad_norm": 1.6117591857910156,
      "learning_rate": 0.0009175766016713092,
      "loss": 2.6878,
      "step": 2960
    },
    {
      "epoch": 0.8272980501392758,
      "grad_norm": 1.055732011795044,
      "learning_rate": 0.0009172980501392759,
      "loss": 2.4587,
      "step": 2970
    },
    {
      "epoch": 0.83008356545961,
      "grad_norm": 1.7295316457748413,
      "learning_rate": 0.0009170194986072423,
      "loss": 2.4898,
      "step": 2980
    },
    {
      "epoch": 0.8328690807799443,
      "grad_norm": 2.995147705078125,
      "learning_rate": 0.0009167409470752089,
      "loss": 2.4622,
      "step": 2990
    },
    {
      "epoch": 0.8356545961002786,
      "grad_norm": 1.1673688888549805,
      "learning_rate": 0.0009164623955431754,
      "loss": 2.5147,
      "step": 3000
    },
    {
      "epoch": 0.8384401114206128,
      "grad_norm": 1.1923869848251343,
      "learning_rate": 0.0009161838440111421,
      "loss": 2.5471,
      "step": 3010
    },
    {
      "epoch": 0.841225626740947,
      "grad_norm": 1.7218519449234009,
      "learning_rate": 0.0009159052924791087,
      "loss": 2.4566,
      "step": 3020
    },
    {
      "epoch": 0.8440111420612814,
      "grad_norm": 1.2605904340744019,
      "learning_rate": 0.0009156267409470752,
      "loss": 2.4132,
      "step": 3030
    },
    {
      "epoch": 0.8467966573816156,
      "grad_norm": 1.4090555906295776,
      "learning_rate": 0.0009153481894150419,
      "loss": 2.539,
      "step": 3040
    },
    {
      "epoch": 0.8495821727019499,
      "grad_norm": 1.4345227479934692,
      "learning_rate": 0.0009150696378830084,
      "loss": 2.6776,
      "step": 3050
    },
    {
      "epoch": 0.8523676880222841,
      "grad_norm": 1.1424943208694458,
      "learning_rate": 0.000914791086350975,
      "loss": 2.658,
      "step": 3060
    },
    {
      "epoch": 0.8551532033426184,
      "grad_norm": 1.2283047437667847,
      "learning_rate": 0.0009145125348189414,
      "loss": 2.618,
      "step": 3070
    },
    {
      "epoch": 0.8579387186629527,
      "grad_norm": 1.3770256042480469,
      "learning_rate": 0.0009142339832869081,
      "loss": 2.4941,
      "step": 3080
    },
    {
      "epoch": 0.8607242339832869,
      "grad_norm": 3.363332748413086,
      "learning_rate": 0.0009139554317548747,
      "loss": 2.4735,
      "step": 3090
    },
    {
      "epoch": 0.8635097493036211,
      "grad_norm": 1.307361364364624,
      "learning_rate": 0.0009136768802228412,
      "loss": 2.4563,
      "step": 3100
    },
    {
      "epoch": 0.8662952646239555,
      "grad_norm": 1.243944525718689,
      "learning_rate": 0.0009133983286908078,
      "loss": 2.559,
      "step": 3110
    },
    {
      "epoch": 0.8690807799442897,
      "grad_norm": 1.2397347688674927,
      "learning_rate": 0.0009131197771587744,
      "loss": 2.4407,
      "step": 3120
    },
    {
      "epoch": 0.871866295264624,
      "grad_norm": 1.4625368118286133,
      "learning_rate": 0.000912841225626741,
      "loss": 2.5931,
      "step": 3130
    },
    {
      "epoch": 0.8746518105849582,
      "grad_norm": 1.2405434846878052,
      "learning_rate": 0.0009125626740947075,
      "loss": 2.5325,
      "step": 3140
    },
    {
      "epoch": 0.8774373259052924,
      "grad_norm": 1.9752392768859863,
      "learning_rate": 0.0009122841225626742,
      "loss": 2.565,
      "step": 3150
    },
    {
      "epoch": 0.8802228412256268,
      "grad_norm": 1.3646939992904663,
      "learning_rate": 0.0009120055710306407,
      "loss": 2.5743,
      "step": 3160
    },
    {
      "epoch": 0.883008356545961,
      "grad_norm": 1.5579534769058228,
      "learning_rate": 0.0009117270194986072,
      "loss": 2.5472,
      "step": 3170
    },
    {
      "epoch": 0.8857938718662952,
      "grad_norm": 2.42226243019104,
      "learning_rate": 0.0009114484679665738,
      "loss": 2.3035,
      "step": 3180
    },
    {
      "epoch": 0.8885793871866295,
      "grad_norm": 1.1674410104751587,
      "learning_rate": 0.0009111699164345404,
      "loss": 2.5894,
      "step": 3190
    },
    {
      "epoch": 0.8913649025069638,
      "grad_norm": 1.3225926160812378,
      "learning_rate": 0.000910891364902507,
      "loss": 2.249,
      "step": 3200
    },
    {
      "epoch": 0.8941504178272981,
      "grad_norm": 1.2591214179992676,
      "learning_rate": 0.0009106128133704735,
      "loss": 2.688,
      "step": 3210
    },
    {
      "epoch": 0.8969359331476323,
      "grad_norm": 0.9997440576553345,
      "learning_rate": 0.0009103342618384402,
      "loss": 2.5501,
      "step": 3220
    },
    {
      "epoch": 0.8997214484679665,
      "grad_norm": 1.4348294734954834,
      "learning_rate": 0.0009100557103064067,
      "loss": 2.4578,
      "step": 3230
    },
    {
      "epoch": 0.9025069637883009,
      "grad_norm": 1.8786579370498657,
      "learning_rate": 0.0009097771587743733,
      "loss": 2.4413,
      "step": 3240
    },
    {
      "epoch": 0.9052924791086351,
      "grad_norm": 1.268146276473999,
      "learning_rate": 0.0009094986072423399,
      "loss": 2.6339,
      "step": 3250
    },
    {
      "epoch": 0.9080779944289693,
      "grad_norm": 1.290724754333496,
      "learning_rate": 0.0009092200557103065,
      "loss": 2.5075,
      "step": 3260
    },
    {
      "epoch": 0.9108635097493036,
      "grad_norm": 1.3573336601257324,
      "learning_rate": 0.000908941504178273,
      "loss": 2.5851,
      "step": 3270
    },
    {
      "epoch": 0.9136490250696379,
      "grad_norm": 1.4669519662857056,
      "learning_rate": 0.0009086629526462395,
      "loss": 2.363,
      "step": 3280
    },
    {
      "epoch": 0.9164345403899722,
      "grad_norm": 1.8897361755371094,
      "learning_rate": 0.0009083844011142061,
      "loss": 2.5062,
      "step": 3290
    },
    {
      "epoch": 0.9192200557103064,
      "grad_norm": 1.1699302196502686,
      "learning_rate": 0.0009081058495821727,
      "loss": 2.5882,
      "step": 3300
    },
    {
      "epoch": 0.9220055710306406,
      "grad_norm": 1.0322651863098145,
      "learning_rate": 0.0009078272980501393,
      "loss": 2.3308,
      "step": 3310
    },
    {
      "epoch": 0.924791086350975,
      "grad_norm": 1.6198482513427734,
      "learning_rate": 0.0009075487465181058,
      "loss": 2.5845,
      "step": 3320
    },
    {
      "epoch": 0.9275766016713092,
      "grad_norm": 1.4595698118209839,
      "learning_rate": 0.0009072701949860725,
      "loss": 2.5231,
      "step": 3330
    },
    {
      "epoch": 0.9303621169916435,
      "grad_norm": 2.1530861854553223,
      "learning_rate": 0.0009069916434540391,
      "loss": 2.6477,
      "step": 3340
    },
    {
      "epoch": 0.9331476323119777,
      "grad_norm": 1.2015318870544434,
      "learning_rate": 0.0009067130919220056,
      "loss": 2.4752,
      "step": 3350
    },
    {
      "epoch": 0.935933147632312,
      "grad_norm": 1.059872031211853,
      "learning_rate": 0.0009064345403899722,
      "loss": 2.4365,
      "step": 3360
    },
    {
      "epoch": 0.9387186629526463,
      "grad_norm": 1.626658320426941,
      "learning_rate": 0.0009061559888579387,
      "loss": 2.4973,
      "step": 3370
    },
    {
      "epoch": 0.9415041782729805,
      "grad_norm": 2.131232738494873,
      "learning_rate": 0.0009058774373259053,
      "loss": 2.4464,
      "step": 3380
    },
    {
      "epoch": 0.9442896935933147,
      "grad_norm": 1.3092647790908813,
      "learning_rate": 0.0009055988857938718,
      "loss": 2.5117,
      "step": 3390
    },
    {
      "epoch": 0.947075208913649,
      "grad_norm": 0.9204092621803284,
      "learning_rate": 0.0009053203342618385,
      "loss": 2.4254,
      "step": 3400
    },
    {
      "epoch": 0.9498607242339833,
      "grad_norm": 1.6563459634780884,
      "learning_rate": 0.0009050417827298051,
      "loss": 2.4794,
      "step": 3410
    },
    {
      "epoch": 0.9526462395543176,
      "grad_norm": 1.1215386390686035,
      "learning_rate": 0.0009047632311977716,
      "loss": 2.6263,
      "step": 3420
    },
    {
      "epoch": 0.9554317548746518,
      "grad_norm": 1.5183919668197632,
      "learning_rate": 0.0009044846796657382,
      "loss": 2.6815,
      "step": 3430
    },
    {
      "epoch": 0.958217270194986,
      "grad_norm": 1.588484525680542,
      "learning_rate": 0.0009042061281337048,
      "loss": 2.5717,
      "step": 3440
    },
    {
      "epoch": 0.9610027855153204,
      "grad_norm": 1.6100398302078247,
      "learning_rate": 0.0009039275766016714,
      "loss": 2.4423,
      "step": 3450
    },
    {
      "epoch": 0.9637883008356546,
      "grad_norm": 1.4368380308151245,
      "learning_rate": 0.0009036490250696378,
      "loss": 2.5486,
      "step": 3460
    },
    {
      "epoch": 0.9665738161559888,
      "grad_norm": 1.3462988138198853,
      "learning_rate": 0.0009033704735376044,
      "loss": 2.8811,
      "step": 3470
    },
    {
      "epoch": 0.9693593314763231,
      "grad_norm": 2.7722113132476807,
      "learning_rate": 0.000903091922005571,
      "loss": 2.5645,
      "step": 3480
    },
    {
      "epoch": 0.9721448467966574,
      "grad_norm": 1.2402198314666748,
      "learning_rate": 0.0009028133704735376,
      "loss": 2.5775,
      "step": 3490
    },
    {
      "epoch": 0.9749303621169917,
      "grad_norm": 1.575783610343933,
      "learning_rate": 0.0009025348189415042,
      "loss": 2.4571,
      "step": 3500
    },
    {
      "epoch": 0.9777158774373259,
      "grad_norm": 1.9783412218093872,
      "learning_rate": 0.0009022562674094708,
      "loss": 2.3175,
      "step": 3510
    },
    {
      "epoch": 0.9805013927576601,
      "grad_norm": 1.3348389863967896,
      "learning_rate": 0.0009019777158774374,
      "loss": 2.5341,
      "step": 3520
    },
    {
      "epoch": 0.9832869080779945,
      "grad_norm": 1.2009682655334473,
      "learning_rate": 0.0009016991643454039,
      "loss": 2.4764,
      "step": 3530
    },
    {
      "epoch": 0.9860724233983287,
      "grad_norm": 1.5894309282302856,
      "learning_rate": 0.0009014206128133705,
      "loss": 2.4736,
      "step": 3540
    },
    {
      "epoch": 0.9888579387186629,
      "grad_norm": 1.083309531211853,
      "learning_rate": 0.0009011420612813371,
      "loss": 2.5119,
      "step": 3550
    },
    {
      "epoch": 0.9916434540389972,
      "grad_norm": 1.9603224992752075,
      "learning_rate": 0.0009008635097493037,
      "loss": 2.2679,
      "step": 3560
    },
    {
      "epoch": 0.9944289693593314,
      "grad_norm": 1.474579095840454,
      "learning_rate": 0.0009005849582172702,
      "loss": 2.529,
      "step": 3570
    },
    {
      "epoch": 0.9972144846796658,
      "grad_norm": 1.3432501554489136,
      "learning_rate": 0.0009003064066852367,
      "loss": 2.7045,
      "step": 3580
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.6508357524871826,
      "learning_rate": 0.0009000278551532034,
      "loss": 2.3741,
      "step": 3590
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.406306028366089,
      "eval_runtime": 8.5545,
      "eval_samples_per_second": 373.136,
      "eval_steps_per_second": 46.642,
      "step": 3590
    },
    {
      "epoch": 1.0027855153203342,
      "grad_norm": 1.4612566232681274,
      "learning_rate": 0.0008997493036211699,
      "loss": 2.4591,
      "step": 3600
    },
    {
      "epoch": 1.0055710306406684,
      "grad_norm": 1.2919337749481201,
      "learning_rate": 0.0008994707520891365,
      "loss": 2.4201,
      "step": 3610
    },
    {
      "epoch": 1.0083565459610029,
      "grad_norm": 1.6829274892807007,
      "learning_rate": 0.0008991922005571031,
      "loss": 2.7765,
      "step": 3620
    },
    {
      "epoch": 1.011142061281337,
      "grad_norm": 1.3170417547225952,
      "learning_rate": 0.0008989136490250697,
      "loss": 2.5798,
      "step": 3630
    },
    {
      "epoch": 1.0139275766016713,
      "grad_norm": 1.157371997833252,
      "learning_rate": 0.0008986350974930362,
      "loss": 2.4809,
      "step": 3640
    },
    {
      "epoch": 1.0167130919220055,
      "grad_norm": 1.3536180257797241,
      "learning_rate": 0.0008983565459610028,
      "loss": 2.4149,
      "step": 3650
    },
    {
      "epoch": 1.0194986072423398,
      "grad_norm": 1.1478989124298096,
      "learning_rate": 0.0008980779944289695,
      "loss": 2.4977,
      "step": 3660
    },
    {
      "epoch": 1.0222841225626742,
      "grad_norm": 1.1603747606277466,
      "learning_rate": 0.0008977994428969359,
      "loss": 2.4717,
      "step": 3670
    },
    {
      "epoch": 1.0250696378830084,
      "grad_norm": 1.632210373878479,
      "learning_rate": 0.0008975208913649025,
      "loss": 2.3034,
      "step": 3680
    },
    {
      "epoch": 1.0278551532033426,
      "grad_norm": 2.301032066345215,
      "learning_rate": 0.0008972423398328691,
      "loss": 2.5353,
      "step": 3690
    },
    {
      "epoch": 1.0306406685236769,
      "grad_norm": 1.5368854999542236,
      "learning_rate": 0.0008969637883008357,
      "loss": 2.5873,
      "step": 3700
    },
    {
      "epoch": 1.033426183844011,
      "grad_norm": 1.2738960981369019,
      "learning_rate": 0.0008966852367688022,
      "loss": 2.49,
      "step": 3710
    },
    {
      "epoch": 1.0362116991643453,
      "grad_norm": 1.8657538890838623,
      "learning_rate": 0.0008964066852367688,
      "loss": 2.3966,
      "step": 3720
    },
    {
      "epoch": 1.0389972144846797,
      "grad_norm": 2.4652726650238037,
      "learning_rate": 0.0008961281337047355,
      "loss": 2.5077,
      "step": 3730
    },
    {
      "epoch": 1.041782729805014,
      "grad_norm": 1.2079960107803345,
      "learning_rate": 0.000895849582172702,
      "loss": 2.4518,
      "step": 3740
    },
    {
      "epoch": 1.0445682451253482,
      "grad_norm": 1.5822467803955078,
      "learning_rate": 0.0008955710306406686,
      "loss": 2.3529,
      "step": 3750
    },
    {
      "epoch": 1.0473537604456824,
      "grad_norm": 1.4387261867523193,
      "learning_rate": 0.000895292479108635,
      "loss": 2.4588,
      "step": 3760
    },
    {
      "epoch": 1.0501392757660166,
      "grad_norm": 1.5031903982162476,
      "learning_rate": 0.0008950139275766017,
      "loss": 2.4766,
      "step": 3770
    },
    {
      "epoch": 1.052924791086351,
      "grad_norm": 1.5071768760681152,
      "learning_rate": 0.0008947353760445682,
      "loss": 2.4465,
      "step": 3780
    },
    {
      "epoch": 1.0557103064066853,
      "grad_norm": 2.0635712146759033,
      "learning_rate": 0.0008944568245125348,
      "loss": 2.6647,
      "step": 3790
    },
    {
      "epoch": 1.0584958217270195,
      "grad_norm": 1.1401463747024536,
      "learning_rate": 0.0008941782729805014,
      "loss": 2.3361,
      "step": 3800
    },
    {
      "epoch": 1.0612813370473537,
      "grad_norm": 1.2125036716461182,
      "learning_rate": 0.000893899721448468,
      "loss": 2.304,
      "step": 3810
    },
    {
      "epoch": 1.064066852367688,
      "grad_norm": 1.3619002103805542,
      "learning_rate": 0.0008936211699164346,
      "loss": 2.5081,
      "step": 3820
    },
    {
      "epoch": 1.0668523676880224,
      "grad_norm": 1.1637217998504639,
      "learning_rate": 0.0008933426183844011,
      "loss": 2.5704,
      "step": 3830
    },
    {
      "epoch": 1.0696378830083566,
      "grad_norm": 1.3032349348068237,
      "learning_rate": 0.0008930640668523678,
      "loss": 2.4805,
      "step": 3840
    },
    {
      "epoch": 1.0724233983286908,
      "grad_norm": 1.1482923030853271,
      "learning_rate": 0.0008927855153203343,
      "loss": 2.3806,
      "step": 3850
    },
    {
      "epoch": 1.075208913649025,
      "grad_norm": 1.4730639457702637,
      "learning_rate": 0.0008925069637883008,
      "loss": 2.6235,
      "step": 3860
    },
    {
      "epoch": 1.0779944289693593,
      "grad_norm": 1.486322045326233,
      "learning_rate": 0.0008922284122562674,
      "loss": 2.4399,
      "step": 3870
    },
    {
      "epoch": 1.0807799442896937,
      "grad_norm": 1.3524055480957031,
      "learning_rate": 0.000891949860724234,
      "loss": 2.36,
      "step": 3880
    },
    {
      "epoch": 1.083565459610028,
      "grad_norm": 1.6733726263046265,
      "learning_rate": 0.0008916713091922006,
      "loss": 2.4964,
      "step": 3890
    },
    {
      "epoch": 1.0863509749303621,
      "grad_norm": 1.3121404647827148,
      "learning_rate": 0.0008913927576601671,
      "loss": 2.5315,
      "step": 3900
    },
    {
      "epoch": 1.0891364902506964,
      "grad_norm": 1.9655492305755615,
      "learning_rate": 0.0008911142061281338,
      "loss": 2.3937,
      "step": 3910
    },
    {
      "epoch": 1.0919220055710306,
      "grad_norm": 1.5588632822036743,
      "learning_rate": 0.0008908356545961003,
      "loss": 2.5644,
      "step": 3920
    },
    {
      "epoch": 1.0947075208913648,
      "grad_norm": 1.5936354398727417,
      "learning_rate": 0.0008905571030640669,
      "loss": 2.3736,
      "step": 3930
    },
    {
      "epoch": 1.0974930362116992,
      "grad_norm": 1.463191032409668,
      "learning_rate": 0.0008902785515320334,
      "loss": 2.5791,
      "step": 3940
    },
    {
      "epoch": 1.1002785515320335,
      "grad_norm": 1.4579893350601196,
      "learning_rate": 0.0008900000000000001,
      "loss": 2.3294,
      "step": 3950
    },
    {
      "epoch": 1.1030640668523677,
      "grad_norm": 1.2750009298324585,
      "learning_rate": 0.0008897214484679665,
      "loss": 2.3095,
      "step": 3960
    },
    {
      "epoch": 1.105849582172702,
      "grad_norm": 1.2659629583358765,
      "learning_rate": 0.0008894428969359331,
      "loss": 2.3713,
      "step": 3970
    },
    {
      "epoch": 1.1086350974930361,
      "grad_norm": 1.2793024778366089,
      "learning_rate": 0.0008891643454038998,
      "loss": 2.4815,
      "step": 3980
    },
    {
      "epoch": 1.1114206128133706,
      "grad_norm": 1.5348848104476929,
      "learning_rate": 0.0008888857938718663,
      "loss": 2.3644,
      "step": 3990
    },
    {
      "epoch": 1.1142061281337048,
      "grad_norm": 1.8643014430999756,
      "learning_rate": 0.0008886072423398329,
      "loss": 2.2616,
      "step": 4000
    },
    {
      "epoch": 1.116991643454039,
      "grad_norm": 1.6432875394821167,
      "learning_rate": 0.0008883286908077994,
      "loss": 2.5874,
      "step": 4010
    },
    {
      "epoch": 1.1197771587743732,
      "grad_norm": 1.241422414779663,
      "learning_rate": 0.0008880501392757661,
      "loss": 2.5802,
      "step": 4020
    },
    {
      "epoch": 1.1225626740947074,
      "grad_norm": 1.4932185411453247,
      "learning_rate": 0.0008877715877437326,
      "loss": 2.4742,
      "step": 4030
    },
    {
      "epoch": 1.1253481894150417,
      "grad_norm": 1.326904058456421,
      "learning_rate": 0.0008874930362116992,
      "loss": 2.47,
      "step": 4040
    },
    {
      "epoch": 1.128133704735376,
      "grad_norm": 1.4641865491867065,
      "learning_rate": 0.0008872144846796659,
      "loss": 2.4871,
      "step": 4050
    },
    {
      "epoch": 1.1309192200557103,
      "grad_norm": 1.4165868759155273,
      "learning_rate": 0.0008869359331476323,
      "loss": 2.5226,
      "step": 4060
    },
    {
      "epoch": 1.1337047353760445,
      "grad_norm": 1.5237942934036255,
      "learning_rate": 0.0008866573816155989,
      "loss": 2.4413,
      "step": 4070
    },
    {
      "epoch": 1.1364902506963788,
      "grad_norm": 1.4232450723648071,
      "learning_rate": 0.0008863788300835654,
      "loss": 2.5276,
      "step": 4080
    },
    {
      "epoch": 1.1392757660167132,
      "grad_norm": 1.457831859588623,
      "learning_rate": 0.0008861002785515321,
      "loss": 2.4979,
      "step": 4090
    },
    {
      "epoch": 1.1420612813370474,
      "grad_norm": 1.1287785768508911,
      "learning_rate": 0.0008858217270194986,
      "loss": 2.5198,
      "step": 4100
    },
    {
      "epoch": 1.1448467966573816,
      "grad_norm": 2.2144477367401123,
      "learning_rate": 0.0008855431754874652,
      "loss": 2.5851,
      "step": 4110
    },
    {
      "epoch": 1.1476323119777159,
      "grad_norm": 1.1904432773590088,
      "learning_rate": 0.0008852646239554317,
      "loss": 2.4309,
      "step": 4120
    },
    {
      "epoch": 1.15041782729805,
      "grad_norm": 1.1705002784729004,
      "learning_rate": 0.0008849860724233984,
      "loss": 2.3675,
      "step": 4130
    },
    {
      "epoch": 1.1532033426183843,
      "grad_norm": 2.155735731124878,
      "learning_rate": 0.000884707520891365,
      "loss": 2.4777,
      "step": 4140
    },
    {
      "epoch": 1.1559888579387188,
      "grad_norm": 1.389912486076355,
      "learning_rate": 0.0008844289693593314,
      "loss": 2.4497,
      "step": 4150
    },
    {
      "epoch": 1.158774373259053,
      "grad_norm": 1.416368007659912,
      "learning_rate": 0.0008841504178272981,
      "loss": 2.3,
      "step": 4160
    },
    {
      "epoch": 1.1615598885793872,
      "grad_norm": 1.5125023126602173,
      "learning_rate": 0.0008838718662952646,
      "loss": 2.3537,
      "step": 4170
    },
    {
      "epoch": 1.1643454038997214,
      "grad_norm": 1.3144712448120117,
      "learning_rate": 0.0008835933147632312,
      "loss": 2.527,
      "step": 4180
    },
    {
      "epoch": 1.1671309192200556,
      "grad_norm": 1.2422128915786743,
      "learning_rate": 0.0008833147632311977,
      "loss": 2.3369,
      "step": 4190
    },
    {
      "epoch": 1.16991643454039,
      "grad_norm": 1.2286884784698486,
      "learning_rate": 0.0008830362116991644,
      "loss": 2.7417,
      "step": 4200
    },
    {
      "epoch": 1.1727019498607243,
      "grad_norm": 1.094249963760376,
      "learning_rate": 0.0008827576601671309,
      "loss": 2.3996,
      "step": 4210
    },
    {
      "epoch": 1.1754874651810585,
      "grad_norm": 1.1764394044876099,
      "learning_rate": 0.0008824791086350975,
      "loss": 2.4468,
      "step": 4220
    },
    {
      "epoch": 1.1782729805013927,
      "grad_norm": 1.2035174369812012,
      "learning_rate": 0.0008822005571030642,
      "loss": 2.3648,
      "step": 4230
    },
    {
      "epoch": 1.181058495821727,
      "grad_norm": 1.5827460289001465,
      "learning_rate": 0.0008819220055710307,
      "loss": 2.5205,
      "step": 4240
    },
    {
      "epoch": 1.1838440111420612,
      "grad_norm": 1.7215116024017334,
      "learning_rate": 0.0008816434540389973,
      "loss": 2.4784,
      "step": 4250
    },
    {
      "epoch": 1.1866295264623956,
      "grad_norm": 1.2432202100753784,
      "learning_rate": 0.0008813649025069637,
      "loss": 2.4597,
      "step": 4260
    },
    {
      "epoch": 1.1894150417827298,
      "grad_norm": 2.093928098678589,
      "learning_rate": 0.0008810863509749304,
      "loss": 2.4894,
      "step": 4270
    },
    {
      "epoch": 1.192200557103064,
      "grad_norm": 1.614795446395874,
      "learning_rate": 0.0008808077994428969,
      "loss": 2.586,
      "step": 4280
    },
    {
      "epoch": 1.1949860724233983,
      "grad_norm": 1.6697884798049927,
      "learning_rate": 0.0008805292479108635,
      "loss": 2.4378,
      "step": 4290
    },
    {
      "epoch": 1.1977715877437327,
      "grad_norm": 1.6672203540802002,
      "learning_rate": 0.0008802506963788301,
      "loss": 2.4101,
      "step": 4300
    },
    {
      "epoch": 1.200557103064067,
      "grad_norm": 1.4323503971099854,
      "learning_rate": 0.0008799721448467967,
      "loss": 2.4614,
      "step": 4310
    },
    {
      "epoch": 1.2033426183844012,
      "grad_norm": 1.6803035736083984,
      "learning_rate": 0.0008796935933147633,
      "loss": 2.4764,
      "step": 4320
    },
    {
      "epoch": 1.2061281337047354,
      "grad_norm": 3.0955936908721924,
      "learning_rate": 0.0008794150417827298,
      "loss": 2.4491,
      "step": 4330
    },
    {
      "epoch": 1.2089136490250696,
      "grad_norm": 1.44463050365448,
      "learning_rate": 0.0008791364902506965,
      "loss": 2.5742,
      "step": 4340
    },
    {
      "epoch": 1.2116991643454038,
      "grad_norm": 1.6345651149749756,
      "learning_rate": 0.000878857938718663,
      "loss": 2.3708,
      "step": 4350
    },
    {
      "epoch": 1.2144846796657383,
      "grad_norm": 1.8529552221298218,
      "learning_rate": 0.0008785793871866295,
      "loss": 2.5324,
      "step": 4360
    },
    {
      "epoch": 1.2172701949860725,
      "grad_norm": 1.1213207244873047,
      "learning_rate": 0.000878300835654596,
      "loss": 2.6369,
      "step": 4370
    },
    {
      "epoch": 1.2200557103064067,
      "grad_norm": 1.5448205471038818,
      "learning_rate": 0.0008780222841225627,
      "loss": 2.5796,
      "step": 4380
    },
    {
      "epoch": 1.222841225626741,
      "grad_norm": 1.3022407293319702,
      "learning_rate": 0.0008777437325905293,
      "loss": 2.4645,
      "step": 4390
    },
    {
      "epoch": 1.2256267409470751,
      "grad_norm": 1.5589473247528076,
      "learning_rate": 0.0008774651810584958,
      "loss": 2.3262,
      "step": 4400
    },
    {
      "epoch": 1.2284122562674096,
      "grad_norm": 1.459229826927185,
      "learning_rate": 0.0008771866295264625,
      "loss": 2.296,
      "step": 4410
    },
    {
      "epoch": 1.2311977715877438,
      "grad_norm": 1.064813494682312,
      "learning_rate": 0.000876908077994429,
      "loss": 2.2014,
      "step": 4420
    },
    {
      "epoch": 1.233983286908078,
      "grad_norm": 1.5980842113494873,
      "learning_rate": 0.0008766295264623956,
      "loss": 2.3131,
      "step": 4430
    },
    {
      "epoch": 1.2367688022284122,
      "grad_norm": 1.0553364753723145,
      "learning_rate": 0.000876350974930362,
      "loss": 2.4306,
      "step": 4440
    },
    {
      "epoch": 1.2395543175487465,
      "grad_norm": 1.982969045639038,
      "learning_rate": 0.0008760724233983288,
      "loss": 2.463,
      "step": 4450
    },
    {
      "epoch": 1.2423398328690807,
      "grad_norm": 1.407447338104248,
      "learning_rate": 0.0008757938718662953,
      "loss": 2.2746,
      "step": 4460
    },
    {
      "epoch": 1.2451253481894151,
      "grad_norm": 1.558633804321289,
      "learning_rate": 0.0008755153203342618,
      "loss": 2.5858,
      "step": 4470
    },
    {
      "epoch": 1.2479108635097493,
      "grad_norm": 1.6369287967681885,
      "learning_rate": 0.0008752367688022284,
      "loss": 2.3685,
      "step": 4480
    },
    {
      "epoch": 1.2506963788300836,
      "grad_norm": 1.4967803955078125,
      "learning_rate": 0.000874958217270195,
      "loss": 2.4378,
      "step": 4490
    },
    {
      "epoch": 1.2534818941504178,
      "grad_norm": 1.395831823348999,
      "learning_rate": 0.0008746796657381616,
      "loss": 2.58,
      "step": 4500
    },
    {
      "epoch": 1.2562674094707522,
      "grad_norm": 1.3929535150527954,
      "learning_rate": 0.0008744011142061281,
      "loss": 2.2813,
      "step": 4510
    },
    {
      "epoch": 1.2590529247910864,
      "grad_norm": 0.9952377080917358,
      "learning_rate": 0.0008741225626740948,
      "loss": 2.41,
      "step": 4520
    },
    {
      "epoch": 1.2618384401114207,
      "grad_norm": 1.4244884252548218,
      "learning_rate": 0.0008738440111420613,
      "loss": 2.6545,
      "step": 4530
    },
    {
      "epoch": 1.2646239554317549,
      "grad_norm": 1.4346792697906494,
      "learning_rate": 0.0008735654596100279,
      "loss": 2.3372,
      "step": 4540
    },
    {
      "epoch": 1.267409470752089,
      "grad_norm": 1.216755986213684,
      "learning_rate": 0.0008732869080779944,
      "loss": 2.2982,
      "step": 4550
    },
    {
      "epoch": 1.2701949860724233,
      "grad_norm": 1.8581485748291016,
      "learning_rate": 0.000873008356545961,
      "loss": 2.6007,
      "step": 4560
    },
    {
      "epoch": 1.2729805013927575,
      "grad_norm": 1.3053561449050903,
      "learning_rate": 0.0008727298050139276,
      "loss": 2.5728,
      "step": 4570
    },
    {
      "epoch": 1.275766016713092,
      "grad_norm": 1.5136610269546509,
      "learning_rate": 0.0008724512534818941,
      "loss": 2.5635,
      "step": 4580
    },
    {
      "epoch": 1.2785515320334262,
      "grad_norm": 1.8121294975280762,
      "learning_rate": 0.0008721727019498608,
      "loss": 2.4537,
      "step": 4590
    },
    {
      "epoch": 1.2813370473537604,
      "grad_norm": 1.2781909704208374,
      "learning_rate": 0.0008718941504178273,
      "loss": 2.2631,
      "step": 4600
    },
    {
      "epoch": 1.2841225626740946,
      "grad_norm": 1.277390480041504,
      "learning_rate": 0.0008716155988857939,
      "loss": 2.2568,
      "step": 4610
    },
    {
      "epoch": 1.286908077994429,
      "grad_norm": 1.217903971672058,
      "learning_rate": 0.0008713370473537605,
      "loss": 2.4022,
      "step": 4620
    },
    {
      "epoch": 1.2896935933147633,
      "grad_norm": 1.4290454387664795,
      "learning_rate": 0.0008710584958217271,
      "loss": 2.4514,
      "step": 4630
    },
    {
      "epoch": 1.2924791086350975,
      "grad_norm": 1.6049617528915405,
      "learning_rate": 0.0008707799442896937,
      "loss": 2.5207,
      "step": 4640
    },
    {
      "epoch": 1.2952646239554317,
      "grad_norm": 2.6267402172088623,
      "learning_rate": 0.0008705013927576601,
      "loss": 2.5118,
      "step": 4650
    },
    {
      "epoch": 1.298050139275766,
      "grad_norm": 1.4410964250564575,
      "learning_rate": 0.0008702228412256267,
      "loss": 2.3168,
      "step": 4660
    },
    {
      "epoch": 1.3008356545961002,
      "grad_norm": 1.5928828716278076,
      "learning_rate": 0.0008699442896935933,
      "loss": 2.4933,
      "step": 4670
    },
    {
      "epoch": 1.3036211699164346,
      "grad_norm": 1.5655015707015991,
      "learning_rate": 0.0008696657381615599,
      "loss": 2.5487,
      "step": 4680
    },
    {
      "epoch": 1.3064066852367688,
      "grad_norm": 1.595541000366211,
      "learning_rate": 0.0008693871866295264,
      "loss": 2.7002,
      "step": 4690
    },
    {
      "epoch": 1.309192200557103,
      "grad_norm": 1.1704699993133545,
      "learning_rate": 0.0008691086350974931,
      "loss": 2.5207,
      "step": 4700
    },
    {
      "epoch": 1.3119777158774373,
      "grad_norm": 1.456167459487915,
      "learning_rate": 0.0008688300835654597,
      "loss": 2.2512,
      "step": 4710
    },
    {
      "epoch": 1.3147632311977717,
      "grad_norm": 1.5502310991287231,
      "learning_rate": 0.0008685515320334262,
      "loss": 2.3478,
      "step": 4720
    },
    {
      "epoch": 1.317548746518106,
      "grad_norm": 1.6640392541885376,
      "learning_rate": 0.0008682729805013928,
      "loss": 2.5919,
      "step": 4730
    },
    {
      "epoch": 1.3203342618384402,
      "grad_norm": 2.0881025791168213,
      "learning_rate": 0.0008679944289693594,
      "loss": 2.4603,
      "step": 4740
    },
    {
      "epoch": 1.3231197771587744,
      "grad_norm": 1.3277233839035034,
      "learning_rate": 0.0008677158774373259,
      "loss": 2.7463,
      "step": 4750
    },
    {
      "epoch": 1.3259052924791086,
      "grad_norm": 1.3412487506866455,
      "learning_rate": 0.0008674373259052924,
      "loss": 2.51,
      "step": 4760
    },
    {
      "epoch": 1.3286908077994428,
      "grad_norm": 1.426458716392517,
      "learning_rate": 0.0008671587743732591,
      "loss": 2.5636,
      "step": 4770
    },
    {
      "epoch": 1.331476323119777,
      "grad_norm": 1.9711438417434692,
      "learning_rate": 0.0008668802228412257,
      "loss": 2.622,
      "step": 4780
    },
    {
      "epoch": 1.3342618384401115,
      "grad_norm": 1.4340852499008179,
      "learning_rate": 0.0008666016713091922,
      "loss": 2.4166,
      "step": 4790
    },
    {
      "epoch": 1.3370473537604457,
      "grad_norm": 1.8819503784179688,
      "learning_rate": 0.0008663231197771588,
      "loss": 2.5511,
      "step": 4800
    },
    {
      "epoch": 1.33983286908078,
      "grad_norm": 1.7910012006759644,
      "learning_rate": 0.0008660445682451254,
      "loss": 2.2883,
      "step": 4810
    },
    {
      "epoch": 1.3426183844011141,
      "grad_norm": 1.11577308177948,
      "learning_rate": 0.000865766016713092,
      "loss": 2.1784,
      "step": 4820
    },
    {
      "epoch": 1.3454038997214486,
      "grad_norm": 1.2146192789077759,
      "learning_rate": 0.0008654874651810585,
      "loss": 2.2619,
      "step": 4830
    },
    {
      "epoch": 1.3481894150417828,
      "grad_norm": 1.162723183631897,
      "learning_rate": 0.000865208913649025,
      "loss": 2.4557,
      "step": 4840
    },
    {
      "epoch": 1.350974930362117,
      "grad_norm": 1.2347052097320557,
      "learning_rate": 0.0008649303621169916,
      "loss": 2.7314,
      "step": 4850
    },
    {
      "epoch": 1.3537604456824512,
      "grad_norm": 1.4150409698486328,
      "learning_rate": 0.0008646518105849582,
      "loss": 2.4856,
      "step": 4860
    },
    {
      "epoch": 1.3565459610027855,
      "grad_norm": 1.5935486555099487,
      "learning_rate": 0.0008643732590529248,
      "loss": 2.3712,
      "step": 4870
    },
    {
      "epoch": 1.3593314763231197,
      "grad_norm": 1.191989541053772,
      "learning_rate": 0.0008640947075208914,
      "loss": 2.3328,
      "step": 4880
    },
    {
      "epoch": 1.362116991643454,
      "grad_norm": 1.498227596282959,
      "learning_rate": 0.000863816155988858,
      "loss": 2.234,
      "step": 4890
    },
    {
      "epoch": 1.3649025069637883,
      "grad_norm": 1.453818678855896,
      "learning_rate": 0.0008635376044568245,
      "loss": 2.4424,
      "step": 4900
    },
    {
      "epoch": 1.3676880222841226,
      "grad_norm": 1.4252535104751587,
      "learning_rate": 0.0008632590529247911,
      "loss": 2.4066,
      "step": 4910
    },
    {
      "epoch": 1.3704735376044568,
      "grad_norm": 1.3077820539474487,
      "learning_rate": 0.0008629805013927577,
      "loss": 2.567,
      "step": 4920
    },
    {
      "epoch": 1.3732590529247912,
      "grad_norm": 1.5380414724349976,
      "learning_rate": 0.0008627019498607243,
      "loss": 2.3976,
      "step": 4930
    },
    {
      "epoch": 1.3760445682451254,
      "grad_norm": 1.2236677408218384,
      "learning_rate": 0.0008624233983286909,
      "loss": 2.5227,
      "step": 4940
    },
    {
      "epoch": 1.3788300835654597,
      "grad_norm": 1.347116231918335,
      "learning_rate": 0.0008621448467966574,
      "loss": 2.4053,
      "step": 4950
    },
    {
      "epoch": 1.3816155988857939,
      "grad_norm": 1.5117372274398804,
      "learning_rate": 0.000861866295264624,
      "loss": 2.4355,
      "step": 4960
    },
    {
      "epoch": 1.384401114206128,
      "grad_norm": 1.423092246055603,
      "learning_rate": 0.0008615877437325905,
      "loss": 2.4882,
      "step": 4970
    },
    {
      "epoch": 1.3871866295264623,
      "grad_norm": 1.3323116302490234,
      "learning_rate": 0.0008613091922005571,
      "loss": 2.4307,
      "step": 4980
    },
    {
      "epoch": 1.3899721448467965,
      "grad_norm": 2.0638983249664307,
      "learning_rate": 0.0008610306406685237,
      "loss": 2.5537,
      "step": 4990
    },
    {
      "epoch": 1.392757660167131,
      "grad_norm": 2.0708186626434326,
      "learning_rate": 0.0008607520891364903,
      "loss": 2.3708,
      "step": 5000
    },
    {
      "epoch": 1.3955431754874652,
      "grad_norm": 2.9912097454071045,
      "learning_rate": 0.0008604735376044568,
      "loss": 2.6424,
      "step": 5010
    },
    {
      "epoch": 1.3983286908077994,
      "grad_norm": 1.644172191619873,
      "learning_rate": 0.0008601949860724234,
      "loss": 2.4277,
      "step": 5020
    },
    {
      "epoch": 1.4011142061281336,
      "grad_norm": 1.307828426361084,
      "learning_rate": 0.0008599164345403901,
      "loss": 2.487,
      "step": 5030
    },
    {
      "epoch": 1.403899721448468,
      "grad_norm": 1.4547945261001587,
      "learning_rate": 0.0008596378830083565,
      "loss": 2.4201,
      "step": 5040
    },
    {
      "epoch": 1.4066852367688023,
      "grad_norm": 1.5435593128204346,
      "learning_rate": 0.0008593593314763231,
      "loss": 2.5642,
      "step": 5050
    },
    {
      "epoch": 1.4094707520891365,
      "grad_norm": 1.1553717851638794,
      "learning_rate": 0.0008590807799442897,
      "loss": 2.3856,
      "step": 5060
    },
    {
      "epoch": 1.4122562674094707,
      "grad_norm": 1.2565151453018188,
      "learning_rate": 0.0008588022284122563,
      "loss": 2.4595,
      "step": 5070
    },
    {
      "epoch": 1.415041782729805,
      "grad_norm": 1.3578641414642334,
      "learning_rate": 0.0008585236768802228,
      "loss": 2.255,
      "step": 5080
    },
    {
      "epoch": 1.4178272980501392,
      "grad_norm": 1.6320995092391968,
      "learning_rate": 0.0008582451253481894,
      "loss": 2.5573,
      "step": 5090
    },
    {
      "epoch": 1.4206128133704734,
      "grad_norm": 1.83077871799469,
      "learning_rate": 0.0008579665738161561,
      "loss": 2.4367,
      "step": 5100
    },
    {
      "epoch": 1.4233983286908078,
      "grad_norm": 1.336504340171814,
      "learning_rate": 0.0008576880222841226,
      "loss": 2.3728,
      "step": 5110
    },
    {
      "epoch": 1.426183844011142,
      "grad_norm": 1.3290684223175049,
      "learning_rate": 0.0008574094707520892,
      "loss": 2.5579,
      "step": 5120
    },
    {
      "epoch": 1.4289693593314763,
      "grad_norm": 1.5716007947921753,
      "learning_rate": 0.0008571309192200557,
      "loss": 2.4424,
      "step": 5130
    },
    {
      "epoch": 1.4317548746518105,
      "grad_norm": 2.309199810028076,
      "learning_rate": 0.0008568523676880224,
      "loss": 2.3258,
      "step": 5140
    },
    {
      "epoch": 1.434540389972145,
      "grad_norm": 2.0193002223968506,
      "learning_rate": 0.0008565738161559888,
      "loss": 2.5223,
      "step": 5150
    },
    {
      "epoch": 1.4373259052924792,
      "grad_norm": 1.114368200302124,
      "learning_rate": 0.0008562952646239554,
      "loss": 2.3876,
      "step": 5160
    },
    {
      "epoch": 1.4401114206128134,
      "grad_norm": 1.0269701480865479,
      "learning_rate": 0.000856016713091922,
      "loss": 2.511,
      "step": 5170
    },
    {
      "epoch": 1.4428969359331476,
      "grad_norm": 1.3479105234146118,
      "learning_rate": 0.0008557381615598886,
      "loss": 2.7617,
      "step": 5180
    },
    {
      "epoch": 1.4456824512534818,
      "grad_norm": 1.152579426765442,
      "learning_rate": 0.0008554596100278552,
      "loss": 2.4129,
      "step": 5190
    },
    {
      "epoch": 1.448467966573816,
      "grad_norm": 1.9077376127243042,
      "learning_rate": 0.0008551810584958217,
      "loss": 2.4303,
      "step": 5200
    },
    {
      "epoch": 1.4512534818941505,
      "grad_norm": 1.5234146118164062,
      "learning_rate": 0.0008549025069637884,
      "loss": 2.4073,
      "step": 5210
    },
    {
      "epoch": 1.4540389972144847,
      "grad_norm": 1.3769124746322632,
      "learning_rate": 0.0008546239554317549,
      "loss": 2.6313,
      "step": 5220
    },
    {
      "epoch": 1.456824512534819,
      "grad_norm": 2.118243932723999,
      "learning_rate": 0.0008543454038997215,
      "loss": 2.4816,
      "step": 5230
    },
    {
      "epoch": 1.4596100278551531,
      "grad_norm": 1.343883752822876,
      "learning_rate": 0.000854066852367688,
      "loss": 2.4175,
      "step": 5240
    },
    {
      "epoch": 1.4623955431754876,
      "grad_norm": 1.85471773147583,
      "learning_rate": 0.0008537883008356546,
      "loss": 2.4087,
      "step": 5250
    },
    {
      "epoch": 1.4651810584958218,
      "grad_norm": 1.2788963317871094,
      "learning_rate": 0.0008535097493036212,
      "loss": 2.4769,
      "step": 5260
    },
    {
      "epoch": 1.467966573816156,
      "grad_norm": 1.537280559539795,
      "learning_rate": 0.0008532311977715877,
      "loss": 2.4658,
      "step": 5270
    },
    {
      "epoch": 1.4707520891364902,
      "grad_norm": 1.3490735292434692,
      "learning_rate": 0.0008529526462395544,
      "loss": 2.5781,
      "step": 5280
    },
    {
      "epoch": 1.4735376044568245,
      "grad_norm": 1.401794195175171,
      "learning_rate": 0.0008526740947075209,
      "loss": 2.4965,
      "step": 5290
    },
    {
      "epoch": 1.4763231197771587,
      "grad_norm": 1.2649232149124146,
      "learning_rate": 0.0008523955431754875,
      "loss": 2.416,
      "step": 5300
    },
    {
      "epoch": 1.479108635097493,
      "grad_norm": 1.411764144897461,
      "learning_rate": 0.000852116991643454,
      "loss": 2.448,
      "step": 5310
    },
    {
      "epoch": 1.4818941504178273,
      "grad_norm": 1.9823192358016968,
      "learning_rate": 0.0008518384401114207,
      "loss": 2.2413,
      "step": 5320
    },
    {
      "epoch": 1.4846796657381616,
      "grad_norm": 1.3081318140029907,
      "learning_rate": 0.0008515598885793872,
      "loss": 2.3889,
      "step": 5330
    },
    {
      "epoch": 1.4874651810584958,
      "grad_norm": 1.4099092483520508,
      "learning_rate": 0.0008512813370473537,
      "loss": 2.6169,
      "step": 5340
    },
    {
      "epoch": 1.49025069637883,
      "grad_norm": 1.5063430070877075,
      "learning_rate": 0.0008510027855153204,
      "loss": 2.533,
      "step": 5350
    },
    {
      "epoch": 1.4930362116991645,
      "grad_norm": 1.7313750982284546,
      "learning_rate": 0.0008507242339832869,
      "loss": 2.4669,
      "step": 5360
    },
    {
      "epoch": 1.4958217270194987,
      "grad_norm": 1.896119475364685,
      "learning_rate": 0.0008504456824512535,
      "loss": 2.6384,
      "step": 5370
    },
    {
      "epoch": 1.498607242339833,
      "grad_norm": 1.1091203689575195,
      "learning_rate": 0.00085016713091922,
      "loss": 2.5833,
      "step": 5380
    },
    {
      "epoch": 1.501392757660167,
      "grad_norm": 1.5639678239822388,
      "learning_rate": 0.0008498885793871867,
      "loss": 2.6135,
      "step": 5390
    },
    {
      "epoch": 1.5041782729805013,
      "grad_norm": 1.9277924299240112,
      "learning_rate": 0.0008496100278551532,
      "loss": 2.4559,
      "step": 5400
    },
    {
      "epoch": 1.5069637883008355,
      "grad_norm": 1.4500378370285034,
      "learning_rate": 0.0008493314763231198,
      "loss": 2.6439,
      "step": 5410
    },
    {
      "epoch": 1.5097493036211698,
      "grad_norm": 1.9656790494918823,
      "learning_rate": 0.0008490529247910865,
      "loss": 2.4827,
      "step": 5420
    },
    {
      "epoch": 1.5125348189415042,
      "grad_norm": 1.4778428077697754,
      "learning_rate": 0.000848774373259053,
      "loss": 2.325,
      "step": 5430
    },
    {
      "epoch": 1.5153203342618384,
      "grad_norm": 1.4972937107086182,
      "learning_rate": 0.0008484958217270195,
      "loss": 2.5825,
      "step": 5440
    },
    {
      "epoch": 1.5181058495821727,
      "grad_norm": 1.6604104042053223,
      "learning_rate": 0.000848217270194986,
      "loss": 2.5202,
      "step": 5450
    },
    {
      "epoch": 1.520891364902507,
      "grad_norm": 2.7263386249542236,
      "learning_rate": 0.0008479387186629527,
      "loss": 2.5038,
      "step": 5460
    },
    {
      "epoch": 1.5236768802228413,
      "grad_norm": 1.2281723022460938,
      "learning_rate": 0.0008476601671309192,
      "loss": 2.5018,
      "step": 5470
    },
    {
      "epoch": 1.5264623955431755,
      "grad_norm": 2.7635321617126465,
      "learning_rate": 0.0008473816155988858,
      "loss": 2.609,
      "step": 5480
    },
    {
      "epoch": 1.5292479108635098,
      "grad_norm": 1.3000730276107788,
      "learning_rate": 0.0008471030640668523,
      "loss": 2.3773,
      "step": 5490
    },
    {
      "epoch": 1.532033426183844,
      "grad_norm": 1.1592458486557007,
      "learning_rate": 0.000846824512534819,
      "loss": 2.3032,
      "step": 5500
    },
    {
      "epoch": 1.5348189415041782,
      "grad_norm": 1.5116746425628662,
      "learning_rate": 0.0008465459610027856,
      "loss": 2.3355,
      "step": 5510
    },
    {
      "epoch": 1.5376044568245124,
      "grad_norm": 1.6036648750305176,
      "learning_rate": 0.0008462674094707521,
      "loss": 2.3178,
      "step": 5520
    },
    {
      "epoch": 1.5403899721448466,
      "grad_norm": 1.2953124046325684,
      "learning_rate": 0.0008459888579387188,
      "loss": 2.5132,
      "step": 5530
    },
    {
      "epoch": 1.543175487465181,
      "grad_norm": 1.2321703433990479,
      "learning_rate": 0.0008457103064066852,
      "loss": 2.3899,
      "step": 5540
    },
    {
      "epoch": 1.5459610027855153,
      "grad_norm": 1.2665367126464844,
      "learning_rate": 0.0008454317548746518,
      "loss": 2.5171,
      "step": 5550
    },
    {
      "epoch": 1.5487465181058497,
      "grad_norm": 5.491153717041016,
      "learning_rate": 0.0008451532033426183,
      "loss": 2.3692,
      "step": 5560
    },
    {
      "epoch": 1.551532033426184,
      "grad_norm": 1.9508066177368164,
      "learning_rate": 0.000844874651810585,
      "loss": 2.0581,
      "step": 5570
    },
    {
      "epoch": 1.5543175487465182,
      "grad_norm": 1.3831769227981567,
      "learning_rate": 0.0008445961002785516,
      "loss": 2.5829,
      "step": 5580
    },
    {
      "epoch": 1.5571030640668524,
      "grad_norm": 1.1479387283325195,
      "learning_rate": 0.0008443175487465181,
      "loss": 2.3515,
      "step": 5590
    },
    {
      "epoch": 1.5598885793871866,
      "grad_norm": 1.3690439462661743,
      "learning_rate": 0.0008440389972144848,
      "loss": 2.4492,
      "step": 5600
    },
    {
      "epoch": 1.5626740947075208,
      "grad_norm": 1.3672510385513306,
      "learning_rate": 0.0008437604456824513,
      "loss": 2.3337,
      "step": 5610
    },
    {
      "epoch": 1.565459610027855,
      "grad_norm": 1.2372057437896729,
      "learning_rate": 0.0008434818941504179,
      "loss": 2.4582,
      "step": 5620
    },
    {
      "epoch": 1.5682451253481893,
      "grad_norm": 1.0453547239303589,
      "learning_rate": 0.0008432033426183843,
      "loss": 2.3788,
      "step": 5630
    },
    {
      "epoch": 1.5710306406685237,
      "grad_norm": 1.5158696174621582,
      "learning_rate": 0.000842924791086351,
      "loss": 2.3808,
      "step": 5640
    },
    {
      "epoch": 1.573816155988858,
      "grad_norm": 1.2887293100357056,
      "learning_rate": 0.0008426462395543175,
      "loss": 2.3433,
      "step": 5650
    },
    {
      "epoch": 1.5766016713091922,
      "grad_norm": 1.4751222133636475,
      "learning_rate": 0.0008423676880222841,
      "loss": 2.5571,
      "step": 5660
    },
    {
      "epoch": 1.5793871866295266,
      "grad_norm": 1.2774983644485474,
      "learning_rate": 0.0008420891364902507,
      "loss": 2.7037,
      "step": 5670
    },
    {
      "epoch": 1.5821727019498608,
      "grad_norm": 1.666767954826355,
      "learning_rate": 0.0008418105849582173,
      "loss": 2.6777,
      "step": 5680
    },
    {
      "epoch": 1.584958217270195,
      "grad_norm": 1.2695374488830566,
      "learning_rate": 0.0008415320334261839,
      "loss": 2.3811,
      "step": 5690
    },
    {
      "epoch": 1.5877437325905293,
      "grad_norm": 1.2878268957138062,
      "learning_rate": 0.0008412534818941504,
      "loss": 2.4794,
      "step": 5700
    },
    {
      "epoch": 1.5905292479108635,
      "grad_norm": 1.232753038406372,
      "learning_rate": 0.0008409749303621171,
      "loss": 2.3441,
      "step": 5710
    },
    {
      "epoch": 1.5933147632311977,
      "grad_norm": 7.615690231323242,
      "learning_rate": 0.0008406963788300836,
      "loss": 2.5287,
      "step": 5720
    },
    {
      "epoch": 1.596100278551532,
      "grad_norm": 1.1922414302825928,
      "learning_rate": 0.0008404178272980501,
      "loss": 2.5361,
      "step": 5730
    },
    {
      "epoch": 1.5988857938718661,
      "grad_norm": 6.901677131652832,
      "learning_rate": 0.0008401392757660166,
      "loss": 2.4592,
      "step": 5740
    },
    {
      "epoch": 1.6016713091922006,
      "grad_norm": 1.0901975631713867,
      "learning_rate": 0.0008398607242339833,
      "loss": 2.4945,
      "step": 5750
    },
    {
      "epoch": 1.6044568245125348,
      "grad_norm": 1.0598312616348267,
      "learning_rate": 0.0008395821727019499,
      "loss": 2.3916,
      "step": 5760
    },
    {
      "epoch": 1.6072423398328692,
      "grad_norm": 1.4701067209243774,
      "learning_rate": 0.0008393036211699164,
      "loss": 2.2952,
      "step": 5770
    },
    {
      "epoch": 1.6100278551532035,
      "grad_norm": 1.685010313987732,
      "learning_rate": 0.0008390250696378831,
      "loss": 2.3241,
      "step": 5780
    },
    {
      "epoch": 1.6128133704735377,
      "grad_norm": 1.1402126550674438,
      "learning_rate": 0.0008387465181058496,
      "loss": 2.4277,
      "step": 5790
    },
    {
      "epoch": 1.615598885793872,
      "grad_norm": 1.5091774463653564,
      "learning_rate": 0.0008384679665738162,
      "loss": 2.3327,
      "step": 5800
    },
    {
      "epoch": 1.6183844011142061,
      "grad_norm": 1.7020095586776733,
      "learning_rate": 0.0008381894150417827,
      "loss": 2.604,
      "step": 5810
    },
    {
      "epoch": 1.6211699164345403,
      "grad_norm": 1.1630966663360596,
      "learning_rate": 0.0008379108635097494,
      "loss": 2.6356,
      "step": 5820
    },
    {
      "epoch": 1.6239554317548746,
      "grad_norm": 1.4947725534439087,
      "learning_rate": 0.000837632311977716,
      "loss": 2.5718,
      "step": 5830
    },
    {
      "epoch": 1.6267409470752088,
      "grad_norm": 1.3917486667633057,
      "learning_rate": 0.0008373537604456824,
      "loss": 2.5618,
      "step": 5840
    },
    {
      "epoch": 1.6295264623955432,
      "grad_norm": 1.2452794313430786,
      "learning_rate": 0.000837075208913649,
      "loss": 2.4527,
      "step": 5850
    },
    {
      "epoch": 1.6323119777158774,
      "grad_norm": 1.2787998914718628,
      "learning_rate": 0.0008367966573816156,
      "loss": 2.4076,
      "step": 5860
    },
    {
      "epoch": 1.6350974930362117,
      "grad_norm": 1.579384207725525,
      "learning_rate": 0.0008365181058495822,
      "loss": 2.3852,
      "step": 5870
    },
    {
      "epoch": 1.637883008356546,
      "grad_norm": 1.3344780206680298,
      "learning_rate": 0.0008362395543175487,
      "loss": 2.391,
      "step": 5880
    },
    {
      "epoch": 1.6406685236768803,
      "grad_norm": 1.2701056003570557,
      "learning_rate": 0.0008359610027855154,
      "loss": 2.3682,
      "step": 5890
    },
    {
      "epoch": 1.6434540389972145,
      "grad_norm": 2.0189638137817383,
      "learning_rate": 0.0008356824512534819,
      "loss": 2.5332,
      "step": 5900
    },
    {
      "epoch": 1.6462395543175488,
      "grad_norm": 1.1444721221923828,
      "learning_rate": 0.0008354038997214485,
      "loss": 2.3393,
      "step": 5910
    },
    {
      "epoch": 1.649025069637883,
      "grad_norm": 1.35390305519104,
      "learning_rate": 0.0008351253481894151,
      "loss": 2.4116,
      "step": 5920
    },
    {
      "epoch": 1.6518105849582172,
      "grad_norm": 1.315775990486145,
      "learning_rate": 0.0008348467966573816,
      "loss": 2.4844,
      "step": 5930
    },
    {
      "epoch": 1.6545961002785514,
      "grad_norm": 1.342215657234192,
      "learning_rate": 0.0008345682451253482,
      "loss": 2.3155,
      "step": 5940
    },
    {
      "epoch": 1.6573816155988856,
      "grad_norm": 1.4621185064315796,
      "learning_rate": 0.0008342896935933147,
      "loss": 2.4632,
      "step": 5950
    },
    {
      "epoch": 1.66016713091922,
      "grad_norm": 1.560271978378296,
      "learning_rate": 0.0008340111420612814,
      "loss": 2.4635,
      "step": 5960
    },
    {
      "epoch": 1.6629526462395543,
      "grad_norm": 1.2337414026260376,
      "learning_rate": 0.0008337325905292479,
      "loss": 2.3149,
      "step": 5970
    },
    {
      "epoch": 1.6657381615598887,
      "grad_norm": 1.6877257823944092,
      "learning_rate": 0.0008334540389972145,
      "loss": 2.2842,
      "step": 5980
    },
    {
      "epoch": 1.668523676880223,
      "grad_norm": 1.5340789556503296,
      "learning_rate": 0.0008331754874651811,
      "loss": 2.4816,
      "step": 5990
    },
    {
      "epoch": 1.6713091922005572,
      "grad_norm": 1.0539878606796265,
      "learning_rate": 0.0008328969359331477,
      "loss": 2.3371,
      "step": 6000
    },
    {
      "epoch": 1.6740947075208914,
      "grad_norm": 1.2730919122695923,
      "learning_rate": 0.0008326183844011143,
      "loss": 2.3748,
      "step": 6010
    },
    {
      "epoch": 1.6768802228412256,
      "grad_norm": 1.3607196807861328,
      "learning_rate": 0.0008323398328690808,
      "loss": 2.4295,
      "step": 6020
    },
    {
      "epoch": 1.6796657381615598,
      "grad_norm": 1.3761805295944214,
      "learning_rate": 0.0008320612813370473,
      "loss": 2.7483,
      "step": 6030
    },
    {
      "epoch": 1.682451253481894,
      "grad_norm": 1.4564507007598877,
      "learning_rate": 0.0008317827298050139,
      "loss": 2.3211,
      "step": 6040
    },
    {
      "epoch": 1.6852367688022283,
      "grad_norm": 1.2749069929122925,
      "learning_rate": 0.0008315041782729805,
      "loss": 2.4103,
      "step": 6050
    },
    {
      "epoch": 1.6880222841225627,
      "grad_norm": 1.183241367340088,
      "learning_rate": 0.000831225626740947,
      "loss": 2.4233,
      "step": 6060
    },
    {
      "epoch": 1.690807799442897,
      "grad_norm": 1.3602735996246338,
      "learning_rate": 0.0008309470752089137,
      "loss": 2.5632,
      "step": 6070
    },
    {
      "epoch": 1.6935933147632312,
      "grad_norm": 2.0438053607940674,
      "learning_rate": 0.0008306685236768803,
      "loss": 2.3983,
      "step": 6080
    },
    {
      "epoch": 1.6963788300835656,
      "grad_norm": 0.9943479895591736,
      "learning_rate": 0.0008303899721448468,
      "loss": 2.536,
      "step": 6090
    },
    {
      "epoch": 1.6991643454038998,
      "grad_norm": 1.4198260307312012,
      "learning_rate": 0.0008301114206128134,
      "loss": 2.6579,
      "step": 6100
    },
    {
      "epoch": 1.701949860724234,
      "grad_norm": 1.9250845909118652,
      "learning_rate": 0.00082983286908078,
      "loss": 2.2715,
      "step": 6110
    },
    {
      "epoch": 1.7047353760445683,
      "grad_norm": 1.8493574857711792,
      "learning_rate": 0.0008295543175487466,
      "loss": 2.4315,
      "step": 6120
    },
    {
      "epoch": 1.7075208913649025,
      "grad_norm": 1.0615828037261963,
      "learning_rate": 0.000829275766016713,
      "loss": 2.3803,
      "step": 6130
    },
    {
      "epoch": 1.7103064066852367,
      "grad_norm": 1.4312912225723267,
      "learning_rate": 0.0008289972144846797,
      "loss": 2.2793,
      "step": 6140
    },
    {
      "epoch": 1.713091922005571,
      "grad_norm": 1.0620651245117188,
      "learning_rate": 0.0008287186629526463,
      "loss": 2.4466,
      "step": 6150
    },
    {
      "epoch": 1.7158774373259051,
      "grad_norm": 1.7495641708374023,
      "learning_rate": 0.0008284401114206128,
      "loss": 2.4945,
      "step": 6160
    },
    {
      "epoch": 1.7186629526462396,
      "grad_norm": 1.5299077033996582,
      "learning_rate": 0.0008281615598885794,
      "loss": 2.3678,
      "step": 6170
    },
    {
      "epoch": 1.7214484679665738,
      "grad_norm": 1.3435090780258179,
      "learning_rate": 0.000827883008356546,
      "loss": 2.3959,
      "step": 6180
    },
    {
      "epoch": 1.724233983286908,
      "grad_norm": 1.240332007408142,
      "learning_rate": 0.0008276044568245126,
      "loss": 2.4142,
      "step": 6190
    },
    {
      "epoch": 1.7270194986072425,
      "grad_norm": 1.466760277748108,
      "learning_rate": 0.0008273259052924791,
      "loss": 2.6543,
      "step": 6200
    },
    {
      "epoch": 1.7298050139275767,
      "grad_norm": 3.769167184829712,
      "learning_rate": 0.0008270473537604457,
      "loss": 2.4302,
      "step": 6210
    },
    {
      "epoch": 1.732590529247911,
      "grad_norm": 1.3401732444763184,
      "learning_rate": 0.0008267688022284122,
      "loss": 2.2791,
      "step": 6220
    },
    {
      "epoch": 1.7353760445682451,
      "grad_norm": 1.4258317947387695,
      "learning_rate": 0.0008264902506963788,
      "loss": 2.4749,
      "step": 6230
    },
    {
      "epoch": 1.7381615598885793,
      "grad_norm": 1.5848344564437866,
      "learning_rate": 0.0008262116991643454,
      "loss": 2.5406,
      "step": 6240
    },
    {
      "epoch": 1.7409470752089136,
      "grad_norm": 1.2846511602401733,
      "learning_rate": 0.000825933147632312,
      "loss": 2.4889,
      "step": 6250
    },
    {
      "epoch": 1.7437325905292478,
      "grad_norm": 1.3900681734085083,
      "learning_rate": 0.0008256545961002786,
      "loss": 2.5036,
      "step": 6260
    },
    {
      "epoch": 1.7465181058495822,
      "grad_norm": 1.317147135734558,
      "learning_rate": 0.0008253760445682451,
      "loss": 2.3661,
      "step": 6270
    },
    {
      "epoch": 1.7493036211699164,
      "grad_norm": 1.7741236686706543,
      "learning_rate": 0.0008250974930362117,
      "loss": 2.3272,
      "step": 6280
    },
    {
      "epoch": 1.7520891364902507,
      "grad_norm": 1.3510454893112183,
      "learning_rate": 0.0008248189415041783,
      "loss": 2.2795,
      "step": 6290
    },
    {
      "epoch": 1.754874651810585,
      "grad_norm": 1.2753394842147827,
      "learning_rate": 0.0008245403899721449,
      "loss": 2.4931,
      "step": 6300
    },
    {
      "epoch": 1.7576601671309193,
      "grad_norm": 1.9052752256393433,
      "learning_rate": 0.0008242618384401115,
      "loss": 2.2879,
      "step": 6310
    },
    {
      "epoch": 1.7604456824512535,
      "grad_norm": 1.6165374517440796,
      "learning_rate": 0.000823983286908078,
      "loss": 2.5197,
      "step": 6320
    },
    {
      "epoch": 1.7632311977715878,
      "grad_norm": 1.2844661474227905,
      "learning_rate": 0.0008237047353760446,
      "loss": 2.2696,
      "step": 6330
    },
    {
      "epoch": 1.766016713091922,
      "grad_norm": 1.3863577842712402,
      "learning_rate": 0.0008234261838440111,
      "loss": 2.5315,
      "step": 6340
    },
    {
      "epoch": 1.7688022284122562,
      "grad_norm": 1.3799529075622559,
      "learning_rate": 0.0008231476323119777,
      "loss": 2.3653,
      "step": 6350
    },
    {
      "epoch": 1.7715877437325904,
      "grad_norm": 1.0960042476654053,
      "learning_rate": 0.0008228690807799443,
      "loss": 2.4842,
      "step": 6360
    },
    {
      "epoch": 1.7743732590529246,
      "grad_norm": 1.3078745603561401,
      "learning_rate": 0.0008225905292479109,
      "loss": 2.3265,
      "step": 6370
    },
    {
      "epoch": 1.777158774373259,
      "grad_norm": 1.4684354066848755,
      "learning_rate": 0.0008223119777158774,
      "loss": 2.3594,
      "step": 6380
    },
    {
      "epoch": 1.7799442896935933,
      "grad_norm": 1.3670852184295654,
      "learning_rate": 0.000822033426183844,
      "loss": 2.5076,
      "step": 6390
    },
    {
      "epoch": 1.7827298050139275,
      "grad_norm": 1.1271620988845825,
      "learning_rate": 0.0008217548746518107,
      "loss": 2.3308,
      "step": 6400
    },
    {
      "epoch": 1.785515320334262,
      "grad_norm": 1.4619067907333374,
      "learning_rate": 0.0008214763231197772,
      "loss": 2.4362,
      "step": 6410
    },
    {
      "epoch": 1.7883008356545962,
      "grad_norm": 1.095137596130371,
      "learning_rate": 0.0008211977715877437,
      "loss": 2.3999,
      "step": 6420
    },
    {
      "epoch": 1.7910863509749304,
      "grad_norm": 1.1994507312774658,
      "learning_rate": 0.0008209192200557103,
      "loss": 2.4043,
      "step": 6430
    },
    {
      "epoch": 1.7938718662952646,
      "grad_norm": 1.6124271154403687,
      "learning_rate": 0.0008206406685236769,
      "loss": 2.4663,
      "step": 6440
    },
    {
      "epoch": 1.7966573816155988,
      "grad_norm": 1.3351354598999023,
      "learning_rate": 0.0008203621169916434,
      "loss": 2.3531,
      "step": 6450
    },
    {
      "epoch": 1.799442896935933,
      "grad_norm": 1.8354355096817017,
      "learning_rate": 0.00082008356545961,
      "loss": 2.3234,
      "step": 6460
    },
    {
      "epoch": 1.8022284122562673,
      "grad_norm": 0.8783612251281738,
      "learning_rate": 0.0008198050139275767,
      "loss": 2.3308,
      "step": 6470
    },
    {
      "epoch": 1.8050139275766015,
      "grad_norm": 1.9236317873001099,
      "learning_rate": 0.0008195264623955432,
      "loss": 2.5139,
      "step": 6480
    },
    {
      "epoch": 1.807799442896936,
      "grad_norm": 0.9463639855384827,
      "learning_rate": 0.0008192479108635098,
      "loss": 2.4948,
      "step": 6490
    },
    {
      "epoch": 1.8105849582172702,
      "grad_norm": 1.4316291809082031,
      "learning_rate": 0.0008189693593314764,
      "loss": 2.4513,
      "step": 6500
    },
    {
      "epoch": 1.8133704735376046,
      "grad_norm": 1.3555768728256226,
      "learning_rate": 0.000818690807799443,
      "loss": 2.4301,
      "step": 6510
    },
    {
      "epoch": 1.8161559888579388,
      "grad_norm": 1.8113563060760498,
      "learning_rate": 0.0008184122562674094,
      "loss": 2.2581,
      "step": 6520
    },
    {
      "epoch": 1.818941504178273,
      "grad_norm": 1.6019859313964844,
      "learning_rate": 0.000818133704735376,
      "loss": 2.5592,
      "step": 6530
    },
    {
      "epoch": 1.8217270194986073,
      "grad_norm": 1.4551405906677246,
      "learning_rate": 0.0008178551532033426,
      "loss": 2.4838,
      "step": 6540
    },
    {
      "epoch": 1.8245125348189415,
      "grad_norm": 1.1586564779281616,
      "learning_rate": 0.0008175766016713092,
      "loss": 2.2018,
      "step": 6550
    },
    {
      "epoch": 1.8272980501392757,
      "grad_norm": 1.094881296157837,
      "learning_rate": 0.0008172980501392758,
      "loss": 2.3093,
      "step": 6560
    },
    {
      "epoch": 1.83008356545961,
      "grad_norm": 1.619089961051941,
      "learning_rate": 0.0008170194986072423,
      "loss": 2.3631,
      "step": 6570
    },
    {
      "epoch": 1.8328690807799441,
      "grad_norm": 1.396796703338623,
      "learning_rate": 0.000816740947075209,
      "loss": 2.4252,
      "step": 6580
    },
    {
      "epoch": 1.8356545961002786,
      "grad_norm": 1.5675032138824463,
      "learning_rate": 0.0008164623955431755,
      "loss": 2.5262,
      "step": 6590
    },
    {
      "epoch": 1.8384401114206128,
      "grad_norm": 1.427345871925354,
      "learning_rate": 0.0008161838440111421,
      "loss": 2.4402,
      "step": 6600
    },
    {
      "epoch": 1.841225626740947,
      "grad_norm": 1.174105167388916,
      "learning_rate": 0.0008159052924791087,
      "loss": 2.2528,
      "step": 6610
    },
    {
      "epoch": 1.8440111420612815,
      "grad_norm": 1.3738512992858887,
      "learning_rate": 0.0008156267409470752,
      "loss": 2.48,
      "step": 6620
    },
    {
      "epoch": 1.8467966573816157,
      "grad_norm": 1.260046362876892,
      "learning_rate": 0.0008153481894150418,
      "loss": 2.3723,
      "step": 6630
    },
    {
      "epoch": 1.84958217270195,
      "grad_norm": 1.9405947923660278,
      "learning_rate": 0.0008150696378830083,
      "loss": 2.3436,
      "step": 6640
    },
    {
      "epoch": 1.8523676880222841,
      "grad_norm": 1.228348731994629,
      "learning_rate": 0.000814791086350975,
      "loss": 2.4296,
      "step": 6650
    },
    {
      "epoch": 1.8551532033426184,
      "grad_norm": 1.2375986576080322,
      "learning_rate": 0.0008145125348189415,
      "loss": 2.485,
      "step": 6660
    },
    {
      "epoch": 1.8579387186629526,
      "grad_norm": 1.0994915962219238,
      "learning_rate": 0.0008142339832869081,
      "loss": 2.2898,
      "step": 6670
    },
    {
      "epoch": 1.8607242339832868,
      "grad_norm": 1.6460318565368652,
      "learning_rate": 0.0008139554317548746,
      "loss": 2.4856,
      "step": 6680
    },
    {
      "epoch": 1.863509749303621,
      "grad_norm": 1.6726387739181519,
      "learning_rate": 0.0008136768802228413,
      "loss": 2.3451,
      "step": 6690
    },
    {
      "epoch": 1.8662952646239555,
      "grad_norm": 1.2864811420440674,
      "learning_rate": 0.0008133983286908078,
      "loss": 2.2958,
      "step": 6700
    },
    {
      "epoch": 1.8690807799442897,
      "grad_norm": 1.158359169960022,
      "learning_rate": 0.0008131197771587744,
      "loss": 2.3702,
      "step": 6710
    },
    {
      "epoch": 1.8718662952646241,
      "grad_norm": 1.398132562637329,
      "learning_rate": 0.000812841225626741,
      "loss": 2.5205,
      "step": 6720
    },
    {
      "epoch": 1.8746518105849583,
      "grad_norm": 1.5372495651245117,
      "learning_rate": 0.0008125626740947075,
      "loss": 2.2513,
      "step": 6730
    },
    {
      "epoch": 1.8774373259052926,
      "grad_norm": 1.239255666732788,
      "learning_rate": 0.0008122841225626741,
      "loss": 2.306,
      "step": 6740
    },
    {
      "epoch": 1.8802228412256268,
      "grad_norm": 1.453840732574463,
      "learning_rate": 0.0008120055710306406,
      "loss": 2.3127,
      "step": 6750
    },
    {
      "epoch": 1.883008356545961,
      "grad_norm": 1.742677927017212,
      "learning_rate": 0.0008117270194986073,
      "loss": 2.5083,
      "step": 6760
    },
    {
      "epoch": 1.8857938718662952,
      "grad_norm": 1.563870906829834,
      "learning_rate": 0.0008114484679665738,
      "loss": 2.3195,
      "step": 6770
    },
    {
      "epoch": 1.8885793871866294,
      "grad_norm": 1.786298155784607,
      "learning_rate": 0.0008111699164345404,
      "loss": 2.4914,
      "step": 6780
    },
    {
      "epoch": 1.8913649025069637,
      "grad_norm": 1.856904149055481,
      "learning_rate": 0.0008108913649025071,
      "loss": 2.3423,
      "step": 6790
    },
    {
      "epoch": 1.894150417827298,
      "grad_norm": 1.3077996969223022,
      "learning_rate": 0.0008106128133704736,
      "loss": 2.381,
      "step": 6800
    },
    {
      "epoch": 1.8969359331476323,
      "grad_norm": 1.6071877479553223,
      "learning_rate": 0.0008103342618384402,
      "loss": 2.3707,
      "step": 6810
    },
    {
      "epoch": 1.8997214484679665,
      "grad_norm": 1.6998291015625,
      "learning_rate": 0.0008100557103064066,
      "loss": 2.3371,
      "step": 6820
    },
    {
      "epoch": 1.902506963788301,
      "grad_norm": 1.3886394500732422,
      "learning_rate": 0.0008097771587743733,
      "loss": 2.325,
      "step": 6830
    },
    {
      "epoch": 1.9052924791086352,
      "grad_norm": 1.2896050214767456,
      "learning_rate": 0.0008094986072423398,
      "loss": 2.5946,
      "step": 6840
    },
    {
      "epoch": 1.9080779944289694,
      "grad_norm": 1.282377004623413,
      "learning_rate": 0.0008092200557103064,
      "loss": 2.4245,
      "step": 6850
    },
    {
      "epoch": 1.9108635097493036,
      "grad_norm": 1.4862234592437744,
      "learning_rate": 0.0008089415041782729,
      "loss": 2.4175,
      "step": 6860
    },
    {
      "epoch": 1.9136490250696379,
      "grad_norm": 1.705293893814087,
      "learning_rate": 0.0008086629526462396,
      "loss": 2.4492,
      "step": 6870
    },
    {
      "epoch": 1.916434540389972,
      "grad_norm": 1.564586877822876,
      "learning_rate": 0.0008083844011142062,
      "loss": 2.4524,
      "step": 6880
    },
    {
      "epoch": 1.9192200557103063,
      "grad_norm": 1.742055058479309,
      "learning_rate": 0.0008081058495821727,
      "loss": 2.2601,
      "step": 6890
    },
    {
      "epoch": 1.9220055710306405,
      "grad_norm": 1.3718092441558838,
      "learning_rate": 0.0008078272980501394,
      "loss": 2.3363,
      "step": 6900
    },
    {
      "epoch": 1.924791086350975,
      "grad_norm": 2.9504823684692383,
      "learning_rate": 0.0008075487465181059,
      "loss": 2.3778,
      "step": 6910
    },
    {
      "epoch": 1.9275766016713092,
      "grad_norm": 1.5638364553451538,
      "learning_rate": 0.0008072701949860724,
      "loss": 2.2609,
      "step": 6920
    },
    {
      "epoch": 1.9303621169916436,
      "grad_norm": 1.2623132467269897,
      "learning_rate": 0.0008069916434540389,
      "loss": 2.4639,
      "step": 6930
    },
    {
      "epoch": 1.9331476323119778,
      "grad_norm": 1.2557053565979004,
      "learning_rate": 0.0008067130919220056,
      "loss": 2.3805,
      "step": 6940
    },
    {
      "epoch": 1.935933147632312,
      "grad_norm": 1.3918931484222412,
      "learning_rate": 0.0008064345403899722,
      "loss": 2.4788,
      "step": 6950
    },
    {
      "epoch": 1.9387186629526463,
      "grad_norm": 1.3058959245681763,
      "learning_rate": 0.0008061559888579387,
      "loss": 2.3741,
      "step": 6960
    },
    {
      "epoch": 1.9415041782729805,
      "grad_norm": 1.588460087776184,
      "learning_rate": 0.0008058774373259054,
      "loss": 2.5464,
      "step": 6970
    },
    {
      "epoch": 1.9442896935933147,
      "grad_norm": 1.1558775901794434,
      "learning_rate": 0.0008055988857938719,
      "loss": 2.4036,
      "step": 6980
    },
    {
      "epoch": 1.947075208913649,
      "grad_norm": 1.438850998878479,
      "learning_rate": 0.0008053203342618385,
      "loss": 2.4994,
      "step": 6990
    },
    {
      "epoch": 1.9498607242339832,
      "grad_norm": 1.253549337387085,
      "learning_rate": 0.000805041782729805,
      "loss": 2.4123,
      "step": 7000
    },
    {
      "epoch": 1.9526462395543176,
      "grad_norm": 1.2849812507629395,
      "learning_rate": 0.0008047632311977717,
      "loss": 2.5346,
      "step": 7010
    },
    {
      "epoch": 1.9554317548746518,
      "grad_norm": 1.3534023761749268,
      "learning_rate": 0.0008044846796657381,
      "loss": 2.37,
      "step": 7020
    },
    {
      "epoch": 1.958217270194986,
      "grad_norm": 1.6432652473449707,
      "learning_rate": 0.0008042061281337047,
      "loss": 2.231,
      "step": 7030
    },
    {
      "epoch": 1.9610027855153205,
      "grad_norm": 1.2143882513046265,
      "learning_rate": 0.0008039275766016713,
      "loss": 2.4258,
      "step": 7040
    },
    {
      "epoch": 1.9637883008356547,
      "grad_norm": 1.1166949272155762,
      "learning_rate": 0.0008036490250696379,
      "loss": 2.5308,
      "step": 7050
    },
    {
      "epoch": 1.966573816155989,
      "grad_norm": 1.7945033311843872,
      "learning_rate": 0.0008033704735376045,
      "loss": 2.4217,
      "step": 7060
    },
    {
      "epoch": 1.9693593314763231,
      "grad_norm": 1.1977458000183105,
      "learning_rate": 0.000803091922005571,
      "loss": 2.4774,
      "step": 7070
    },
    {
      "epoch": 1.9721448467966574,
      "grad_norm": 1.7178343534469604,
      "learning_rate": 0.0008028133704735377,
      "loss": 2.4478,
      "step": 7080
    },
    {
      "epoch": 1.9749303621169916,
      "grad_norm": 1.3880826234817505,
      "learning_rate": 0.0008025348189415042,
      "loss": 2.359,
      "step": 7090
    },
    {
      "epoch": 1.9777158774373258,
      "grad_norm": 1.7445032596588135,
      "learning_rate": 0.0008022562674094708,
      "loss": 2.2314,
      "step": 7100
    },
    {
      "epoch": 1.98050139275766,
      "grad_norm": 1.630293846130371,
      "learning_rate": 0.0008019777158774373,
      "loss": 2.4,
      "step": 7110
    },
    {
      "epoch": 1.9832869080779945,
      "grad_norm": 1.4134390354156494,
      "learning_rate": 0.0008016991643454039,
      "loss": 2.4731,
      "step": 7120
    },
    {
      "epoch": 1.9860724233983287,
      "grad_norm": 1.3871678113937378,
      "learning_rate": 0.0008014206128133705,
      "loss": 2.5525,
      "step": 7130
    },
    {
      "epoch": 1.988857938718663,
      "grad_norm": 1.3510398864746094,
      "learning_rate": 0.000801142061281337,
      "loss": 2.671,
      "step": 7140
    },
    {
      "epoch": 1.9916434540389973,
      "grad_norm": 2.066843271255493,
      "learning_rate": 0.0008008635097493037,
      "loss": 2.5491,
      "step": 7150
    },
    {
      "epoch": 1.9944289693593316,
      "grad_norm": 1.6416343450546265,
      "learning_rate": 0.0008005849582172702,
      "loss": 2.4984,
      "step": 7160
    },
    {
      "epoch": 1.9972144846796658,
      "grad_norm": 1.50349760055542,
      "learning_rate": 0.0008003064066852368,
      "loss": 2.3535,
      "step": 7170
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.689961314201355,
      "learning_rate": 0.0008000278551532033,
      "loss": 2.4883,
      "step": 7180
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.356095314025879,
      "eval_runtime": 5.7728,
      "eval_samples_per_second": 552.934,
      "eval_steps_per_second": 69.117,
      "step": 7180
    },
    {
      "epoch": 2.002785515320334,
      "grad_norm": 1.3793178796768188,
      "learning_rate": 0.00079974930362117,
      "loss": 2.6602,
      "step": 7190
    },
    {
      "epoch": 2.0055710306406684,
      "grad_norm": 1.1510647535324097,
      "learning_rate": 0.0007994707520891366,
      "loss": 2.2967,
      "step": 7200
    },
    {
      "epoch": 2.0083565459610027,
      "grad_norm": 1.4528188705444336,
      "learning_rate": 0.000799192200557103,
      "loss": 2.2547,
      "step": 7210
    },
    {
      "epoch": 2.011142061281337,
      "grad_norm": 2.173064708709717,
      "learning_rate": 0.0007989136490250696,
      "loss": 2.3586,
      "step": 7220
    },
    {
      "epoch": 2.013927576601671,
      "grad_norm": 1.2183516025543213,
      "learning_rate": 0.0007986350974930362,
      "loss": 2.3783,
      "step": 7230
    },
    {
      "epoch": 2.0167130919220058,
      "grad_norm": 1.4444009065628052,
      "learning_rate": 0.0007983565459610028,
      "loss": 2.3445,
      "step": 7240
    },
    {
      "epoch": 2.01949860724234,
      "grad_norm": 1.5153805017471313,
      "learning_rate": 0.0007980779944289693,
      "loss": 2.521,
      "step": 7250
    },
    {
      "epoch": 2.022284122562674,
      "grad_norm": 1.600487470626831,
      "learning_rate": 0.000797799442896936,
      "loss": 2.7149,
      "step": 7260
    },
    {
      "epoch": 2.0250696378830084,
      "grad_norm": 1.5241098403930664,
      "learning_rate": 0.0007975208913649026,
      "loss": 2.4539,
      "step": 7270
    },
    {
      "epoch": 2.0278551532033426,
      "grad_norm": 1.3366420269012451,
      "learning_rate": 0.0007972423398328691,
      "loss": 2.3003,
      "step": 7280
    },
    {
      "epoch": 2.030640668523677,
      "grad_norm": 1.7221125364303589,
      "learning_rate": 0.0007969637883008357,
      "loss": 2.3157,
      "step": 7290
    },
    {
      "epoch": 2.033426183844011,
      "grad_norm": 2.60722017288208,
      "learning_rate": 0.0007966852367688023,
      "loss": 2.3295,
      "step": 7300
    },
    {
      "epoch": 2.0362116991643453,
      "grad_norm": 1.5543239116668701,
      "learning_rate": 0.0007964066852367688,
      "loss": 2.3506,
      "step": 7310
    },
    {
      "epoch": 2.0389972144846795,
      "grad_norm": 1.3007012605667114,
      "learning_rate": 0.0007961281337047353,
      "loss": 2.1479,
      "step": 7320
    },
    {
      "epoch": 2.0417827298050137,
      "grad_norm": 1.315839171409607,
      "learning_rate": 0.000795849582172702,
      "loss": 2.3968,
      "step": 7330
    },
    {
      "epoch": 2.0445682451253484,
      "grad_norm": 1.063215970993042,
      "learning_rate": 0.0007955710306406685,
      "loss": 2.31,
      "step": 7340
    },
    {
      "epoch": 2.0473537604456826,
      "grad_norm": 1.4033770561218262,
      "learning_rate": 0.0007952924791086351,
      "loss": 2.4664,
      "step": 7350
    },
    {
      "epoch": 2.050139275766017,
      "grad_norm": 2.025193929672241,
      "learning_rate": 0.0007950139275766017,
      "loss": 2.3436,
      "step": 7360
    },
    {
      "epoch": 2.052924791086351,
      "grad_norm": 1.1773039102554321,
      "learning_rate": 0.0007947353760445683,
      "loss": 2.2739,
      "step": 7370
    },
    {
      "epoch": 2.0557103064066853,
      "grad_norm": 2.040565013885498,
      "learning_rate": 0.0007944568245125349,
      "loss": 2.4466,
      "step": 7380
    },
    {
      "epoch": 2.0584958217270195,
      "grad_norm": 2.174027919769287,
      "learning_rate": 0.0007941782729805014,
      "loss": 2.4705,
      "step": 7390
    },
    {
      "epoch": 2.0612813370473537,
      "grad_norm": 1.6786584854125977,
      "learning_rate": 0.000793899721448468,
      "loss": 2.5039,
      "step": 7400
    },
    {
      "epoch": 2.064066852367688,
      "grad_norm": 1.4325050115585327,
      "learning_rate": 0.0007936211699164345,
      "loss": 2.2407,
      "step": 7410
    },
    {
      "epoch": 2.066852367688022,
      "grad_norm": 1.2426438331604004,
      "learning_rate": 0.0007933426183844011,
      "loss": 2.2403,
      "step": 7420
    },
    {
      "epoch": 2.0696378830083564,
      "grad_norm": 1.4592970609664917,
      "learning_rate": 0.0007930640668523676,
      "loss": 2.4394,
      "step": 7430
    },
    {
      "epoch": 2.0724233983286906,
      "grad_norm": 2.2457756996154785,
      "learning_rate": 0.0007927855153203343,
      "loss": 2.2899,
      "step": 7440
    },
    {
      "epoch": 2.0752089136490253,
      "grad_norm": 1.8023048639297485,
      "learning_rate": 0.0007925069637883009,
      "loss": 2.4203,
      "step": 7450
    },
    {
      "epoch": 2.0779944289693595,
      "grad_norm": 1.2521294355392456,
      "learning_rate": 0.0007922284122562674,
      "loss": 2.3121,
      "step": 7460
    },
    {
      "epoch": 2.0807799442896937,
      "grad_norm": 1.755453109741211,
      "learning_rate": 0.000791949860724234,
      "loss": 2.4003,
      "step": 7470
    },
    {
      "epoch": 2.083565459610028,
      "grad_norm": 1.4372684955596924,
      "learning_rate": 0.0007916713091922006,
      "loss": 2.5087,
      "step": 7480
    },
    {
      "epoch": 2.086350974930362,
      "grad_norm": 1.170839548110962,
      "learning_rate": 0.0007913927576601672,
      "loss": 2.3172,
      "step": 7490
    },
    {
      "epoch": 2.0891364902506964,
      "grad_norm": 1.3947033882141113,
      "learning_rate": 0.0007911142061281336,
      "loss": 2.3193,
      "step": 7500
    },
    {
      "epoch": 2.0919220055710306,
      "grad_norm": 1.286779761314392,
      "learning_rate": 0.0007908356545961003,
      "loss": 2.4351,
      "step": 7510
    },
    {
      "epoch": 2.094707520891365,
      "grad_norm": 1.6179249286651611,
      "learning_rate": 0.0007905571030640669,
      "loss": 2.274,
      "step": 7520
    },
    {
      "epoch": 2.097493036211699,
      "grad_norm": 1.0042928457260132,
      "learning_rate": 0.0007902785515320334,
      "loss": 2.362,
      "step": 7530
    },
    {
      "epoch": 2.1002785515320332,
      "grad_norm": 1.6619367599487305,
      "learning_rate": 0.00079,
      "loss": 2.7206,
      "step": 7540
    },
    {
      "epoch": 2.103064066852368,
      "grad_norm": 1.549983024597168,
      "learning_rate": 0.0007897214484679666,
      "loss": 2.3101,
      "step": 7550
    },
    {
      "epoch": 2.105849582172702,
      "grad_norm": 1.8498156070709229,
      "learning_rate": 0.0007894428969359332,
      "loss": 2.6339,
      "step": 7560
    },
    {
      "epoch": 2.1086350974930363,
      "grad_norm": 1.1128922700881958,
      "learning_rate": 0.0007891643454038997,
      "loss": 2.2494,
      "step": 7570
    },
    {
      "epoch": 2.1114206128133706,
      "grad_norm": 1.8395735025405884,
      "learning_rate": 0.0007888857938718663,
      "loss": 2.2708,
      "step": 7580
    },
    {
      "epoch": 2.114206128133705,
      "grad_norm": 1.224404215812683,
      "learning_rate": 0.0007886072423398329,
      "loss": 2.31,
      "step": 7590
    },
    {
      "epoch": 2.116991643454039,
      "grad_norm": 1.3247249126434326,
      "learning_rate": 0.0007883286908077995,
      "loss": 2.4229,
      "step": 7600
    },
    {
      "epoch": 2.1197771587743732,
      "grad_norm": 1.2941913604736328,
      "learning_rate": 0.000788050139275766,
      "loss": 2.4451,
      "step": 7610
    },
    {
      "epoch": 2.1225626740947074,
      "grad_norm": 1.2908804416656494,
      "learning_rate": 0.0007877715877437326,
      "loss": 2.4553,
      "step": 7620
    },
    {
      "epoch": 2.1253481894150417,
      "grad_norm": 1.212195634841919,
      "learning_rate": 0.0007874930362116992,
      "loss": 2.3415,
      "step": 7630
    },
    {
      "epoch": 2.128133704735376,
      "grad_norm": 2.0850789546966553,
      "learning_rate": 0.0007872144846796657,
      "loss": 2.4066,
      "step": 7640
    },
    {
      "epoch": 2.13091922005571,
      "grad_norm": 2.9325308799743652,
      "learning_rate": 0.0007869359331476323,
      "loss": 2.214,
      "step": 7650
    },
    {
      "epoch": 2.1337047353760448,
      "grad_norm": 1.2413291931152344,
      "learning_rate": 0.0007866573816155989,
      "loss": 2.4421,
      "step": 7660
    },
    {
      "epoch": 2.136490250696379,
      "grad_norm": 1.529618740081787,
      "learning_rate": 0.0007863788300835655,
      "loss": 2.4597,
      "step": 7670
    },
    {
      "epoch": 2.139275766016713,
      "grad_norm": 1.0800505876541138,
      "learning_rate": 0.0007861002785515321,
      "loss": 2.3512,
      "step": 7680
    },
    {
      "epoch": 2.1420612813370474,
      "grad_norm": 1.2398654222488403,
      "learning_rate": 0.0007858217270194987,
      "loss": 2.2443,
      "step": 7690
    },
    {
      "epoch": 2.1448467966573816,
      "grad_norm": 1.345249056816101,
      "learning_rate": 0.0007855431754874653,
      "loss": 2.3927,
      "step": 7700
    },
    {
      "epoch": 2.147632311977716,
      "grad_norm": 2.6803183555603027,
      "learning_rate": 0.0007852646239554317,
      "loss": 2.5656,
      "step": 7710
    },
    {
      "epoch": 2.15041782729805,
      "grad_norm": 1.486061453819275,
      "learning_rate": 0.0007849860724233983,
      "loss": 2.4712,
      "step": 7720
    },
    {
      "epoch": 2.1532033426183843,
      "grad_norm": 1.1676629781723022,
      "learning_rate": 0.0007847075208913649,
      "loss": 2.3517,
      "step": 7730
    },
    {
      "epoch": 2.1559888579387185,
      "grad_norm": 1.4803147315979004,
      "learning_rate": 0.0007844289693593315,
      "loss": 2.2347,
      "step": 7740
    },
    {
      "epoch": 2.1587743732590527,
      "grad_norm": 1.4599446058273315,
      "learning_rate": 0.000784150417827298,
      "loss": 2.2711,
      "step": 7750
    },
    {
      "epoch": 2.1615598885793874,
      "grad_norm": 2.29618239402771,
      "learning_rate": 0.0007838718662952646,
      "loss": 2.5363,
      "step": 7760
    },
    {
      "epoch": 2.1643454038997216,
      "grad_norm": 1.233139991760254,
      "learning_rate": 0.0007835933147632313,
      "loss": 2.3714,
      "step": 7770
    },
    {
      "epoch": 2.167130919220056,
      "grad_norm": 1.291534185409546,
      "learning_rate": 0.0007833147632311978,
      "loss": 2.3307,
      "step": 7780
    },
    {
      "epoch": 2.16991643454039,
      "grad_norm": 1.416357159614563,
      "learning_rate": 0.0007830362116991644,
      "loss": 2.2829,
      "step": 7790
    },
    {
      "epoch": 2.1727019498607243,
      "grad_norm": 1.8830618858337402,
      "learning_rate": 0.000782757660167131,
      "loss": 2.3532,
      "step": 7800
    },
    {
      "epoch": 2.1754874651810585,
      "grad_norm": 1.617240071296692,
      "learning_rate": 0.0007824791086350975,
      "loss": 2.2518,
      "step": 7810
    },
    {
      "epoch": 2.1782729805013927,
      "grad_norm": 1.6248215436935425,
      "learning_rate": 0.000782200557103064,
      "loss": 2.3946,
      "step": 7820
    },
    {
      "epoch": 2.181058495821727,
      "grad_norm": 1.6357499361038208,
      "learning_rate": 0.0007819220055710306,
      "loss": 2.369,
      "step": 7830
    },
    {
      "epoch": 2.183844011142061,
      "grad_norm": 3.4977359771728516,
      "learning_rate": 0.0007816434540389973,
      "loss": 2.4964,
      "step": 7840
    },
    {
      "epoch": 2.1866295264623954,
      "grad_norm": 1.6003689765930176,
      "learning_rate": 0.0007813649025069638,
      "loss": 2.3617,
      "step": 7850
    },
    {
      "epoch": 2.1894150417827296,
      "grad_norm": 2.413916826248169,
      "learning_rate": 0.0007810863509749304,
      "loss": 2.3578,
      "step": 7860
    },
    {
      "epoch": 2.1922005571030643,
      "grad_norm": 1.767979621887207,
      "learning_rate": 0.000780807799442897,
      "loss": 2.4948,
      "step": 7870
    },
    {
      "epoch": 2.1949860724233985,
      "grad_norm": 1.1983895301818848,
      "learning_rate": 0.0007805292479108636,
      "loss": 2.2524,
      "step": 7880
    },
    {
      "epoch": 2.1977715877437327,
      "grad_norm": 1.4050227403640747,
      "learning_rate": 0.00078025069637883,
      "loss": 2.4398,
      "step": 7890
    },
    {
      "epoch": 2.200557103064067,
      "grad_norm": 1.4456746578216553,
      "learning_rate": 0.0007799721448467966,
      "loss": 2.2924,
      "step": 7900
    },
    {
      "epoch": 2.203342618384401,
      "grad_norm": 1.2584569454193115,
      "learning_rate": 0.0007796935933147632,
      "loss": 2.2934,
      "step": 7910
    },
    {
      "epoch": 2.2061281337047354,
      "grad_norm": 1.1840765476226807,
      "learning_rate": 0.0007794150417827298,
      "loss": 2.5694,
      "step": 7920
    },
    {
      "epoch": 2.2089136490250696,
      "grad_norm": 1.0272057056427002,
      "learning_rate": 0.0007791364902506964,
      "loss": 2.312,
      "step": 7930
    },
    {
      "epoch": 2.211699164345404,
      "grad_norm": 1.6382324695587158,
      "learning_rate": 0.0007788579387186629,
      "loss": 2.4767,
      "step": 7940
    },
    {
      "epoch": 2.214484679665738,
      "grad_norm": 1.7364943027496338,
      "learning_rate": 0.0007785793871866296,
      "loss": 2.456,
      "step": 7950
    },
    {
      "epoch": 2.2172701949860723,
      "grad_norm": 2.1002166271209717,
      "learning_rate": 0.0007783008356545961,
      "loss": 2.4556,
      "step": 7960
    },
    {
      "epoch": 2.220055710306407,
      "grad_norm": 2.6632468700408936,
      "learning_rate": 0.0007780222841225627,
      "loss": 2.3219,
      "step": 7970
    },
    {
      "epoch": 2.222841225626741,
      "grad_norm": 1.9678758382797241,
      "learning_rate": 0.0007777437325905293,
      "loss": 2.4836,
      "step": 7980
    },
    {
      "epoch": 2.2256267409470754,
      "grad_norm": 1.3856940269470215,
      "learning_rate": 0.0007774651810584959,
      "loss": 2.3394,
      "step": 7990
    },
    {
      "epoch": 2.2284122562674096,
      "grad_norm": 1.5227327346801758,
      "learning_rate": 0.0007771866295264624,
      "loss": 2.1959,
      "step": 8000
    },
    {
      "epoch": 2.231197771587744,
      "grad_norm": 1.6829389333724976,
      "learning_rate": 0.0007769080779944289,
      "loss": 2.482,
      "step": 8010
    },
    {
      "epoch": 2.233983286908078,
      "grad_norm": 1.4815946817398071,
      "learning_rate": 0.0007766295264623956,
      "loss": 2.3526,
      "step": 8020
    },
    {
      "epoch": 2.2367688022284122,
      "grad_norm": 1.3893436193466187,
      "learning_rate": 0.0007763509749303621,
      "loss": 2.3119,
      "step": 8030
    },
    {
      "epoch": 2.2395543175487465,
      "grad_norm": 1.7107305526733398,
      "learning_rate": 0.0007760724233983287,
      "loss": 2.3304,
      "step": 8040
    },
    {
      "epoch": 2.2423398328690807,
      "grad_norm": 1.6443315744400024,
      "learning_rate": 0.0007757938718662953,
      "loss": 2.3207,
      "step": 8050
    },
    {
      "epoch": 2.245125348189415,
      "grad_norm": 1.4696248769760132,
      "learning_rate": 0.0007755153203342619,
      "loss": 2.524,
      "step": 8060
    },
    {
      "epoch": 2.247910863509749,
      "grad_norm": 1.2282191514968872,
      "learning_rate": 0.0007752367688022284,
      "loss": 2.3386,
      "step": 8070
    },
    {
      "epoch": 2.2506963788300833,
      "grad_norm": 1.4982227087020874,
      "learning_rate": 0.000774958217270195,
      "loss": 2.2215,
      "step": 8080
    },
    {
      "epoch": 2.253481894150418,
      "grad_norm": 1.635749340057373,
      "learning_rate": 0.0007746796657381617,
      "loss": 2.1394,
      "step": 8090
    },
    {
      "epoch": 2.256267409470752,
      "grad_norm": 1.1851848363876343,
      "learning_rate": 0.0007744011142061281,
      "loss": 2.4592,
      "step": 8100
    },
    {
      "epoch": 2.2590529247910864,
      "grad_norm": 1.368833065032959,
      "learning_rate": 0.0007741225626740947,
      "loss": 2.3099,
      "step": 8110
    },
    {
      "epoch": 2.2618384401114207,
      "grad_norm": 1.2497309446334839,
      "learning_rate": 0.0007738440111420612,
      "loss": 2.2162,
      "step": 8120
    },
    {
      "epoch": 2.264623955431755,
      "grad_norm": 1.2516252994537354,
      "learning_rate": 0.0007735654596100279,
      "loss": 2.2302,
      "step": 8130
    },
    {
      "epoch": 2.267409470752089,
      "grad_norm": 1.9017127752304077,
      "learning_rate": 0.0007732869080779944,
      "loss": 2.3997,
      "step": 8140
    },
    {
      "epoch": 2.2701949860724233,
      "grad_norm": 1.4533798694610596,
      "learning_rate": 0.000773008356545961,
      "loss": 2.3677,
      "step": 8150
    },
    {
      "epoch": 2.2729805013927575,
      "grad_norm": 1.3774884939193726,
      "learning_rate": 0.0007727298050139277,
      "loss": 2.1289,
      "step": 8160
    },
    {
      "epoch": 2.2757660167130918,
      "grad_norm": 1.7429136037826538,
      "learning_rate": 0.0007724512534818942,
      "loss": 2.426,
      "step": 8170
    },
    {
      "epoch": 2.2785515320334264,
      "grad_norm": 1.778224229812622,
      "learning_rate": 0.0007721727019498608,
      "loss": 2.5779,
      "step": 8180
    },
    {
      "epoch": 2.2813370473537606,
      "grad_norm": 1.448362946510315,
      "learning_rate": 0.0007718941504178272,
      "loss": 2.3049,
      "step": 8190
    },
    {
      "epoch": 2.284122562674095,
      "grad_norm": 1.3694400787353516,
      "learning_rate": 0.000771615598885794,
      "loss": 2.4632,
      "step": 8200
    },
    {
      "epoch": 2.286908077994429,
      "grad_norm": 1.714797854423523,
      "learning_rate": 0.0007713370473537604,
      "loss": 2.3518,
      "step": 8210
    },
    {
      "epoch": 2.2896935933147633,
      "grad_norm": 1.830570936203003,
      "learning_rate": 0.000771058495821727,
      "loss": 2.3129,
      "step": 8220
    },
    {
      "epoch": 2.2924791086350975,
      "grad_norm": 1.3123340606689453,
      "learning_rate": 0.0007707799442896935,
      "loss": 2.2331,
      "step": 8230
    },
    {
      "epoch": 2.2952646239554317,
      "grad_norm": 1.469950556755066,
      "learning_rate": 0.0007705013927576602,
      "loss": 2.5594,
      "step": 8240
    },
    {
      "epoch": 2.298050139275766,
      "grad_norm": 1.4407873153686523,
      "learning_rate": 0.0007702228412256268,
      "loss": 2.1466,
      "step": 8250
    },
    {
      "epoch": 2.3008356545961,
      "grad_norm": 1.3625340461730957,
      "learning_rate": 0.0007699442896935933,
      "loss": 2.4319,
      "step": 8260
    },
    {
      "epoch": 2.3036211699164344,
      "grad_norm": 2.89880633354187,
      "learning_rate": 0.00076966573816156,
      "loss": 2.2805,
      "step": 8270
    },
    {
      "epoch": 2.3064066852367686,
      "grad_norm": 1.121453881263733,
      "learning_rate": 0.0007693871866295265,
      "loss": 2.5179,
      "step": 8280
    },
    {
      "epoch": 2.309192200557103,
      "grad_norm": 1.5246977806091309,
      "learning_rate": 0.000769108635097493,
      "loss": 2.3963,
      "step": 8290
    },
    {
      "epoch": 2.3119777158774375,
      "grad_norm": 1.162003993988037,
      "learning_rate": 0.0007688300835654595,
      "loss": 2.2096,
      "step": 8300
    },
    {
      "epoch": 2.3147632311977717,
      "grad_norm": 1.563053846359253,
      "learning_rate": 0.0007685515320334262,
      "loss": 2.6406,
      "step": 8310
    },
    {
      "epoch": 2.317548746518106,
      "grad_norm": 2.127366781234741,
      "learning_rate": 0.0007682729805013928,
      "loss": 2.3171,
      "step": 8320
    },
    {
      "epoch": 2.32033426183844,
      "grad_norm": 1.3918875455856323,
      "learning_rate": 0.0007679944289693593,
      "loss": 2.4912,
      "step": 8330
    },
    {
      "epoch": 2.3231197771587744,
      "grad_norm": 1.7538429498672485,
      "learning_rate": 0.000767715877437326,
      "loss": 2.2249,
      "step": 8340
    },
    {
      "epoch": 2.3259052924791086,
      "grad_norm": 1.2900848388671875,
      "learning_rate": 0.0007674373259052925,
      "loss": 2.3947,
      "step": 8350
    },
    {
      "epoch": 2.328690807799443,
      "grad_norm": 1.7530219554901123,
      "learning_rate": 0.0007671587743732591,
      "loss": 2.5832,
      "step": 8360
    },
    {
      "epoch": 2.331476323119777,
      "grad_norm": 1.5113074779510498,
      "learning_rate": 0.0007668802228412256,
      "loss": 2.267,
      "step": 8370
    },
    {
      "epoch": 2.3342618384401113,
      "grad_norm": 1.936834454536438,
      "learning_rate": 0.0007666016713091923,
      "loss": 2.312,
      "step": 8380
    },
    {
      "epoch": 2.337047353760446,
      "grad_norm": 1.2367008924484253,
      "learning_rate": 0.0007663231197771587,
      "loss": 2.5133,
      "step": 8390
    },
    {
      "epoch": 2.33983286908078,
      "grad_norm": 1.314180612564087,
      "learning_rate": 0.0007660445682451253,
      "loss": 2.2657,
      "step": 8400
    },
    {
      "epoch": 2.3426183844011144,
      "grad_norm": 1.5724931955337524,
      "learning_rate": 0.000765766016713092,
      "loss": 2.4004,
      "step": 8410
    },
    {
      "epoch": 2.3454038997214486,
      "grad_norm": 1.3105064630508423,
      "learning_rate": 0.0007654874651810585,
      "loss": 2.1789,
      "step": 8420
    },
    {
      "epoch": 2.348189415041783,
      "grad_norm": 1.3063901662826538,
      "learning_rate": 0.0007652089136490251,
      "loss": 2.3734,
      "step": 8430
    },
    {
      "epoch": 2.350974930362117,
      "grad_norm": 1.6439136266708374,
      "learning_rate": 0.0007649303621169916,
      "loss": 2.6272,
      "step": 8440
    },
    {
      "epoch": 2.3537604456824512,
      "grad_norm": 1.624767541885376,
      "learning_rate": 0.0007646518105849583,
      "loss": 2.4229,
      "step": 8450
    },
    {
      "epoch": 2.3565459610027855,
      "grad_norm": 2.2184672355651855,
      "learning_rate": 0.0007643732590529248,
      "loss": 2.5629,
      "step": 8460
    },
    {
      "epoch": 2.3593314763231197,
      "grad_norm": 1.570643424987793,
      "learning_rate": 0.0007640947075208914,
      "loss": 2.339,
      "step": 8470
    },
    {
      "epoch": 2.362116991643454,
      "grad_norm": 1.904431939125061,
      "learning_rate": 0.000763816155988858,
      "loss": 2.5417,
      "step": 8480
    },
    {
      "epoch": 2.364902506963788,
      "grad_norm": 1.178659439086914,
      "learning_rate": 0.0007635376044568246,
      "loss": 2.3118,
      "step": 8490
    },
    {
      "epoch": 2.3676880222841223,
      "grad_norm": 1.5227019786834717,
      "learning_rate": 0.0007632590529247911,
      "loss": 2.4768,
      "step": 8500
    },
    {
      "epoch": 2.370473537604457,
      "grad_norm": 1.5698143243789673,
      "learning_rate": 0.0007629805013927576,
      "loss": 2.2912,
      "step": 8510
    },
    {
      "epoch": 2.3732590529247912,
      "grad_norm": 1.7420145273208618,
      "learning_rate": 0.0007627019498607243,
      "loss": 2.5274,
      "step": 8520
    },
    {
      "epoch": 2.3760445682451254,
      "grad_norm": 1.3122910261154175,
      "learning_rate": 0.0007624233983286908,
      "loss": 2.3093,
      "step": 8530
    },
    {
      "epoch": 2.3788300835654597,
      "grad_norm": 1.3436490297317505,
      "learning_rate": 0.0007621448467966574,
      "loss": 2.384,
      "step": 8540
    },
    {
      "epoch": 2.381615598885794,
      "grad_norm": 1.3395259380340576,
      "learning_rate": 0.0007618662952646239,
      "loss": 2.2628,
      "step": 8550
    },
    {
      "epoch": 2.384401114206128,
      "grad_norm": 1.487441897392273,
      "learning_rate": 0.0007615877437325906,
      "loss": 2.3133,
      "step": 8560
    },
    {
      "epoch": 2.3871866295264623,
      "grad_norm": 1.7779827117919922,
      "learning_rate": 0.0007613091922005572,
      "loss": 2.4927,
      "step": 8570
    },
    {
      "epoch": 2.3899721448467965,
      "grad_norm": 1.993438482284546,
      "learning_rate": 0.0007610306406685237,
      "loss": 2.3462,
      "step": 8580
    },
    {
      "epoch": 2.3927576601671308,
      "grad_norm": 1.9813001155853271,
      "learning_rate": 0.0007607520891364902,
      "loss": 2.4056,
      "step": 8590
    },
    {
      "epoch": 2.3955431754874654,
      "grad_norm": 2.6499109268188477,
      "learning_rate": 0.0007604735376044568,
      "loss": 2.4174,
      "step": 8600
    },
    {
      "epoch": 2.3983286908077996,
      "grad_norm": 1.554140567779541,
      "learning_rate": 0.0007601949860724234,
      "loss": 2.4704,
      "step": 8610
    },
    {
      "epoch": 2.401114206128134,
      "grad_norm": 2.1478075981140137,
      "learning_rate": 0.0007599164345403899,
      "loss": 2.4283,
      "step": 8620
    },
    {
      "epoch": 2.403899721448468,
      "grad_norm": 2.0523488521575928,
      "learning_rate": 0.0007596378830083566,
      "loss": 2.2976,
      "step": 8630
    },
    {
      "epoch": 2.4066852367688023,
      "grad_norm": 1.4595967531204224,
      "learning_rate": 0.0007593593314763232,
      "loss": 2.4668,
      "step": 8640
    },
    {
      "epoch": 2.4094707520891365,
      "grad_norm": 1.5554499626159668,
      "learning_rate": 0.0007590807799442897,
      "loss": 2.4671,
      "step": 8650
    },
    {
      "epoch": 2.4122562674094707,
      "grad_norm": 1.3619202375411987,
      "learning_rate": 0.0007588022284122563,
      "loss": 2.5394,
      "step": 8660
    },
    {
      "epoch": 2.415041782729805,
      "grad_norm": 1.9726306200027466,
      "learning_rate": 0.0007585236768802229,
      "loss": 2.4969,
      "step": 8670
    },
    {
      "epoch": 2.417827298050139,
      "grad_norm": 1.6018552780151367,
      "learning_rate": 0.0007582451253481895,
      "loss": 2.3237,
      "step": 8680
    },
    {
      "epoch": 2.4206128133704734,
      "grad_norm": 1.5730338096618652,
      "learning_rate": 0.0007579665738161559,
      "loss": 2.3257,
      "step": 8690
    },
    {
      "epoch": 2.4233983286908076,
      "grad_norm": 1.596053957939148,
      "learning_rate": 0.0007576880222841226,
      "loss": 2.1993,
      "step": 8700
    },
    {
      "epoch": 2.426183844011142,
      "grad_norm": 1.0325525999069214,
      "learning_rate": 0.0007574094707520891,
      "loss": 2.2756,
      "step": 8710
    },
    {
      "epoch": 2.4289693593314765,
      "grad_norm": 1.5277142524719238,
      "learning_rate": 0.0007571309192200557,
      "loss": 2.3867,
      "step": 8720
    },
    {
      "epoch": 2.4317548746518107,
      "grad_norm": 1.7261697053909302,
      "learning_rate": 0.0007568523676880223,
      "loss": 2.4966,
      "step": 8730
    },
    {
      "epoch": 2.434540389972145,
      "grad_norm": 1.198167324066162,
      "learning_rate": 0.0007565738161559889,
      "loss": 2.375,
      "step": 8740
    },
    {
      "epoch": 2.437325905292479,
      "grad_norm": 1.3675048351287842,
      "learning_rate": 0.0007562952646239555,
      "loss": 2.3003,
      "step": 8750
    },
    {
      "epoch": 2.4401114206128134,
      "grad_norm": 1.2398661375045776,
      "learning_rate": 0.000756016713091922,
      "loss": 2.3475,
      "step": 8760
    },
    {
      "epoch": 2.4428969359331476,
      "grad_norm": 1.387608289718628,
      "learning_rate": 0.0007557381615598886,
      "loss": 2.3729,
      "step": 8770
    },
    {
      "epoch": 2.445682451253482,
      "grad_norm": 1.8169025182724,
      "learning_rate": 0.0007554596100278552,
      "loss": 2.3103,
      "step": 8780
    },
    {
      "epoch": 2.448467966573816,
      "grad_norm": 1.247129201889038,
      "learning_rate": 0.0007551810584958217,
      "loss": 2.3791,
      "step": 8790
    },
    {
      "epoch": 2.4512534818941503,
      "grad_norm": 1.9401276111602783,
      "learning_rate": 0.0007549025069637883,
      "loss": 2.5758,
      "step": 8800
    },
    {
      "epoch": 2.4540389972144845,
      "grad_norm": 1.3087321519851685,
      "learning_rate": 0.0007546239554317549,
      "loss": 2.3698,
      "step": 8810
    },
    {
      "epoch": 2.456824512534819,
      "grad_norm": 1.3112348318099976,
      "learning_rate": 0.0007543454038997215,
      "loss": 2.5264,
      "step": 8820
    },
    {
      "epoch": 2.4596100278551534,
      "grad_norm": 1.5737661123275757,
      "learning_rate": 0.000754066852367688,
      "loss": 2.4782,
      "step": 8830
    },
    {
      "epoch": 2.4623955431754876,
      "grad_norm": 1.3336981534957886,
      "learning_rate": 0.0007537883008356546,
      "loss": 2.3378,
      "step": 8840
    },
    {
      "epoch": 2.465181058495822,
      "grad_norm": 1.5192991495132446,
      "learning_rate": 0.0007535097493036212,
      "loss": 2.455,
      "step": 8850
    },
    {
      "epoch": 2.467966573816156,
      "grad_norm": 1.3848764896392822,
      "learning_rate": 0.0007532311977715878,
      "loss": 2.4082,
      "step": 8860
    },
    {
      "epoch": 2.4707520891364902,
      "grad_norm": 1.334181785583496,
      "learning_rate": 0.0007529526462395543,
      "loss": 2.4552,
      "step": 8870
    },
    {
      "epoch": 2.4735376044568245,
      "grad_norm": 1.5878896713256836,
      "learning_rate": 0.000752674094707521,
      "loss": 2.6678,
      "step": 8880
    },
    {
      "epoch": 2.4763231197771587,
      "grad_norm": 1.5463829040527344,
      "learning_rate": 0.0007523955431754875,
      "loss": 2.3074,
      "step": 8890
    },
    {
      "epoch": 2.479108635097493,
      "grad_norm": 1.666717767715454,
      "learning_rate": 0.000752116991643454,
      "loss": 2.293,
      "step": 8900
    },
    {
      "epoch": 2.481894150417827,
      "grad_norm": 1.6671006679534912,
      "learning_rate": 0.0007518384401114206,
      "loss": 2.2659,
      "step": 8910
    },
    {
      "epoch": 2.4846796657381613,
      "grad_norm": 2.653984308242798,
      "learning_rate": 0.0007515598885793872,
      "loss": 2.4211,
      "step": 8920
    },
    {
      "epoch": 2.487465181058496,
      "grad_norm": 1.5195986032485962,
      "learning_rate": 0.0007512813370473538,
      "loss": 2.299,
      "step": 8930
    },
    {
      "epoch": 2.4902506963788302,
      "grad_norm": 1.6021162271499634,
      "learning_rate": 0.0007510027855153203,
      "loss": 2.3544,
      "step": 8940
    },
    {
      "epoch": 2.4930362116991645,
      "grad_norm": 1.6794383525848389,
      "learning_rate": 0.0007507242339832869,
      "loss": 2.3324,
      "step": 8950
    },
    {
      "epoch": 2.4958217270194987,
      "grad_norm": 1.4451088905334473,
      "learning_rate": 0.0007504456824512536,
      "loss": 2.1859,
      "step": 8960
    },
    {
      "epoch": 2.498607242339833,
      "grad_norm": 1.2979377508163452,
      "learning_rate": 0.0007501671309192201,
      "loss": 2.4303,
      "step": 8970
    },
    {
      "epoch": 2.501392757660167,
      "grad_norm": 1.4720878601074219,
      "learning_rate": 0.0007498885793871867,
      "loss": 2.2534,
      "step": 8980
    },
    {
      "epoch": 2.5041782729805013,
      "grad_norm": 2.3555307388305664,
      "learning_rate": 0.0007496100278551532,
      "loss": 2.327,
      "step": 8990
    },
    {
      "epoch": 2.5069637883008355,
      "grad_norm": 1.5677169561386108,
      "learning_rate": 0.0007493314763231198,
      "loss": 2.3774,
      "step": 9000
    },
    {
      "epoch": 2.5097493036211698,
      "grad_norm": 1.2421962022781372,
      "learning_rate": 0.0007490529247910863,
      "loss": 2.3632,
      "step": 9010
    },
    {
      "epoch": 2.5125348189415044,
      "grad_norm": 1.7879527807235718,
      "learning_rate": 0.0007487743732590529,
      "loss": 2.3029,
      "step": 9020
    },
    {
      "epoch": 2.5153203342618387,
      "grad_norm": 1.556707739830017,
      "learning_rate": 0.0007484958217270195,
      "loss": 2.3812,
      "step": 9030
    },
    {
      "epoch": 2.518105849582173,
      "grad_norm": 1.109598994255066,
      "learning_rate": 0.0007482172701949861,
      "loss": 2.3609,
      "step": 9040
    },
    {
      "epoch": 2.520891364902507,
      "grad_norm": 1.5781301259994507,
      "learning_rate": 0.0007479387186629527,
      "loss": 2.3106,
      "step": 9050
    },
    {
      "epoch": 2.5236768802228413,
      "grad_norm": 1.21300208568573,
      "learning_rate": 0.0007476601671309193,
      "loss": 2.3911,
      "step": 9060
    },
    {
      "epoch": 2.5264623955431755,
      "grad_norm": 1.3286482095718384,
      "learning_rate": 0.0007473816155988859,
      "loss": 2.1381,
      "step": 9070
    },
    {
      "epoch": 2.5292479108635098,
      "grad_norm": 1.794359564781189,
      "learning_rate": 0.0007471030640668523,
      "loss": 2.3971,
      "step": 9080
    },
    {
      "epoch": 2.532033426183844,
      "grad_norm": 1.722793459892273,
      "learning_rate": 0.0007468245125348189,
      "loss": 2.3995,
      "step": 9090
    },
    {
      "epoch": 2.534818941504178,
      "grad_norm": 1.1472413539886475,
      "learning_rate": 0.0007465459610027855,
      "loss": 2.2193,
      "step": 9100
    },
    {
      "epoch": 2.5376044568245124,
      "grad_norm": 1.8194706439971924,
      "learning_rate": 0.0007462674094707521,
      "loss": 2.2814,
      "step": 9110
    },
    {
      "epoch": 2.5403899721448466,
      "grad_norm": 1.7438243627548218,
      "learning_rate": 0.0007459888579387186,
      "loss": 2.2686,
      "step": 9120
    },
    {
      "epoch": 2.543175487465181,
      "grad_norm": 1.3716647624969482,
      "learning_rate": 0.0007457103064066852,
      "loss": 2.3614,
      "step": 9130
    },
    {
      "epoch": 2.545961002785515,
      "grad_norm": 1.5468199253082275,
      "learning_rate": 0.0007454317548746519,
      "loss": 2.4391,
      "step": 9140
    },
    {
      "epoch": 2.5487465181058497,
      "grad_norm": 1.3654786348342896,
      "learning_rate": 0.0007451532033426184,
      "loss": 2.3694,
      "step": 9150
    },
    {
      "epoch": 2.551532033426184,
      "grad_norm": 1.6226969957351685,
      "learning_rate": 0.000744874651810585,
      "loss": 2.0601,
      "step": 9160
    },
    {
      "epoch": 2.554317548746518,
      "grad_norm": 1.1148478984832764,
      "learning_rate": 0.0007445961002785516,
      "loss": 2.4211,
      "step": 9170
    },
    {
      "epoch": 2.5571030640668524,
      "grad_norm": 1.480338454246521,
      "learning_rate": 0.0007443175487465182,
      "loss": 2.4525,
      "step": 9180
    },
    {
      "epoch": 2.5598885793871866,
      "grad_norm": 1.3976490497589111,
      "learning_rate": 0.0007440389972144846,
      "loss": 2.3754,
      "step": 9190
    },
    {
      "epoch": 2.562674094707521,
      "grad_norm": 1.0736980438232422,
      "learning_rate": 0.0007437604456824512,
      "loss": 2.3318,
      "step": 9200
    },
    {
      "epoch": 2.565459610027855,
      "grad_norm": 1.3330775499343872,
      "learning_rate": 0.0007434818941504179,
      "loss": 2.2294,
      "step": 9210
    },
    {
      "epoch": 2.5682451253481893,
      "grad_norm": 1.637626051902771,
      "learning_rate": 0.0007432033426183844,
      "loss": 2.2904,
      "step": 9220
    },
    {
      "epoch": 2.571030640668524,
      "grad_norm": 1.7723501920700073,
      "learning_rate": 0.000742924791086351,
      "loss": 2.4553,
      "step": 9230
    },
    {
      "epoch": 2.573816155988858,
      "grad_norm": 1.3246526718139648,
      "learning_rate": 0.0007426462395543176,
      "loss": 2.212,
      "step": 9240
    },
    {
      "epoch": 2.5766016713091924,
      "grad_norm": 1.504364252090454,
      "learning_rate": 0.0007423676880222842,
      "loss": 2.3168,
      "step": 9250
    },
    {
      "epoch": 2.5793871866295266,
      "grad_norm": 1.5926234722137451,
      "learning_rate": 0.0007420891364902507,
      "loss": 2.4193,
      "step": 9260
    },
    {
      "epoch": 2.582172701949861,
      "grad_norm": 1.6257201433181763,
      "learning_rate": 0.0007418105849582173,
      "loss": 2.2705,
      "step": 9270
    },
    {
      "epoch": 2.584958217270195,
      "grad_norm": 2.2648043632507324,
      "learning_rate": 0.0007415320334261838,
      "loss": 2.4435,
      "step": 9280
    },
    {
      "epoch": 2.5877437325905293,
      "grad_norm": 1.5594255924224854,
      "learning_rate": 0.0007412534818941504,
      "loss": 2.6378,
      "step": 9290
    },
    {
      "epoch": 2.5905292479108635,
      "grad_norm": 1.1702359914779663,
      "learning_rate": 0.000740974930362117,
      "loss": 2.3537,
      "step": 9300
    },
    {
      "epoch": 2.5933147632311977,
      "grad_norm": 1.6769099235534668,
      "learning_rate": 0.0007406963788300835,
      "loss": 2.4533,
      "step": 9310
    },
    {
      "epoch": 2.596100278551532,
      "grad_norm": 1.4550331830978394,
      "learning_rate": 0.0007404178272980502,
      "loss": 2.3985,
      "step": 9320
    },
    {
      "epoch": 2.598885793871866,
      "grad_norm": 1.2039984464645386,
      "learning_rate": 0.0007401392757660167,
      "loss": 2.1376,
      "step": 9330
    },
    {
      "epoch": 2.6016713091922004,
      "grad_norm": 1.6952106952667236,
      "learning_rate": 0.0007398607242339833,
      "loss": 2.4797,
      "step": 9340
    },
    {
      "epoch": 2.6044568245125346,
      "grad_norm": 1.956058382987976,
      "learning_rate": 0.0007395821727019499,
      "loss": 2.3874,
      "step": 9350
    },
    {
      "epoch": 2.6072423398328692,
      "grad_norm": 1.5059070587158203,
      "learning_rate": 0.0007393036211699165,
      "loss": 2.1977,
      "step": 9360
    },
    {
      "epoch": 2.6100278551532035,
      "grad_norm": 1.4696749448776245,
      "learning_rate": 0.0007390250696378831,
      "loss": 2.5206,
      "step": 9370
    },
    {
      "epoch": 2.6128133704735377,
      "grad_norm": 1.5965481996536255,
      "learning_rate": 0.0007387465181058495,
      "loss": 2.3574,
      "step": 9380
    },
    {
      "epoch": 2.615598885793872,
      "grad_norm": 1.5031001567840576,
      "learning_rate": 0.0007384679665738162,
      "loss": 2.3692,
      "step": 9390
    },
    {
      "epoch": 2.618384401114206,
      "grad_norm": 1.3284039497375488,
      "learning_rate": 0.0007381894150417827,
      "loss": 2.2412,
      "step": 9400
    },
    {
      "epoch": 2.6211699164345403,
      "grad_norm": 1.3027822971343994,
      "learning_rate": 0.0007379108635097493,
      "loss": 2.5613,
      "step": 9410
    },
    {
      "epoch": 2.6239554317548746,
      "grad_norm": 1.6971338987350464,
      "learning_rate": 0.0007376323119777159,
      "loss": 2.5161,
      "step": 9420
    },
    {
      "epoch": 2.6267409470752088,
      "grad_norm": 1.2475694417953491,
      "learning_rate": 0.0007373537604456825,
      "loss": 2.3944,
      "step": 9430
    },
    {
      "epoch": 2.6295264623955434,
      "grad_norm": 1.5631567239761353,
      "learning_rate": 0.000737075208913649,
      "loss": 2.5269,
      "step": 9440
    },
    {
      "epoch": 2.6323119777158777,
      "grad_norm": 1.6893374919891357,
      "learning_rate": 0.0007367966573816156,
      "loss": 2.2431,
      "step": 9450
    },
    {
      "epoch": 2.635097493036212,
      "grad_norm": 1.1720528602600098,
      "learning_rate": 0.0007365181058495823,
      "loss": 2.3721,
      "step": 9460
    },
    {
      "epoch": 2.637883008356546,
      "grad_norm": 1.3217604160308838,
      "learning_rate": 0.0007362395543175488,
      "loss": 2.1472,
      "step": 9470
    },
    {
      "epoch": 2.6406685236768803,
      "grad_norm": 2.0891952514648438,
      "learning_rate": 0.0007359610027855153,
      "loss": 2.5066,
      "step": 9480
    },
    {
      "epoch": 2.6434540389972145,
      "grad_norm": 1.7775816917419434,
      "learning_rate": 0.0007356824512534818,
      "loss": 2.4175,
      "step": 9490
    },
    {
      "epoch": 2.6462395543175488,
      "grad_norm": 1.34629225730896,
      "learning_rate": 0.0007354038997214485,
      "loss": 2.468,
      "step": 9500
    },
    {
      "epoch": 2.649025069637883,
      "grad_norm": 1.3562535047531128,
      "learning_rate": 0.000735125348189415,
      "loss": 2.2678,
      "step": 9510
    },
    {
      "epoch": 2.651810584958217,
      "grad_norm": 1.36008882522583,
      "learning_rate": 0.0007348467966573816,
      "loss": 2.2684,
      "step": 9520
    },
    {
      "epoch": 2.6545961002785514,
      "grad_norm": 1.4720635414123535,
      "learning_rate": 0.0007345682451253483,
      "loss": 2.4464,
      "step": 9530
    },
    {
      "epoch": 2.6573816155988856,
      "grad_norm": 1.325239658355713,
      "learning_rate": 0.0007342896935933148,
      "loss": 2.4295,
      "step": 9540
    },
    {
      "epoch": 2.66016713091922,
      "grad_norm": 1.874584436416626,
      "learning_rate": 0.0007340111420612814,
      "loss": 2.3812,
      "step": 9550
    },
    {
      "epoch": 2.662952646239554,
      "grad_norm": 1.7597228288650513,
      "learning_rate": 0.0007337325905292479,
      "loss": 2.3276,
      "step": 9560
    },
    {
      "epoch": 2.6657381615598887,
      "grad_norm": 1.8239531517028809,
      "learning_rate": 0.0007334540389972146,
      "loss": 2.3247,
      "step": 9570
    },
    {
      "epoch": 2.668523676880223,
      "grad_norm": 1.4959743022918701,
      "learning_rate": 0.000733175487465181,
      "loss": 2.3597,
      "step": 9580
    },
    {
      "epoch": 2.671309192200557,
      "grad_norm": 1.416974663734436,
      "learning_rate": 0.0007328969359331476,
      "loss": 2.4877,
      "step": 9590
    },
    {
      "epoch": 2.6740947075208914,
      "grad_norm": 1.3195037841796875,
      "learning_rate": 0.0007326183844011142,
      "loss": 2.2093,
      "step": 9600
    },
    {
      "epoch": 2.6768802228412256,
      "grad_norm": 1.398378849029541,
      "learning_rate": 0.0007323398328690808,
      "loss": 2.2048,
      "step": 9610
    },
    {
      "epoch": 2.67966573816156,
      "grad_norm": 1.1144508123397827,
      "learning_rate": 0.0007320612813370474,
      "loss": 2.322,
      "step": 9620
    },
    {
      "epoch": 2.682451253481894,
      "grad_norm": 1.6750462055206299,
      "learning_rate": 0.0007317827298050139,
      "loss": 2.4373,
      "step": 9630
    },
    {
      "epoch": 2.6852367688022283,
      "grad_norm": 1.552621603012085,
      "learning_rate": 0.0007315041782729806,
      "loss": 2.3268,
      "step": 9640
    },
    {
      "epoch": 2.688022284122563,
      "grad_norm": 1.1667364835739136,
      "learning_rate": 0.0007312256267409471,
      "loss": 2.4738,
      "step": 9650
    },
    {
      "epoch": 2.690807799442897,
      "grad_norm": 1.544327974319458,
      "learning_rate": 0.0007309470752089137,
      "loss": 2.513,
      "step": 9660
    },
    {
      "epoch": 2.6935933147632314,
      "grad_norm": 1.1829460859298706,
      "learning_rate": 0.0007306685236768801,
      "loss": 2.4636,
      "step": 9670
    },
    {
      "epoch": 2.6963788300835656,
      "grad_norm": 1.6325079202651978,
      "learning_rate": 0.0007303899721448468,
      "loss": 2.3958,
      "step": 9680
    },
    {
      "epoch": 2.6991643454039,
      "grad_norm": 1.7510958909988403,
      "learning_rate": 0.0007301114206128134,
      "loss": 2.4509,
      "step": 9690
    },
    {
      "epoch": 2.701949860724234,
      "grad_norm": 1.3734521865844727,
      "learning_rate": 0.0007298328690807799,
      "loss": 2.5549,
      "step": 9700
    },
    {
      "epoch": 2.7047353760445683,
      "grad_norm": 1.2595340013504028,
      "learning_rate": 0.0007295543175487466,
      "loss": 2.423,
      "step": 9710
    },
    {
      "epoch": 2.7075208913649025,
      "grad_norm": 1.8345084190368652,
      "learning_rate": 0.0007292757660167131,
      "loss": 2.4191,
      "step": 9720
    },
    {
      "epoch": 2.7103064066852367,
      "grad_norm": 1.1468111276626587,
      "learning_rate": 0.0007289972144846797,
      "loss": 2.4281,
      "step": 9730
    },
    {
      "epoch": 2.713091922005571,
      "grad_norm": 1.244115948677063,
      "learning_rate": 0.0007287186629526462,
      "loss": 2.3016,
      "step": 9740
    },
    {
      "epoch": 2.715877437325905,
      "grad_norm": 1.242820382118225,
      "learning_rate": 0.0007284401114206129,
      "loss": 2.2272,
      "step": 9750
    },
    {
      "epoch": 2.7186629526462394,
      "grad_norm": 1.5161426067352295,
      "learning_rate": 0.0007281615598885794,
      "loss": 2.1928,
      "step": 9760
    },
    {
      "epoch": 2.7214484679665736,
      "grad_norm": 1.5532574653625488,
      "learning_rate": 0.000727883008356546,
      "loss": 2.3537,
      "step": 9770
    },
    {
      "epoch": 2.724233983286908,
      "grad_norm": 1.4049654006958008,
      "learning_rate": 0.0007276044568245126,
      "loss": 2.4776,
      "step": 9780
    },
    {
      "epoch": 2.7270194986072425,
      "grad_norm": 1.5402261018753052,
      "learning_rate": 0.0007273259052924791,
      "loss": 2.3465,
      "step": 9790
    },
    {
      "epoch": 2.7298050139275767,
      "grad_norm": 1.4677605628967285,
      "learning_rate": 0.0007270473537604457,
      "loss": 2.4594,
      "step": 9800
    },
    {
      "epoch": 2.732590529247911,
      "grad_norm": 1.7739280462265015,
      "learning_rate": 0.0007267688022284122,
      "loss": 2.2504,
      "step": 9810
    },
    {
      "epoch": 2.735376044568245,
      "grad_norm": 1.6115583181381226,
      "learning_rate": 0.0007264902506963789,
      "loss": 2.2652,
      "step": 9820
    },
    {
      "epoch": 2.7381615598885793,
      "grad_norm": 1.702363133430481,
      "learning_rate": 0.0007262116991643454,
      "loss": 2.3407,
      "step": 9830
    },
    {
      "epoch": 2.7409470752089136,
      "grad_norm": 1.5057480335235596,
      "learning_rate": 0.000725933147632312,
      "loss": 2.172,
      "step": 9840
    },
    {
      "epoch": 2.743732590529248,
      "grad_norm": 1.4189131259918213,
      "learning_rate": 0.0007256545961002786,
      "loss": 2.374,
      "step": 9850
    },
    {
      "epoch": 2.7465181058495824,
      "grad_norm": 1.1436545848846436,
      "learning_rate": 0.0007253760445682452,
      "loss": 2.3111,
      "step": 9860
    },
    {
      "epoch": 2.7493036211699167,
      "grad_norm": 1.2805694341659546,
      "learning_rate": 0.0007250974930362118,
      "loss": 2.3262,
      "step": 9870
    },
    {
      "epoch": 2.752089136490251,
      "grad_norm": 1.3823996782302856,
      "learning_rate": 0.0007248189415041782,
      "loss": 2.466,
      "step": 9880
    },
    {
      "epoch": 2.754874651810585,
      "grad_norm": 1.8634110689163208,
      "learning_rate": 0.0007245403899721449,
      "loss": 2.2591,
      "step": 9890
    },
    {
      "epoch": 2.7576601671309193,
      "grad_norm": 1.4808744192123413,
      "learning_rate": 0.0007242618384401114,
      "loss": 2.2995,
      "step": 9900
    },
    {
      "epoch": 2.7604456824512535,
      "grad_norm": 1.3758004903793335,
      "learning_rate": 0.000723983286908078,
      "loss": 2.4486,
      "step": 9910
    },
    {
      "epoch": 2.7632311977715878,
      "grad_norm": 1.4143640995025635,
      "learning_rate": 0.0007237047353760445,
      "loss": 2.4955,
      "step": 9920
    },
    {
      "epoch": 2.766016713091922,
      "grad_norm": 1.6344133615493774,
      "learning_rate": 0.0007234261838440112,
      "loss": 2.4394,
      "step": 9930
    },
    {
      "epoch": 2.768802228412256,
      "grad_norm": 1.2570867538452148,
      "learning_rate": 0.0007231476323119778,
      "loss": 2.2664,
      "step": 9940
    },
    {
      "epoch": 2.7715877437325904,
      "grad_norm": 1.690895915031433,
      "learning_rate": 0.0007228690807799443,
      "loss": 2.3687,
      "step": 9950
    },
    {
      "epoch": 2.7743732590529246,
      "grad_norm": 1.4593535661697388,
      "learning_rate": 0.000722590529247911,
      "loss": 2.0733,
      "step": 9960
    },
    {
      "epoch": 2.777158774373259,
      "grad_norm": 1.3056105375289917,
      "learning_rate": 0.0007223119777158774,
      "loss": 2.3548,
      "step": 9970
    },
    {
      "epoch": 2.779944289693593,
      "grad_norm": 1.3442400693893433,
      "learning_rate": 0.000722033426183844,
      "loss": 2.4566,
      "step": 9980
    },
    {
      "epoch": 2.7827298050139273,
      "grad_norm": 1.4143109321594238,
      "learning_rate": 0.0007217548746518105,
      "loss": 2.4555,
      "step": 9990
    },
    {
      "epoch": 2.785515320334262,
      "grad_norm": 1.2888611555099487,
      "learning_rate": 0.0007214763231197772,
      "loss": 2.3781,
      "step": 10000
    },
    {
      "epoch": 2.788300835654596,
      "grad_norm": 1.5559262037277222,
      "learning_rate": 0.0007211977715877438,
      "loss": 2.5863,
      "step": 10010
    },
    {
      "epoch": 2.7910863509749304,
      "grad_norm": 1.263867974281311,
      "learning_rate": 0.0007209192200557103,
      "loss": 2.452,
      "step": 10020
    },
    {
      "epoch": 2.7938718662952646,
      "grad_norm": 1.2461581230163574,
      "learning_rate": 0.0007206406685236769,
      "loss": 2.4872,
      "step": 10030
    },
    {
      "epoch": 2.796657381615599,
      "grad_norm": 1.8553184270858765,
      "learning_rate": 0.0007203621169916435,
      "loss": 2.4772,
      "step": 10040
    },
    {
      "epoch": 2.799442896935933,
      "grad_norm": 1.520257592201233,
      "learning_rate": 0.0007200835654596101,
      "loss": 2.4424,
      "step": 10050
    },
    {
      "epoch": 2.8022284122562673,
      "grad_norm": 1.4219661951065063,
      "learning_rate": 0.0007198050139275766,
      "loss": 2.5352,
      "step": 10060
    },
    {
      "epoch": 2.8050139275766015,
      "grad_norm": 1.6734304428100586,
      "learning_rate": 0.0007195264623955433,
      "loss": 2.3102,
      "step": 10070
    },
    {
      "epoch": 2.807799442896936,
      "grad_norm": 1.9599668979644775,
      "learning_rate": 0.0007192479108635097,
      "loss": 2.226,
      "step": 10080
    },
    {
      "epoch": 2.8105849582172704,
      "grad_norm": 1.9473886489868164,
      "learning_rate": 0.0007189693593314763,
      "loss": 2.3393,
      "step": 10090
    },
    {
      "epoch": 2.8133704735376046,
      "grad_norm": 1.3158667087554932,
      "learning_rate": 0.0007186908077994429,
      "loss": 2.5351,
      "step": 10100
    },
    {
      "epoch": 2.816155988857939,
      "grad_norm": 1.3466463088989258,
      "learning_rate": 0.0007184122562674095,
      "loss": 2.3272,
      "step": 10110
    },
    {
      "epoch": 2.818941504178273,
      "grad_norm": 1.6411749124526978,
      "learning_rate": 0.0007181337047353761,
      "loss": 2.2262,
      "step": 10120
    },
    {
      "epoch": 2.8217270194986073,
      "grad_norm": 1.3673292398452759,
      "learning_rate": 0.0007178551532033426,
      "loss": 2.3275,
      "step": 10130
    },
    {
      "epoch": 2.8245125348189415,
      "grad_norm": 3.0413944721221924,
      "learning_rate": 0.0007175766016713092,
      "loss": 2.3846,
      "step": 10140
    },
    {
      "epoch": 2.8272980501392757,
      "grad_norm": 1.3676607608795166,
      "learning_rate": 0.0007172980501392758,
      "loss": 2.3295,
      "step": 10150
    },
    {
      "epoch": 2.83008356545961,
      "grad_norm": 1.7347034215927124,
      "learning_rate": 0.0007170194986072424,
      "loss": 2.5547,
      "step": 10160
    },
    {
      "epoch": 2.832869080779944,
      "grad_norm": 1.5477913618087769,
      "learning_rate": 0.0007167409470752089,
      "loss": 2.4441,
      "step": 10170
    },
    {
      "epoch": 2.8356545961002784,
      "grad_norm": 1.0937153100967407,
      "learning_rate": 0.0007164623955431755,
      "loss": 2.3343,
      "step": 10180
    },
    {
      "epoch": 2.8384401114206126,
      "grad_norm": 1.3143982887268066,
      "learning_rate": 0.0007161838440111421,
      "loss": 2.3928,
      "step": 10190
    },
    {
      "epoch": 2.841225626740947,
      "grad_norm": 1.4250246286392212,
      "learning_rate": 0.0007159052924791086,
      "loss": 2.4672,
      "step": 10200
    },
    {
      "epoch": 2.8440111420612815,
      "grad_norm": 1.435807466506958,
      "learning_rate": 0.0007156267409470752,
      "loss": 2.2628,
      "step": 10210
    },
    {
      "epoch": 2.8467966573816157,
      "grad_norm": 1.0603989362716675,
      "learning_rate": 0.0007153481894150418,
      "loss": 2.3025,
      "step": 10220
    },
    {
      "epoch": 2.84958217270195,
      "grad_norm": 1.1977171897888184,
      "learning_rate": 0.0007150696378830084,
      "loss": 2.2101,
      "step": 10230
    },
    {
      "epoch": 2.852367688022284,
      "grad_norm": 1.2057386636734009,
      "learning_rate": 0.0007147910863509749,
      "loss": 2.397,
      "step": 10240
    },
    {
      "epoch": 2.8551532033426184,
      "grad_norm": 1.3507020473480225,
      "learning_rate": 0.0007145125348189416,
      "loss": 2.5731,
      "step": 10250
    },
    {
      "epoch": 2.8579387186629526,
      "grad_norm": 1.2909852266311646,
      "learning_rate": 0.0007142339832869082,
      "loss": 2.4819,
      "step": 10260
    },
    {
      "epoch": 2.860724233983287,
      "grad_norm": 1.3957420587539673,
      "learning_rate": 0.0007139554317548746,
      "loss": 2.4421,
      "step": 10270
    },
    {
      "epoch": 2.863509749303621,
      "grad_norm": 1.2625739574432373,
      "learning_rate": 0.0007136768802228412,
      "loss": 2.3188,
      "step": 10280
    },
    {
      "epoch": 2.8662952646239557,
      "grad_norm": 1.3548649549484253,
      "learning_rate": 0.0007133983286908078,
      "loss": 2.4103,
      "step": 10290
    },
    {
      "epoch": 2.86908077994429,
      "grad_norm": 2.166933298110962,
      "learning_rate": 0.0007131197771587744,
      "loss": 2.3218,
      "step": 10300
    },
    {
      "epoch": 2.871866295264624,
      "grad_norm": 1.4916737079620361,
      "learning_rate": 0.0007128412256267409,
      "loss": 2.5591,
      "step": 10310
    },
    {
      "epoch": 2.8746518105849583,
      "grad_norm": 1.52994704246521,
      "learning_rate": 0.0007125626740947075,
      "loss": 2.2849,
      "step": 10320
    },
    {
      "epoch": 2.8774373259052926,
      "grad_norm": 1.9716960191726685,
      "learning_rate": 0.0007122841225626742,
      "loss": 2.311,
      "step": 10330
    },
    {
      "epoch": 2.8802228412256268,
      "grad_norm": 2.116320848464966,
      "learning_rate": 0.0007120055710306407,
      "loss": 2.197,
      "step": 10340
    },
    {
      "epoch": 2.883008356545961,
      "grad_norm": 1.2003915309906006,
      "learning_rate": 0.0007117270194986073,
      "loss": 2.3619,
      "step": 10350
    },
    {
      "epoch": 2.885793871866295,
      "grad_norm": 1.2737764120101929,
      "learning_rate": 0.0007114484679665739,
      "loss": 2.3713,
      "step": 10360
    },
    {
      "epoch": 2.8885793871866294,
      "grad_norm": 1.449415922164917,
      "learning_rate": 0.0007111699164345404,
      "loss": 2.4678,
      "step": 10370
    },
    {
      "epoch": 2.8913649025069637,
      "grad_norm": 1.3802268505096436,
      "learning_rate": 0.0007108913649025069,
      "loss": 2.5444,
      "step": 10380
    },
    {
      "epoch": 2.894150417827298,
      "grad_norm": 1.6831403970718384,
      "learning_rate": 0.0007106128133704735,
      "loss": 2.4699,
      "step": 10390
    },
    {
      "epoch": 2.896935933147632,
      "grad_norm": 1.3345669507980347,
      "learning_rate": 0.0007103342618384401,
      "loss": 2.3795,
      "step": 10400
    },
    {
      "epoch": 2.8997214484679663,
      "grad_norm": 1.7766412496566772,
      "learning_rate": 0.0007100557103064067,
      "loss": 2.6337,
      "step": 10410
    },
    {
      "epoch": 2.902506963788301,
      "grad_norm": 1.2396695613861084,
      "learning_rate": 0.0007097771587743733,
      "loss": 2.4191,
      "step": 10420
    },
    {
      "epoch": 2.905292479108635,
      "grad_norm": 1.343619704246521,
      "learning_rate": 0.0007094986072423399,
      "loss": 2.3358,
      "step": 10430
    },
    {
      "epoch": 2.9080779944289694,
      "grad_norm": 1.6909853219985962,
      "learning_rate": 0.0007092200557103065,
      "loss": 2.4362,
      "step": 10440
    },
    {
      "epoch": 2.9108635097493036,
      "grad_norm": 1.1793484687805176,
      "learning_rate": 0.000708941504178273,
      "loss": 2.2321,
      "step": 10450
    },
    {
      "epoch": 2.913649025069638,
      "grad_norm": 1.4729446172714233,
      "learning_rate": 0.0007086629526462395,
      "loss": 2.37,
      "step": 10460
    },
    {
      "epoch": 2.916434540389972,
      "grad_norm": 2.000260353088379,
      "learning_rate": 0.0007083844011142061,
      "loss": 2.3251,
      "step": 10470
    },
    {
      "epoch": 2.9192200557103063,
      "grad_norm": 1.316098928451538,
      "learning_rate": 0.0007081058495821727,
      "loss": 2.3425,
      "step": 10480
    },
    {
      "epoch": 2.9220055710306405,
      "grad_norm": 1.5097795724868774,
      "learning_rate": 0.0007078272980501393,
      "loss": 2.1667,
      "step": 10490
    },
    {
      "epoch": 2.924791086350975,
      "grad_norm": 1.5296151638031006,
      "learning_rate": 0.0007075487465181058,
      "loss": 2.4232,
      "step": 10500
    },
    {
      "epoch": 2.9275766016713094,
      "grad_norm": 1.553838849067688,
      "learning_rate": 0.0007072701949860725,
      "loss": 2.276,
      "step": 10510
    },
    {
      "epoch": 2.9303621169916436,
      "grad_norm": 1.3141677379608154,
      "learning_rate": 0.000706991643454039,
      "loss": 2.3956,
      "step": 10520
    },
    {
      "epoch": 2.933147632311978,
      "grad_norm": 1.7851088047027588,
      "learning_rate": 0.0007067130919220056,
      "loss": 2.3782,
      "step": 10530
    },
    {
      "epoch": 2.935933147632312,
      "grad_norm": 1.2855138778686523,
      "learning_rate": 0.0007064345403899722,
      "loss": 2.1889,
      "step": 10540
    },
    {
      "epoch": 2.9387186629526463,
      "grad_norm": 1.7497972249984741,
      "learning_rate": 0.0007061559888579388,
      "loss": 2.3371,
      "step": 10550
    },
    {
      "epoch": 2.9415041782729805,
      "grad_norm": 1.4410792589187622,
      "learning_rate": 0.0007058774373259052,
      "loss": 2.3559,
      "step": 10560
    },
    {
      "epoch": 2.9442896935933147,
      "grad_norm": 1.2176616191864014,
      "learning_rate": 0.0007055988857938718,
      "loss": 2.2964,
      "step": 10570
    },
    {
      "epoch": 2.947075208913649,
      "grad_norm": 1.2907081842422485,
      "learning_rate": 0.0007053203342618385,
      "loss": 2.3275,
      "step": 10580
    },
    {
      "epoch": 2.949860724233983,
      "grad_norm": 2.1762454509735107,
      "learning_rate": 0.000705041782729805,
      "loss": 2.5924,
      "step": 10590
    },
    {
      "epoch": 2.9526462395543174,
      "grad_norm": 2.007300853729248,
      "learning_rate": 0.0007047632311977716,
      "loss": 2.5487,
      "step": 10600
    },
    {
      "epoch": 2.9554317548746516,
      "grad_norm": 1.6153416633605957,
      "learning_rate": 0.0007044846796657382,
      "loss": 2.4156,
      "step": 10610
    },
    {
      "epoch": 2.958217270194986,
      "grad_norm": 1.2136789560317993,
      "learning_rate": 0.0007042061281337048,
      "loss": 2.3244,
      "step": 10620
    },
    {
      "epoch": 2.9610027855153205,
      "grad_norm": 1.5773162841796875,
      "learning_rate": 0.0007039275766016713,
      "loss": 2.4532,
      "step": 10630
    },
    {
      "epoch": 2.9637883008356547,
      "grad_norm": 1.6605310440063477,
      "learning_rate": 0.0007036490250696379,
      "loss": 2.4558,
      "step": 10640
    },
    {
      "epoch": 2.966573816155989,
      "grad_norm": 1.382476806640625,
      "learning_rate": 0.0007033704735376045,
      "loss": 2.3329,
      "step": 10650
    },
    {
      "epoch": 2.969359331476323,
      "grad_norm": 1.4320905208587646,
      "learning_rate": 0.000703091922005571,
      "loss": 2.2923,
      "step": 10660
    },
    {
      "epoch": 2.9721448467966574,
      "grad_norm": 1.213707685470581,
      "learning_rate": 0.0007028133704735376,
      "loss": 2.2542,
      "step": 10670
    },
    {
      "epoch": 2.9749303621169916,
      "grad_norm": 1.3783247470855713,
      "learning_rate": 0.0007025348189415041,
      "loss": 2.2337,
      "step": 10680
    },
    {
      "epoch": 2.977715877437326,
      "grad_norm": 1.3815464973449707,
      "learning_rate": 0.0007022562674094708,
      "loss": 2.4456,
      "step": 10690
    },
    {
      "epoch": 2.98050139275766,
      "grad_norm": 1.932098388671875,
      "learning_rate": 0.0007019777158774373,
      "loss": 2.552,
      "step": 10700
    },
    {
      "epoch": 2.9832869080779947,
      "grad_norm": 1.522336721420288,
      "learning_rate": 0.0007016991643454039,
      "loss": 2.5597,
      "step": 10710
    },
    {
      "epoch": 2.986072423398329,
      "grad_norm": 1.7078583240509033,
      "learning_rate": 0.0007014206128133705,
      "loss": 2.3063,
      "step": 10720
    },
    {
      "epoch": 2.988857938718663,
      "grad_norm": 1.653467059135437,
      "learning_rate": 0.0007011420612813371,
      "loss": 2.3933,
      "step": 10730
    },
    {
      "epoch": 2.9916434540389973,
      "grad_norm": 1.296477198600769,
      "learning_rate": 0.0007008635097493037,
      "loss": 2.3973,
      "step": 10740
    },
    {
      "epoch": 2.9944289693593316,
      "grad_norm": 1.25565505027771,
      "learning_rate": 0.0007005849582172702,
      "loss": 2.2079,
      "step": 10750
    },
    {
      "epoch": 2.997214484679666,
      "grad_norm": 1.7394973039627075,
      "learning_rate": 0.0007003064066852369,
      "loss": 2.378,
      "step": 10760
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.9593335390090942,
      "learning_rate": 0.0007000278551532033,
      "loss": 2.3467,
      "step": 10770
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.315568685531616,
      "eval_runtime": 5.3712,
      "eval_samples_per_second": 594.282,
      "eval_steps_per_second": 74.285,
      "step": 10770
    },
    {
      "epoch": 3.002785515320334,
      "grad_norm": 1.334330439567566,
      "learning_rate": 0.0006997493036211699,
      "loss": 2.2857,
      "step": 10780
    },
    {
      "epoch": 3.0055710306406684,
      "grad_norm": 1.3440320491790771,
      "learning_rate": 0.0006994707520891365,
      "loss": 2.2342,
      "step": 10790
    },
    {
      "epoch": 3.0083565459610027,
      "grad_norm": 1.442097783088684,
      "learning_rate": 0.0006991922005571031,
      "loss": 2.2745,
      "step": 10800
    },
    {
      "epoch": 3.011142061281337,
      "grad_norm": 1.9926830530166626,
      "learning_rate": 0.0006989136490250696,
      "loss": 2.1582,
      "step": 10810
    },
    {
      "epoch": 3.013927576601671,
      "grad_norm": 1.4142866134643555,
      "learning_rate": 0.0006986350974930362,
      "loss": 2.3609,
      "step": 10820
    },
    {
      "epoch": 3.0167130919220058,
      "grad_norm": 1.3299813270568848,
      "learning_rate": 0.0006983565459610029,
      "loss": 2.1591,
      "step": 10830
    },
    {
      "epoch": 3.01949860724234,
      "grad_norm": 1.1343332529067993,
      "learning_rate": 0.0006980779944289694,
      "loss": 2.3264,
      "step": 10840
    },
    {
      "epoch": 3.022284122562674,
      "grad_norm": 2.0583078861236572,
      "learning_rate": 0.000697799442896936,
      "loss": 2.4349,
      "step": 10850
    },
    {
      "epoch": 3.0250696378830084,
      "grad_norm": 2.08536958694458,
      "learning_rate": 0.0006975208913649024,
      "loss": 2.2887,
      "step": 10860
    },
    {
      "epoch": 3.0278551532033426,
      "grad_norm": 1.4799771308898926,
      "learning_rate": 0.0006972423398328691,
      "loss": 2.3641,
      "step": 10870
    },
    {
      "epoch": 3.030640668523677,
      "grad_norm": 1.6490578651428223,
      "learning_rate": 0.0006969637883008356,
      "loss": 2.5135,
      "step": 10880
    },
    {
      "epoch": 3.033426183844011,
      "grad_norm": 1.959760069847107,
      "learning_rate": 0.0006966852367688022,
      "loss": 2.3297,
      "step": 10890
    },
    {
      "epoch": 3.0362116991643453,
      "grad_norm": 1.689745545387268,
      "learning_rate": 0.0006964066852367689,
      "loss": 2.2573,
      "step": 10900
    },
    {
      "epoch": 3.0389972144846795,
      "grad_norm": 1.6763256788253784,
      "learning_rate": 0.0006961281337047354,
      "loss": 2.2336,
      "step": 10910
    },
    {
      "epoch": 3.0417827298050137,
      "grad_norm": 1.6861227750778198,
      "learning_rate": 0.000695849582172702,
      "loss": 2.5177,
      "step": 10920
    },
    {
      "epoch": 3.0445682451253484,
      "grad_norm": 1.2797569036483765,
      "learning_rate": 0.0006955710306406685,
      "loss": 2.219,
      "step": 10930
    },
    {
      "epoch": 3.0473537604456826,
      "grad_norm": 1.4141643047332764,
      "learning_rate": 0.0006952924791086352,
      "loss": 2.4451,
      "step": 10940
    },
    {
      "epoch": 3.050139275766017,
      "grad_norm": 1.1971104145050049,
      "learning_rate": 0.0006950139275766017,
      "loss": 2.2346,
      "step": 10950
    },
    {
      "epoch": 3.052924791086351,
      "grad_norm": 2.0279126167297363,
      "learning_rate": 0.0006947353760445682,
      "loss": 2.4757,
      "step": 10960
    },
    {
      "epoch": 3.0557103064066853,
      "grad_norm": 1.6788012981414795,
      "learning_rate": 0.0006944568245125348,
      "loss": 2.3081,
      "step": 10970
    },
    {
      "epoch": 3.0584958217270195,
      "grad_norm": 1.4989509582519531,
      "learning_rate": 0.0006941782729805014,
      "loss": 2.3705,
      "step": 10980
    },
    {
      "epoch": 3.0612813370473537,
      "grad_norm": 1.5092414617538452,
      "learning_rate": 0.000693899721448468,
      "loss": 2.3945,
      "step": 10990
    },
    {
      "epoch": 3.064066852367688,
      "grad_norm": 1.2165988683700562,
      "learning_rate": 0.0006936211699164345,
      "loss": 2.2367,
      "step": 11000
    },
    {
      "epoch": 3.066852367688022,
      "grad_norm": 1.0267361402511597,
      "learning_rate": 0.0006933426183844012,
      "loss": 2.1734,
      "step": 11010
    },
    {
      "epoch": 3.0696378830083564,
      "grad_norm": 1.4651734828948975,
      "learning_rate": 0.0006930640668523677,
      "loss": 2.2543,
      "step": 11020
    },
    {
      "epoch": 3.0724233983286906,
      "grad_norm": 1.9580475091934204,
      "learning_rate": 0.0006927855153203343,
      "loss": 2.4312,
      "step": 11030
    },
    {
      "epoch": 3.0752089136490253,
      "grad_norm": 1.6410049200057983,
      "learning_rate": 0.0006925069637883008,
      "loss": 2.4323,
      "step": 11040
    },
    {
      "epoch": 3.0779944289693595,
      "grad_norm": 1.4049475193023682,
      "learning_rate": 0.0006922284122562675,
      "loss": 2.415,
      "step": 11050
    },
    {
      "epoch": 3.0807799442896937,
      "grad_norm": 1.7643928527832031,
      "learning_rate": 0.000691949860724234,
      "loss": 2.3312,
      "step": 11060
    },
    {
      "epoch": 3.083565459610028,
      "grad_norm": 1.4554303884506226,
      "learning_rate": 0.0006916713091922005,
      "loss": 2.2375,
      "step": 11070
    },
    {
      "epoch": 3.086350974930362,
      "grad_norm": 1.4979705810546875,
      "learning_rate": 0.0006913927576601672,
      "loss": 2.2517,
      "step": 11080
    },
    {
      "epoch": 3.0891364902506964,
      "grad_norm": 1.3321504592895508,
      "learning_rate": 0.0006911142061281337,
      "loss": 2.3529,
      "step": 11090
    },
    {
      "epoch": 3.0919220055710306,
      "grad_norm": 1.846463918685913,
      "learning_rate": 0.0006908356545961003,
      "loss": 2.3132,
      "step": 11100
    },
    {
      "epoch": 3.094707520891365,
      "grad_norm": 1.2359659671783447,
      "learning_rate": 0.0006905571030640668,
      "loss": 2.0859,
      "step": 11110
    },
    {
      "epoch": 3.097493036211699,
      "grad_norm": 1.3283135890960693,
      "learning_rate": 0.0006902785515320335,
      "loss": 2.5187,
      "step": 11120
    },
    {
      "epoch": 3.1002785515320332,
      "grad_norm": 1.7035953998565674,
      "learning_rate": 0.00069,
      "loss": 2.3907,
      "step": 11130
    },
    {
      "epoch": 3.103064066852368,
      "grad_norm": 1.2901904582977295,
      "learning_rate": 0.0006897214484679666,
      "loss": 2.3691,
      "step": 11140
    },
    {
      "epoch": 3.105849582172702,
      "grad_norm": 1.6034349203109741,
      "learning_rate": 0.0006894428969359333,
      "loss": 2.3657,
      "step": 11150
    },
    {
      "epoch": 3.1086350974930363,
      "grad_norm": 1.2295947074890137,
      "learning_rate": 0.0006891643454038997,
      "loss": 2.2888,
      "step": 11160
    },
    {
      "epoch": 3.1114206128133706,
      "grad_norm": 1.4204400777816772,
      "learning_rate": 0.0006888857938718663,
      "loss": 2.0287,
      "step": 11170
    },
    {
      "epoch": 3.114206128133705,
      "grad_norm": 1.5670419931411743,
      "learning_rate": 0.0006886072423398328,
      "loss": 2.3335,
      "step": 11180
    },
    {
      "epoch": 3.116991643454039,
      "grad_norm": 1.5849183797836304,
      "learning_rate": 0.0006883286908077995,
      "loss": 2.2639,
      "step": 11190
    },
    {
      "epoch": 3.1197771587743732,
      "grad_norm": 1.2492289543151855,
      "learning_rate": 0.000688050139275766,
      "loss": 2.2456,
      "step": 11200
    },
    {
      "epoch": 3.1225626740947074,
      "grad_norm": 1.8725464344024658,
      "learning_rate": 0.0006877715877437326,
      "loss": 2.2572,
      "step": 11210
    },
    {
      "epoch": 3.1253481894150417,
      "grad_norm": 1.6757550239562988,
      "learning_rate": 0.0006874930362116992,
      "loss": 2.2245,
      "step": 11220
    },
    {
      "epoch": 3.128133704735376,
      "grad_norm": 1.2970085144042969,
      "learning_rate": 0.0006872144846796658,
      "loss": 2.6199,
      "step": 11230
    },
    {
      "epoch": 3.13091922005571,
      "grad_norm": 1.5082409381866455,
      "learning_rate": 0.0006869359331476324,
      "loss": 2.2582,
      "step": 11240
    },
    {
      "epoch": 3.1337047353760448,
      "grad_norm": 1.6220455169677734,
      "learning_rate": 0.0006866573816155988,
      "loss": 2.2368,
      "step": 11250
    },
    {
      "epoch": 3.136490250696379,
      "grad_norm": 1.2128148078918457,
      "learning_rate": 0.0006863788300835655,
      "loss": 2.3653,
      "step": 11260
    },
    {
      "epoch": 3.139275766016713,
      "grad_norm": 1.862984538078308,
      "learning_rate": 0.000686100278551532,
      "loss": 2.4136,
      "step": 11270
    },
    {
      "epoch": 3.1420612813370474,
      "grad_norm": 1.2187355756759644,
      "learning_rate": 0.0006858217270194986,
      "loss": 2.286,
      "step": 11280
    },
    {
      "epoch": 3.1448467966573816,
      "grad_norm": 2.768639326095581,
      "learning_rate": 0.0006855431754874651,
      "loss": 2.2518,
      "step": 11290
    },
    {
      "epoch": 3.147632311977716,
      "grad_norm": 2.2767341136932373,
      "learning_rate": 0.0006852646239554318,
      "loss": 2.4299,
      "step": 11300
    },
    {
      "epoch": 3.15041782729805,
      "grad_norm": 1.2363550662994385,
      "learning_rate": 0.0006849860724233984,
      "loss": 2.3455,
      "step": 11310
    },
    {
      "epoch": 3.1532033426183843,
      "grad_norm": 1.3353663682937622,
      "learning_rate": 0.0006847075208913649,
      "loss": 2.4402,
      "step": 11320
    },
    {
      "epoch": 3.1559888579387185,
      "grad_norm": 1.5702191591262817,
      "learning_rate": 0.0006844289693593316,
      "loss": 2.3544,
      "step": 11330
    },
    {
      "epoch": 3.1587743732590527,
      "grad_norm": 1.87246835231781,
      "learning_rate": 0.0006841504178272981,
      "loss": 2.3157,
      "step": 11340
    },
    {
      "epoch": 3.1615598885793874,
      "grad_norm": 1.619587779045105,
      "learning_rate": 0.0006838718662952646,
      "loss": 2.3756,
      "step": 11350
    },
    {
      "epoch": 3.1643454038997216,
      "grad_norm": 1.5300110578536987,
      "learning_rate": 0.0006835933147632311,
      "loss": 2.3127,
      "step": 11360
    },
    {
      "epoch": 3.167130919220056,
      "grad_norm": 1.424216866493225,
      "learning_rate": 0.0006833147632311978,
      "loss": 2.1369,
      "step": 11370
    },
    {
      "epoch": 3.16991643454039,
      "grad_norm": 1.2420849800109863,
      "learning_rate": 0.0006830362116991644,
      "loss": 2.2635,
      "step": 11380
    },
    {
      "epoch": 3.1727019498607243,
      "grad_norm": 2.0106077194213867,
      "learning_rate": 0.0006827576601671309,
      "loss": 2.4471,
      "step": 11390
    },
    {
      "epoch": 3.1754874651810585,
      "grad_norm": 1.280112624168396,
      "learning_rate": 0.0006824791086350975,
      "loss": 2.3982,
      "step": 11400
    },
    {
      "epoch": 3.1782729805013927,
      "grad_norm": 1.4067001342773438,
      "learning_rate": 0.0006822005571030641,
      "loss": 2.2861,
      "step": 11410
    },
    {
      "epoch": 3.181058495821727,
      "grad_norm": 1.3846855163574219,
      "learning_rate": 0.0006819220055710307,
      "loss": 2.0731,
      "step": 11420
    },
    {
      "epoch": 3.183844011142061,
      "grad_norm": 1.5332492589950562,
      "learning_rate": 0.0006816434540389972,
      "loss": 2.545,
      "step": 11430
    },
    {
      "epoch": 3.1866295264623954,
      "grad_norm": 1.5308562517166138,
      "learning_rate": 0.0006813649025069639,
      "loss": 2.179,
      "step": 11440
    },
    {
      "epoch": 3.1894150417827296,
      "grad_norm": 1.257583737373352,
      "learning_rate": 0.0006810863509749303,
      "loss": 2.2043,
      "step": 11450
    },
    {
      "epoch": 3.1922005571030643,
      "grad_norm": 1.9841477870941162,
      "learning_rate": 0.0006808077994428969,
      "loss": 2.2262,
      "step": 11460
    },
    {
      "epoch": 3.1949860724233985,
      "grad_norm": 1.7657321691513062,
      "learning_rate": 0.0006805292479108635,
      "loss": 2.3825,
      "step": 11470
    },
    {
      "epoch": 3.1977715877437327,
      "grad_norm": 1.4417176246643066,
      "learning_rate": 0.0006802506963788301,
      "loss": 2.2818,
      "step": 11480
    },
    {
      "epoch": 3.200557103064067,
      "grad_norm": 1.6027814149856567,
      "learning_rate": 0.0006799721448467967,
      "loss": 2.2256,
      "step": 11490
    },
    {
      "epoch": 3.203342618384401,
      "grad_norm": 1.1666828393936157,
      "learning_rate": 0.0006796935933147632,
      "loss": 2.3218,
      "step": 11500
    },
    {
      "epoch": 3.2061281337047354,
      "grad_norm": 1.2694981098175049,
      "learning_rate": 0.0006794150417827299,
      "loss": 2.3901,
      "step": 11510
    },
    {
      "epoch": 3.2089136490250696,
      "grad_norm": 1.408821702003479,
      "learning_rate": 0.0006791364902506964,
      "loss": 2.2231,
      "step": 11520
    },
    {
      "epoch": 3.211699164345404,
      "grad_norm": 1.3457497358322144,
      "learning_rate": 0.000678857938718663,
      "loss": 2.4681,
      "step": 11530
    },
    {
      "epoch": 3.214484679665738,
      "grad_norm": 1.387740135192871,
      "learning_rate": 0.0006785793871866296,
      "loss": 2.421,
      "step": 11540
    },
    {
      "epoch": 3.2172701949860723,
      "grad_norm": 1.2911399602890015,
      "learning_rate": 0.0006783008356545961,
      "loss": 2.1782,
      "step": 11550
    },
    {
      "epoch": 3.220055710306407,
      "grad_norm": 1.662872552871704,
      "learning_rate": 0.0006780222841225627,
      "loss": 2.175,
      "step": 11560
    },
    {
      "epoch": 3.222841225626741,
      "grad_norm": 2.1195790767669678,
      "learning_rate": 0.0006777437325905292,
      "loss": 2.2983,
      "step": 11570
    },
    {
      "epoch": 3.2256267409470754,
      "grad_norm": 1.5581862926483154,
      "learning_rate": 0.0006774651810584958,
      "loss": 2.2634,
      "step": 11580
    },
    {
      "epoch": 3.2284122562674096,
      "grad_norm": 1.1506202220916748,
      "learning_rate": 0.0006771866295264624,
      "loss": 2.223,
      "step": 11590
    },
    {
      "epoch": 3.231197771587744,
      "grad_norm": 1.3945058584213257,
      "learning_rate": 0.000676908077994429,
      "loss": 2.3548,
      "step": 11600
    },
    {
      "epoch": 3.233983286908078,
      "grad_norm": 1.3837746381759644,
      "learning_rate": 0.0006766295264623955,
      "loss": 2.2835,
      "step": 11610
    },
    {
      "epoch": 3.2367688022284122,
      "grad_norm": 1.489894986152649,
      "learning_rate": 0.0006763509749303622,
      "loss": 2.2601,
      "step": 11620
    },
    {
      "epoch": 3.2395543175487465,
      "grad_norm": 1.4130359888076782,
      "learning_rate": 0.0006760724233983288,
      "loss": 2.2818,
      "step": 11630
    },
    {
      "epoch": 3.2423398328690807,
      "grad_norm": 1.5713595151901245,
      "learning_rate": 0.0006757938718662953,
      "loss": 2.2614,
      "step": 11640
    },
    {
      "epoch": 3.245125348189415,
      "grad_norm": 1.2919553518295288,
      "learning_rate": 0.0006755153203342618,
      "loss": 2.4215,
      "step": 11650
    },
    {
      "epoch": 3.247910863509749,
      "grad_norm": 1.512909173965454,
      "learning_rate": 0.0006752367688022284,
      "loss": 2.3662,
      "step": 11660
    },
    {
      "epoch": 3.2506963788300833,
      "grad_norm": 1.3643431663513184,
      "learning_rate": 0.000674958217270195,
      "loss": 2.3023,
      "step": 11670
    },
    {
      "epoch": 3.253481894150418,
      "grad_norm": 1.3367573022842407,
      "learning_rate": 0.0006746796657381615,
      "loss": 2.2722,
      "step": 11680
    },
    {
      "epoch": 3.256267409470752,
      "grad_norm": 1.3609519004821777,
      "learning_rate": 0.0006744011142061281,
      "loss": 2.2058,
      "step": 11690
    },
    {
      "epoch": 3.2590529247910864,
      "grad_norm": 1.2678591012954712,
      "learning_rate": 0.0006741225626740948,
      "loss": 2.275,
      "step": 11700
    },
    {
      "epoch": 3.2618384401114207,
      "grad_norm": 2.073680877685547,
      "learning_rate": 0.0006738440111420613,
      "loss": 2.4169,
      "step": 11710
    },
    {
      "epoch": 3.264623955431755,
      "grad_norm": 1.6156624555587769,
      "learning_rate": 0.0006735654596100279,
      "loss": 2.4418,
      "step": 11720
    },
    {
      "epoch": 3.267409470752089,
      "grad_norm": 1.0550742149353027,
      "learning_rate": 0.0006732869080779945,
      "loss": 2.3641,
      "step": 11730
    },
    {
      "epoch": 3.2701949860724233,
      "grad_norm": 2.28971004486084,
      "learning_rate": 0.0006730083565459611,
      "loss": 2.4806,
      "step": 11740
    },
    {
      "epoch": 3.2729805013927575,
      "grad_norm": 1.2767773866653442,
      "learning_rate": 0.0006727298050139275,
      "loss": 2.2304,
      "step": 11750
    },
    {
      "epoch": 3.2757660167130918,
      "grad_norm": 1.595889925956726,
      "learning_rate": 0.0006724512534818941,
      "loss": 2.5027,
      "step": 11760
    },
    {
      "epoch": 3.2785515320334264,
      "grad_norm": 1.2632372379302979,
      "learning_rate": 0.0006721727019498607,
      "loss": 2.4061,
      "step": 11770
    },
    {
      "epoch": 3.2813370473537606,
      "grad_norm": 1.573546051979065,
      "learning_rate": 0.0006718941504178273,
      "loss": 2.2616,
      "step": 11780
    },
    {
      "epoch": 3.284122562674095,
      "grad_norm": 1.6670316457748413,
      "learning_rate": 0.0006716155988857939,
      "loss": 2.2027,
      "step": 11790
    },
    {
      "epoch": 3.286908077994429,
      "grad_norm": 1.4563919305801392,
      "learning_rate": 0.0006713370473537605,
      "loss": 2.3032,
      "step": 11800
    },
    {
      "epoch": 3.2896935933147633,
      "grad_norm": 1.477880597114563,
      "learning_rate": 0.0006710584958217271,
      "loss": 2.2772,
      "step": 11810
    },
    {
      "epoch": 3.2924791086350975,
      "grad_norm": 1.979149580001831,
      "learning_rate": 0.0006707799442896936,
      "loss": 2.245,
      "step": 11820
    },
    {
      "epoch": 3.2952646239554317,
      "grad_norm": 1.4658639430999756,
      "learning_rate": 0.0006705013927576602,
      "loss": 2.5012,
      "step": 11830
    },
    {
      "epoch": 3.298050139275766,
      "grad_norm": 1.7725045680999756,
      "learning_rate": 0.0006702228412256267,
      "loss": 2.4697,
      "step": 11840
    },
    {
      "epoch": 3.3008356545961,
      "grad_norm": 2.5845119953155518,
      "learning_rate": 0.0006699442896935933,
      "loss": 2.3524,
      "step": 11850
    },
    {
      "epoch": 3.3036211699164344,
      "grad_norm": 1.1936419010162354,
      "learning_rate": 0.0006696657381615599,
      "loss": 2.2516,
      "step": 11860
    },
    {
      "epoch": 3.3064066852367686,
      "grad_norm": 1.728630542755127,
      "learning_rate": 0.0006693871866295264,
      "loss": 2.2881,
      "step": 11870
    },
    {
      "epoch": 3.309192200557103,
      "grad_norm": 1.4624457359313965,
      "learning_rate": 0.0006691086350974931,
      "loss": 2.4233,
      "step": 11880
    },
    {
      "epoch": 3.3119777158774375,
      "grad_norm": 1.5634623765945435,
      "learning_rate": 0.0006688300835654596,
      "loss": 2.3965,
      "step": 11890
    },
    {
      "epoch": 3.3147632311977717,
      "grad_norm": 1.3752130270004272,
      "learning_rate": 0.0006685515320334262,
      "loss": 2.3669,
      "step": 11900
    },
    {
      "epoch": 3.317548746518106,
      "grad_norm": 1.4791605472564697,
      "learning_rate": 0.0006682729805013928,
      "loss": 2.2765,
      "step": 11910
    },
    {
      "epoch": 3.32033426183844,
      "grad_norm": 1.4900710582733154,
      "learning_rate": 0.0006679944289693594,
      "loss": 2.3567,
      "step": 11920
    },
    {
      "epoch": 3.3231197771587744,
      "grad_norm": 1.3093379735946655,
      "learning_rate": 0.0006677158774373259,
      "loss": 2.1855,
      "step": 11930
    },
    {
      "epoch": 3.3259052924791086,
      "grad_norm": 1.3417317867279053,
      "learning_rate": 0.0006674373259052924,
      "loss": 2.1333,
      "step": 11940
    },
    {
      "epoch": 3.328690807799443,
      "grad_norm": 1.8623708486557007,
      "learning_rate": 0.0006671587743732591,
      "loss": 2.3417,
      "step": 11950
    },
    {
      "epoch": 3.331476323119777,
      "grad_norm": 1.6992326974868774,
      "learning_rate": 0.0006668802228412256,
      "loss": 2.4347,
      "step": 11960
    },
    {
      "epoch": 3.3342618384401113,
      "grad_norm": 1.228739857673645,
      "learning_rate": 0.0006666016713091922,
      "loss": 2.3812,
      "step": 11970
    },
    {
      "epoch": 3.337047353760446,
      "grad_norm": 1.2220752239227295,
      "learning_rate": 0.0006663231197771588,
      "loss": 2.3952,
      "step": 11980
    },
    {
      "epoch": 3.33983286908078,
      "grad_norm": 0.9991467595100403,
      "learning_rate": 0.0006660445682451254,
      "loss": 2.1538,
      "step": 11990
    },
    {
      "epoch": 3.3426183844011144,
      "grad_norm": 1.538361668586731,
      "learning_rate": 0.0006657660167130919,
      "loss": 2.2707,
      "step": 12000
    },
    {
      "epoch": 3.3454038997214486,
      "grad_norm": 1.2403862476348877,
      "learning_rate": 0.0006654874651810585,
      "loss": 2.4196,
      "step": 12010
    },
    {
      "epoch": 3.348189415041783,
      "grad_norm": 1.5140234231948853,
      "learning_rate": 0.0006652089136490252,
      "loss": 2.3053,
      "step": 12020
    },
    {
      "epoch": 3.350974930362117,
      "grad_norm": 1.441936731338501,
      "learning_rate": 0.0006649303621169917,
      "loss": 2.3389,
      "step": 12030
    },
    {
      "epoch": 3.3537604456824512,
      "grad_norm": 1.3239376544952393,
      "learning_rate": 0.0006646518105849582,
      "loss": 2.3971,
      "step": 12040
    },
    {
      "epoch": 3.3565459610027855,
      "grad_norm": 1.626596450805664,
      "learning_rate": 0.0006643732590529247,
      "loss": 2.328,
      "step": 12050
    },
    {
      "epoch": 3.3593314763231197,
      "grad_norm": 2.221458911895752,
      "learning_rate": 0.0006640947075208914,
      "loss": 2.2658,
      "step": 12060
    },
    {
      "epoch": 3.362116991643454,
      "grad_norm": 1.0347603559494019,
      "learning_rate": 0.0006638161559888579,
      "loss": 2.2351,
      "step": 12070
    },
    {
      "epoch": 3.364902506963788,
      "grad_norm": 1.5313184261322021,
      "learning_rate": 0.0006635376044568245,
      "loss": 2.419,
      "step": 12080
    },
    {
      "epoch": 3.3676880222841223,
      "grad_norm": 1.310180425643921,
      "learning_rate": 0.0006632590529247911,
      "loss": 2.2368,
      "step": 12090
    },
    {
      "epoch": 3.370473537604457,
      "grad_norm": 1.1661341190338135,
      "learning_rate": 0.0006629805013927577,
      "loss": 2.4412,
      "step": 12100
    },
    {
      "epoch": 3.3732590529247912,
      "grad_norm": 1.8823796510696411,
      "learning_rate": 0.0006627019498607243,
      "loss": 2.1777,
      "step": 12110
    },
    {
      "epoch": 3.3760445682451254,
      "grad_norm": 1.3188045024871826,
      "learning_rate": 0.0006624233983286908,
      "loss": 2.0762,
      "step": 12120
    },
    {
      "epoch": 3.3788300835654597,
      "grad_norm": 1.559186339378357,
      "learning_rate": 0.0006621448467966575,
      "loss": 2.1902,
      "step": 12130
    },
    {
      "epoch": 3.381615598885794,
      "grad_norm": 1.4226784706115723,
      "learning_rate": 0.0006618662952646239,
      "loss": 2.4048,
      "step": 12140
    },
    {
      "epoch": 3.384401114206128,
      "grad_norm": 1.9022228717803955,
      "learning_rate": 0.0006615877437325905,
      "loss": 2.2861,
      "step": 12150
    },
    {
      "epoch": 3.3871866295264623,
      "grad_norm": 1.3374511003494263,
      "learning_rate": 0.0006613091922005571,
      "loss": 2.3139,
      "step": 12160
    },
    {
      "epoch": 3.3899721448467965,
      "grad_norm": 1.603153944015503,
      "learning_rate": 0.0006610306406685237,
      "loss": 2.3966,
      "step": 12170
    },
    {
      "epoch": 3.3927576601671308,
      "grad_norm": 1.6281628608703613,
      "learning_rate": 0.0006607520891364903,
      "loss": 2.3591,
      "step": 12180
    },
    {
      "epoch": 3.3955431754874654,
      "grad_norm": 1.6746869087219238,
      "learning_rate": 0.0006604735376044568,
      "loss": 2.3934,
      "step": 12190
    },
    {
      "epoch": 3.3983286908077996,
      "grad_norm": 1.7931455373764038,
      "learning_rate": 0.0006601949860724235,
      "loss": 2.2755,
      "step": 12200
    },
    {
      "epoch": 3.401114206128134,
      "grad_norm": 1.784530520439148,
      "learning_rate": 0.00065991643454039,
      "loss": 2.3225,
      "step": 12210
    },
    {
      "epoch": 3.403899721448468,
      "grad_norm": 1.1602891683578491,
      "learning_rate": 0.0006596378830083566,
      "loss": 2.2952,
      "step": 12220
    },
    {
      "epoch": 3.4066852367688023,
      "grad_norm": 2.243532180786133,
      "learning_rate": 0.000659359331476323,
      "loss": 2.3304,
      "step": 12230
    },
    {
      "epoch": 3.4094707520891365,
      "grad_norm": 1.5971020460128784,
      "learning_rate": 0.0006590807799442897,
      "loss": 2.4767,
      "step": 12240
    },
    {
      "epoch": 3.4122562674094707,
      "grad_norm": 1.73195481300354,
      "learning_rate": 0.0006588022284122562,
      "loss": 2.35,
      "step": 12250
    },
    {
      "epoch": 3.415041782729805,
      "grad_norm": 2.323052406311035,
      "learning_rate": 0.0006585236768802228,
      "loss": 2.3712,
      "step": 12260
    },
    {
      "epoch": 3.417827298050139,
      "grad_norm": 1.523945689201355,
      "learning_rate": 0.0006582451253481895,
      "loss": 2.3539,
      "step": 12270
    },
    {
      "epoch": 3.4206128133704734,
      "grad_norm": 1.5844286680221558,
      "learning_rate": 0.000657966573816156,
      "loss": 2.3226,
      "step": 12280
    },
    {
      "epoch": 3.4233983286908076,
      "grad_norm": 1.8073376417160034,
      "learning_rate": 0.0006576880222841226,
      "loss": 2.2325,
      "step": 12290
    },
    {
      "epoch": 3.426183844011142,
      "grad_norm": 1.3973417282104492,
      "learning_rate": 0.0006574094707520891,
      "loss": 2.0623,
      "step": 12300
    },
    {
      "epoch": 3.4289693593314765,
      "grad_norm": 1.9088879823684692,
      "learning_rate": 0.0006571309192200558,
      "loss": 2.5725,
      "step": 12310
    },
    {
      "epoch": 3.4317548746518107,
      "grad_norm": 1.236739158630371,
      "learning_rate": 0.0006568523676880223,
      "loss": 2.3093,
      "step": 12320
    },
    {
      "epoch": 3.434540389972145,
      "grad_norm": 1.6687108278274536,
      "learning_rate": 0.0006565738161559889,
      "loss": 2.3488,
      "step": 12330
    },
    {
      "epoch": 3.437325905292479,
      "grad_norm": 1.0843071937561035,
      "learning_rate": 0.0006562952646239554,
      "loss": 2.0809,
      "step": 12340
    },
    {
      "epoch": 3.4401114206128134,
      "grad_norm": 1.208906650543213,
      "learning_rate": 0.000656016713091922,
      "loss": 2.2109,
      "step": 12350
    },
    {
      "epoch": 3.4428969359331476,
      "grad_norm": 1.4876632690429688,
      "learning_rate": 0.0006557381615598886,
      "loss": 2.1942,
      "step": 12360
    },
    {
      "epoch": 3.445682451253482,
      "grad_norm": 2.2942562103271484,
      "learning_rate": 0.0006554596100278551,
      "loss": 2.2813,
      "step": 12370
    },
    {
      "epoch": 3.448467966573816,
      "grad_norm": 1.4417495727539062,
      "learning_rate": 0.0006551810584958218,
      "loss": 2.167,
      "step": 12380
    },
    {
      "epoch": 3.4512534818941503,
      "grad_norm": 1.3321492671966553,
      "learning_rate": 0.0006549025069637883,
      "loss": 2.4176,
      "step": 12390
    },
    {
      "epoch": 3.4540389972144845,
      "grad_norm": 1.5537824630737305,
      "learning_rate": 0.0006546239554317549,
      "loss": 2.2855,
      "step": 12400
    },
    {
      "epoch": 3.456824512534819,
      "grad_norm": 1.466306447982788,
      "learning_rate": 0.0006543454038997214,
      "loss": 2.4036,
      "step": 12410
    },
    {
      "epoch": 3.4596100278551534,
      "grad_norm": 1.257235050201416,
      "learning_rate": 0.0006540668523676881,
      "loss": 2.335,
      "step": 12420
    },
    {
      "epoch": 3.4623955431754876,
      "grad_norm": 1.9970041513442993,
      "learning_rate": 0.0006537883008356547,
      "loss": 2.3814,
      "step": 12430
    },
    {
      "epoch": 3.465181058495822,
      "grad_norm": 1.541783332824707,
      "learning_rate": 0.0006535097493036211,
      "loss": 2.2983,
      "step": 12440
    },
    {
      "epoch": 3.467966573816156,
      "grad_norm": 1.5805977582931519,
      "learning_rate": 0.0006532311977715878,
      "loss": 2.4364,
      "step": 12450
    },
    {
      "epoch": 3.4707520891364902,
      "grad_norm": 1.4406286478042603,
      "learning_rate": 0.0006529526462395543,
      "loss": 2.326,
      "step": 12460
    },
    {
      "epoch": 3.4735376044568245,
      "grad_norm": 5.667898178100586,
      "learning_rate": 0.0006526740947075209,
      "loss": 2.273,
      "step": 12470
    },
    {
      "epoch": 3.4763231197771587,
      "grad_norm": 1.6012698411941528,
      "learning_rate": 0.0006523955431754874,
      "loss": 2.4959,
      "step": 12480
    },
    {
      "epoch": 3.479108635097493,
      "grad_norm": 1.2822974920272827,
      "learning_rate": 0.0006521169916434541,
      "loss": 2.3418,
      "step": 12490
    },
    {
      "epoch": 3.481894150417827,
      "grad_norm": 1.4207894802093506,
      "learning_rate": 0.0006518384401114206,
      "loss": 2.2612,
      "step": 12500
    },
    {
      "epoch": 3.4846796657381613,
      "grad_norm": 1.7914904356002808,
      "learning_rate": 0.0006515598885793872,
      "loss": 2.2371,
      "step": 12510
    },
    {
      "epoch": 3.487465181058496,
      "grad_norm": 1.8419209718704224,
      "learning_rate": 0.0006512813370473539,
      "loss": 2.1577,
      "step": 12520
    },
    {
      "epoch": 3.4902506963788302,
      "grad_norm": 1.8476974964141846,
      "learning_rate": 0.0006510027855153204,
      "loss": 2.3279,
      "step": 12530
    },
    {
      "epoch": 3.4930362116991645,
      "grad_norm": 1.3231638669967651,
      "learning_rate": 0.0006507242339832869,
      "loss": 2.2161,
      "step": 12540
    },
    {
      "epoch": 3.4958217270194987,
      "grad_norm": 1.2319260835647583,
      "learning_rate": 0.0006504456824512534,
      "loss": 2.5047,
      "step": 12550
    },
    {
      "epoch": 3.498607242339833,
      "grad_norm": 1.894384503364563,
      "learning_rate": 0.0006501671309192201,
      "loss": 2.314,
      "step": 12560
    },
    {
      "epoch": 3.501392757660167,
      "grad_norm": 1.2017067670822144,
      "learning_rate": 0.0006498885793871866,
      "loss": 2.3939,
      "step": 12570
    },
    {
      "epoch": 3.5041782729805013,
      "grad_norm": 1.055229902267456,
      "learning_rate": 0.0006496100278551532,
      "loss": 2.369,
      "step": 12580
    },
    {
      "epoch": 3.5069637883008355,
      "grad_norm": 1.2799218893051147,
      "learning_rate": 0.0006493314763231198,
      "loss": 2.4228,
      "step": 12590
    },
    {
      "epoch": 3.5097493036211698,
      "grad_norm": 1.6954010725021362,
      "learning_rate": 0.0006490529247910864,
      "loss": 2.3853,
      "step": 12600
    },
    {
      "epoch": 3.5125348189415044,
      "grad_norm": 1.7093684673309326,
      "learning_rate": 0.000648774373259053,
      "loss": 2.3301,
      "step": 12610
    },
    {
      "epoch": 3.5153203342618387,
      "grad_norm": 1.5871191024780273,
      "learning_rate": 0.0006484958217270195,
      "loss": 2.1454,
      "step": 12620
    },
    {
      "epoch": 3.518105849582173,
      "grad_norm": 1.9851709604263306,
      "learning_rate": 0.0006482172701949862,
      "loss": 2.4577,
      "step": 12630
    },
    {
      "epoch": 3.520891364902507,
      "grad_norm": 1.6808902025222778,
      "learning_rate": 0.0006479387186629526,
      "loss": 2.184,
      "step": 12640
    },
    {
      "epoch": 3.5236768802228413,
      "grad_norm": 1.6830973625183105,
      "learning_rate": 0.0006476601671309192,
      "loss": 2.4553,
      "step": 12650
    },
    {
      "epoch": 3.5264623955431755,
      "grad_norm": 1.0739703178405762,
      "learning_rate": 0.0006473816155988857,
      "loss": 2.3542,
      "step": 12660
    },
    {
      "epoch": 3.5292479108635098,
      "grad_norm": 1.4496790170669556,
      "learning_rate": 0.0006471030640668524,
      "loss": 2.4179,
      "step": 12670
    },
    {
      "epoch": 3.532033426183844,
      "grad_norm": 1.3839253187179565,
      "learning_rate": 0.000646824512534819,
      "loss": 2.2822,
      "step": 12680
    },
    {
      "epoch": 3.534818941504178,
      "grad_norm": 1.4457019567489624,
      "learning_rate": 0.0006465459610027855,
      "loss": 2.3162,
      "step": 12690
    },
    {
      "epoch": 3.5376044568245124,
      "grad_norm": 1.544791340827942,
      "learning_rate": 0.0006462674094707522,
      "loss": 2.0725,
      "step": 12700
    },
    {
      "epoch": 3.5403899721448466,
      "grad_norm": 1.6511585712432861,
      "learning_rate": 0.0006459888579387187,
      "loss": 2.3934,
      "step": 12710
    },
    {
      "epoch": 3.543175487465181,
      "grad_norm": 2.311314821243286,
      "learning_rate": 0.0006457103064066853,
      "loss": 2.2981,
      "step": 12720
    },
    {
      "epoch": 3.545961002785515,
      "grad_norm": 1.3572282791137695,
      "learning_rate": 0.0006454317548746517,
      "loss": 2.4382,
      "step": 12730
    },
    {
      "epoch": 3.5487465181058497,
      "grad_norm": 1.1056126356124878,
      "learning_rate": 0.0006451532033426184,
      "loss": 2.3861,
      "step": 12740
    },
    {
      "epoch": 3.551532033426184,
      "grad_norm": 0.9075063467025757,
      "learning_rate": 0.000644874651810585,
      "loss": 2.4881,
      "step": 12750
    },
    {
      "epoch": 3.554317548746518,
      "grad_norm": 1.2031193971633911,
      "learning_rate": 0.0006445961002785515,
      "loss": 2.3472,
      "step": 12760
    },
    {
      "epoch": 3.5571030640668524,
      "grad_norm": 2.09230637550354,
      "learning_rate": 0.0006443175487465181,
      "loss": 2.1419,
      "step": 12770
    },
    {
      "epoch": 3.5598885793871866,
      "grad_norm": 2.1864519119262695,
      "learning_rate": 0.0006440389972144847,
      "loss": 2.1857,
      "step": 12780
    },
    {
      "epoch": 3.562674094707521,
      "grad_norm": 1.4485856294631958,
      "learning_rate": 0.0006437604456824513,
      "loss": 2.2476,
      "step": 12790
    },
    {
      "epoch": 3.565459610027855,
      "grad_norm": 1.2525800466537476,
      "learning_rate": 0.0006434818941504178,
      "loss": 2.5048,
      "step": 12800
    },
    {
      "epoch": 3.5682451253481893,
      "grad_norm": 1.364533543586731,
      "learning_rate": 0.0006432033426183845,
      "loss": 2.1669,
      "step": 12810
    },
    {
      "epoch": 3.571030640668524,
      "grad_norm": 1.2718236446380615,
      "learning_rate": 0.000642924791086351,
      "loss": 2.4807,
      "step": 12820
    },
    {
      "epoch": 3.573816155988858,
      "grad_norm": 1.346458911895752,
      "learning_rate": 0.0006426462395543175,
      "loss": 2.4016,
      "step": 12830
    },
    {
      "epoch": 3.5766016713091924,
      "grad_norm": 2.3737971782684326,
      "learning_rate": 0.0006423676880222841,
      "loss": 2.456,
      "step": 12840
    },
    {
      "epoch": 3.5793871866295266,
      "grad_norm": 1.4083935022354126,
      "learning_rate": 0.0006420891364902507,
      "loss": 2.2949,
      "step": 12850
    },
    {
      "epoch": 3.582172701949861,
      "grad_norm": 1.7620437145233154,
      "learning_rate": 0.0006418105849582173,
      "loss": 2.1492,
      "step": 12860
    },
    {
      "epoch": 3.584958217270195,
      "grad_norm": 1.4648255109786987,
      "learning_rate": 0.0006415320334261838,
      "loss": 2.3091,
      "step": 12870
    },
    {
      "epoch": 3.5877437325905293,
      "grad_norm": 1.3133126497268677,
      "learning_rate": 0.0006412534818941505,
      "loss": 2.1502,
      "step": 12880
    },
    {
      "epoch": 3.5905292479108635,
      "grad_norm": 1.469413161277771,
      "learning_rate": 0.000640974930362117,
      "loss": 2.2078,
      "step": 12890
    },
    {
      "epoch": 3.5933147632311977,
      "grad_norm": 1.3214819431304932,
      "learning_rate": 0.0006406963788300836,
      "loss": 2.2567,
      "step": 12900
    },
    {
      "epoch": 3.596100278551532,
      "grad_norm": 1.4502009153366089,
      "learning_rate": 0.0006404178272980502,
      "loss": 2.2523,
      "step": 12910
    },
    {
      "epoch": 3.598885793871866,
      "grad_norm": 1.2360239028930664,
      "learning_rate": 0.0006401392757660168,
      "loss": 2.494,
      "step": 12920
    },
    {
      "epoch": 3.6016713091922004,
      "grad_norm": 1.3741497993469238,
      "learning_rate": 0.0006398607242339833,
      "loss": 2.2623,
      "step": 12930
    },
    {
      "epoch": 3.6044568245125346,
      "grad_norm": 1.230825424194336,
      "learning_rate": 0.0006395821727019498,
      "loss": 2.3802,
      "step": 12940
    },
    {
      "epoch": 3.6072423398328692,
      "grad_norm": 1.5098012685775757,
      "learning_rate": 0.0006393036211699164,
      "loss": 2.1806,
      "step": 12950
    },
    {
      "epoch": 3.6100278551532035,
      "grad_norm": 2.572859048843384,
      "learning_rate": 0.000639025069637883,
      "loss": 2.2614,
      "step": 12960
    },
    {
      "epoch": 3.6128133704735377,
      "grad_norm": 1.114989161491394,
      "learning_rate": 0.0006387465181058496,
      "loss": 2.2206,
      "step": 12970
    },
    {
      "epoch": 3.615598885793872,
      "grad_norm": 1.9090431928634644,
      "learning_rate": 0.0006384679665738161,
      "loss": 2.4642,
      "step": 12980
    },
    {
      "epoch": 3.618384401114206,
      "grad_norm": 1.9084835052490234,
      "learning_rate": 0.0006381894150417828,
      "loss": 2.2643,
      "step": 12990
    },
    {
      "epoch": 3.6211699164345403,
      "grad_norm": 1.2896595001220703,
      "learning_rate": 0.0006379108635097494,
      "loss": 2.3619,
      "step": 13000
    },
    {
      "epoch": 3.6239554317548746,
      "grad_norm": 1.7779409885406494,
      "learning_rate": 0.0006376323119777159,
      "loss": 2.4154,
      "step": 13010
    },
    {
      "epoch": 3.6267409470752088,
      "grad_norm": 1.7269325256347656,
      "learning_rate": 0.0006373537604456825,
      "loss": 2.5499,
      "step": 13020
    },
    {
      "epoch": 3.6295264623955434,
      "grad_norm": 1.6264753341674805,
      "learning_rate": 0.000637075208913649,
      "loss": 2.4261,
      "step": 13030
    },
    {
      "epoch": 3.6323119777158777,
      "grad_norm": 1.2531251907348633,
      "learning_rate": 0.0006367966573816156,
      "loss": 2.4592,
      "step": 13040
    },
    {
      "epoch": 3.635097493036212,
      "grad_norm": 1.309495449066162,
      "learning_rate": 0.0006365181058495821,
      "loss": 2.33,
      "step": 13050
    },
    {
      "epoch": 3.637883008356546,
      "grad_norm": 1.5598701238632202,
      "learning_rate": 0.0006362395543175488,
      "loss": 2.3241,
      "step": 13060
    },
    {
      "epoch": 3.6406685236768803,
      "grad_norm": 1.6879950761795044,
      "learning_rate": 0.0006359610027855154,
      "loss": 2.2921,
      "step": 13070
    },
    {
      "epoch": 3.6434540389972145,
      "grad_norm": 1.4357479810714722,
      "learning_rate": 0.0006356824512534819,
      "loss": 2.4383,
      "step": 13080
    },
    {
      "epoch": 3.6462395543175488,
      "grad_norm": 1.6731765270233154,
      "learning_rate": 0.0006354038997214485,
      "loss": 2.2732,
      "step": 13090
    },
    {
      "epoch": 3.649025069637883,
      "grad_norm": 1.4927427768707275,
      "learning_rate": 0.0006351253481894151,
      "loss": 2.2547,
      "step": 13100
    },
    {
      "epoch": 3.651810584958217,
      "grad_norm": 1.961484432220459,
      "learning_rate": 0.0006348467966573817,
      "loss": 2.3954,
      "step": 13110
    },
    {
      "epoch": 3.6545961002785514,
      "grad_norm": 1.2974268198013306,
      "learning_rate": 0.0006345682451253481,
      "loss": 2.4397,
      "step": 13120
    },
    {
      "epoch": 3.6573816155988856,
      "grad_norm": 1.7746433019638062,
      "learning_rate": 0.0006342896935933147,
      "loss": 2.3968,
      "step": 13130
    },
    {
      "epoch": 3.66016713091922,
      "grad_norm": 1.6231231689453125,
      "learning_rate": 0.0006340111420612813,
      "loss": 2.2922,
      "step": 13140
    },
    {
      "epoch": 3.662952646239554,
      "grad_norm": 1.9083393812179565,
      "learning_rate": 0.0006337325905292479,
      "loss": 2.3565,
      "step": 13150
    },
    {
      "epoch": 3.6657381615598887,
      "grad_norm": 1.903537392616272,
      "learning_rate": 0.0006334540389972145,
      "loss": 2.1208,
      "step": 13160
    },
    {
      "epoch": 3.668523676880223,
      "grad_norm": 2.1222188472747803,
      "learning_rate": 0.0006331754874651811,
      "loss": 2.5778,
      "step": 13170
    },
    {
      "epoch": 3.671309192200557,
      "grad_norm": 1.489466667175293,
      "learning_rate": 0.0006328969359331477,
      "loss": 2.1953,
      "step": 13180
    },
    {
      "epoch": 3.6740947075208914,
      "grad_norm": 1.4898117780685425,
      "learning_rate": 0.0006326183844011142,
      "loss": 1.9508,
      "step": 13190
    },
    {
      "epoch": 3.6768802228412256,
      "grad_norm": 2.163832664489746,
      "learning_rate": 0.0006323398328690808,
      "loss": 2.4407,
      "step": 13200
    },
    {
      "epoch": 3.67966573816156,
      "grad_norm": 2.001377820968628,
      "learning_rate": 0.0006320612813370474,
      "loss": 2.3468,
      "step": 13210
    },
    {
      "epoch": 3.682451253481894,
      "grad_norm": 1.590067982673645,
      "learning_rate": 0.000631782729805014,
      "loss": 2.2935,
      "step": 13220
    },
    {
      "epoch": 3.6852367688022283,
      "grad_norm": 1.4998323917388916,
      "learning_rate": 0.0006315041782729805,
      "loss": 2.2075,
      "step": 13230
    },
    {
      "epoch": 3.688022284122563,
      "grad_norm": 1.3125863075256348,
      "learning_rate": 0.000631225626740947,
      "loss": 2.4239,
      "step": 13240
    },
    {
      "epoch": 3.690807799442897,
      "grad_norm": 1.2090623378753662,
      "learning_rate": 0.0006309470752089137,
      "loss": 2.3062,
      "step": 13250
    },
    {
      "epoch": 3.6935933147632314,
      "grad_norm": 1.2177094221115112,
      "learning_rate": 0.0006306685236768802,
      "loss": 2.3169,
      "step": 13260
    },
    {
      "epoch": 3.6963788300835656,
      "grad_norm": 1.5273774862289429,
      "learning_rate": 0.0006303899721448468,
      "loss": 2.3751,
      "step": 13270
    },
    {
      "epoch": 3.6991643454039,
      "grad_norm": 1.3542211055755615,
      "learning_rate": 0.0006301114206128134,
      "loss": 2.3518,
      "step": 13280
    },
    {
      "epoch": 3.701949860724234,
      "grad_norm": 1.4010863304138184,
      "learning_rate": 0.00062983286908078,
      "loss": 2.3022,
      "step": 13290
    },
    {
      "epoch": 3.7047353760445683,
      "grad_norm": 1.675050139427185,
      "learning_rate": 0.0006295543175487465,
      "loss": 2.1931,
      "step": 13300
    },
    {
      "epoch": 3.7075208913649025,
      "grad_norm": 2.0827834606170654,
      "learning_rate": 0.0006292757660167131,
      "loss": 2.2564,
      "step": 13310
    },
    {
      "epoch": 3.7103064066852367,
      "grad_norm": 1.39237642288208,
      "learning_rate": 0.0006289972144846798,
      "loss": 2.3385,
      "step": 13320
    },
    {
      "epoch": 3.713091922005571,
      "grad_norm": 1.393566608428955,
      "learning_rate": 0.0006287186629526462,
      "loss": 2.285,
      "step": 13330
    },
    {
      "epoch": 3.715877437325905,
      "grad_norm": 1.5327445268630981,
      "learning_rate": 0.0006284401114206128,
      "loss": 2.1713,
      "step": 13340
    },
    {
      "epoch": 3.7186629526462394,
      "grad_norm": 1.7326716184616089,
      "learning_rate": 0.0006281615598885794,
      "loss": 2.316,
      "step": 13350
    },
    {
      "epoch": 3.7214484679665736,
      "grad_norm": 1.5277678966522217,
      "learning_rate": 0.000627883008356546,
      "loss": 2.4548,
      "step": 13360
    },
    {
      "epoch": 3.724233983286908,
      "grad_norm": 1.5624381303787231,
      "learning_rate": 0.0006276044568245125,
      "loss": 2.3272,
      "step": 13370
    },
    {
      "epoch": 3.7270194986072425,
      "grad_norm": 1.29640793800354,
      "learning_rate": 0.0006273259052924791,
      "loss": 2.1703,
      "step": 13380
    },
    {
      "epoch": 3.7298050139275767,
      "grad_norm": 1.7886282205581665,
      "learning_rate": 0.0006270473537604458,
      "loss": 2.4282,
      "step": 13390
    },
    {
      "epoch": 3.732590529247911,
      "grad_norm": 1.2356090545654297,
      "learning_rate": 0.0006267688022284123,
      "loss": 2.5151,
      "step": 13400
    },
    {
      "epoch": 3.735376044568245,
      "grad_norm": 2.1555418968200684,
      "learning_rate": 0.0006264902506963789,
      "loss": 2.6235,
      "step": 13410
    },
    {
      "epoch": 3.7381615598885793,
      "grad_norm": 1.6376566886901855,
      "learning_rate": 0.0006262116991643453,
      "loss": 2.4102,
      "step": 13420
    },
    {
      "epoch": 3.7409470752089136,
      "grad_norm": 1.0738029479980469,
      "learning_rate": 0.000625933147632312,
      "loss": 2.4024,
      "step": 13430
    },
    {
      "epoch": 3.743732590529248,
      "grad_norm": 5.732860565185547,
      "learning_rate": 0.0006256545961002785,
      "loss": 2.1975,
      "step": 13440
    },
    {
      "epoch": 3.7465181058495824,
      "grad_norm": 1.6386504173278809,
      "learning_rate": 0.0006253760445682451,
      "loss": 2.3787,
      "step": 13450
    },
    {
      "epoch": 3.7493036211699167,
      "grad_norm": 1.4560370445251465,
      "learning_rate": 0.0006250974930362117,
      "loss": 2.333,
      "step": 13460
    },
    {
      "epoch": 3.752089136490251,
      "grad_norm": 1.4362564086914062,
      "learning_rate": 0.0006248189415041783,
      "loss": 2.2419,
      "step": 13470
    },
    {
      "epoch": 3.754874651810585,
      "grad_norm": 1.3968470096588135,
      "learning_rate": 0.0006245403899721449,
      "loss": 2.2122,
      "step": 13480
    },
    {
      "epoch": 3.7576601671309193,
      "grad_norm": 1.7118297815322876,
      "learning_rate": 0.0006242618384401114,
      "loss": 2.5762,
      "step": 13490
    },
    {
      "epoch": 3.7604456824512535,
      "grad_norm": 1.2799633741378784,
      "learning_rate": 0.0006239832869080781,
      "loss": 2.3231,
      "step": 13500
    },
    {
      "epoch": 3.7632311977715878,
      "grad_norm": 1.198039174079895,
      "learning_rate": 0.0006237047353760446,
      "loss": 2.3337,
      "step": 13510
    },
    {
      "epoch": 3.766016713091922,
      "grad_norm": 1.5535297393798828,
      "learning_rate": 0.0006234261838440111,
      "loss": 2.1361,
      "step": 13520
    },
    {
      "epoch": 3.768802228412256,
      "grad_norm": 2.00439453125,
      "learning_rate": 0.0006231476323119777,
      "loss": 2.4691,
      "step": 13530
    },
    {
      "epoch": 3.7715877437325904,
      "grad_norm": 2.088627815246582,
      "learning_rate": 0.0006228690807799443,
      "loss": 2.2857,
      "step": 13540
    },
    {
      "epoch": 3.7743732590529246,
      "grad_norm": 2.6765966415405273,
      "learning_rate": 0.0006225905292479109,
      "loss": 2.2734,
      "step": 13550
    },
    {
      "epoch": 3.777158774373259,
      "grad_norm": 2.0737738609313965,
      "learning_rate": 0.0006223119777158774,
      "loss": 2.2201,
      "step": 13560
    },
    {
      "epoch": 3.779944289693593,
      "grad_norm": 1.2318596839904785,
      "learning_rate": 0.0006220334261838441,
      "loss": 2.1785,
      "step": 13570
    },
    {
      "epoch": 3.7827298050139273,
      "grad_norm": 2.109170436859131,
      "learning_rate": 0.0006217548746518106,
      "loss": 2.4415,
      "step": 13580
    },
    {
      "epoch": 3.785515320334262,
      "grad_norm": 1.340998888015747,
      "learning_rate": 0.0006214763231197772,
      "loss": 2.2296,
      "step": 13590
    },
    {
      "epoch": 3.788300835654596,
      "grad_norm": 1.5060312747955322,
      "learning_rate": 0.0006211977715877437,
      "loss": 2.1448,
      "step": 13600
    },
    {
      "epoch": 3.7910863509749304,
      "grad_norm": 1.9406999349594116,
      "learning_rate": 0.0006209192200557104,
      "loss": 2.4383,
      "step": 13610
    },
    {
      "epoch": 3.7938718662952646,
      "grad_norm": 1.3203506469726562,
      "learning_rate": 0.0006206406685236768,
      "loss": 2.2722,
      "step": 13620
    },
    {
      "epoch": 3.796657381615599,
      "grad_norm": 1.4145845174789429,
      "learning_rate": 0.0006203621169916434,
      "loss": 2.4443,
      "step": 13630
    },
    {
      "epoch": 3.799442896935933,
      "grad_norm": 1.7279268503189087,
      "learning_rate": 0.0006200835654596101,
      "loss": 2.1316,
      "step": 13640
    },
    {
      "epoch": 3.8022284122562673,
      "grad_norm": 2.6757171154022217,
      "learning_rate": 0.0006198050139275766,
      "loss": 2.1634,
      "step": 13650
    },
    {
      "epoch": 3.8050139275766015,
      "grad_norm": 1.3096617460250854,
      "learning_rate": 0.0006195264623955432,
      "loss": 2.4399,
      "step": 13660
    },
    {
      "epoch": 3.807799442896936,
      "grad_norm": 1.4000792503356934,
      "learning_rate": 0.0006192479108635097,
      "loss": 2.1826,
      "step": 13670
    },
    {
      "epoch": 3.8105849582172704,
      "grad_norm": 1.1935285329818726,
      "learning_rate": 0.0006189693593314764,
      "loss": 2.2974,
      "step": 13680
    },
    {
      "epoch": 3.8133704735376046,
      "grad_norm": 1.4418857097625732,
      "learning_rate": 0.0006186908077994429,
      "loss": 2.3065,
      "step": 13690
    },
    {
      "epoch": 3.816155988857939,
      "grad_norm": 2.08838152885437,
      "learning_rate": 0.0006184122562674095,
      "loss": 2.2891,
      "step": 13700
    },
    {
      "epoch": 3.818941504178273,
      "grad_norm": 2.6418778896331787,
      "learning_rate": 0.0006181337047353762,
      "loss": 2.2765,
      "step": 13710
    },
    {
      "epoch": 3.8217270194986073,
      "grad_norm": 1.4695005416870117,
      "learning_rate": 0.0006178551532033426,
      "loss": 2.4332,
      "step": 13720
    },
    {
      "epoch": 3.8245125348189415,
      "grad_norm": 1.0630613565444946,
      "learning_rate": 0.0006175766016713092,
      "loss": 2.3612,
      "step": 13730
    },
    {
      "epoch": 3.8272980501392757,
      "grad_norm": 1.323739767074585,
      "learning_rate": 0.0006172980501392757,
      "loss": 2.4001,
      "step": 13740
    },
    {
      "epoch": 3.83008356545961,
      "grad_norm": 1.364373803138733,
      "learning_rate": 0.0006170194986072424,
      "loss": 2.3918,
      "step": 13750
    },
    {
      "epoch": 3.832869080779944,
      "grad_norm": 2.0041770935058594,
      "learning_rate": 0.0006167409470752089,
      "loss": 2.1838,
      "step": 13760
    },
    {
      "epoch": 3.8356545961002784,
      "grad_norm": 1.127997636795044,
      "learning_rate": 0.0006164623955431755,
      "loss": 2.3794,
      "step": 13770
    },
    {
      "epoch": 3.8384401114206126,
      "grad_norm": 1.3770983219146729,
      "learning_rate": 0.000616183844011142,
      "loss": 2.4163,
      "step": 13780
    },
    {
      "epoch": 3.841225626740947,
      "grad_norm": 1.6192904710769653,
      "learning_rate": 0.0006159052924791087,
      "loss": 2.4612,
      "step": 13790
    },
    {
      "epoch": 3.8440111420612815,
      "grad_norm": 1.3391081094741821,
      "learning_rate": 0.0006156267409470753,
      "loss": 2.5068,
      "step": 13800
    },
    {
      "epoch": 3.8467966573816157,
      "grad_norm": 1.5269739627838135,
      "learning_rate": 0.0006153481894150417,
      "loss": 2.2432,
      "step": 13810
    },
    {
      "epoch": 3.84958217270195,
      "grad_norm": 1.5659023523330688,
      "learning_rate": 0.0006150696378830084,
      "loss": 2.2154,
      "step": 13820
    },
    {
      "epoch": 3.852367688022284,
      "grad_norm": 1.4080133438110352,
      "learning_rate": 0.0006147910863509749,
      "loss": 2.4552,
      "step": 13830
    },
    {
      "epoch": 3.8551532033426184,
      "grad_norm": 1.577031135559082,
      "learning_rate": 0.0006145125348189415,
      "loss": 2.1555,
      "step": 13840
    },
    {
      "epoch": 3.8579387186629526,
      "grad_norm": 1.5950101613998413,
      "learning_rate": 0.000614233983286908,
      "loss": 2.2564,
      "step": 13850
    },
    {
      "epoch": 3.860724233983287,
      "grad_norm": 1.2344335317611694,
      "learning_rate": 0.0006139554317548747,
      "loss": 2.2925,
      "step": 13860
    },
    {
      "epoch": 3.863509749303621,
      "grad_norm": 1.7785578966140747,
      "learning_rate": 0.0006136768802228413,
      "loss": 2.2407,
      "step": 13870
    },
    {
      "epoch": 3.8662952646239557,
      "grad_norm": 2.613541841506958,
      "learning_rate": 0.0006133983286908078,
      "loss": 2.5341,
      "step": 13880
    },
    {
      "epoch": 3.86908077994429,
      "grad_norm": 1.8005751371383667,
      "learning_rate": 0.0006131197771587745,
      "loss": 2.4236,
      "step": 13890
    },
    {
      "epoch": 3.871866295264624,
      "grad_norm": 1.5647118091583252,
      "learning_rate": 0.000612841225626741,
      "loss": 2.5212,
      "step": 13900
    },
    {
      "epoch": 3.8746518105849583,
      "grad_norm": 1.1672208309173584,
      "learning_rate": 0.0006125626740947076,
      "loss": 2.2433,
      "step": 13910
    },
    {
      "epoch": 3.8774373259052926,
      "grad_norm": 1.323865532875061,
      "learning_rate": 0.000612284122562674,
      "loss": 2.3449,
      "step": 13920
    },
    {
      "epoch": 3.8802228412256268,
      "grad_norm": 1.6491001844406128,
      "learning_rate": 0.0006120055710306407,
      "loss": 2.2749,
      "step": 13930
    },
    {
      "epoch": 3.883008356545961,
      "grad_norm": 1.5053249597549438,
      "learning_rate": 0.0006117270194986072,
      "loss": 2.2855,
      "step": 13940
    },
    {
      "epoch": 3.885793871866295,
      "grad_norm": 1.982690691947937,
      "learning_rate": 0.0006114484679665738,
      "loss": 2.4336,
      "step": 13950
    },
    {
      "epoch": 3.8885793871866294,
      "grad_norm": 2.0627126693725586,
      "learning_rate": 0.0006111699164345404,
      "loss": 2.3823,
      "step": 13960
    },
    {
      "epoch": 3.8913649025069637,
      "grad_norm": 1.0806740522384644,
      "learning_rate": 0.000610891364902507,
      "loss": 2.1817,
      "step": 13970
    },
    {
      "epoch": 3.894150417827298,
      "grad_norm": 1.1616339683532715,
      "learning_rate": 0.0006106128133704736,
      "loss": 2.452,
      "step": 13980
    },
    {
      "epoch": 3.896935933147632,
      "grad_norm": 1.6819462776184082,
      "learning_rate": 0.0006103342618384401,
      "loss": 2.3086,
      "step": 13990
    },
    {
      "epoch": 3.8997214484679663,
      "grad_norm": 1.1682155132293701,
      "learning_rate": 0.0006100557103064068,
      "loss": 2.3773,
      "step": 14000
    },
    {
      "epoch": 3.902506963788301,
      "grad_norm": 1.4419502019882202,
      "learning_rate": 0.0006097771587743732,
      "loss": 2.2992,
      "step": 14010
    },
    {
      "epoch": 3.905292479108635,
      "grad_norm": 1.3555113077163696,
      "learning_rate": 0.0006094986072423398,
      "loss": 2.3394,
      "step": 14020
    },
    {
      "epoch": 3.9080779944289694,
      "grad_norm": 2.283500909805298,
      "learning_rate": 0.0006092200557103063,
      "loss": 2.3579,
      "step": 14030
    },
    {
      "epoch": 3.9108635097493036,
      "grad_norm": 1.3312451839447021,
      "learning_rate": 0.000608941504178273,
      "loss": 2.2477,
      "step": 14040
    },
    {
      "epoch": 3.913649025069638,
      "grad_norm": 1.2014777660369873,
      "learning_rate": 0.0006086629526462396,
      "loss": 2.2884,
      "step": 14050
    },
    {
      "epoch": 3.916434540389972,
      "grad_norm": 1.4588004350662231,
      "learning_rate": 0.0006083844011142061,
      "loss": 2.4098,
      "step": 14060
    },
    {
      "epoch": 3.9192200557103063,
      "grad_norm": 1.4915868043899536,
      "learning_rate": 0.0006081058495821728,
      "loss": 2.164,
      "step": 14070
    },
    {
      "epoch": 3.9220055710306405,
      "grad_norm": 1.392350435256958,
      "learning_rate": 0.0006078272980501393,
      "loss": 2.1953,
      "step": 14080
    },
    {
      "epoch": 3.924791086350975,
      "grad_norm": 1.5342321395874023,
      "learning_rate": 0.0006075487465181059,
      "loss": 2.3332,
      "step": 14090
    },
    {
      "epoch": 3.9275766016713094,
      "grad_norm": 1.3370779752731323,
      "learning_rate": 0.0006072701949860724,
      "loss": 2.517,
      "step": 14100
    },
    {
      "epoch": 3.9303621169916436,
      "grad_norm": 1.3041504621505737,
      "learning_rate": 0.000606991643454039,
      "loss": 2.4116,
      "step": 14110
    },
    {
      "epoch": 3.933147632311978,
      "grad_norm": 1.0169172286987305,
      "learning_rate": 0.0006067130919220056,
      "loss": 2.2788,
      "step": 14120
    },
    {
      "epoch": 3.935933147632312,
      "grad_norm": 2.128232002258301,
      "learning_rate": 0.0006064345403899721,
      "loss": 2.3141,
      "step": 14130
    },
    {
      "epoch": 3.9387186629526463,
      "grad_norm": 1.4834986925125122,
      "learning_rate": 0.0006061559888579387,
      "loss": 2.4281,
      "step": 14140
    },
    {
      "epoch": 3.9415041782729805,
      "grad_norm": 1.6608153581619263,
      "learning_rate": 0.0006058774373259053,
      "loss": 2.4912,
      "step": 14150
    },
    {
      "epoch": 3.9442896935933147,
      "grad_norm": 1.4153929948806763,
      "learning_rate": 0.0006055988857938719,
      "loss": 2.4465,
      "step": 14160
    },
    {
      "epoch": 3.947075208913649,
      "grad_norm": 1.3556253910064697,
      "learning_rate": 0.0006053203342618384,
      "loss": 2.0883,
      "step": 14170
    },
    {
      "epoch": 3.949860724233983,
      "grad_norm": 1.8780953884124756,
      "learning_rate": 0.0006050417827298051,
      "loss": 2.2217,
      "step": 14180
    },
    {
      "epoch": 3.9526462395543174,
      "grad_norm": 1.7089242935180664,
      "learning_rate": 0.0006047632311977716,
      "loss": 2.3737,
      "step": 14190
    },
    {
      "epoch": 3.9554317548746516,
      "grad_norm": 2.668304204940796,
      "learning_rate": 0.0006044846796657382,
      "loss": 2.361,
      "step": 14200
    },
    {
      "epoch": 3.958217270194986,
      "grad_norm": 1.1752207279205322,
      "learning_rate": 0.0006042061281337047,
      "loss": 2.3516,
      "step": 14210
    },
    {
      "epoch": 3.9610027855153205,
      "grad_norm": 0.9602178335189819,
      "learning_rate": 0.0006039275766016713,
      "loss": 2.2185,
      "step": 14220
    },
    {
      "epoch": 3.9637883008356547,
      "grad_norm": 1.7957921028137207,
      "learning_rate": 0.0006036490250696379,
      "loss": 2.3224,
      "step": 14230
    },
    {
      "epoch": 3.966573816155989,
      "grad_norm": 1.4005470275878906,
      "learning_rate": 0.0006033704735376044,
      "loss": 2.2388,
      "step": 14240
    },
    {
      "epoch": 3.969359331476323,
      "grad_norm": 1.2645385265350342,
      "learning_rate": 0.0006030919220055711,
      "loss": 2.4453,
      "step": 14250
    },
    {
      "epoch": 3.9721448467966574,
      "grad_norm": 2.088280439376831,
      "learning_rate": 0.0006028133704735376,
      "loss": 2.1654,
      "step": 14260
    },
    {
      "epoch": 3.9749303621169916,
      "grad_norm": 1.889398455619812,
      "learning_rate": 0.0006025348189415042,
      "loss": 2.3453,
      "step": 14270
    },
    {
      "epoch": 3.977715877437326,
      "grad_norm": 1.5112652778625488,
      "learning_rate": 0.0006022562674094708,
      "loss": 2.2797,
      "step": 14280
    },
    {
      "epoch": 3.98050139275766,
      "grad_norm": 1.3182045221328735,
      "learning_rate": 0.0006019777158774374,
      "loss": 2.1235,
      "step": 14290
    },
    {
      "epoch": 3.9832869080779947,
      "grad_norm": 1.5027703046798706,
      "learning_rate": 0.000601699164345404,
      "loss": 2.2445,
      "step": 14300
    },
    {
      "epoch": 3.986072423398329,
      "grad_norm": 1.4088374376296997,
      "learning_rate": 0.0006014206128133704,
      "loss": 2.5385,
      "step": 14310
    },
    {
      "epoch": 3.988857938718663,
      "grad_norm": 1.4582080841064453,
      "learning_rate": 0.000601142061281337,
      "loss": 2.3136,
      "step": 14320
    },
    {
      "epoch": 3.9916434540389973,
      "grad_norm": 1.0363234281539917,
      "learning_rate": 0.0006008635097493036,
      "loss": 2.2557,
      "step": 14330
    },
    {
      "epoch": 3.9944289693593316,
      "grad_norm": 1.3371258974075317,
      "learning_rate": 0.0006005849582172702,
      "loss": 2.2123,
      "step": 14340
    },
    {
      "epoch": 3.997214484679666,
      "grad_norm": 1.4910093545913696,
      "learning_rate": 0.0006003064066852367,
      "loss": 2.1979,
      "step": 14350
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.3127790689468384,
      "learning_rate": 0.0006000278551532034,
      "loss": 2.1229,
      "step": 14360
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.303037643432617,
      "eval_runtime": 7.4671,
      "eval_samples_per_second": 427.475,
      "eval_steps_per_second": 53.434,
      "step": 14360
    },
    {
      "epoch": 4.002785515320334,
      "grad_norm": 1.7211501598358154,
      "learning_rate": 0.00059974930362117,
      "loss": 2.1479,
      "step": 14370
    },
    {
      "epoch": 4.005571030640668,
      "grad_norm": 1.9111131429672241,
      "learning_rate": 0.0005994707520891365,
      "loss": 2.2563,
      "step": 14380
    },
    {
      "epoch": 4.008356545961003,
      "grad_norm": 1.4512723684310913,
      "learning_rate": 0.0005991922005571031,
      "loss": 2.2352,
      "step": 14390
    },
    {
      "epoch": 4.011142061281337,
      "grad_norm": 1.8751652240753174,
      "learning_rate": 0.0005989136490250697,
      "loss": 2.4024,
      "step": 14400
    },
    {
      "epoch": 4.013927576601671,
      "grad_norm": 1.541527271270752,
      "learning_rate": 0.0005986350974930362,
      "loss": 2.2115,
      "step": 14410
    },
    {
      "epoch": 4.016713091922005,
      "grad_norm": 1.5012550354003906,
      "learning_rate": 0.0005983565459610027,
      "loss": 2.3306,
      "step": 14420
    },
    {
      "epoch": 4.0194986072423395,
      "grad_norm": 1.344869613647461,
      "learning_rate": 0.0005980779944289694,
      "loss": 2.2062,
      "step": 14430
    },
    {
      "epoch": 4.022284122562674,
      "grad_norm": 1.3758901357650757,
      "learning_rate": 0.000597799442896936,
      "loss": 2.2116,
      "step": 14440
    },
    {
      "epoch": 4.025069637883008,
      "grad_norm": 1.2528748512268066,
      "learning_rate": 0.0005975208913649025,
      "loss": 2.2076,
      "step": 14450
    },
    {
      "epoch": 4.027855153203342,
      "grad_norm": 1.385085105895996,
      "learning_rate": 0.0005972423398328691,
      "loss": 2.0469,
      "step": 14460
    },
    {
      "epoch": 4.030640668523677,
      "grad_norm": 1.5531824827194214,
      "learning_rate": 0.0005969637883008357,
      "loss": 2.2146,
      "step": 14470
    },
    {
      "epoch": 4.0334261838440115,
      "grad_norm": 1.4322782754898071,
      "learning_rate": 0.0005966852367688023,
      "loss": 2.3429,
      "step": 14480
    },
    {
      "epoch": 4.036211699164346,
      "grad_norm": 1.8767220973968506,
      "learning_rate": 0.0005964066852367688,
      "loss": 2.2029,
      "step": 14490
    },
    {
      "epoch": 4.03899721448468,
      "grad_norm": 1.9717566967010498,
      "learning_rate": 0.0005961281337047353,
      "loss": 2.256,
      "step": 14500
    },
    {
      "epoch": 4.041782729805014,
      "grad_norm": 1.9657717943191528,
      "learning_rate": 0.0005958495821727019,
      "loss": 2.444,
      "step": 14510
    },
    {
      "epoch": 4.044568245125348,
      "grad_norm": 1.4995956420898438,
      "learning_rate": 0.0005955710306406685,
      "loss": 2.3662,
      "step": 14520
    },
    {
      "epoch": 4.047353760445683,
      "grad_norm": 1.4484071731567383,
      "learning_rate": 0.0005952924791086351,
      "loss": 2.322,
      "step": 14530
    },
    {
      "epoch": 4.050139275766017,
      "grad_norm": 1.449535846710205,
      "learning_rate": 0.0005950139275766017,
      "loss": 2.3129,
      "step": 14540
    },
    {
      "epoch": 4.052924791086351,
      "grad_norm": 2.45548152923584,
      "learning_rate": 0.0005947353760445683,
      "loss": 2.3166,
      "step": 14550
    },
    {
      "epoch": 4.055710306406685,
      "grad_norm": 1.266051173210144,
      "learning_rate": 0.0005944568245125348,
      "loss": 2.1397,
      "step": 14560
    },
    {
      "epoch": 4.0584958217270195,
      "grad_norm": 1.9865046739578247,
      "learning_rate": 0.0005941782729805014,
      "loss": 2.2457,
      "step": 14570
    },
    {
      "epoch": 4.061281337047354,
      "grad_norm": 1.3924604654312134,
      "learning_rate": 0.000593899721448468,
      "loss": 2.2838,
      "step": 14580
    },
    {
      "epoch": 4.064066852367688,
      "grad_norm": 1.349166750907898,
      "learning_rate": 0.0005936211699164346,
      "loss": 2.4162,
      "step": 14590
    },
    {
      "epoch": 4.066852367688022,
      "grad_norm": 1.469250202178955,
      "learning_rate": 0.0005933426183844012,
      "loss": 2.0466,
      "step": 14600
    },
    {
      "epoch": 4.069637883008356,
      "grad_norm": 1.8411563634872437,
      "learning_rate": 0.0005930640668523677,
      "loss": 2.4152,
      "step": 14610
    },
    {
      "epoch": 4.072423398328691,
      "grad_norm": 1.5608642101287842,
      "learning_rate": 0.0005927855153203343,
      "loss": 2.2453,
      "step": 14620
    },
    {
      "epoch": 4.075208913649025,
      "grad_norm": 1.8815317153930664,
      "learning_rate": 0.0005925069637883008,
      "loss": 2.4237,
      "step": 14630
    },
    {
      "epoch": 4.077994428969359,
      "grad_norm": 1.5334280729293823,
      "learning_rate": 0.0005922284122562674,
      "loss": 2.2353,
      "step": 14640
    },
    {
      "epoch": 4.080779944289693,
      "grad_norm": 2.2637670040130615,
      "learning_rate": 0.000591949860724234,
      "loss": 2.2661,
      "step": 14650
    },
    {
      "epoch": 4.0835654596100275,
      "grad_norm": 1.4410696029663086,
      "learning_rate": 0.0005916713091922006,
      "loss": 2.1674,
      "step": 14660
    },
    {
      "epoch": 4.086350974930362,
      "grad_norm": 1.3641817569732666,
      "learning_rate": 0.0005913927576601671,
      "loss": 2.2652,
      "step": 14670
    },
    {
      "epoch": 4.089136490250697,
      "grad_norm": 1.4518961906433105,
      "learning_rate": 0.0005911142061281337,
      "loss": 2.1824,
      "step": 14680
    },
    {
      "epoch": 4.091922005571031,
      "grad_norm": 1.6551156044006348,
      "learning_rate": 0.0005908356545961004,
      "loss": 2.1841,
      "step": 14690
    },
    {
      "epoch": 4.094707520891365,
      "grad_norm": 1.3264793157577515,
      "learning_rate": 0.0005905571030640668,
      "loss": 2.2867,
      "step": 14700
    },
    {
      "epoch": 4.0974930362116995,
      "grad_norm": 1.1034002304077148,
      "learning_rate": 0.0005902785515320334,
      "loss": 2.345,
      "step": 14710
    },
    {
      "epoch": 4.100278551532034,
      "grad_norm": 1.3071985244750977,
      "learning_rate": 0.00059,
      "loss": 2.0856,
      "step": 14720
    },
    {
      "epoch": 4.103064066852368,
      "grad_norm": 1.9600307941436768,
      "learning_rate": 0.0005897214484679666,
      "loss": 2.2531,
      "step": 14730
    },
    {
      "epoch": 4.105849582172702,
      "grad_norm": 3.0325570106506348,
      "learning_rate": 0.0005894428969359331,
      "loss": 2.1955,
      "step": 14740
    },
    {
      "epoch": 4.108635097493036,
      "grad_norm": 1.5314841270446777,
      "learning_rate": 0.0005891643454038997,
      "loss": 2.4581,
      "step": 14750
    },
    {
      "epoch": 4.111420612813371,
      "grad_norm": 1.5978096723556519,
      "learning_rate": 0.0005888857938718664,
      "loss": 2.2086,
      "step": 14760
    },
    {
      "epoch": 4.114206128133705,
      "grad_norm": 1.435271143913269,
      "learning_rate": 0.0005886072423398329,
      "loss": 2.0678,
      "step": 14770
    },
    {
      "epoch": 4.116991643454039,
      "grad_norm": 2.2829911708831787,
      "learning_rate": 0.0005883286908077995,
      "loss": 2.3048,
      "step": 14780
    },
    {
      "epoch": 4.119777158774373,
      "grad_norm": 1.541003704071045,
      "learning_rate": 0.000588050139275766,
      "loss": 2.4319,
      "step": 14790
    },
    {
      "epoch": 4.1225626740947074,
      "grad_norm": 1.7037361860275269,
      "learning_rate": 0.0005877715877437327,
      "loss": 2.3005,
      "step": 14800
    },
    {
      "epoch": 4.125348189415042,
      "grad_norm": 1.425915002822876,
      "learning_rate": 0.0005874930362116991,
      "loss": 2.37,
      "step": 14810
    },
    {
      "epoch": 4.128133704735376,
      "grad_norm": 1.3565561771392822,
      "learning_rate": 0.0005872144846796657,
      "loss": 2.2272,
      "step": 14820
    },
    {
      "epoch": 4.13091922005571,
      "grad_norm": 2.377429485321045,
      "learning_rate": 0.0005869359331476323,
      "loss": 2.3349,
      "step": 14830
    },
    {
      "epoch": 4.133704735376044,
      "grad_norm": 1.552560567855835,
      "learning_rate": 0.0005866573816155989,
      "loss": 2.2039,
      "step": 14840
    },
    {
      "epoch": 4.1364902506963785,
      "grad_norm": 1.9603928327560425,
      "learning_rate": 0.0005863788300835655,
      "loss": 2.1792,
      "step": 14850
    },
    {
      "epoch": 4.139275766016713,
      "grad_norm": 1.4121944904327393,
      "learning_rate": 0.000586100278551532,
      "loss": 2.3357,
      "step": 14860
    },
    {
      "epoch": 4.142061281337047,
      "grad_norm": 1.9297597408294678,
      "learning_rate": 0.0005858217270194987,
      "loss": 2.2619,
      "step": 14870
    },
    {
      "epoch": 4.144846796657381,
      "grad_norm": 1.302758812904358,
      "learning_rate": 0.0005855431754874652,
      "loss": 2.3194,
      "step": 14880
    },
    {
      "epoch": 4.147632311977716,
      "grad_norm": 1.1900750398635864,
      "learning_rate": 0.0005852646239554318,
      "loss": 2.2073,
      "step": 14890
    },
    {
      "epoch": 4.1504178272980505,
      "grad_norm": 2.0786492824554443,
      "learning_rate": 0.0005849860724233983,
      "loss": 2.5882,
      "step": 14900
    },
    {
      "epoch": 4.153203342618385,
      "grad_norm": 1.424649953842163,
      "learning_rate": 0.0005847075208913649,
      "loss": 2.2862,
      "step": 14910
    },
    {
      "epoch": 4.155988857938719,
      "grad_norm": 2.1116364002227783,
      "learning_rate": 0.0005844289693593315,
      "loss": 2.3851,
      "step": 14920
    },
    {
      "epoch": 4.158774373259053,
      "grad_norm": 1.3568559885025024,
      "learning_rate": 0.000584150417827298,
      "loss": 2.3145,
      "step": 14930
    },
    {
      "epoch": 4.161559888579387,
      "grad_norm": 1.647481083869934,
      "learning_rate": 0.0005838718662952647,
      "loss": 2.3139,
      "step": 14940
    },
    {
      "epoch": 4.164345403899722,
      "grad_norm": 1.825411319732666,
      "learning_rate": 0.0005835933147632312,
      "loss": 2.3517,
      "step": 14950
    },
    {
      "epoch": 4.167130919220056,
      "grad_norm": 1.5015562772750854,
      "learning_rate": 0.0005833147632311978,
      "loss": 2.3594,
      "step": 14960
    },
    {
      "epoch": 4.16991643454039,
      "grad_norm": 1.4484899044036865,
      "learning_rate": 0.0005830362116991643,
      "loss": 2.3464,
      "step": 14970
    },
    {
      "epoch": 4.172701949860724,
      "grad_norm": 1.968000054359436,
      "learning_rate": 0.000582757660167131,
      "loss": 2.2113,
      "step": 14980
    },
    {
      "epoch": 4.1754874651810585,
      "grad_norm": 2.0083940029144287,
      "learning_rate": 0.0005824791086350975,
      "loss": 2.2748,
      "step": 14990
    },
    {
      "epoch": 4.178272980501393,
      "grad_norm": 1.7102161645889282,
      "learning_rate": 0.000582200557103064,
      "loss": 2.2811,
      "step": 15000
    },
    {
      "epoch": 4.181058495821727,
      "grad_norm": 1.6540075540542603,
      "learning_rate": 0.0005819220055710307,
      "loss": 2.3317,
      "step": 15010
    },
    {
      "epoch": 4.183844011142061,
      "grad_norm": 1.7932406663894653,
      "learning_rate": 0.0005816434540389972,
      "loss": 2.1698,
      "step": 15020
    },
    {
      "epoch": 4.186629526462395,
      "grad_norm": 1.471985936164856,
      "learning_rate": 0.0005813649025069638,
      "loss": 2.2749,
      "step": 15030
    },
    {
      "epoch": 4.18941504178273,
      "grad_norm": 1.500661015510559,
      "learning_rate": 0.0005810863509749303,
      "loss": 2.3881,
      "step": 15040
    },
    {
      "epoch": 4.192200557103064,
      "grad_norm": 1.35371732711792,
      "learning_rate": 0.000580807799442897,
      "loss": 2.5114,
      "step": 15050
    },
    {
      "epoch": 4.194986072423398,
      "grad_norm": 1.1617164611816406,
      "learning_rate": 0.0005805292479108635,
      "loss": 2.1911,
      "step": 15060
    },
    {
      "epoch": 4.197771587743732,
      "grad_norm": 1.529843807220459,
      "learning_rate": 0.0005802506963788301,
      "loss": 2.1165,
      "step": 15070
    },
    {
      "epoch": 4.2005571030640665,
      "grad_norm": 1.661721110343933,
      "learning_rate": 0.0005799721448467968,
      "loss": 2.275,
      "step": 15080
    },
    {
      "epoch": 4.203342618384401,
      "grad_norm": 1.763130784034729,
      "learning_rate": 0.0005796935933147633,
      "loss": 2.3049,
      "step": 15090
    },
    {
      "epoch": 4.206128133704736,
      "grad_norm": 1.722551703453064,
      "learning_rate": 0.0005794150417827298,
      "loss": 2.2769,
      "step": 15100
    },
    {
      "epoch": 4.20891364902507,
      "grad_norm": 1.3210653066635132,
      "learning_rate": 0.0005791364902506963,
      "loss": 2.3048,
      "step": 15110
    },
    {
      "epoch": 4.211699164345404,
      "grad_norm": 1.911927342414856,
      "learning_rate": 0.000578857938718663,
      "loss": 2.3387,
      "step": 15120
    },
    {
      "epoch": 4.2144846796657385,
      "grad_norm": 1.1150304079055786,
      "learning_rate": 0.0005785793871866295,
      "loss": 2.1197,
      "step": 15130
    },
    {
      "epoch": 4.217270194986073,
      "grad_norm": 1.2875853776931763,
      "learning_rate": 0.0005783008356545961,
      "loss": 2.2205,
      "step": 15140
    },
    {
      "epoch": 4.220055710306407,
      "grad_norm": 1.155649185180664,
      "learning_rate": 0.0005780222841225626,
      "loss": 2.3905,
      "step": 15150
    },
    {
      "epoch": 4.222841225626741,
      "grad_norm": 2.3093862533569336,
      "learning_rate": 0.0005777437325905293,
      "loss": 2.3696,
      "step": 15160
    },
    {
      "epoch": 4.225626740947075,
      "grad_norm": 1.631727933883667,
      "learning_rate": 0.0005774651810584959,
      "loss": 2.2695,
      "step": 15170
    },
    {
      "epoch": 4.22841225626741,
      "grad_norm": 1.4838320016860962,
      "learning_rate": 0.0005771866295264624,
      "loss": 2.2719,
      "step": 15180
    },
    {
      "epoch": 4.231197771587744,
      "grad_norm": 1.6492424011230469,
      "learning_rate": 0.0005769080779944291,
      "loss": 2.3499,
      "step": 15190
    },
    {
      "epoch": 4.233983286908078,
      "grad_norm": 1.8083651065826416,
      "learning_rate": 0.0005766295264623955,
      "loss": 2.1621,
      "step": 15200
    },
    {
      "epoch": 4.236768802228412,
      "grad_norm": 1.8096024990081787,
      "learning_rate": 0.0005763509749303621,
      "loss": 2.2862,
      "step": 15210
    },
    {
      "epoch": 4.2395543175487465,
      "grad_norm": 1.553142786026001,
      "learning_rate": 0.0005760724233983286,
      "loss": 2.1597,
      "step": 15220
    },
    {
      "epoch": 4.242339832869081,
      "grad_norm": 1.5264595746994019,
      "learning_rate": 0.0005757938718662953,
      "loss": 2.3355,
      "step": 15230
    },
    {
      "epoch": 4.245125348189415,
      "grad_norm": 1.6521337032318115,
      "learning_rate": 0.0005755153203342619,
      "loss": 2.2875,
      "step": 15240
    },
    {
      "epoch": 4.247910863509749,
      "grad_norm": 1.5602887868881226,
      "learning_rate": 0.0005752367688022284,
      "loss": 2.338,
      "step": 15250
    },
    {
      "epoch": 4.250696378830083,
      "grad_norm": 1.5607866048812866,
      "learning_rate": 0.0005749582172701951,
      "loss": 2.1569,
      "step": 15260
    },
    {
      "epoch": 4.2534818941504176,
      "grad_norm": 1.623724341392517,
      "learning_rate": 0.0005746796657381616,
      "loss": 2.4119,
      "step": 15270
    },
    {
      "epoch": 4.256267409470752,
      "grad_norm": 1.2421801090240479,
      "learning_rate": 0.0005744011142061282,
      "loss": 2.1526,
      "step": 15280
    },
    {
      "epoch": 4.259052924791086,
      "grad_norm": 1.6486455202102661,
      "learning_rate": 0.0005741225626740946,
      "loss": 2.3704,
      "step": 15290
    },
    {
      "epoch": 4.26183844011142,
      "grad_norm": 2.3398211002349854,
      "learning_rate": 0.0005738440111420613,
      "loss": 2.1877,
      "step": 15300
    },
    {
      "epoch": 4.264623955431755,
      "grad_norm": 1.1976968050003052,
      "learning_rate": 0.0005735654596100278,
      "loss": 2.2312,
      "step": 15310
    },
    {
      "epoch": 4.2674094707520895,
      "grad_norm": 1.1566706895828247,
      "learning_rate": 0.0005732869080779944,
      "loss": 2.1434,
      "step": 15320
    },
    {
      "epoch": 4.270194986072424,
      "grad_norm": 1.2232005596160889,
      "learning_rate": 0.000573008356545961,
      "loss": 2.2996,
      "step": 15330
    },
    {
      "epoch": 4.272980501392758,
      "grad_norm": 1.8922219276428223,
      "learning_rate": 0.0005727298050139276,
      "loss": 2.1679,
      "step": 15340
    },
    {
      "epoch": 4.275766016713092,
      "grad_norm": 1.4461169242858887,
      "learning_rate": 0.0005724512534818942,
      "loss": 2.1866,
      "step": 15350
    },
    {
      "epoch": 4.278551532033426,
      "grad_norm": 2.8974685668945312,
      "learning_rate": 0.0005721727019498607,
      "loss": 2.248,
      "step": 15360
    },
    {
      "epoch": 4.281337047353761,
      "grad_norm": 1.3743562698364258,
      "learning_rate": 0.0005718941504178274,
      "loss": 2.2139,
      "step": 15370
    },
    {
      "epoch": 4.284122562674095,
      "grad_norm": 1.9262303113937378,
      "learning_rate": 0.0005716155988857939,
      "loss": 2.1669,
      "step": 15380
    },
    {
      "epoch": 4.286908077994429,
      "grad_norm": 1.1794800758361816,
      "learning_rate": 0.0005713370473537604,
      "loss": 2.1112,
      "step": 15390
    },
    {
      "epoch": 4.289693593314763,
      "grad_norm": 1.707641363143921,
      "learning_rate": 0.000571058495821727,
      "loss": 2.2703,
      "step": 15400
    },
    {
      "epoch": 4.2924791086350975,
      "grad_norm": 1.4144830703735352,
      "learning_rate": 0.0005707799442896936,
      "loss": 2.2095,
      "step": 15410
    },
    {
      "epoch": 4.295264623955432,
      "grad_norm": 1.3467133045196533,
      "learning_rate": 0.0005705013927576602,
      "loss": 2.199,
      "step": 15420
    },
    {
      "epoch": 4.298050139275766,
      "grad_norm": 1.2861568927764893,
      "learning_rate": 0.0005702228412256267,
      "loss": 2.2724,
      "step": 15430
    },
    {
      "epoch": 4.3008356545961,
      "grad_norm": 1.564134120941162,
      "learning_rate": 0.0005699442896935934,
      "loss": 2.1822,
      "step": 15440
    },
    {
      "epoch": 4.303621169916434,
      "grad_norm": 1.1119892597198486,
      "learning_rate": 0.0005696657381615599,
      "loss": 2.1429,
      "step": 15450
    },
    {
      "epoch": 4.306406685236769,
      "grad_norm": 1.1917943954467773,
      "learning_rate": 0.0005693871866295265,
      "loss": 2.2418,
      "step": 15460
    },
    {
      "epoch": 4.309192200557103,
      "grad_norm": 1.0957452058792114,
      "learning_rate": 0.000569108635097493,
      "loss": 2.1393,
      "step": 15470
    },
    {
      "epoch": 4.311977715877437,
      "grad_norm": 1.342321515083313,
      "learning_rate": 0.0005688300835654597,
      "loss": 2.3052,
      "step": 15480
    },
    {
      "epoch": 4.314763231197771,
      "grad_norm": 1.4726409912109375,
      "learning_rate": 0.0005685515320334263,
      "loss": 2.1896,
      "step": 15490
    },
    {
      "epoch": 4.3175487465181055,
      "grad_norm": 1.4825185537338257,
      "learning_rate": 0.0005682729805013927,
      "loss": 2.3636,
      "step": 15500
    },
    {
      "epoch": 4.32033426183844,
      "grad_norm": 2.000559091567993,
      "learning_rate": 0.0005679944289693593,
      "loss": 2.1671,
      "step": 15510
    },
    {
      "epoch": 4.323119777158775,
      "grad_norm": 1.5653886795043945,
      "learning_rate": 0.0005677158774373259,
      "loss": 2.2671,
      "step": 15520
    },
    {
      "epoch": 4.325905292479109,
      "grad_norm": 1.5921636819839478,
      "learning_rate": 0.0005674373259052925,
      "loss": 2.2419,
      "step": 15530
    },
    {
      "epoch": 4.328690807799443,
      "grad_norm": 1.1974326372146606,
      "learning_rate": 0.000567158774373259,
      "loss": 2.1482,
      "step": 15540
    },
    {
      "epoch": 4.3314763231197775,
      "grad_norm": 1.340927004814148,
      "learning_rate": 0.0005668802228412257,
      "loss": 2.4833,
      "step": 15550
    },
    {
      "epoch": 4.334261838440112,
      "grad_norm": 1.1183279752731323,
      "learning_rate": 0.0005666016713091922,
      "loss": 2.4002,
      "step": 15560
    },
    {
      "epoch": 4.337047353760446,
      "grad_norm": 1.3969544172286987,
      "learning_rate": 0.0005663231197771588,
      "loss": 2.2939,
      "step": 15570
    },
    {
      "epoch": 4.33983286908078,
      "grad_norm": 1.647425651550293,
      "learning_rate": 0.0005660445682451254,
      "loss": 1.9411,
      "step": 15580
    },
    {
      "epoch": 4.342618384401114,
      "grad_norm": 1.4710263013839722,
      "learning_rate": 0.000565766016713092,
      "loss": 2.3385,
      "step": 15590
    },
    {
      "epoch": 4.345403899721449,
      "grad_norm": 1.4216434955596924,
      "learning_rate": 0.0005654874651810585,
      "loss": 2.2263,
      "step": 15600
    },
    {
      "epoch": 4.348189415041783,
      "grad_norm": 1.387345552444458,
      "learning_rate": 0.000565208913649025,
      "loss": 2.2222,
      "step": 15610
    },
    {
      "epoch": 4.350974930362117,
      "grad_norm": 2.034116268157959,
      "learning_rate": 0.0005649303621169917,
      "loss": 2.3418,
      "step": 15620
    },
    {
      "epoch": 4.353760445682451,
      "grad_norm": 2.015183925628662,
      "learning_rate": 0.0005646518105849582,
      "loss": 2.3324,
      "step": 15630
    },
    {
      "epoch": 4.3565459610027855,
      "grad_norm": 1.53713858127594,
      "learning_rate": 0.0005643732590529248,
      "loss": 2.356,
      "step": 15640
    },
    {
      "epoch": 4.35933147632312,
      "grad_norm": 1.4293432235717773,
      "learning_rate": 0.0005640947075208914,
      "loss": 2.2883,
      "step": 15650
    },
    {
      "epoch": 4.362116991643454,
      "grad_norm": 1.9506398439407349,
      "learning_rate": 0.000563816155988858,
      "loss": 2.311,
      "step": 15660
    },
    {
      "epoch": 4.364902506963788,
      "grad_norm": 1.5502456426620483,
      "learning_rate": 0.0005635376044568246,
      "loss": 2.1512,
      "step": 15670
    },
    {
      "epoch": 4.367688022284122,
      "grad_norm": 1.7916501760482788,
      "learning_rate": 0.000563259052924791,
      "loss": 2.3915,
      "step": 15680
    },
    {
      "epoch": 4.370473537604457,
      "grad_norm": 1.8738164901733398,
      "learning_rate": 0.0005629805013927576,
      "loss": 2.1796,
      "step": 15690
    },
    {
      "epoch": 4.373259052924791,
      "grad_norm": 1.4822648763656616,
      "learning_rate": 0.0005627019498607242,
      "loss": 2.2155,
      "step": 15700
    },
    {
      "epoch": 4.376044568245125,
      "grad_norm": 1.531183123588562,
      "learning_rate": 0.0005624233983286908,
      "loss": 2.1063,
      "step": 15710
    },
    {
      "epoch": 4.378830083565459,
      "grad_norm": 1.7767345905303955,
      "learning_rate": 0.0005621448467966573,
      "loss": 2.327,
      "step": 15720
    },
    {
      "epoch": 4.381615598885794,
      "grad_norm": 1.41966712474823,
      "learning_rate": 0.000561866295264624,
      "loss": 2.213,
      "step": 15730
    },
    {
      "epoch": 4.3844011142061285,
      "grad_norm": 1.2733031511306763,
      "learning_rate": 0.0005615877437325906,
      "loss": 2.184,
      "step": 15740
    },
    {
      "epoch": 4.387186629526463,
      "grad_norm": 2.1533522605895996,
      "learning_rate": 0.0005613091922005571,
      "loss": 2.5383,
      "step": 15750
    },
    {
      "epoch": 4.389972144846797,
      "grad_norm": 1.3536320924758911,
      "learning_rate": 0.0005610306406685237,
      "loss": 2.3612,
      "step": 15760
    },
    {
      "epoch": 4.392757660167131,
      "grad_norm": 1.486376404762268,
      "learning_rate": 0.0005607520891364903,
      "loss": 2.0468,
      "step": 15770
    },
    {
      "epoch": 4.395543175487465,
      "grad_norm": 1.4719821214675903,
      "learning_rate": 0.0005604735376044569,
      "loss": 2.3709,
      "step": 15780
    },
    {
      "epoch": 4.3983286908078,
      "grad_norm": 2.2954800128936768,
      "learning_rate": 0.0005601949860724233,
      "loss": 2.347,
      "step": 15790
    },
    {
      "epoch": 4.401114206128134,
      "grad_norm": 1.9465268850326538,
      "learning_rate": 0.00055991643454039,
      "loss": 2.1489,
      "step": 15800
    },
    {
      "epoch": 4.403899721448468,
      "grad_norm": 1.8258708715438843,
      "learning_rate": 0.0005596378830083566,
      "loss": 2.4865,
      "step": 15810
    },
    {
      "epoch": 4.406685236768802,
      "grad_norm": 1.4746390581130981,
      "learning_rate": 0.0005593593314763231,
      "loss": 2.4169,
      "step": 15820
    },
    {
      "epoch": 4.4094707520891365,
      "grad_norm": 1.4859880208969116,
      "learning_rate": 0.0005590807799442897,
      "loss": 2.3495,
      "step": 15830
    },
    {
      "epoch": 4.412256267409471,
      "grad_norm": 1.4606701135635376,
      "learning_rate": 0.0005588022284122563,
      "loss": 2.2115,
      "step": 15840
    },
    {
      "epoch": 4.415041782729805,
      "grad_norm": 2.117438316345215,
      "learning_rate": 0.0005585236768802229,
      "loss": 2.3061,
      "step": 15850
    },
    {
      "epoch": 4.417827298050139,
      "grad_norm": 1.353198766708374,
      "learning_rate": 0.0005582451253481894,
      "loss": 2.2538,
      "step": 15860
    },
    {
      "epoch": 4.420612813370473,
      "grad_norm": 1.5408591032028198,
      "learning_rate": 0.000557966573816156,
      "loss": 2.2362,
      "step": 15870
    },
    {
      "epoch": 4.423398328690808,
      "grad_norm": 2.154747486114502,
      "learning_rate": 0.0005576880222841225,
      "loss": 2.4267,
      "step": 15880
    },
    {
      "epoch": 4.426183844011142,
      "grad_norm": 1.4361196756362915,
      "learning_rate": 0.0005574094707520891,
      "loss": 2.3178,
      "step": 15890
    },
    {
      "epoch": 4.428969359331476,
      "grad_norm": 1.912140965461731,
      "learning_rate": 0.0005571309192200557,
      "loss": 2.3718,
      "step": 15900
    },
    {
      "epoch": 4.43175487465181,
      "grad_norm": 1.6180187463760376,
      "learning_rate": 0.0005568523676880223,
      "loss": 2.2969,
      "step": 15910
    },
    {
      "epoch": 4.4345403899721445,
      "grad_norm": 1.3013750314712524,
      "learning_rate": 0.0005565738161559889,
      "loss": 2.4047,
      "step": 15920
    },
    {
      "epoch": 4.437325905292479,
      "grad_norm": 1.1165848970413208,
      "learning_rate": 0.0005562952646239554,
      "loss": 2.299,
      "step": 15930
    },
    {
      "epoch": 4.440111420612814,
      "grad_norm": 1.3780080080032349,
      "learning_rate": 0.000556016713091922,
      "loss": 2.189,
      "step": 15940
    },
    {
      "epoch": 4.442896935933147,
      "grad_norm": 1.865444302558899,
      "learning_rate": 0.0005557381615598886,
      "loss": 2.2714,
      "step": 15950
    },
    {
      "epoch": 4.445682451253482,
      "grad_norm": 1.795111894607544,
      "learning_rate": 0.0005554596100278552,
      "loss": 2.2501,
      "step": 15960
    },
    {
      "epoch": 4.4484679665738165,
      "grad_norm": 1.9234904050827026,
      "learning_rate": 0.0005551810584958218,
      "loss": 2.3943,
      "step": 15970
    },
    {
      "epoch": 4.451253481894151,
      "grad_norm": 1.3206453323364258,
      "learning_rate": 0.0005549025069637884,
      "loss": 2.2537,
      "step": 15980
    },
    {
      "epoch": 4.454038997214485,
      "grad_norm": 0.9544207453727722,
      "learning_rate": 0.0005546239554317549,
      "loss": 2.2083,
      "step": 15990
    },
    {
      "epoch": 4.456824512534819,
      "grad_norm": 1.9728907346725464,
      "learning_rate": 0.0005543454038997214,
      "loss": 2.3901,
      "step": 16000
    },
    {
      "epoch": 4.459610027855153,
      "grad_norm": 1.4736210107803345,
      "learning_rate": 0.000554066852367688,
      "loss": 1.974,
      "step": 16010
    },
    {
      "epoch": 4.462395543175488,
      "grad_norm": 1.4385279417037964,
      "learning_rate": 0.0005537883008356546,
      "loss": 2.3464,
      "step": 16020
    },
    {
      "epoch": 4.465181058495822,
      "grad_norm": 1.4234726428985596,
      "learning_rate": 0.0005535097493036212,
      "loss": 2.1914,
      "step": 16030
    },
    {
      "epoch": 4.467966573816156,
      "grad_norm": 1.7274354696273804,
      "learning_rate": 0.0005532311977715877,
      "loss": 2.2217,
      "step": 16040
    },
    {
      "epoch": 4.47075208913649,
      "grad_norm": 1.8316495418548584,
      "learning_rate": 0.0005529526462395543,
      "loss": 2.4146,
      "step": 16050
    },
    {
      "epoch": 4.4735376044568245,
      "grad_norm": 1.3563557863235474,
      "learning_rate": 0.000552674094707521,
      "loss": 1.9233,
      "step": 16060
    },
    {
      "epoch": 4.476323119777159,
      "grad_norm": 2.0503318309783936,
      "learning_rate": 0.0005523955431754875,
      "loss": 2.167,
      "step": 16070
    },
    {
      "epoch": 4.479108635097493,
      "grad_norm": 1.2871688604354858,
      "learning_rate": 0.000552116991643454,
      "loss": 2.3641,
      "step": 16080
    },
    {
      "epoch": 4.481894150417827,
      "grad_norm": 1.4674335718154907,
      "learning_rate": 0.0005518384401114206,
      "loss": 2.3281,
      "step": 16090
    },
    {
      "epoch": 4.484679665738161,
      "grad_norm": 1.677626371383667,
      "learning_rate": 0.0005515598885793872,
      "loss": 2.176,
      "step": 16100
    },
    {
      "epoch": 4.487465181058496,
      "grad_norm": 1.5232881307601929,
      "learning_rate": 0.0005512813370473537,
      "loss": 2.5053,
      "step": 16110
    },
    {
      "epoch": 4.49025069637883,
      "grad_norm": 1.5353071689605713,
      "learning_rate": 0.0005510027855153203,
      "loss": 2.3494,
      "step": 16120
    },
    {
      "epoch": 4.493036211699164,
      "grad_norm": 0.9570493698120117,
      "learning_rate": 0.000550724233983287,
      "loss": 2.2281,
      "step": 16130
    },
    {
      "epoch": 4.495821727019498,
      "grad_norm": 1.9378842115402222,
      "learning_rate": 0.0005504456824512535,
      "loss": 2.0915,
      "step": 16140
    },
    {
      "epoch": 4.498607242339833,
      "grad_norm": 1.8367396593093872,
      "learning_rate": 0.0005501671309192201,
      "loss": 2.3151,
      "step": 16150
    },
    {
      "epoch": 4.501392757660167,
      "grad_norm": 1.721219778060913,
      "learning_rate": 0.0005498885793871867,
      "loss": 2.4098,
      "step": 16160
    },
    {
      "epoch": 4.504178272980502,
      "grad_norm": 2.3533411026000977,
      "learning_rate": 0.0005496100278551533,
      "loss": 2.2595,
      "step": 16170
    },
    {
      "epoch": 4.506963788300836,
      "grad_norm": 2.0327234268188477,
      "learning_rate": 0.0005493314763231197,
      "loss": 2.3873,
      "step": 16180
    },
    {
      "epoch": 4.50974930362117,
      "grad_norm": 1.717708945274353,
      "learning_rate": 0.0005490529247910863,
      "loss": 2.4781,
      "step": 16190
    },
    {
      "epoch": 4.512534818941504,
      "grad_norm": 1.2897508144378662,
      "learning_rate": 0.0005487743732590529,
      "loss": 2.2525,
      "step": 16200
    },
    {
      "epoch": 4.515320334261839,
      "grad_norm": 1.182836651802063,
      "learning_rate": 0.0005484958217270195,
      "loss": 2.1884,
      "step": 16210
    },
    {
      "epoch": 4.518105849582173,
      "grad_norm": 1.5115723609924316,
      "learning_rate": 0.0005482172701949861,
      "loss": 2.2027,
      "step": 16220
    },
    {
      "epoch": 4.520891364902507,
      "grad_norm": 1.522653579711914,
      "learning_rate": 0.0005479387186629526,
      "loss": 2.2816,
      "step": 16230
    },
    {
      "epoch": 4.523676880222841,
      "grad_norm": 1.4455091953277588,
      "learning_rate": 0.0005476601671309193,
      "loss": 2.3566,
      "step": 16240
    },
    {
      "epoch": 4.5264623955431755,
      "grad_norm": 2.5405189990997314,
      "learning_rate": 0.0005473816155988858,
      "loss": 2.2608,
      "step": 16250
    },
    {
      "epoch": 4.52924791086351,
      "grad_norm": 2.5624618530273438,
      "learning_rate": 0.0005471030640668524,
      "loss": 2.246,
      "step": 16260
    },
    {
      "epoch": 4.532033426183844,
      "grad_norm": 1.7104967832565308,
      "learning_rate": 0.000546824512534819,
      "loss": 2.0054,
      "step": 16270
    },
    {
      "epoch": 4.534818941504178,
      "grad_norm": 1.3018944263458252,
      "learning_rate": 0.0005465459610027855,
      "loss": 2.3026,
      "step": 16280
    },
    {
      "epoch": 4.537604456824512,
      "grad_norm": 1.4970920085906982,
      "learning_rate": 0.0005462674094707521,
      "loss": 2.1839,
      "step": 16290
    },
    {
      "epoch": 4.540389972144847,
      "grad_norm": 1.444639801979065,
      "learning_rate": 0.0005459888579387186,
      "loss": 2.3255,
      "step": 16300
    },
    {
      "epoch": 4.543175487465181,
      "grad_norm": 1.03452730178833,
      "learning_rate": 0.0005457103064066853,
      "loss": 2.2834,
      "step": 16310
    },
    {
      "epoch": 4.545961002785515,
      "grad_norm": 1.2085180282592773,
      "learning_rate": 0.0005454317548746518,
      "loss": 2.1844,
      "step": 16320
    },
    {
      "epoch": 4.548746518105849,
      "grad_norm": 1.5241613388061523,
      "learning_rate": 0.0005451532033426184,
      "loss": 2.1281,
      "step": 16330
    },
    {
      "epoch": 4.5515320334261835,
      "grad_norm": 1.4051626920700073,
      "learning_rate": 0.0005448746518105849,
      "loss": 2.1302,
      "step": 16340
    },
    {
      "epoch": 4.554317548746518,
      "grad_norm": 1.2081053256988525,
      "learning_rate": 0.0005445961002785516,
      "loss": 2.1567,
      "step": 16350
    },
    {
      "epoch": 4.557103064066853,
      "grad_norm": 1.7022186517715454,
      "learning_rate": 0.0005443175487465181,
      "loss": 2.3196,
      "step": 16360
    },
    {
      "epoch": 4.559888579387186,
      "grad_norm": 1.2409201860427856,
      "learning_rate": 0.0005440389972144847,
      "loss": 2.1326,
      "step": 16370
    },
    {
      "epoch": 4.562674094707521,
      "grad_norm": 1.333455204963684,
      "learning_rate": 0.0005437604456824514,
      "loss": 2.2235,
      "step": 16380
    },
    {
      "epoch": 4.5654596100278555,
      "grad_norm": 1.945243000984192,
      "learning_rate": 0.0005434818941504178,
      "loss": 2.22,
      "step": 16390
    },
    {
      "epoch": 4.56824512534819,
      "grad_norm": 1.8619798421859741,
      "learning_rate": 0.0005432033426183844,
      "loss": 2.2923,
      "step": 16400
    },
    {
      "epoch": 4.571030640668524,
      "grad_norm": 1.701438546180725,
      "learning_rate": 0.0005429247910863509,
      "loss": 2.2093,
      "step": 16410
    },
    {
      "epoch": 4.573816155988858,
      "grad_norm": 1.4026994705200195,
      "learning_rate": 0.0005426462395543176,
      "loss": 2.4895,
      "step": 16420
    },
    {
      "epoch": 4.576601671309192,
      "grad_norm": 1.3296853303909302,
      "learning_rate": 0.0005423676880222841,
      "loss": 2.2086,
      "step": 16430
    },
    {
      "epoch": 4.579387186629527,
      "grad_norm": 1.4088828563690186,
      "learning_rate": 0.0005420891364902507,
      "loss": 2.2125,
      "step": 16440
    },
    {
      "epoch": 4.582172701949861,
      "grad_norm": 1.5609772205352783,
      "learning_rate": 0.0005418105849582174,
      "loss": 2.3492,
      "step": 16450
    },
    {
      "epoch": 4.584958217270195,
      "grad_norm": 1.3184624910354614,
      "learning_rate": 0.0005415320334261839,
      "loss": 2.2701,
      "step": 16460
    },
    {
      "epoch": 4.587743732590529,
      "grad_norm": 1.6179651021957397,
      "learning_rate": 0.0005412534818941505,
      "loss": 2.3313,
      "step": 16470
    },
    {
      "epoch": 4.5905292479108635,
      "grad_norm": 1.4619897603988647,
      "learning_rate": 0.0005409749303621169,
      "loss": 2.25,
      "step": 16480
    },
    {
      "epoch": 4.593314763231198,
      "grad_norm": 1.4716017246246338,
      "learning_rate": 0.0005406963788300836,
      "loss": 2.3993,
      "step": 16490
    },
    {
      "epoch": 4.596100278551532,
      "grad_norm": 1.4016814231872559,
      "learning_rate": 0.0005404178272980501,
      "loss": 2.1413,
      "step": 16500
    },
    {
      "epoch": 4.598885793871866,
      "grad_norm": 1.1978555917739868,
      "learning_rate": 0.0005401392757660167,
      "loss": 2.1789,
      "step": 16510
    },
    {
      "epoch": 4.6016713091922,
      "grad_norm": 2.198603868484497,
      "learning_rate": 0.0005398607242339832,
      "loss": 2.3287,
      "step": 16520
    },
    {
      "epoch": 4.604456824512535,
      "grad_norm": 1.3651939630508423,
      "learning_rate": 0.0005395821727019499,
      "loss": 2.2317,
      "step": 16530
    },
    {
      "epoch": 4.607242339832869,
      "grad_norm": 1.75315523147583,
      "learning_rate": 0.0005393036211699165,
      "loss": 2.3916,
      "step": 16540
    },
    {
      "epoch": 4.610027855153203,
      "grad_norm": 1.6372971534729004,
      "learning_rate": 0.000539025069637883,
      "loss": 2.3581,
      "step": 16550
    },
    {
      "epoch": 4.612813370473537,
      "grad_norm": 1.954630970954895,
      "learning_rate": 0.0005387465181058497,
      "loss": 2.2965,
      "step": 16560
    },
    {
      "epoch": 4.615598885793872,
      "grad_norm": 1.3320279121398926,
      "learning_rate": 0.0005384679665738162,
      "loss": 2.2308,
      "step": 16570
    },
    {
      "epoch": 4.618384401114206,
      "grad_norm": 1.4013609886169434,
      "learning_rate": 0.0005381894150417827,
      "loss": 2.4211,
      "step": 16580
    },
    {
      "epoch": 4.621169916434541,
      "grad_norm": 1.3366938829421997,
      "learning_rate": 0.0005379108635097492,
      "loss": 2.279,
      "step": 16590
    },
    {
      "epoch": 4.623955431754875,
      "grad_norm": 1.841843605041504,
      "learning_rate": 0.0005376323119777159,
      "loss": 2.3248,
      "step": 16600
    },
    {
      "epoch": 4.626740947075209,
      "grad_norm": 1.7206388711929321,
      "learning_rate": 0.0005373537604456825,
      "loss": 2.114,
      "step": 16610
    },
    {
      "epoch": 4.629526462395543,
      "grad_norm": 1.4702303409576416,
      "learning_rate": 0.000537075208913649,
      "loss": 2.3748,
      "step": 16620
    },
    {
      "epoch": 4.632311977715878,
      "grad_norm": 1.5407755374908447,
      "learning_rate": 0.0005367966573816157,
      "loss": 2.2419,
      "step": 16630
    },
    {
      "epoch": 4.635097493036212,
      "grad_norm": 2.0379464626312256,
      "learning_rate": 0.0005365181058495822,
      "loss": 2.1141,
      "step": 16640
    },
    {
      "epoch": 4.637883008356546,
      "grad_norm": 1.5647960901260376,
      "learning_rate": 0.0005362395543175488,
      "loss": 2.3312,
      "step": 16650
    },
    {
      "epoch": 4.64066852367688,
      "grad_norm": 1.8776838779449463,
      "learning_rate": 0.0005359610027855153,
      "loss": 2.2479,
      "step": 16660
    },
    {
      "epoch": 4.6434540389972145,
      "grad_norm": 1.7570383548736572,
      "learning_rate": 0.000535682451253482,
      "loss": 2.2549,
      "step": 16670
    },
    {
      "epoch": 4.646239554317549,
      "grad_norm": 1.4998717308044434,
      "learning_rate": 0.0005354038997214484,
      "loss": 2.5228,
      "step": 16680
    },
    {
      "epoch": 4.649025069637883,
      "grad_norm": 1.3070337772369385,
      "learning_rate": 0.000535125348189415,
      "loss": 2.2412,
      "step": 16690
    },
    {
      "epoch": 4.651810584958217,
      "grad_norm": 2.5812504291534424,
      "learning_rate": 0.0005348467966573817,
      "loss": 2.2756,
      "step": 16700
    },
    {
      "epoch": 4.654596100278551,
      "grad_norm": 1.3985767364501953,
      "learning_rate": 0.0005345682451253482,
      "loss": 2.3976,
      "step": 16710
    },
    {
      "epoch": 4.657381615598886,
      "grad_norm": 1.3532713651657104,
      "learning_rate": 0.0005342896935933148,
      "loss": 2.3028,
      "step": 16720
    },
    {
      "epoch": 4.66016713091922,
      "grad_norm": 1.8780378103256226,
      "learning_rate": 0.0005340111420612813,
      "loss": 2.1614,
      "step": 16730
    },
    {
      "epoch": 4.662952646239554,
      "grad_norm": 2.7269656658172607,
      "learning_rate": 0.000533732590529248,
      "loss": 2.2936,
      "step": 16740
    },
    {
      "epoch": 4.665738161559888,
      "grad_norm": 1.8735227584838867,
      "learning_rate": 0.0005334540389972145,
      "loss": 2.2264,
      "step": 16750
    },
    {
      "epoch": 4.6685236768802225,
      "grad_norm": 1.8337252140045166,
      "learning_rate": 0.0005331754874651811,
      "loss": 2.341,
      "step": 16760
    },
    {
      "epoch": 4.671309192200557,
      "grad_norm": 1.3665982484817505,
      "learning_rate": 0.0005328969359331476,
      "loss": 2.3036,
      "step": 16770
    },
    {
      "epoch": 4.674094707520892,
      "grad_norm": 2.048271894454956,
      "learning_rate": 0.0005326183844011142,
      "loss": 2.2746,
      "step": 16780
    },
    {
      "epoch": 4.676880222841225,
      "grad_norm": 1.2199989557266235,
      "learning_rate": 0.0005323398328690808,
      "loss": 2.288,
      "step": 16790
    },
    {
      "epoch": 4.67966573816156,
      "grad_norm": 1.0948222875595093,
      "learning_rate": 0.0005320612813370473,
      "loss": 2.0421,
      "step": 16800
    },
    {
      "epoch": 4.6824512534818945,
      "grad_norm": 1.6799792051315308,
      "learning_rate": 0.000531782729805014,
      "loss": 2.2619,
      "step": 16810
    },
    {
      "epoch": 4.685236768802229,
      "grad_norm": 1.8817970752716064,
      "learning_rate": 0.0005315041782729805,
      "loss": 2.2321,
      "step": 16820
    },
    {
      "epoch": 4.688022284122563,
      "grad_norm": 1.4759167432785034,
      "learning_rate": 0.0005312256267409471,
      "loss": 2.2412,
      "step": 16830
    },
    {
      "epoch": 4.690807799442897,
      "grad_norm": 1.5111277103424072,
      "learning_rate": 0.0005309470752089136,
      "loss": 2.2588,
      "step": 16840
    },
    {
      "epoch": 4.693593314763231,
      "grad_norm": 1.8225572109222412,
      "learning_rate": 0.0005306685236768803,
      "loss": 2.3052,
      "step": 16850
    },
    {
      "epoch": 4.696378830083566,
      "grad_norm": 1.0332533121109009,
      "learning_rate": 0.0005303899721448469,
      "loss": 2.153,
      "step": 16860
    },
    {
      "epoch": 4.6991643454039,
      "grad_norm": 1.336027979850769,
      "learning_rate": 0.0005301114206128133,
      "loss": 2.4827,
      "step": 16870
    },
    {
      "epoch": 4.701949860724234,
      "grad_norm": 1.416619062423706,
      "learning_rate": 0.0005298328690807799,
      "loss": 2.1029,
      "step": 16880
    },
    {
      "epoch": 4.704735376044568,
      "grad_norm": 2.0473387241363525,
      "learning_rate": 0.0005295543175487465,
      "loss": 2.2028,
      "step": 16890
    },
    {
      "epoch": 4.7075208913649025,
      "grad_norm": 1.2255572080612183,
      "learning_rate": 0.0005292757660167131,
      "loss": 2.257,
      "step": 16900
    },
    {
      "epoch": 4.710306406685237,
      "grad_norm": 1.8140289783477783,
      "learning_rate": 0.0005289972144846796,
      "loss": 2.326,
      "step": 16910
    },
    {
      "epoch": 4.713091922005571,
      "grad_norm": 1.348267912864685,
      "learning_rate": 0.0005287186629526463,
      "loss": 2.4319,
      "step": 16920
    },
    {
      "epoch": 4.715877437325905,
      "grad_norm": 1.5026248693466187,
      "learning_rate": 0.0005284401114206129,
      "loss": 2.2627,
      "step": 16930
    },
    {
      "epoch": 4.718662952646239,
      "grad_norm": 1.4615098237991333,
      "learning_rate": 0.0005281615598885794,
      "loss": 2.3897,
      "step": 16940
    },
    {
      "epoch": 4.721448467966574,
      "grad_norm": 1.483906626701355,
      "learning_rate": 0.000527883008356546,
      "loss": 2.1591,
      "step": 16950
    },
    {
      "epoch": 4.724233983286908,
      "grad_norm": 1.9005173444747925,
      "learning_rate": 0.0005276044568245126,
      "loss": 2.2702,
      "step": 16960
    },
    {
      "epoch": 4.727019498607242,
      "grad_norm": 1.4476028680801392,
      "learning_rate": 0.0005273259052924791,
      "loss": 2.2932,
      "step": 16970
    },
    {
      "epoch": 4.729805013927576,
      "grad_norm": 1.040763258934021,
      "learning_rate": 0.0005270473537604456,
      "loss": 2.2305,
      "step": 16980
    },
    {
      "epoch": 4.732590529247911,
      "grad_norm": 1.4665640592575073,
      "learning_rate": 0.0005267688022284123,
      "loss": 2.3058,
      "step": 16990
    },
    {
      "epoch": 4.735376044568245,
      "grad_norm": 1.2189621925354004,
      "learning_rate": 0.0005264902506963788,
      "loss": 2.148,
      "step": 17000
    },
    {
      "epoch": 4.73816155988858,
      "grad_norm": 1.3853625059127808,
      "learning_rate": 0.0005262116991643454,
      "loss": 2.2731,
      "step": 17010
    },
    {
      "epoch": 4.740947075208914,
      "grad_norm": 1.2985515594482422,
      "learning_rate": 0.000525933147632312,
      "loss": 2.114,
      "step": 17020
    },
    {
      "epoch": 4.743732590529248,
      "grad_norm": 1.3046430349349976,
      "learning_rate": 0.0005256545961002786,
      "loss": 2.1827,
      "step": 17030
    },
    {
      "epoch": 4.7465181058495824,
      "grad_norm": 1.638580322265625,
      "learning_rate": 0.0005253760445682452,
      "loss": 2.4195,
      "step": 17040
    },
    {
      "epoch": 4.749303621169917,
      "grad_norm": 1.2639834880828857,
      "learning_rate": 0.0005250974930362117,
      "loss": 2.3786,
      "step": 17050
    },
    {
      "epoch": 4.752089136490251,
      "grad_norm": 1.3515698909759521,
      "learning_rate": 0.0005248189415041783,
      "loss": 2.4092,
      "step": 17060
    },
    {
      "epoch": 4.754874651810585,
      "grad_norm": 1.7933555841445923,
      "learning_rate": 0.0005245403899721448,
      "loss": 2.3816,
      "step": 17070
    },
    {
      "epoch": 4.757660167130919,
      "grad_norm": 1.3910198211669922,
      "learning_rate": 0.0005242618384401114,
      "loss": 2.1841,
      "step": 17080
    },
    {
      "epoch": 4.7604456824512535,
      "grad_norm": 1.7955135107040405,
      "learning_rate": 0.000523983286908078,
      "loss": 2.2751,
      "step": 17090
    },
    {
      "epoch": 4.763231197771588,
      "grad_norm": 1.8407762050628662,
      "learning_rate": 0.0005237047353760446,
      "loss": 2.3963,
      "step": 17100
    },
    {
      "epoch": 4.766016713091922,
      "grad_norm": 1.2253904342651367,
      "learning_rate": 0.0005234261838440112,
      "loss": 2.2435,
      "step": 17110
    },
    {
      "epoch": 4.768802228412256,
      "grad_norm": 1.4580764770507812,
      "learning_rate": 0.0005231476323119777,
      "loss": 2.1878,
      "step": 17120
    },
    {
      "epoch": 4.77158774373259,
      "grad_norm": 1.449989676475525,
      "learning_rate": 0.0005228690807799443,
      "loss": 2.457,
      "step": 17130
    },
    {
      "epoch": 4.774373259052925,
      "grad_norm": 1.4739845991134644,
      "learning_rate": 0.0005225905292479109,
      "loss": 2.3064,
      "step": 17140
    },
    {
      "epoch": 4.777158774373259,
      "grad_norm": 1.7628964185714722,
      "learning_rate": 0.0005223119777158775,
      "loss": 2.4463,
      "step": 17150
    },
    {
      "epoch": 4.779944289693593,
      "grad_norm": 1.2698633670806885,
      "learning_rate": 0.000522033426183844,
      "loss": 2.3662,
      "step": 17160
    },
    {
      "epoch": 4.782729805013927,
      "grad_norm": 1.352885365486145,
      "learning_rate": 0.0005217548746518106,
      "loss": 2.4723,
      "step": 17170
    },
    {
      "epoch": 4.7855153203342615,
      "grad_norm": 1.5390342473983765,
      "learning_rate": 0.0005214763231197772,
      "loss": 2.1204,
      "step": 17180
    },
    {
      "epoch": 4.788300835654596,
      "grad_norm": 1.666794776916504,
      "learning_rate": 0.0005211977715877437,
      "loss": 2.3796,
      "step": 17190
    },
    {
      "epoch": 4.791086350974931,
      "grad_norm": 1.0997958183288574,
      "learning_rate": 0.0005209192200557103,
      "loss": 2.121,
      "step": 17200
    },
    {
      "epoch": 4.793871866295264,
      "grad_norm": 1.796330451965332,
      "learning_rate": 0.0005206406685236769,
      "loss": 2.4642,
      "step": 17210
    },
    {
      "epoch": 4.796657381615599,
      "grad_norm": 2.1827971935272217,
      "learning_rate": 0.0005203621169916435,
      "loss": 2.3361,
      "step": 17220
    },
    {
      "epoch": 4.7994428969359335,
      "grad_norm": 1.3553181886672974,
      "learning_rate": 0.00052008356545961,
      "loss": 2.1862,
      "step": 17230
    },
    {
      "epoch": 4.802228412256268,
      "grad_norm": 1.4133481979370117,
      "learning_rate": 0.0005198050139275766,
      "loss": 2.2653,
      "step": 17240
    },
    {
      "epoch": 4.805013927576602,
      "grad_norm": 1.1172800064086914,
      "learning_rate": 0.0005195264623955432,
      "loss": 2.1207,
      "step": 17250
    },
    {
      "epoch": 4.807799442896936,
      "grad_norm": 1.2946281433105469,
      "learning_rate": 0.0005192479108635098,
      "loss": 2.3184,
      "step": 17260
    },
    {
      "epoch": 4.81058495821727,
      "grad_norm": 1.3063596487045288,
      "learning_rate": 0.0005189693593314763,
      "loss": 2.2875,
      "step": 17270
    },
    {
      "epoch": 4.813370473537605,
      "grad_norm": 1.1822731494903564,
      "learning_rate": 0.0005186908077994429,
      "loss": 2.1485,
      "step": 17280
    },
    {
      "epoch": 4.816155988857939,
      "grad_norm": 2.1314473152160645,
      "learning_rate": 0.0005184122562674095,
      "loss": 2.1571,
      "step": 17290
    },
    {
      "epoch": 4.818941504178273,
      "grad_norm": 1.4603991508483887,
      "learning_rate": 0.000518133704735376,
      "loss": 2.3031,
      "step": 17300
    },
    {
      "epoch": 4.821727019498607,
      "grad_norm": 1.8216513395309448,
      "learning_rate": 0.0005178551532033426,
      "loss": 2.3059,
      "step": 17310
    },
    {
      "epoch": 4.8245125348189415,
      "grad_norm": 1.41097891330719,
      "learning_rate": 0.0005175766016713092,
      "loss": 2.2795,
      "step": 17320
    },
    {
      "epoch": 4.827298050139276,
      "grad_norm": 1.719465732574463,
      "learning_rate": 0.0005172980501392758,
      "loss": 2.2604,
      "step": 17330
    },
    {
      "epoch": 4.83008356545961,
      "grad_norm": 1.9101277589797974,
      "learning_rate": 0.0005170194986072424,
      "loss": 2.3578,
      "step": 17340
    },
    {
      "epoch": 4.832869080779944,
      "grad_norm": 1.8667137622833252,
      "learning_rate": 0.000516740947075209,
      "loss": 2.1294,
      "step": 17350
    },
    {
      "epoch": 4.835654596100278,
      "grad_norm": 1.2866971492767334,
      "learning_rate": 0.0005164623955431756,
      "loss": 2.3761,
      "step": 17360
    },
    {
      "epoch": 4.838440111420613,
      "grad_norm": 1.715620517730713,
      "learning_rate": 0.000516183844011142,
      "loss": 2.3865,
      "step": 17370
    },
    {
      "epoch": 4.841225626740947,
      "grad_norm": 1.399402141571045,
      "learning_rate": 0.0005159052924791086,
      "loss": 2.3542,
      "step": 17380
    },
    {
      "epoch": 4.844011142061281,
      "grad_norm": 1.777377963066101,
      "learning_rate": 0.0005156267409470752,
      "loss": 2.3135,
      "step": 17390
    },
    {
      "epoch": 4.846796657381615,
      "grad_norm": 2.1167185306549072,
      "learning_rate": 0.0005153481894150418,
      "loss": 2.2952,
      "step": 17400
    },
    {
      "epoch": 4.84958217270195,
      "grad_norm": 1.294167399406433,
      "learning_rate": 0.0005150696378830083,
      "loss": 2.3352,
      "step": 17410
    },
    {
      "epoch": 4.852367688022284,
      "grad_norm": 1.950681209564209,
      "learning_rate": 0.0005147910863509749,
      "loss": 2.2456,
      "step": 17420
    },
    {
      "epoch": 4.855153203342619,
      "grad_norm": 1.3205703496932983,
      "learning_rate": 0.0005145125348189416,
      "loss": 2.2351,
      "step": 17430
    },
    {
      "epoch": 4.857938718662953,
      "grad_norm": 1.2208930253982544,
      "learning_rate": 0.0005142339832869081,
      "loss": 2.1062,
      "step": 17440
    },
    {
      "epoch": 4.860724233983287,
      "grad_norm": 1.4720431566238403,
      "learning_rate": 0.0005139554317548747,
      "loss": 2.2218,
      "step": 17450
    },
    {
      "epoch": 4.8635097493036215,
      "grad_norm": 1.7172777652740479,
      "learning_rate": 0.0005136768802228412,
      "loss": 2.2252,
      "step": 17460
    },
    {
      "epoch": 4.866295264623956,
      "grad_norm": 1.3702058792114258,
      "learning_rate": 0.0005133983286908078,
      "loss": 2.2829,
      "step": 17470
    },
    {
      "epoch": 4.86908077994429,
      "grad_norm": 1.3106170892715454,
      "learning_rate": 0.0005131197771587743,
      "loss": 2.1535,
      "step": 17480
    },
    {
      "epoch": 4.871866295264624,
      "grad_norm": 1.9590429067611694,
      "learning_rate": 0.0005128412256267409,
      "loss": 2.2553,
      "step": 17490
    },
    {
      "epoch": 4.874651810584958,
      "grad_norm": 1.4740890264511108,
      "learning_rate": 0.0005125626740947076,
      "loss": 2.3416,
      "step": 17500
    },
    {
      "epoch": 4.8774373259052926,
      "grad_norm": 1.7594777345657349,
      "learning_rate": 0.0005122841225626741,
      "loss": 2.2789,
      "step": 17510
    },
    {
      "epoch": 4.880222841225627,
      "grad_norm": 1.1775588989257812,
      "learning_rate": 0.0005120055710306407,
      "loss": 2.1648,
      "step": 17520
    },
    {
      "epoch": 4.883008356545961,
      "grad_norm": 1.4620171785354614,
      "learning_rate": 0.0005117270194986073,
      "loss": 2.3747,
      "step": 17530
    },
    {
      "epoch": 4.885793871866295,
      "grad_norm": 1.2768298387527466,
      "learning_rate": 0.0005114484679665739,
      "loss": 2.3492,
      "step": 17540
    },
    {
      "epoch": 4.888579387186629,
      "grad_norm": 1.881622314453125,
      "learning_rate": 0.0005111699164345404,
      "loss": 2.1671,
      "step": 17550
    },
    {
      "epoch": 4.891364902506964,
      "grad_norm": 1.6050128936767578,
      "learning_rate": 0.0005108913649025069,
      "loss": 2.2996,
      "step": 17560
    },
    {
      "epoch": 4.894150417827298,
      "grad_norm": 1.6289857625961304,
      "learning_rate": 0.0005106128133704735,
      "loss": 2.2758,
      "step": 17570
    },
    {
      "epoch": 4.896935933147632,
      "grad_norm": 2.243773937225342,
      "learning_rate": 0.0005103342618384401,
      "loss": 2.4533,
      "step": 17580
    },
    {
      "epoch": 4.899721448467966,
      "grad_norm": 1.4050899744033813,
      "learning_rate": 0.0005100557103064067,
      "loss": 2.3515,
      "step": 17590
    },
    {
      "epoch": 4.9025069637883005,
      "grad_norm": 1.5016822814941406,
      "learning_rate": 0.0005097771587743732,
      "loss": 2.3613,
      "step": 17600
    },
    {
      "epoch": 4.905292479108635,
      "grad_norm": 1.654454231262207,
      "learning_rate": 0.0005094986072423399,
      "loss": 2.3449,
      "step": 17610
    },
    {
      "epoch": 4.908077994428969,
      "grad_norm": 1.4811958074569702,
      "learning_rate": 0.0005092200557103064,
      "loss": 2.3086,
      "step": 17620
    },
    {
      "epoch": 4.910863509749303,
      "grad_norm": 1.3663369417190552,
      "learning_rate": 0.000508941504178273,
      "loss": 2.278,
      "step": 17630
    },
    {
      "epoch": 4.913649025069638,
      "grad_norm": 1.6025705337524414,
      "learning_rate": 0.0005086629526462396,
      "loss": 2.161,
      "step": 17640
    },
    {
      "epoch": 4.9164345403899725,
      "grad_norm": 1.6350971460342407,
      "learning_rate": 0.0005083844011142062,
      "loss": 2.2793,
      "step": 17650
    },
    {
      "epoch": 4.919220055710307,
      "grad_norm": 1.8437550067901611,
      "learning_rate": 0.0005081058495821727,
      "loss": 2.2854,
      "step": 17660
    },
    {
      "epoch": 4.922005571030641,
      "grad_norm": 1.42860746383667,
      "learning_rate": 0.0005078272980501392,
      "loss": 2.1316,
      "step": 17670
    },
    {
      "epoch": 4.924791086350975,
      "grad_norm": 1.580441951751709,
      "learning_rate": 0.0005075487465181059,
      "loss": 2.1972,
      "step": 17680
    },
    {
      "epoch": 4.927576601671309,
      "grad_norm": 1.9858804941177368,
      "learning_rate": 0.0005072701949860724,
      "loss": 2.248,
      "step": 17690
    },
    {
      "epoch": 4.930362116991644,
      "grad_norm": 2.092789888381958,
      "learning_rate": 0.000506991643454039,
      "loss": 2.3166,
      "step": 17700
    },
    {
      "epoch": 4.933147632311978,
      "grad_norm": 1.5661741495132446,
      "learning_rate": 0.0005067130919220056,
      "loss": 2.3853,
      "step": 17710
    },
    {
      "epoch": 4.935933147632312,
      "grad_norm": 1.2403414249420166,
      "learning_rate": 0.0005064345403899722,
      "loss": 2.1951,
      "step": 17720
    },
    {
      "epoch": 4.938718662952646,
      "grad_norm": 1.3715742826461792,
      "learning_rate": 0.0005061559888579387,
      "loss": 2.2082,
      "step": 17730
    },
    {
      "epoch": 4.9415041782729805,
      "grad_norm": 1.4738293886184692,
      "learning_rate": 0.0005058774373259053,
      "loss": 2.1366,
      "step": 17740
    },
    {
      "epoch": 4.944289693593315,
      "grad_norm": 1.605117678642273,
      "learning_rate": 0.000505598885793872,
      "loss": 2.1871,
      "step": 17750
    },
    {
      "epoch": 4.947075208913649,
      "grad_norm": 1.6356253623962402,
      "learning_rate": 0.0005053203342618384,
      "loss": 2.3342,
      "step": 17760
    },
    {
      "epoch": 4.949860724233983,
      "grad_norm": 1.3043177127838135,
      "learning_rate": 0.000505041782729805,
      "loss": 2.1471,
      "step": 17770
    },
    {
      "epoch": 4.952646239554317,
      "grad_norm": 1.2387511730194092,
      "learning_rate": 0.0005047632311977715,
      "loss": 2.2667,
      "step": 17780
    },
    {
      "epoch": 4.955431754874652,
      "grad_norm": 1.8818614482879639,
      "learning_rate": 0.0005044846796657382,
      "loss": 2.3009,
      "step": 17790
    },
    {
      "epoch": 4.958217270194986,
      "grad_norm": 1.3252300024032593,
      "learning_rate": 0.0005042061281337047,
      "loss": 2.3613,
      "step": 17800
    },
    {
      "epoch": 4.96100278551532,
      "grad_norm": 2.7484054565429688,
      "learning_rate": 0.0005039275766016713,
      "loss": 2.3153,
      "step": 17810
    },
    {
      "epoch": 4.963788300835654,
      "grad_norm": 1.6434224843978882,
      "learning_rate": 0.000503649025069638,
      "loss": 2.1881,
      "step": 17820
    },
    {
      "epoch": 4.9665738161559885,
      "grad_norm": 1.8651015758514404,
      "learning_rate": 0.0005033704735376045,
      "loss": 2.2586,
      "step": 17830
    },
    {
      "epoch": 4.969359331476323,
      "grad_norm": 1.318866491317749,
      "learning_rate": 0.0005030919220055711,
      "loss": 2.397,
      "step": 17840
    },
    {
      "epoch": 4.972144846796658,
      "grad_norm": 2.140472412109375,
      "learning_rate": 0.0005028133704735375,
      "loss": 2.4226,
      "step": 17850
    },
    {
      "epoch": 4.974930362116992,
      "grad_norm": 1.5910037755966187,
      "learning_rate": 0.0005025348189415042,
      "loss": 2.3367,
      "step": 17860
    },
    {
      "epoch": 4.977715877437326,
      "grad_norm": 1.3083618879318237,
      "learning_rate": 0.0005022562674094707,
      "loss": 2.2089,
      "step": 17870
    },
    {
      "epoch": 4.9805013927576605,
      "grad_norm": 1.5178142786026,
      "learning_rate": 0.0005019777158774373,
      "loss": 2.3071,
      "step": 17880
    },
    {
      "epoch": 4.983286908077995,
      "grad_norm": 2.1697492599487305,
      "learning_rate": 0.0005016991643454038,
      "loss": 2.5551,
      "step": 17890
    },
    {
      "epoch": 4.986072423398329,
      "grad_norm": 1.2968639135360718,
      "learning_rate": 0.0005014206128133705,
      "loss": 2.3113,
      "step": 17900
    },
    {
      "epoch": 4.988857938718663,
      "grad_norm": 1.0567294359207153,
      "learning_rate": 0.0005011420612813371,
      "loss": 2.1813,
      "step": 17910
    },
    {
      "epoch": 4.991643454038997,
      "grad_norm": 1.3325912952423096,
      "learning_rate": 0.0005008635097493036,
      "loss": 2.1207,
      "step": 17920
    },
    {
      "epoch": 4.994428969359332,
      "grad_norm": 1.4998035430908203,
      "learning_rate": 0.0005005849582172703,
      "loss": 2.2615,
      "step": 17930
    },
    {
      "epoch": 4.997214484679666,
      "grad_norm": 1.7247203588485718,
      "learning_rate": 0.0005003064066852368,
      "loss": 2.1269,
      "step": 17940
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.9343196153640747,
      "learning_rate": 0.0005000278551532034,
      "loss": 2.3174,
      "step": 17950
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.298664093017578,
      "eval_runtime": 5.9753,
      "eval_samples_per_second": 534.199,
      "eval_steps_per_second": 66.775,
      "step": 17950
    },
    {
      "epoch": 5.002785515320334,
      "grad_norm": 1.5156561136245728,
      "learning_rate": 0.0004997493036211699,
      "loss": 2.1331,
      "step": 17960
    },
    {
      "epoch": 5.005571030640668,
      "grad_norm": 1.573203206062317,
      "learning_rate": 0.0004994707520891365,
      "loss": 2.0965,
      "step": 17970
    },
    {
      "epoch": 5.008356545961003,
      "grad_norm": 1.6331983804702759,
      "learning_rate": 0.0004991922005571031,
      "loss": 2.2442,
      "step": 17980
    },
    {
      "epoch": 5.011142061281337,
      "grad_norm": 1.1784179210662842,
      "learning_rate": 0.0004989136490250696,
      "loss": 2.1582,
      "step": 17990
    },
    {
      "epoch": 5.013927576601671,
      "grad_norm": 2.1019675731658936,
      "learning_rate": 0.0004986350974930362,
      "loss": 2.1012,
      "step": 18000
    },
    {
      "epoch": 5.016713091922005,
      "grad_norm": 1.7725952863693237,
      "learning_rate": 0.0004983565459610028,
      "loss": 2.3199,
      "step": 18010
    },
    {
      "epoch": 5.0194986072423395,
      "grad_norm": 2.4285576343536377,
      "learning_rate": 0.0004980779944289694,
      "loss": 2.1411,
      "step": 18020
    },
    {
      "epoch": 5.022284122562674,
      "grad_norm": 2.528287410736084,
      "learning_rate": 0.000497799442896936,
      "loss": 2.2577,
      "step": 18030
    },
    {
      "epoch": 5.025069637883008,
      "grad_norm": 1.1760447025299072,
      "learning_rate": 0.0004975208913649026,
      "loss": 1.9511,
      "step": 18040
    },
    {
      "epoch": 5.027855153203342,
      "grad_norm": 1.7976702451705933,
      "learning_rate": 0.000497242339832869,
      "loss": 2.2568,
      "step": 18050
    },
    {
      "epoch": 5.030640668523677,
      "grad_norm": 1.7306040525436401,
      "learning_rate": 0.0004969637883008356,
      "loss": 2.4006,
      "step": 18060
    },
    {
      "epoch": 5.0334261838440115,
      "grad_norm": 1.7712208032608032,
      "learning_rate": 0.0004966852367688022,
      "loss": 2.1731,
      "step": 18070
    },
    {
      "epoch": 5.036211699164346,
      "grad_norm": 1.6307333707809448,
      "learning_rate": 0.0004964066852367688,
      "loss": 2.1118,
      "step": 18080
    },
    {
      "epoch": 5.03899721448468,
      "grad_norm": 1.8322936296463013,
      "learning_rate": 0.0004961281337047354,
      "loss": 2.1971,
      "step": 18090
    },
    {
      "epoch": 5.041782729805014,
      "grad_norm": 1.3193928003311157,
      "learning_rate": 0.000495849582172702,
      "loss": 2.1722,
      "step": 18100
    },
    {
      "epoch": 5.044568245125348,
      "grad_norm": 1.7683979272842407,
      "learning_rate": 0.0004955710306406686,
      "loss": 2.2107,
      "step": 18110
    },
    {
      "epoch": 5.047353760445683,
      "grad_norm": 1.325799822807312,
      "learning_rate": 0.0004952924791086351,
      "loss": 2.1265,
      "step": 18120
    },
    {
      "epoch": 5.050139275766017,
      "grad_norm": 1.5468837022781372,
      "learning_rate": 0.0004950139275766017,
      "loss": 2.3711,
      "step": 18130
    },
    {
      "epoch": 5.052924791086351,
      "grad_norm": 3.332176446914673,
      "learning_rate": 0.0004947353760445683,
      "loss": 2.2237,
      "step": 18140
    },
    {
      "epoch": 5.055710306406685,
      "grad_norm": 1.9390535354614258,
      "learning_rate": 0.0004944568245125349,
      "loss": 2.3024,
      "step": 18150
    },
    {
      "epoch": 5.0584958217270195,
      "grad_norm": 1.3628212213516235,
      "learning_rate": 0.0004941782729805013,
      "loss": 2.1789,
      "step": 18160
    },
    {
      "epoch": 5.061281337047354,
      "grad_norm": 1.9089230298995972,
      "learning_rate": 0.0004938997214484679,
      "loss": 2.3609,
      "step": 18170
    },
    {
      "epoch": 5.064066852367688,
      "grad_norm": 1.307629108428955,
      "learning_rate": 0.0004936211699164346,
      "loss": 2.1313,
      "step": 18180
    },
    {
      "epoch": 5.066852367688022,
      "grad_norm": 1.7855916023254395,
      "learning_rate": 0.0004933426183844011,
      "loss": 2.1859,
      "step": 18190
    },
    {
      "epoch": 5.069637883008356,
      "grad_norm": 1.993854284286499,
      "learning_rate": 0.0004930640668523677,
      "loss": 2.4884,
      "step": 18200
    },
    {
      "epoch": 5.072423398328691,
      "grad_norm": 1.8335723876953125,
      "learning_rate": 0.0004927855153203343,
      "loss": 2.2527,
      "step": 18210
    },
    {
      "epoch": 5.075208913649025,
      "grad_norm": 1.6089966297149658,
      "learning_rate": 0.0004925069637883009,
      "loss": 2.283,
      "step": 18220
    },
    {
      "epoch": 5.077994428969359,
      "grad_norm": 1.4781243801116943,
      "learning_rate": 0.0004922284122562674,
      "loss": 2.1142,
      "step": 18230
    },
    {
      "epoch": 5.080779944289693,
      "grad_norm": 1.864298701286316,
      "learning_rate": 0.000491949860724234,
      "loss": 2.2165,
      "step": 18240
    },
    {
      "epoch": 5.0835654596100275,
      "grad_norm": 1.8931764364242554,
      "learning_rate": 0.0004916713091922005,
      "loss": 2.1309,
      "step": 18250
    },
    {
      "epoch": 5.086350974930362,
      "grad_norm": 1.3541245460510254,
      "learning_rate": 0.0004913927576601671,
      "loss": 2.0146,
      "step": 18260
    },
    {
      "epoch": 5.089136490250697,
      "grad_norm": 1.3111774921417236,
      "learning_rate": 0.0004911142061281337,
      "loss": 2.1027,
      "step": 18270
    },
    {
      "epoch": 5.091922005571031,
      "grad_norm": 1.926859974861145,
      "learning_rate": 0.0004908356545961003,
      "loss": 2.34,
      "step": 18280
    },
    {
      "epoch": 5.094707520891365,
      "grad_norm": 1.2850139141082764,
      "learning_rate": 0.0004905571030640669,
      "loss": 2.0577,
      "step": 18290
    },
    {
      "epoch": 5.0974930362116995,
      "grad_norm": 0.9911344647407532,
      "learning_rate": 0.0004902785515320334,
      "loss": 2.092,
      "step": 18300
    },
    {
      "epoch": 5.100278551532034,
      "grad_norm": 1.6143571138381958,
      "learning_rate": 0.00049,
      "loss": 2.2987,
      "step": 18310
    },
    {
      "epoch": 5.103064066852368,
      "grad_norm": 1.3757697343826294,
      "learning_rate": 0.0004897214484679666,
      "loss": 2.2717,
      "step": 18320
    },
    {
      "epoch": 5.105849582172702,
      "grad_norm": 1.6560955047607422,
      "learning_rate": 0.0004894428969359332,
      "loss": 2.1764,
      "step": 18330
    },
    {
      "epoch": 5.108635097493036,
      "grad_norm": 2.6715524196624756,
      "learning_rate": 0.0004891643454038998,
      "loss": 2.041,
      "step": 18340
    },
    {
      "epoch": 5.111420612813371,
      "grad_norm": 1.3334238529205322,
      "learning_rate": 0.0004888857938718663,
      "loss": 2.2968,
      "step": 18350
    },
    {
      "epoch": 5.114206128133705,
      "grad_norm": 1.6724234819412231,
      "learning_rate": 0.0004886072423398329,
      "loss": 2.1917,
      "step": 18360
    },
    {
      "epoch": 5.116991643454039,
      "grad_norm": 1.716543197631836,
      "learning_rate": 0.0004883286908077994,
      "loss": 2.1573,
      "step": 18370
    },
    {
      "epoch": 5.119777158774373,
      "grad_norm": 2.2427945137023926,
      "learning_rate": 0.000488050139275766,
      "loss": 2.2626,
      "step": 18380
    },
    {
      "epoch": 5.1225626740947074,
      "grad_norm": 1.5039277076721191,
      "learning_rate": 0.0004877715877437326,
      "loss": 2.2083,
      "step": 18390
    },
    {
      "epoch": 5.125348189415042,
      "grad_norm": 1.7280160188674927,
      "learning_rate": 0.00048749303621169914,
      "loss": 2.2791,
      "step": 18400
    },
    {
      "epoch": 5.128133704735376,
      "grad_norm": 1.5270507335662842,
      "learning_rate": 0.00048721448467966573,
      "loss": 2.2275,
      "step": 18410
    },
    {
      "epoch": 5.13091922005571,
      "grad_norm": 1.6570667028427124,
      "learning_rate": 0.0004869359331476323,
      "loss": 2.2243,
      "step": 18420
    },
    {
      "epoch": 5.133704735376044,
      "grad_norm": 1.916176438331604,
      "learning_rate": 0.0004866573816155989,
      "loss": 2.1574,
      "step": 18430
    },
    {
      "epoch": 5.1364902506963785,
      "grad_norm": 1.7683693170547485,
      "learning_rate": 0.0004863788300835655,
      "loss": 2.1284,
      "step": 18440
    },
    {
      "epoch": 5.139275766016713,
      "grad_norm": 1.7889628410339355,
      "learning_rate": 0.00048610027855153204,
      "loss": 2.439,
      "step": 18450
    },
    {
      "epoch": 5.142061281337047,
      "grad_norm": 1.6670876741409302,
      "learning_rate": 0.00048582172701949864,
      "loss": 2.2312,
      "step": 18460
    },
    {
      "epoch": 5.144846796657381,
      "grad_norm": 1.5793068408966064,
      "learning_rate": 0.00048554317548746517,
      "loss": 2.1644,
      "step": 18470
    },
    {
      "epoch": 5.147632311977716,
      "grad_norm": 1.9663872718811035,
      "learning_rate": 0.00048526462395543176,
      "loss": 2.1417,
      "step": 18480
    },
    {
      "epoch": 5.1504178272980505,
      "grad_norm": 1.4817262887954712,
      "learning_rate": 0.0004849860724233983,
      "loss": 2.1092,
      "step": 18490
    },
    {
      "epoch": 5.153203342618385,
      "grad_norm": 1.4700850248336792,
      "learning_rate": 0.00048470752089136495,
      "loss": 2.13,
      "step": 18500
    },
    {
      "epoch": 5.155988857938719,
      "grad_norm": 1.572393536567688,
      "learning_rate": 0.0004844289693593315,
      "loss": 2.1686,
      "step": 18510
    },
    {
      "epoch": 5.158774373259053,
      "grad_norm": 1.4896461963653564,
      "learning_rate": 0.0004841504178272981,
      "loss": 2.2742,
      "step": 18520
    },
    {
      "epoch": 5.161559888579387,
      "grad_norm": 1.408266305923462,
      "learning_rate": 0.00048387186629526467,
      "loss": 2.1926,
      "step": 18530
    },
    {
      "epoch": 5.164345403899722,
      "grad_norm": 1.6808210611343384,
      "learning_rate": 0.0004835933147632312,
      "loss": 2.3298,
      "step": 18540
    },
    {
      "epoch": 5.167130919220056,
      "grad_norm": 3.3804519176483154,
      "learning_rate": 0.0004833147632311978,
      "loss": 2.1957,
      "step": 18550
    },
    {
      "epoch": 5.16991643454039,
      "grad_norm": 1.506984829902649,
      "learning_rate": 0.00048303621169916433,
      "loss": 2.2128,
      "step": 18560
    },
    {
      "epoch": 5.172701949860724,
      "grad_norm": 1.4756970405578613,
      "learning_rate": 0.0004827576601671309,
      "loss": 2.1905,
      "step": 18570
    },
    {
      "epoch": 5.1754874651810585,
      "grad_norm": 1.5185104608535767,
      "learning_rate": 0.0004824791086350975,
      "loss": 2.1441,
      "step": 18580
    },
    {
      "epoch": 5.178272980501393,
      "grad_norm": 1.3005750179290771,
      "learning_rate": 0.0004822005571030641,
      "loss": 2.2708,
      "step": 18590
    },
    {
      "epoch": 5.181058495821727,
      "grad_norm": 1.1196666955947876,
      "learning_rate": 0.00048192200557103064,
      "loss": 2.1764,
      "step": 18600
    },
    {
      "epoch": 5.183844011142061,
      "grad_norm": 3.4468648433685303,
      "learning_rate": 0.00048164345403899723,
      "loss": 2.2751,
      "step": 18610
    },
    {
      "epoch": 5.186629526462395,
      "grad_norm": 1.6884629726409912,
      "learning_rate": 0.0004813649025069638,
      "loss": 2.356,
      "step": 18620
    },
    {
      "epoch": 5.18941504178273,
      "grad_norm": 1.398298978805542,
      "learning_rate": 0.00048108635097493036,
      "loss": 2.0021,
      "step": 18630
    },
    {
      "epoch": 5.192200557103064,
      "grad_norm": 1.6005494594573975,
      "learning_rate": 0.00048080779944289695,
      "loss": 2.0283,
      "step": 18640
    },
    {
      "epoch": 5.194986072423398,
      "grad_norm": 1.7812445163726807,
      "learning_rate": 0.0004805292479108635,
      "loss": 2.3342,
      "step": 18650
    },
    {
      "epoch": 5.197771587743732,
      "grad_norm": 1.6310611963272095,
      "learning_rate": 0.00048025069637883013,
      "loss": 2.0573,
      "step": 18660
    },
    {
      "epoch": 5.2005571030640665,
      "grad_norm": 2.2848150730133057,
      "learning_rate": 0.00047997214484679667,
      "loss": 2.2004,
      "step": 18670
    },
    {
      "epoch": 5.203342618384401,
      "grad_norm": 1.507892370223999,
      "learning_rate": 0.00047969359331476326,
      "loss": 2.2722,
      "step": 18680
    },
    {
      "epoch": 5.206128133704736,
      "grad_norm": 1.456809401512146,
      "learning_rate": 0.0004794150417827298,
      "loss": 2.3745,
      "step": 18690
    },
    {
      "epoch": 5.20891364902507,
      "grad_norm": 1.6179757118225098,
      "learning_rate": 0.0004791364902506964,
      "loss": 2.334,
      "step": 18700
    },
    {
      "epoch": 5.211699164345404,
      "grad_norm": 1.4820024967193604,
      "learning_rate": 0.000478857938718663,
      "loss": 2.2642,
      "step": 18710
    },
    {
      "epoch": 5.2144846796657385,
      "grad_norm": 1.824309229850769,
      "learning_rate": 0.0004785793871866295,
      "loss": 2.1943,
      "step": 18720
    },
    {
      "epoch": 5.217270194986073,
      "grad_norm": 1.4417446851730347,
      "learning_rate": 0.0004783008356545961,
      "loss": 2.2233,
      "step": 18730
    },
    {
      "epoch": 5.220055710306407,
      "grad_norm": 1.5623376369476318,
      "learning_rate": 0.0004780222841225627,
      "loss": 2.14,
      "step": 18740
    },
    {
      "epoch": 5.222841225626741,
      "grad_norm": 1.5317449569702148,
      "learning_rate": 0.0004777437325905293,
      "loss": 2.1456,
      "step": 18750
    },
    {
      "epoch": 5.225626740947075,
      "grad_norm": 2.2741751670837402,
      "learning_rate": 0.00047746518105849583,
      "loss": 2.3465,
      "step": 18760
    },
    {
      "epoch": 5.22841225626741,
      "grad_norm": 1.6653279066085815,
      "learning_rate": 0.0004771866295264624,
      "loss": 2.2477,
      "step": 18770
    },
    {
      "epoch": 5.231197771587744,
      "grad_norm": 1.377361536026001,
      "learning_rate": 0.00047690807799442896,
      "loss": 2.363,
      "step": 18780
    },
    {
      "epoch": 5.233983286908078,
      "grad_norm": 1.969075083732605,
      "learning_rate": 0.00047662952646239555,
      "loss": 2.3972,
      "step": 18790
    },
    {
      "epoch": 5.236768802228412,
      "grad_norm": 1.213692545890808,
      "learning_rate": 0.00047635097493036214,
      "loss": 2.1579,
      "step": 18800
    },
    {
      "epoch": 5.2395543175487465,
      "grad_norm": 2.15010666847229,
      "learning_rate": 0.0004760724233983287,
      "loss": 2.314,
      "step": 18810
    },
    {
      "epoch": 5.242339832869081,
      "grad_norm": 1.4180222749710083,
      "learning_rate": 0.00047579387186629527,
      "loss": 2.2245,
      "step": 18820
    },
    {
      "epoch": 5.245125348189415,
      "grad_norm": 1.6843900680541992,
      "learning_rate": 0.00047551532033426186,
      "loss": 2.2729,
      "step": 18830
    },
    {
      "epoch": 5.247910863509749,
      "grad_norm": 1.3700318336486816,
      "learning_rate": 0.00047523676880222845,
      "loss": 2.2395,
      "step": 18840
    },
    {
      "epoch": 5.250696378830083,
      "grad_norm": 2.035118341445923,
      "learning_rate": 0.000474958217270195,
      "loss": 2.1457,
      "step": 18850
    },
    {
      "epoch": 5.2534818941504176,
      "grad_norm": 1.5480608940124512,
      "learning_rate": 0.0004746796657381616,
      "loss": 2.2896,
      "step": 18860
    },
    {
      "epoch": 5.256267409470752,
      "grad_norm": 2.5046558380126953,
      "learning_rate": 0.0004744011142061281,
      "loss": 2.2678,
      "step": 18870
    },
    {
      "epoch": 5.259052924791086,
      "grad_norm": 1.495804786682129,
      "learning_rate": 0.0004741225626740947,
      "loss": 2.2386,
      "step": 18880
    },
    {
      "epoch": 5.26183844011142,
      "grad_norm": 1.6592559814453125,
      "learning_rate": 0.00047384401114206124,
      "loss": 2.302,
      "step": 18890
    },
    {
      "epoch": 5.264623955431755,
      "grad_norm": 1.4584788084030151,
      "learning_rate": 0.00047356545961002784,
      "loss": 2.1572,
      "step": 18900
    },
    {
      "epoch": 5.2674094707520895,
      "grad_norm": 1.2544790506362915,
      "learning_rate": 0.0004732869080779945,
      "loss": 2.182,
      "step": 18910
    },
    {
      "epoch": 5.270194986072424,
      "grad_norm": 1.7203224897384644,
      "learning_rate": 0.000473008356545961,
      "loss": 2.0528,
      "step": 18920
    },
    {
      "epoch": 5.272980501392758,
      "grad_norm": 1.2359672784805298,
      "learning_rate": 0.0004727298050139276,
      "loss": 2.124,
      "step": 18930
    },
    {
      "epoch": 5.275766016713092,
      "grad_norm": 1.6507518291473389,
      "learning_rate": 0.00047245125348189415,
      "loss": 2.3113,
      "step": 18940
    },
    {
      "epoch": 5.278551532033426,
      "grad_norm": 1.8767732381820679,
      "learning_rate": 0.00047217270194986074,
      "loss": 2.129,
      "step": 18950
    },
    {
      "epoch": 5.281337047353761,
      "grad_norm": 1.8185663223266602,
      "learning_rate": 0.0004718941504178273,
      "loss": 2.1364,
      "step": 18960
    },
    {
      "epoch": 5.284122562674095,
      "grad_norm": 1.704988718032837,
      "learning_rate": 0.00047161559888579387,
      "loss": 2.3581,
      "step": 18970
    },
    {
      "epoch": 5.286908077994429,
      "grad_norm": 1.1494865417480469,
      "learning_rate": 0.0004713370473537604,
      "loss": 2.2932,
      "step": 18980
    },
    {
      "epoch": 5.289693593314763,
      "grad_norm": 1.9106636047363281,
      "learning_rate": 0.00047105849582172705,
      "loss": 2.2203,
      "step": 18990
    },
    {
      "epoch": 5.2924791086350975,
      "grad_norm": 1.4974727630615234,
      "learning_rate": 0.00047077994428969364,
      "loss": 2.3976,
      "step": 19000
    },
    {
      "epoch": 5.295264623955432,
      "grad_norm": 1.2751277685165405,
      "learning_rate": 0.0004705013927576602,
      "loss": 2.3446,
      "step": 19010
    },
    {
      "epoch": 5.298050139275766,
      "grad_norm": 1.2226650714874268,
      "learning_rate": 0.00047022284122562677,
      "loss": 2.2601,
      "step": 19020
    },
    {
      "epoch": 5.3008356545961,
      "grad_norm": 1.157899260520935,
      "learning_rate": 0.0004699442896935933,
      "loss": 2.265,
      "step": 19030
    },
    {
      "epoch": 5.303621169916434,
      "grad_norm": 1.6336333751678467,
      "learning_rate": 0.0004696657381615599,
      "loss": 2.3095,
      "step": 19040
    },
    {
      "epoch": 5.306406685236769,
      "grad_norm": 1.1876083612442017,
      "learning_rate": 0.00046938718662952643,
      "loss": 2.0347,
      "step": 19050
    },
    {
      "epoch": 5.309192200557103,
      "grad_norm": 1.7084386348724365,
      "learning_rate": 0.000469108635097493,
      "loss": 2.1515,
      "step": 19060
    },
    {
      "epoch": 5.311977715877437,
      "grad_norm": 1.4367051124572754,
      "learning_rate": 0.00046883008356545967,
      "loss": 2.2358,
      "step": 19070
    },
    {
      "epoch": 5.314763231197771,
      "grad_norm": 1.4803001880645752,
      "learning_rate": 0.0004685515320334262,
      "loss": 2.4083,
      "step": 19080
    },
    {
      "epoch": 5.3175487465181055,
      "grad_norm": 1.3896044492721558,
      "learning_rate": 0.0004682729805013928,
      "loss": 2.256,
      "step": 19090
    },
    {
      "epoch": 5.32033426183844,
      "grad_norm": 1.4954841136932373,
      "learning_rate": 0.00046799442896935933,
      "loss": 2.1519,
      "step": 19100
    },
    {
      "epoch": 5.323119777158775,
      "grad_norm": 1.8002430200576782,
      "learning_rate": 0.0004677158774373259,
      "loss": 2.4419,
      "step": 19110
    },
    {
      "epoch": 5.325905292479109,
      "grad_norm": 1.3823015689849854,
      "learning_rate": 0.00046743732590529246,
      "loss": 2.2029,
      "step": 19120
    },
    {
      "epoch": 5.328690807799443,
      "grad_norm": 1.546189546585083,
      "learning_rate": 0.00046715877437325905,
      "loss": 2.373,
      "step": 19130
    },
    {
      "epoch": 5.3314763231197775,
      "grad_norm": 1.586955189704895,
      "learning_rate": 0.0004668802228412256,
      "loss": 2.2386,
      "step": 19140
    },
    {
      "epoch": 5.334261838440112,
      "grad_norm": 1.665737271308899,
      "learning_rate": 0.00046660167130919224,
      "loss": 2.3303,
      "step": 19150
    },
    {
      "epoch": 5.337047353760446,
      "grad_norm": 1.1540862321853638,
      "learning_rate": 0.0004663231197771588,
      "loss": 2.2245,
      "step": 19160
    },
    {
      "epoch": 5.33983286908078,
      "grad_norm": 1.8550491333007812,
      "learning_rate": 0.00046604456824512536,
      "loss": 2.3013,
      "step": 19170
    },
    {
      "epoch": 5.342618384401114,
      "grad_norm": 2.614760398864746,
      "learning_rate": 0.00046576601671309196,
      "loss": 2.1136,
      "step": 19180
    },
    {
      "epoch": 5.345403899721449,
      "grad_norm": 1.573931097984314,
      "learning_rate": 0.0004654874651810585,
      "loss": 2.0864,
      "step": 19190
    },
    {
      "epoch": 5.348189415041783,
      "grad_norm": 1.2593286037445068,
      "learning_rate": 0.0004652089136490251,
      "loss": 2.2313,
      "step": 19200
    },
    {
      "epoch": 5.350974930362117,
      "grad_norm": 1.4465219974517822,
      "learning_rate": 0.0004649303621169916,
      "loss": 2.2882,
      "step": 19210
    },
    {
      "epoch": 5.353760445682451,
      "grad_norm": 1.9517697095870972,
      "learning_rate": 0.0004646518105849582,
      "loss": 2.2166,
      "step": 19220
    },
    {
      "epoch": 5.3565459610027855,
      "grad_norm": 1.825854778289795,
      "learning_rate": 0.0004643732590529248,
      "loss": 2.2173,
      "step": 19230
    },
    {
      "epoch": 5.35933147632312,
      "grad_norm": 1.4023149013519287,
      "learning_rate": 0.0004640947075208914,
      "loss": 2.2899,
      "step": 19240
    },
    {
      "epoch": 5.362116991643454,
      "grad_norm": 1.5155247449874878,
      "learning_rate": 0.00046381615598885793,
      "loss": 2.1987,
      "step": 19250
    },
    {
      "epoch": 5.364902506963788,
      "grad_norm": 1.735092282295227,
      "learning_rate": 0.0004635376044568245,
      "loss": 2.2844,
      "step": 19260
    },
    {
      "epoch": 5.367688022284122,
      "grad_norm": 1.5716347694396973,
      "learning_rate": 0.0004632590529247911,
      "loss": 2.1537,
      "step": 19270
    },
    {
      "epoch": 5.370473537604457,
      "grad_norm": 1.3954979181289673,
      "learning_rate": 0.00046298050139275765,
      "loss": 2.3827,
      "step": 19280
    },
    {
      "epoch": 5.373259052924791,
      "grad_norm": 3.249175786972046,
      "learning_rate": 0.00046270194986072424,
      "loss": 2.2499,
      "step": 19290
    },
    {
      "epoch": 5.376044568245125,
      "grad_norm": 1.5923482179641724,
      "learning_rate": 0.0004624233983286908,
      "loss": 2.1162,
      "step": 19300
    },
    {
      "epoch": 5.378830083565459,
      "grad_norm": 1.5626879930496216,
      "learning_rate": 0.0004621448467966574,
      "loss": 2.2184,
      "step": 19310
    },
    {
      "epoch": 5.381615598885794,
      "grad_norm": 1.5877629518508911,
      "learning_rate": 0.00046186629526462396,
      "loss": 2.108,
      "step": 19320
    },
    {
      "epoch": 5.3844011142061285,
      "grad_norm": 1.7900123596191406,
      "learning_rate": 0.00046158774373259055,
      "loss": 2.2376,
      "step": 19330
    },
    {
      "epoch": 5.387186629526463,
      "grad_norm": 1.5311572551727295,
      "learning_rate": 0.0004613091922005571,
      "loss": 2.111,
      "step": 19340
    },
    {
      "epoch": 5.389972144846797,
      "grad_norm": 1.2822519540786743,
      "learning_rate": 0.0004610306406685237,
      "loss": 2.2393,
      "step": 19350
    },
    {
      "epoch": 5.392757660167131,
      "grad_norm": 1.5679457187652588,
      "learning_rate": 0.00046075208913649027,
      "loss": 2.2475,
      "step": 19360
    },
    {
      "epoch": 5.395543175487465,
      "grad_norm": 1.8058197498321533,
      "learning_rate": 0.0004604735376044568,
      "loss": 2.1477,
      "step": 19370
    },
    {
      "epoch": 5.3983286908078,
      "grad_norm": 1.561852216720581,
      "learning_rate": 0.0004601949860724234,
      "loss": 2.2966,
      "step": 19380
    },
    {
      "epoch": 5.401114206128134,
      "grad_norm": 1.7563929557800293,
      "learning_rate": 0.00045991643454039,
      "loss": 2.207,
      "step": 19390
    },
    {
      "epoch": 5.403899721448468,
      "grad_norm": 1.11000657081604,
      "learning_rate": 0.0004596378830083566,
      "loss": 2.0771,
      "step": 19400
    },
    {
      "epoch": 5.406685236768802,
      "grad_norm": 1.698267936706543,
      "learning_rate": 0.0004593593314763231,
      "loss": 2.3326,
      "step": 19410
    },
    {
      "epoch": 5.4094707520891365,
      "grad_norm": 1.7477384805679321,
      "learning_rate": 0.0004590807799442897,
      "loss": 2.0924,
      "step": 19420
    },
    {
      "epoch": 5.412256267409471,
      "grad_norm": 3.228384256362915,
      "learning_rate": 0.00045880222841225625,
      "loss": 2.2286,
      "step": 19430
    },
    {
      "epoch": 5.415041782729805,
      "grad_norm": 1.2983492612838745,
      "learning_rate": 0.00045852367688022284,
      "loss": 2.1886,
      "step": 19440
    },
    {
      "epoch": 5.417827298050139,
      "grad_norm": 1.220240592956543,
      "learning_rate": 0.00045824512534818943,
      "loss": 2.1469,
      "step": 19450
    },
    {
      "epoch": 5.420612813370473,
      "grad_norm": 2.822993755340576,
      "learning_rate": 0.00045796657381615597,
      "loss": 2.219,
      "step": 19460
    },
    {
      "epoch": 5.423398328690808,
      "grad_norm": 1.3179007768630981,
      "learning_rate": 0.0004576880222841226,
      "loss": 2.2667,
      "step": 19470
    },
    {
      "epoch": 5.426183844011142,
      "grad_norm": 1.4274420738220215,
      "learning_rate": 0.00045740947075208915,
      "loss": 2.1361,
      "step": 19480
    },
    {
      "epoch": 5.428969359331476,
      "grad_norm": 1.3847272396087646,
      "learning_rate": 0.00045713091922005574,
      "loss": 2.1002,
      "step": 19490
    },
    {
      "epoch": 5.43175487465181,
      "grad_norm": 2.1538398265838623,
      "learning_rate": 0.0004568523676880223,
      "loss": 2.1789,
      "step": 19500
    },
    {
      "epoch": 5.4345403899721445,
      "grad_norm": 1.581729531288147,
      "learning_rate": 0.00045657381615598887,
      "loss": 2.3091,
      "step": 19510
    },
    {
      "epoch": 5.437325905292479,
      "grad_norm": 1.2675281763076782,
      "learning_rate": 0.0004562952646239554,
      "loss": 2.0319,
      "step": 19520
    },
    {
      "epoch": 5.440111420612814,
      "grad_norm": 1.5133603811264038,
      "learning_rate": 0.000456016713091922,
      "loss": 2.2249,
      "step": 19530
    },
    {
      "epoch": 5.442896935933147,
      "grad_norm": 1.8077561855316162,
      "learning_rate": 0.0004557381615598886,
      "loss": 2.2665,
      "step": 19540
    },
    {
      "epoch": 5.445682451253482,
      "grad_norm": 1.3204923868179321,
      "learning_rate": 0.0004554596100278552,
      "loss": 2.3071,
      "step": 19550
    },
    {
      "epoch": 5.4484679665738165,
      "grad_norm": 1.113595962524414,
      "learning_rate": 0.00045518105849582177,
      "loss": 2.1873,
      "step": 19560
    },
    {
      "epoch": 5.451253481894151,
      "grad_norm": 1.581396222114563,
      "learning_rate": 0.0004549025069637883,
      "loss": 2.2839,
      "step": 19570
    },
    {
      "epoch": 5.454038997214485,
      "grad_norm": 1.4738115072250366,
      "learning_rate": 0.0004546239554317549,
      "loss": 2.1981,
      "step": 19580
    },
    {
      "epoch": 5.456824512534819,
      "grad_norm": 1.1387423276901245,
      "learning_rate": 0.00045434540389972144,
      "loss": 2.1715,
      "step": 19590
    },
    {
      "epoch": 5.459610027855153,
      "grad_norm": 1.5093591213226318,
      "learning_rate": 0.00045406685236768803,
      "loss": 2.1566,
      "step": 19600
    },
    {
      "epoch": 5.462395543175488,
      "grad_norm": 1.7693694829940796,
      "learning_rate": 0.00045378830083565456,
      "loss": 2.2945,
      "step": 19610
    },
    {
      "epoch": 5.465181058495822,
      "grad_norm": 1.3906291723251343,
      "learning_rate": 0.00045350974930362116,
      "loss": 2.1544,
      "step": 19620
    },
    {
      "epoch": 5.467966573816156,
      "grad_norm": 1.2303438186645508,
      "learning_rate": 0.0004532311977715878,
      "loss": 2.1083,
      "step": 19630
    },
    {
      "epoch": 5.47075208913649,
      "grad_norm": 1.8185442686080933,
      "learning_rate": 0.00045295264623955434,
      "loss": 2.2629,
      "step": 19640
    },
    {
      "epoch": 5.4735376044568245,
      "grad_norm": 1.7988146543502808,
      "learning_rate": 0.00045267409470752093,
      "loss": 2.3255,
      "step": 19650
    },
    {
      "epoch": 5.476323119777159,
      "grad_norm": 1.5837771892547607,
      "learning_rate": 0.00045239554317548747,
      "loss": 2.276,
      "step": 19660
    },
    {
      "epoch": 5.479108635097493,
      "grad_norm": 1.3450919389724731,
      "learning_rate": 0.00045211699164345406,
      "loss": 2.2547,
      "step": 19670
    },
    {
      "epoch": 5.481894150417827,
      "grad_norm": 1.3404123783111572,
      "learning_rate": 0.0004518384401114206,
      "loss": 2.3057,
      "step": 19680
    },
    {
      "epoch": 5.484679665738161,
      "grad_norm": 1.5604857206344604,
      "learning_rate": 0.0004515598885793872,
      "loss": 2.0405,
      "step": 19690
    },
    {
      "epoch": 5.487465181058496,
      "grad_norm": 1.680656909942627,
      "learning_rate": 0.0004512813370473537,
      "loss": 2.4104,
      "step": 19700
    },
    {
      "epoch": 5.49025069637883,
      "grad_norm": 1.7735861539840698,
      "learning_rate": 0.00045100278551532037,
      "loss": 2.3026,
      "step": 19710
    },
    {
      "epoch": 5.493036211699164,
      "grad_norm": 1.5858711004257202,
      "learning_rate": 0.00045072423398328696,
      "loss": 2.4268,
      "step": 19720
    },
    {
      "epoch": 5.495821727019498,
      "grad_norm": 1.3558086156845093,
      "learning_rate": 0.0004504456824512535,
      "loss": 2.1669,
      "step": 19730
    },
    {
      "epoch": 5.498607242339833,
      "grad_norm": 1.4481565952301025,
      "learning_rate": 0.0004501671309192201,
      "loss": 2.1147,
      "step": 19740
    },
    {
      "epoch": 5.501392757660167,
      "grad_norm": 1.947507619857788,
      "learning_rate": 0.0004498885793871866,
      "loss": 2.3521,
      "step": 19750
    },
    {
      "epoch": 5.504178272980502,
      "grad_norm": 1.3329236507415771,
      "learning_rate": 0.0004496100278551532,
      "loss": 2.3667,
      "step": 19760
    },
    {
      "epoch": 5.506963788300836,
      "grad_norm": 1.2525089979171753,
      "learning_rate": 0.00044933147632311975,
      "loss": 2.2776,
      "step": 19770
    },
    {
      "epoch": 5.50974930362117,
      "grad_norm": 1.6431999206542969,
      "learning_rate": 0.00044905292479108634,
      "loss": 2.2138,
      "step": 19780
    },
    {
      "epoch": 5.512534818941504,
      "grad_norm": 1.6359038352966309,
      "learning_rate": 0.00044877437325905293,
      "loss": 2.2367,
      "step": 19790
    },
    {
      "epoch": 5.515320334261839,
      "grad_norm": 1.9955295324325562,
      "learning_rate": 0.0004484958217270195,
      "loss": 2.1496,
      "step": 19800
    },
    {
      "epoch": 5.518105849582173,
      "grad_norm": 1.412561058998108,
      "learning_rate": 0.0004482172701949861,
      "loss": 2.2253,
      "step": 19810
    },
    {
      "epoch": 5.520891364902507,
      "grad_norm": 1.5964422225952148,
      "learning_rate": 0.00044793871866295265,
      "loss": 2.2253,
      "step": 19820
    },
    {
      "epoch": 5.523676880222841,
      "grad_norm": 1.6141175031661987,
      "learning_rate": 0.00044766016713091925,
      "loss": 2.3277,
      "step": 19830
    },
    {
      "epoch": 5.5264623955431755,
      "grad_norm": 1.4811407327651978,
      "learning_rate": 0.0004473816155988858,
      "loss": 2.2114,
      "step": 19840
    },
    {
      "epoch": 5.52924791086351,
      "grad_norm": 2.0114428997039795,
      "learning_rate": 0.0004471030640668524,
      "loss": 2.4548,
      "step": 19850
    },
    {
      "epoch": 5.532033426183844,
      "grad_norm": 1.2644215822219849,
      "learning_rate": 0.0004468245125348189,
      "loss": 2.2162,
      "step": 19860
    },
    {
      "epoch": 5.534818941504178,
      "grad_norm": 1.2450599670410156,
      "learning_rate": 0.00044654596100278556,
      "loss": 2.0767,
      "step": 19870
    },
    {
      "epoch": 5.537604456824512,
      "grad_norm": 2.0158281326293945,
      "learning_rate": 0.0004462674094707521,
      "loss": 2.195,
      "step": 19880
    },
    {
      "epoch": 5.540389972144847,
      "grad_norm": 1.652182698249817,
      "learning_rate": 0.0004459888579387187,
      "loss": 2.219,
      "step": 19890
    },
    {
      "epoch": 5.543175487465181,
      "grad_norm": 1.739815354347229,
      "learning_rate": 0.0004457103064066853,
      "loss": 2.3152,
      "step": 19900
    },
    {
      "epoch": 5.545961002785515,
      "grad_norm": 2.5238072872161865,
      "learning_rate": 0.0004454317548746518,
      "loss": 2.3716,
      "step": 19910
    },
    {
      "epoch": 5.548746518105849,
      "grad_norm": 1.621534824371338,
      "learning_rate": 0.0004451532033426184,
      "loss": 2.1081,
      "step": 19920
    },
    {
      "epoch": 5.5515320334261835,
      "grad_norm": 1.885473608970642,
      "learning_rate": 0.00044487465181058494,
      "loss": 2.4575,
      "step": 19930
    },
    {
      "epoch": 5.554317548746518,
      "grad_norm": 1.8545597791671753,
      "learning_rate": 0.00044459610027855153,
      "loss": 2.0561,
      "step": 19940
    },
    {
      "epoch": 5.557103064066853,
      "grad_norm": 1.261991024017334,
      "learning_rate": 0.0004443175487465181,
      "loss": 2.1091,
      "step": 19950
    },
    {
      "epoch": 5.559888579387186,
      "grad_norm": 1.5505168437957764,
      "learning_rate": 0.0004440389972144847,
      "loss": 2.203,
      "step": 19960
    },
    {
      "epoch": 5.562674094707521,
      "grad_norm": 1.9542005062103271,
      "learning_rate": 0.00044376044568245125,
      "loss": 2.3362,
      "step": 19970
    },
    {
      "epoch": 5.5654596100278555,
      "grad_norm": 2.323450803756714,
      "learning_rate": 0.00044348189415041784,
      "loss": 2.0444,
      "step": 19980
    },
    {
      "epoch": 5.56824512534819,
      "grad_norm": 2.44745135307312,
      "learning_rate": 0.00044320334261838443,
      "loss": 2.1649,
      "step": 19990
    },
    {
      "epoch": 5.571030640668524,
      "grad_norm": 1.2857649326324463,
      "learning_rate": 0.00044292479108635097,
      "loss": 2.2066,
      "step": 20000
    },
    {
      "epoch": 5.573816155988858,
      "grad_norm": 1.5808955430984497,
      "learning_rate": 0.00044264623955431756,
      "loss": 2.2954,
      "step": 20010
    },
    {
      "epoch": 5.576601671309192,
      "grad_norm": 2.3602609634399414,
      "learning_rate": 0.0004423676880222841,
      "loss": 2.3532,
      "step": 20020
    },
    {
      "epoch": 5.579387186629527,
      "grad_norm": 1.8334084749221802,
      "learning_rate": 0.00044208913649025074,
      "loss": 2.1389,
      "step": 20030
    },
    {
      "epoch": 5.582172701949861,
      "grad_norm": 1.9500689506530762,
      "learning_rate": 0.0004418105849582173,
      "loss": 2.2669,
      "step": 20040
    },
    {
      "epoch": 5.584958217270195,
      "grad_norm": 1.6371116638183594,
      "learning_rate": 0.00044153203342618387,
      "loss": 2.1837,
      "step": 20050
    },
    {
      "epoch": 5.587743732590529,
      "grad_norm": 1.1593555212020874,
      "learning_rate": 0.0004412534818941504,
      "loss": 2.2142,
      "step": 20060
    },
    {
      "epoch": 5.5905292479108635,
      "grad_norm": 1.6023327112197876,
      "learning_rate": 0.000440974930362117,
      "loss": 2.197,
      "step": 20070
    },
    {
      "epoch": 5.593314763231198,
      "grad_norm": 1.6888794898986816,
      "learning_rate": 0.0004406963788300836,
      "loss": 2.4338,
      "step": 20080
    },
    {
      "epoch": 5.596100278551532,
      "grad_norm": 1.1532655954360962,
      "learning_rate": 0.00044041782729805013,
      "loss": 2.2483,
      "step": 20090
    },
    {
      "epoch": 5.598885793871866,
      "grad_norm": 1.8033536672592163,
      "learning_rate": 0.0004401392757660167,
      "loss": 2.2087,
      "step": 20100
    },
    {
      "epoch": 5.6016713091922,
      "grad_norm": 1.3810527324676514,
      "learning_rate": 0.0004398607242339833,
      "loss": 2.1354,
      "step": 20110
    },
    {
      "epoch": 5.604456824512535,
      "grad_norm": 1.331829309463501,
      "learning_rate": 0.0004395821727019499,
      "loss": 2.2314,
      "step": 20120
    },
    {
      "epoch": 5.607242339832869,
      "grad_norm": 1.5617964267730713,
      "learning_rate": 0.00043930362116991644,
      "loss": 2.1941,
      "step": 20130
    },
    {
      "epoch": 5.610027855153203,
      "grad_norm": 2.635798454284668,
      "learning_rate": 0.00043902506963788303,
      "loss": 2.1087,
      "step": 20140
    },
    {
      "epoch": 5.612813370473537,
      "grad_norm": 1.3306585550308228,
      "learning_rate": 0.00043874651810584957,
      "loss": 2.2474,
      "step": 20150
    },
    {
      "epoch": 5.615598885793872,
      "grad_norm": 1.992262601852417,
      "learning_rate": 0.00043846796657381616,
      "loss": 2.3222,
      "step": 20160
    },
    {
      "epoch": 5.618384401114206,
      "grad_norm": 1.7388395071029663,
      "learning_rate": 0.00043818941504178275,
      "loss": 2.3126,
      "step": 20170
    },
    {
      "epoch": 5.621169916434541,
      "grad_norm": 1.8233307600021362,
      "learning_rate": 0.0004379108635097493,
      "loss": 2.2049,
      "step": 20180
    },
    {
      "epoch": 5.623955431754875,
      "grad_norm": 1.7418055534362793,
      "learning_rate": 0.00043763231197771593,
      "loss": 2.4762,
      "step": 20190
    },
    {
      "epoch": 5.626740947075209,
      "grad_norm": 1.8426117897033691,
      "learning_rate": 0.00043735376044568247,
      "loss": 2.2563,
      "step": 20200
    },
    {
      "epoch": 5.629526462395543,
      "grad_norm": 2.159808397293091,
      "learning_rate": 0.00043707520891364906,
      "loss": 2.2872,
      "step": 20210
    },
    {
      "epoch": 5.632311977715878,
      "grad_norm": 1.5679161548614502,
      "learning_rate": 0.0004367966573816156,
      "loss": 2.0284,
      "step": 20220
    },
    {
      "epoch": 5.635097493036212,
      "grad_norm": 2.3398168087005615,
      "learning_rate": 0.0004365181058495822,
      "loss": 2.3614,
      "step": 20230
    },
    {
      "epoch": 5.637883008356546,
      "grad_norm": 1.8138545751571655,
      "learning_rate": 0.0004362395543175487,
      "loss": 2.138,
      "step": 20240
    },
    {
      "epoch": 5.64066852367688,
      "grad_norm": 1.940418004989624,
      "learning_rate": 0.0004359610027855153,
      "loss": 2.2145,
      "step": 20250
    },
    {
      "epoch": 5.6434540389972145,
      "grad_norm": 1.0054805278778076,
      "learning_rate": 0.0004356824512534819,
      "loss": 2.0983,
      "step": 20260
    },
    {
      "epoch": 5.646239554317549,
      "grad_norm": 1.3070017099380493,
      "learning_rate": 0.0004354038997214485,
      "loss": 2.1838,
      "step": 20270
    },
    {
      "epoch": 5.649025069637883,
      "grad_norm": 1.68082857131958,
      "learning_rate": 0.0004351253481894151,
      "loss": 2.1735,
      "step": 20280
    },
    {
      "epoch": 5.651810584958217,
      "grad_norm": 1.525887131690979,
      "learning_rate": 0.00043484679665738163,
      "loss": 2.5181,
      "step": 20290
    },
    {
      "epoch": 5.654596100278551,
      "grad_norm": 1.6495976448059082,
      "learning_rate": 0.0004345682451253482,
      "loss": 2.2535,
      "step": 20300
    },
    {
      "epoch": 5.657381615598886,
      "grad_norm": 1.4949249029159546,
      "learning_rate": 0.00043428969359331476,
      "loss": 2.2054,
      "step": 20310
    },
    {
      "epoch": 5.66016713091922,
      "grad_norm": 1.3869329690933228,
      "learning_rate": 0.00043401114206128135,
      "loss": 2.1912,
      "step": 20320
    },
    {
      "epoch": 5.662952646239554,
      "grad_norm": 1.8482661247253418,
      "learning_rate": 0.0004337325905292479,
      "loss": 2.298,
      "step": 20330
    },
    {
      "epoch": 5.665738161559888,
      "grad_norm": 1.6998138427734375,
      "learning_rate": 0.0004334540389972145,
      "loss": 2.2849,
      "step": 20340
    },
    {
      "epoch": 5.6685236768802225,
      "grad_norm": 1.1743769645690918,
      "learning_rate": 0.0004331754874651811,
      "loss": 2.1375,
      "step": 20350
    },
    {
      "epoch": 5.671309192200557,
      "grad_norm": 1.7271666526794434,
      "learning_rate": 0.00043289693593314766,
      "loss": 2.3017,
      "step": 20360
    },
    {
      "epoch": 5.674094707520892,
      "grad_norm": 1.9092469215393066,
      "learning_rate": 0.00043261838440111425,
      "loss": 2.1566,
      "step": 20370
    },
    {
      "epoch": 5.676880222841225,
      "grad_norm": 1.1655848026275635,
      "learning_rate": 0.0004323398328690808,
      "loss": 2.1984,
      "step": 20380
    },
    {
      "epoch": 5.67966573816156,
      "grad_norm": 1.6940444707870483,
      "learning_rate": 0.0004320612813370474,
      "loss": 2.1722,
      "step": 20390
    },
    {
      "epoch": 5.6824512534818945,
      "grad_norm": 1.7617788314819336,
      "learning_rate": 0.0004317827298050139,
      "loss": 2.1383,
      "step": 20400
    },
    {
      "epoch": 5.685236768802229,
      "grad_norm": 1.4655672311782837,
      "learning_rate": 0.0004315041782729805,
      "loss": 2.2016,
      "step": 20410
    },
    {
      "epoch": 5.688022284122563,
      "grad_norm": 1.9419392347335815,
      "learning_rate": 0.00043122562674094704,
      "loss": 2.3327,
      "step": 20420
    },
    {
      "epoch": 5.690807799442897,
      "grad_norm": 1.3370052576065063,
      "learning_rate": 0.00043094707520891363,
      "loss": 2.1505,
      "step": 20430
    },
    {
      "epoch": 5.693593314763231,
      "grad_norm": 1.392093300819397,
      "learning_rate": 0.0004306685236768803,
      "loss": 2.3601,
      "step": 20440
    },
    {
      "epoch": 5.696378830083566,
      "grad_norm": 1.6568658351898193,
      "learning_rate": 0.0004303899721448468,
      "loss": 2.2572,
      "step": 20450
    },
    {
      "epoch": 5.6991643454039,
      "grad_norm": 1.5267665386199951,
      "learning_rate": 0.0004301114206128134,
      "loss": 2.2227,
      "step": 20460
    },
    {
      "epoch": 5.701949860724234,
      "grad_norm": 1.6355681419372559,
      "learning_rate": 0.00042983286908077994,
      "loss": 2.2857,
      "step": 20470
    },
    {
      "epoch": 5.704735376044568,
      "grad_norm": 1.6137028932571411,
      "learning_rate": 0.00042955431754874654,
      "loss": 2.1618,
      "step": 20480
    },
    {
      "epoch": 5.7075208913649025,
      "grad_norm": 2.535545825958252,
      "learning_rate": 0.00042927576601671307,
      "loss": 2.2338,
      "step": 20490
    },
    {
      "epoch": 5.710306406685237,
      "grad_norm": 1.782832384109497,
      "learning_rate": 0.00042899721448467966,
      "loss": 2.3068,
      "step": 20500
    },
    {
      "epoch": 5.713091922005571,
      "grad_norm": 1.5072635412216187,
      "learning_rate": 0.0004287186629526462,
      "loss": 2.2797,
      "step": 20510
    },
    {
      "epoch": 5.715877437325905,
      "grad_norm": 1.6542608737945557,
      "learning_rate": 0.00042844011142061285,
      "loss": 2.2084,
      "step": 20520
    },
    {
      "epoch": 5.718662952646239,
      "grad_norm": 1.7306287288665771,
      "learning_rate": 0.00042816155988857944,
      "loss": 2.2208,
      "step": 20530
    },
    {
      "epoch": 5.721448467966574,
      "grad_norm": 1.8048630952835083,
      "learning_rate": 0.000427883008356546,
      "loss": 2.1972,
      "step": 20540
    },
    {
      "epoch": 5.724233983286908,
      "grad_norm": 1.5877951383590698,
      "learning_rate": 0.00042760445682451257,
      "loss": 2.2564,
      "step": 20550
    },
    {
      "epoch": 5.727019498607242,
      "grad_norm": 1.2408580780029297,
      "learning_rate": 0.0004273259052924791,
      "loss": 2.1469,
      "step": 20560
    },
    {
      "epoch": 5.729805013927576,
      "grad_norm": 1.979148030281067,
      "learning_rate": 0.0004270473537604457,
      "loss": 2.2103,
      "step": 20570
    },
    {
      "epoch": 5.732590529247911,
      "grad_norm": 1.78566312789917,
      "learning_rate": 0.00042676880222841223,
      "loss": 2.2659,
      "step": 20580
    },
    {
      "epoch": 5.735376044568245,
      "grad_norm": 2.2684340476989746,
      "learning_rate": 0.0004264902506963788,
      "loss": 2.3643,
      "step": 20590
    },
    {
      "epoch": 5.73816155988858,
      "grad_norm": 3.1018500328063965,
      "learning_rate": 0.0004262116991643454,
      "loss": 2.2287,
      "step": 20600
    },
    {
      "epoch": 5.740947075208914,
      "grad_norm": 1.412534236907959,
      "learning_rate": 0.000425933147632312,
      "loss": 2.2975,
      "step": 20610
    },
    {
      "epoch": 5.743732590529248,
      "grad_norm": 1.683462381362915,
      "learning_rate": 0.00042565459610027854,
      "loss": 2.3231,
      "step": 20620
    },
    {
      "epoch": 5.7465181058495824,
      "grad_norm": 1.8366531133651733,
      "learning_rate": 0.00042537604456824513,
      "loss": 2.1398,
      "step": 20630
    },
    {
      "epoch": 5.749303621169917,
      "grad_norm": 1.2274845838546753,
      "learning_rate": 0.0004250974930362117,
      "loss": 2.1261,
      "step": 20640
    },
    {
      "epoch": 5.752089136490251,
      "grad_norm": 1.5143139362335205,
      "learning_rate": 0.00042481894150417826,
      "loss": 2.1682,
      "step": 20650
    },
    {
      "epoch": 5.754874651810585,
      "grad_norm": 1.2664042711257935,
      "learning_rate": 0.00042454038997214485,
      "loss": 2.3007,
      "step": 20660
    },
    {
      "epoch": 5.757660167130919,
      "grad_norm": 1.5467829704284668,
      "learning_rate": 0.0004242618384401114,
      "loss": 2.2212,
      "step": 20670
    },
    {
      "epoch": 5.7604456824512535,
      "grad_norm": 1.4945006370544434,
      "learning_rate": 0.00042398328690807803,
      "loss": 2.1397,
      "step": 20680
    },
    {
      "epoch": 5.763231197771588,
      "grad_norm": 1.3483164310455322,
      "learning_rate": 0.00042370473537604457,
      "loss": 2.1449,
      "step": 20690
    },
    {
      "epoch": 5.766016713091922,
      "grad_norm": 1.169914722442627,
      "learning_rate": 0.00042342618384401116,
      "loss": 2.1821,
      "step": 20700
    },
    {
      "epoch": 5.768802228412256,
      "grad_norm": 1.0145108699798584,
      "learning_rate": 0.0004231476323119777,
      "loss": 2.3206,
      "step": 20710
    },
    {
      "epoch": 5.77158774373259,
      "grad_norm": 1.6005014181137085,
      "learning_rate": 0.0004228690807799443,
      "loss": 2.1697,
      "step": 20720
    },
    {
      "epoch": 5.774373259052925,
      "grad_norm": 1.4778388738632202,
      "learning_rate": 0.0004225905292479109,
      "loss": 2.2762,
      "step": 20730
    },
    {
      "epoch": 5.777158774373259,
      "grad_norm": 1.1991448402404785,
      "learning_rate": 0.0004223119777158774,
      "loss": 2.1349,
      "step": 20740
    },
    {
      "epoch": 5.779944289693593,
      "grad_norm": 2.660997152328491,
      "learning_rate": 0.000422033426183844,
      "loss": 2.2487,
      "step": 20750
    },
    {
      "epoch": 5.782729805013927,
      "grad_norm": 2.2280123233795166,
      "learning_rate": 0.0004217548746518106,
      "loss": 2.1436,
      "step": 20760
    },
    {
      "epoch": 5.7855153203342615,
      "grad_norm": 1.2892529964447021,
      "learning_rate": 0.0004214763231197772,
      "loss": 2.2384,
      "step": 20770
    },
    {
      "epoch": 5.788300835654596,
      "grad_norm": 1.5893003940582275,
      "learning_rate": 0.00042119777158774373,
      "loss": 2.0861,
      "step": 20780
    },
    {
      "epoch": 5.791086350974931,
      "grad_norm": 1.5130208730697632,
      "learning_rate": 0.0004209192200557103,
      "loss": 2.2004,
      "step": 20790
    },
    {
      "epoch": 5.793871866295264,
      "grad_norm": 1.5977447032928467,
      "learning_rate": 0.00042064066852367686,
      "loss": 2.2564,
      "step": 20800
    },
    {
      "epoch": 5.796657381615599,
      "grad_norm": 1.3838318586349487,
      "learning_rate": 0.00042036211699164345,
      "loss": 2.2649,
      "step": 20810
    },
    {
      "epoch": 5.7994428969359335,
      "grad_norm": 1.7645399570465088,
      "learning_rate": 0.00042008356545961004,
      "loss": 2.1503,
      "step": 20820
    },
    {
      "epoch": 5.802228412256268,
      "grad_norm": 1.5342655181884766,
      "learning_rate": 0.0004198050139275766,
      "loss": 2.3395,
      "step": 20830
    },
    {
      "epoch": 5.805013927576602,
      "grad_norm": 1.4143928289413452,
      "learning_rate": 0.0004195264623955432,
      "loss": 2.1796,
      "step": 20840
    },
    {
      "epoch": 5.807799442896936,
      "grad_norm": 1.5337166786193848,
      "learning_rate": 0.00041924791086350976,
      "loss": 2.5484,
      "step": 20850
    },
    {
      "epoch": 5.81058495821727,
      "grad_norm": 1.462047815322876,
      "learning_rate": 0.00041896935933147635,
      "loss": 2.1934,
      "step": 20860
    },
    {
      "epoch": 5.813370473537605,
      "grad_norm": 2.3453259468078613,
      "learning_rate": 0.0004186908077994429,
      "loss": 2.2734,
      "step": 20870
    },
    {
      "epoch": 5.816155988857939,
      "grad_norm": 1.2771342992782593,
      "learning_rate": 0.0004184122562674095,
      "loss": 2.2447,
      "step": 20880
    },
    {
      "epoch": 5.818941504178273,
      "grad_norm": 2.266714572906494,
      "learning_rate": 0.000418133704735376,
      "loss": 2.1558,
      "step": 20890
    },
    {
      "epoch": 5.821727019498607,
      "grad_norm": 1.5958269834518433,
      "learning_rate": 0.0004178551532033426,
      "loss": 2.3968,
      "step": 20900
    },
    {
      "epoch": 5.8245125348189415,
      "grad_norm": 1.757320523262024,
      "learning_rate": 0.0004175766016713092,
      "loss": 2.1292,
      "step": 20910
    },
    {
      "epoch": 5.827298050139276,
      "grad_norm": 1.5488662719726562,
      "learning_rate": 0.0004172980501392758,
      "loss": 2.2133,
      "step": 20920
    },
    {
      "epoch": 5.83008356545961,
      "grad_norm": 2.5498127937316895,
      "learning_rate": 0.0004170194986072424,
      "loss": 2.2431,
      "step": 20930
    },
    {
      "epoch": 5.832869080779944,
      "grad_norm": 1.479016661643982,
      "learning_rate": 0.0004167409470752089,
      "loss": 2.1507,
      "step": 20940
    },
    {
      "epoch": 5.835654596100278,
      "grad_norm": 1.629003643989563,
      "learning_rate": 0.0004164623955431755,
      "loss": 2.2825,
      "step": 20950
    },
    {
      "epoch": 5.838440111420613,
      "grad_norm": 2.0119967460632324,
      "learning_rate": 0.00041618384401114205,
      "loss": 2.1673,
      "step": 20960
    },
    {
      "epoch": 5.841225626740947,
      "grad_norm": 1.7298213243484497,
      "learning_rate": 0.00041590529247910864,
      "loss": 2.2529,
      "step": 20970
    },
    {
      "epoch": 5.844011142061281,
      "grad_norm": 1.4047514200210571,
      "learning_rate": 0.0004156267409470752,
      "loss": 2.3812,
      "step": 20980
    },
    {
      "epoch": 5.846796657381615,
      "grad_norm": 1.7538390159606934,
      "learning_rate": 0.00041534818941504177,
      "loss": 2.2582,
      "step": 20990
    },
    {
      "epoch": 5.84958217270195,
      "grad_norm": 1.746319055557251,
      "learning_rate": 0.0004150696378830084,
      "loss": 2.2516,
      "step": 21000
    },
    {
      "epoch": 5.852367688022284,
      "grad_norm": 1.1470032930374146,
      "learning_rate": 0.00041479108635097495,
      "loss": 2.1033,
      "step": 21010
    },
    {
      "epoch": 5.855153203342619,
      "grad_norm": 1.8036998510360718,
      "learning_rate": 0.00041451253481894154,
      "loss": 2.2633,
      "step": 21020
    },
    {
      "epoch": 5.857938718662953,
      "grad_norm": 1.6771798133850098,
      "learning_rate": 0.0004142339832869081,
      "loss": 2.3133,
      "step": 21030
    },
    {
      "epoch": 5.860724233983287,
      "grad_norm": 1.5441282987594604,
      "learning_rate": 0.00041395543175487467,
      "loss": 2.1905,
      "step": 21040
    },
    {
      "epoch": 5.8635097493036215,
      "grad_norm": 1.6081645488739014,
      "learning_rate": 0.0004136768802228412,
      "loss": 2.2126,
      "step": 21050
    },
    {
      "epoch": 5.866295264623956,
      "grad_norm": 1.9075753688812256,
      "learning_rate": 0.0004133983286908078,
      "loss": 2.2795,
      "step": 21060
    },
    {
      "epoch": 5.86908077994429,
      "grad_norm": 2.042358636856079,
      "learning_rate": 0.00041311977715877433,
      "loss": 2.2476,
      "step": 21070
    },
    {
      "epoch": 5.871866295264624,
      "grad_norm": 1.4292787313461304,
      "learning_rate": 0.000412841225626741,
      "loss": 2.0752,
      "step": 21080
    },
    {
      "epoch": 5.874651810584958,
      "grad_norm": 1.7299103736877441,
      "learning_rate": 0.00041256267409470757,
      "loss": 2.2277,
      "step": 21090
    },
    {
      "epoch": 5.8774373259052926,
      "grad_norm": 1.8136487007141113,
      "learning_rate": 0.0004122841225626741,
      "loss": 2.2643,
      "step": 21100
    },
    {
      "epoch": 5.880222841225627,
      "grad_norm": 1.1822527647018433,
      "learning_rate": 0.0004120055710306407,
      "loss": 2.2807,
      "step": 21110
    },
    {
      "epoch": 5.883008356545961,
      "grad_norm": 1.7139428853988647,
      "learning_rate": 0.00041172701949860723,
      "loss": 2.1855,
      "step": 21120
    },
    {
      "epoch": 5.885793871866295,
      "grad_norm": 1.1143933534622192,
      "learning_rate": 0.0004114484679665738,
      "loss": 2.1256,
      "step": 21130
    },
    {
      "epoch": 5.888579387186629,
      "grad_norm": 1.839493989944458,
      "learning_rate": 0.00041116991643454036,
      "loss": 2.3174,
      "step": 21140
    },
    {
      "epoch": 5.891364902506964,
      "grad_norm": 1.409981608390808,
      "learning_rate": 0.00041089136490250695,
      "loss": 2.2535,
      "step": 21150
    },
    {
      "epoch": 5.894150417827298,
      "grad_norm": 1.535382628440857,
      "learning_rate": 0.00041061281337047354,
      "loss": 2.3443,
      "step": 21160
    },
    {
      "epoch": 5.896935933147632,
      "grad_norm": 1.2015506029129028,
      "learning_rate": 0.00041033426183844014,
      "loss": 2.3759,
      "step": 21170
    },
    {
      "epoch": 5.899721448467966,
      "grad_norm": 1.4616953134536743,
      "learning_rate": 0.00041005571030640673,
      "loss": 2.1657,
      "step": 21180
    },
    {
      "epoch": 5.9025069637883005,
      "grad_norm": 1.9110337495803833,
      "learning_rate": 0.00040977715877437326,
      "loss": 2.2108,
      "step": 21190
    },
    {
      "epoch": 5.905292479108635,
      "grad_norm": 1.9140979051589966,
      "learning_rate": 0.00040949860724233986,
      "loss": 2.137,
      "step": 21200
    },
    {
      "epoch": 5.908077994428969,
      "grad_norm": 2.245647430419922,
      "learning_rate": 0.0004092200557103064,
      "loss": 2.2231,
      "step": 21210
    },
    {
      "epoch": 5.910863509749303,
      "grad_norm": 2.078469753265381,
      "learning_rate": 0.000408941504178273,
      "loss": 2.3174,
      "step": 21220
    },
    {
      "epoch": 5.913649025069638,
      "grad_norm": 1.4712697267532349,
      "learning_rate": 0.0004086629526462395,
      "loss": 2.2835,
      "step": 21230
    },
    {
      "epoch": 5.9164345403899725,
      "grad_norm": 1.2840403318405151,
      "learning_rate": 0.00040838440111420617,
      "loss": 2.1843,
      "step": 21240
    },
    {
      "epoch": 5.919220055710307,
      "grad_norm": 1.7367442846298218,
      "learning_rate": 0.0004081058495821727,
      "loss": 2.1028,
      "step": 21250
    },
    {
      "epoch": 5.922005571030641,
      "grad_norm": 1.8178157806396484,
      "learning_rate": 0.0004078272980501393,
      "loss": 2.2095,
      "step": 21260
    },
    {
      "epoch": 5.924791086350975,
      "grad_norm": 1.2638404369354248,
      "learning_rate": 0.0004075487465181059,
      "loss": 2.2621,
      "step": 21270
    },
    {
      "epoch": 5.927576601671309,
      "grad_norm": 1.326457142829895,
      "learning_rate": 0.0004072701949860724,
      "loss": 2.287,
      "step": 21280
    },
    {
      "epoch": 5.930362116991644,
      "grad_norm": 1.8931175470352173,
      "learning_rate": 0.000406991643454039,
      "loss": 2.1893,
      "step": 21290
    },
    {
      "epoch": 5.933147632311978,
      "grad_norm": 1.9977854490280151,
      "learning_rate": 0.00040671309192200555,
      "loss": 2.288,
      "step": 21300
    },
    {
      "epoch": 5.935933147632312,
      "grad_norm": 1.6344817876815796,
      "learning_rate": 0.00040643454038997214,
      "loss": 2.2516,
      "step": 21310
    },
    {
      "epoch": 5.938718662952646,
      "grad_norm": 1.407922387123108,
      "learning_rate": 0.00040615598885793873,
      "loss": 2.3913,
      "step": 21320
    },
    {
      "epoch": 5.9415041782729805,
      "grad_norm": 1.3918356895446777,
      "learning_rate": 0.0004058774373259053,
      "loss": 2.0835,
      "step": 21330
    },
    {
      "epoch": 5.944289693593315,
      "grad_norm": 1.4930477142333984,
      "learning_rate": 0.00040559888579387186,
      "loss": 2.2209,
      "step": 21340
    },
    {
      "epoch": 5.947075208913649,
      "grad_norm": 1.3993446826934814,
      "learning_rate": 0.00040532033426183845,
      "loss": 2.1429,
      "step": 21350
    },
    {
      "epoch": 5.949860724233983,
      "grad_norm": 1.3777263164520264,
      "learning_rate": 0.00040504178272980504,
      "loss": 2.1914,
      "step": 21360
    },
    {
      "epoch": 5.952646239554317,
      "grad_norm": 1.6111464500427246,
      "learning_rate": 0.0004047632311977716,
      "loss": 2.2228,
      "step": 21370
    },
    {
      "epoch": 5.955431754874652,
      "grad_norm": 2.0665924549102783,
      "learning_rate": 0.00040448467966573817,
      "loss": 2.2637,
      "step": 21380
    },
    {
      "epoch": 5.958217270194986,
      "grad_norm": 1.7851094007492065,
      "learning_rate": 0.0004042061281337047,
      "loss": 2.4699,
      "step": 21390
    },
    {
      "epoch": 5.96100278551532,
      "grad_norm": 1.352546215057373,
      "learning_rate": 0.00040392757660167135,
      "loss": 2.2884,
      "step": 21400
    },
    {
      "epoch": 5.963788300835654,
      "grad_norm": 1.46172297000885,
      "learning_rate": 0.0004036490250696379,
      "loss": 2.2918,
      "step": 21410
    },
    {
      "epoch": 5.9665738161559885,
      "grad_norm": 1.4802569150924683,
      "learning_rate": 0.0004033704735376045,
      "loss": 2.2954,
      "step": 21420
    },
    {
      "epoch": 5.969359331476323,
      "grad_norm": 1.4669644832611084,
      "learning_rate": 0.000403091922005571,
      "loss": 2.2766,
      "step": 21430
    },
    {
      "epoch": 5.972144846796658,
      "grad_norm": 1.9830994606018066,
      "learning_rate": 0.0004028133704735376,
      "loss": 2.088,
      "step": 21440
    },
    {
      "epoch": 5.974930362116992,
      "grad_norm": 1.59975004196167,
      "learning_rate": 0.0004025348189415042,
      "loss": 2.2441,
      "step": 21450
    },
    {
      "epoch": 5.977715877437326,
      "grad_norm": 2.0155985355377197,
      "learning_rate": 0.00040225626740947074,
      "loss": 2.2495,
      "step": 21460
    },
    {
      "epoch": 5.9805013927576605,
      "grad_norm": 1.6575325727462769,
      "learning_rate": 0.00040197771587743733,
      "loss": 2.2005,
      "step": 21470
    },
    {
      "epoch": 5.983286908077995,
      "grad_norm": 1.1574392318725586,
      "learning_rate": 0.0004016991643454039,
      "loss": 2.2045,
      "step": 21480
    },
    {
      "epoch": 5.986072423398329,
      "grad_norm": 1.5788445472717285,
      "learning_rate": 0.0004014206128133705,
      "loss": 2.2547,
      "step": 21490
    },
    {
      "epoch": 5.988857938718663,
      "grad_norm": 1.750434160232544,
      "learning_rate": 0.00040114206128133705,
      "loss": 2.3923,
      "step": 21500
    }
  ],
  "logging_steps": 10,
  "max_steps": 35900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2847763253329920.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
