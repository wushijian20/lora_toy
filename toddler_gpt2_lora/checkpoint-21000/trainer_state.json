{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.84958217270195,
  "eval_steps": 500,
  "global_step": 21000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002785515320334262,
      "grad_norm": 1.3184306621551514,
      "learning_rate": 0.00099974930362117,
      "loss": 4.4109,
      "step": 10
    },
    {
      "epoch": 0.005571030640668524,
      "grad_norm": 1.5813078880310059,
      "learning_rate": 0.0009994707520891365,
      "loss": 3.4999,
      "step": 20
    },
    {
      "epoch": 0.008356545961002786,
      "grad_norm": 1.0860689878463745,
      "learning_rate": 0.0009991922005571031,
      "loss": 3.1849,
      "step": 30
    },
    {
      "epoch": 0.011142061281337047,
      "grad_norm": 1.8006736040115356,
      "learning_rate": 0.0009989136490250697,
      "loss": 3.4449,
      "step": 40
    },
    {
      "epoch": 0.013927576601671309,
      "grad_norm": 1.356768012046814,
      "learning_rate": 0.0009986350974930363,
      "loss": 3.2374,
      "step": 50
    },
    {
      "epoch": 0.016713091922005572,
      "grad_norm": 1.5551269054412842,
      "learning_rate": 0.0009983565459610027,
      "loss": 2.9415,
      "step": 60
    },
    {
      "epoch": 0.019498607242339833,
      "grad_norm": 1.409018635749817,
      "learning_rate": 0.0009980779944289695,
      "loss": 3.2863,
      "step": 70
    },
    {
      "epoch": 0.022284122562674095,
      "grad_norm": 1.1689125299453735,
      "learning_rate": 0.0009977994428969359,
      "loss": 2.9452,
      "step": 80
    },
    {
      "epoch": 0.025069637883008356,
      "grad_norm": 1.0413603782653809,
      "learning_rate": 0.0009975208913649025,
      "loss": 2.8685,
      "step": 90
    },
    {
      "epoch": 0.027855153203342618,
      "grad_norm": 1.33734130859375,
      "learning_rate": 0.000997242339832869,
      "loss": 2.7454,
      "step": 100
    },
    {
      "epoch": 0.03064066852367688,
      "grad_norm": 1.4368760585784912,
      "learning_rate": 0.0009969637883008356,
      "loss": 3.0147,
      "step": 110
    },
    {
      "epoch": 0.033426183844011144,
      "grad_norm": 1.1257479190826416,
      "learning_rate": 0.0009966852367688022,
      "loss": 3.1734,
      "step": 120
    },
    {
      "epoch": 0.036211699164345405,
      "grad_norm": 1.3167701959609985,
      "learning_rate": 0.0009964066852367688,
      "loss": 2.9545,
      "step": 130
    },
    {
      "epoch": 0.03899721448467967,
      "grad_norm": 1.4054332971572876,
      "learning_rate": 0.0009961281337047354,
      "loss": 3.1557,
      "step": 140
    },
    {
      "epoch": 0.04178272980501393,
      "grad_norm": 1.3727281093597412,
      "learning_rate": 0.000995849582172702,
      "loss": 2.8601,
      "step": 150
    },
    {
      "epoch": 0.04456824512534819,
      "grad_norm": 1.6955479383468628,
      "learning_rate": 0.0009955710306406686,
      "loss": 2.7495,
      "step": 160
    },
    {
      "epoch": 0.04735376044568245,
      "grad_norm": 1.3450806140899658,
      "learning_rate": 0.000995292479108635,
      "loss": 2.8783,
      "step": 170
    },
    {
      "epoch": 0.05013927576601671,
      "grad_norm": 1.459574818611145,
      "learning_rate": 0.0009950139275766018,
      "loss": 2.7604,
      "step": 180
    },
    {
      "epoch": 0.052924791086350974,
      "grad_norm": 1.1801546812057495,
      "learning_rate": 0.0009947353760445684,
      "loss": 3.1179,
      "step": 190
    },
    {
      "epoch": 0.055710306406685235,
      "grad_norm": 1.2009811401367188,
      "learning_rate": 0.0009944568245125348,
      "loss": 2.8204,
      "step": 200
    },
    {
      "epoch": 0.0584958217270195,
      "grad_norm": 1.065401315689087,
      "learning_rate": 0.0009941782729805016,
      "loss": 3.0881,
      "step": 210
    },
    {
      "epoch": 0.06128133704735376,
      "grad_norm": 1.1269153356552124,
      "learning_rate": 0.000993899721448468,
      "loss": 2.799,
      "step": 220
    },
    {
      "epoch": 0.06406685236768803,
      "grad_norm": 2.266690492630005,
      "learning_rate": 0.0009936211699164345,
      "loss": 3.0054,
      "step": 230
    },
    {
      "epoch": 0.06685236768802229,
      "grad_norm": 1.279406189918518,
      "learning_rate": 0.0009933426183844011,
      "loss": 2.8444,
      "step": 240
    },
    {
      "epoch": 0.06963788300835655,
      "grad_norm": 1.3437994718551636,
      "learning_rate": 0.0009930640668523677,
      "loss": 2.9032,
      "step": 250
    },
    {
      "epoch": 0.07242339832869081,
      "grad_norm": 1.764569878578186,
      "learning_rate": 0.0009927855153203343,
      "loss": 3.1219,
      "step": 260
    },
    {
      "epoch": 0.07520891364902507,
      "grad_norm": 1.3152501583099365,
      "learning_rate": 0.0009925069637883009,
      "loss": 2.913,
      "step": 270
    },
    {
      "epoch": 0.07799442896935933,
      "grad_norm": 1.1941207647323608,
      "learning_rate": 0.0009922284122562675,
      "loss": 2.7426,
      "step": 280
    },
    {
      "epoch": 0.0807799442896936,
      "grad_norm": 1.124518632888794,
      "learning_rate": 0.000991949860724234,
      "loss": 2.7929,
      "step": 290
    },
    {
      "epoch": 0.08356545961002786,
      "grad_norm": 1.2781908512115479,
      "learning_rate": 0.0009916713091922007,
      "loss": 2.9162,
      "step": 300
    },
    {
      "epoch": 0.08635097493036212,
      "grad_norm": 1.12647545337677,
      "learning_rate": 0.000991392757660167,
      "loss": 2.9383,
      "step": 310
    },
    {
      "epoch": 0.08913649025069638,
      "grad_norm": 1.097976803779602,
      "learning_rate": 0.0009911142061281338,
      "loss": 2.7453,
      "step": 320
    },
    {
      "epoch": 0.09192200557103064,
      "grad_norm": 1.774233341217041,
      "learning_rate": 0.0009908356545961002,
      "loss": 2.8101,
      "step": 330
    },
    {
      "epoch": 0.0947075208913649,
      "grad_norm": 1.5990629196166992,
      "learning_rate": 0.0009905571030640668,
      "loss": 2.8902,
      "step": 340
    },
    {
      "epoch": 0.09749303621169916,
      "grad_norm": 1.1920967102050781,
      "learning_rate": 0.0009902785515320334,
      "loss": 2.5888,
      "step": 350
    },
    {
      "epoch": 0.10027855153203342,
      "grad_norm": 1.2843753099441528,
      "learning_rate": 0.00099,
      "loss": 2.8326,
      "step": 360
    },
    {
      "epoch": 0.10306406685236769,
      "grad_norm": 1.135982632637024,
      "learning_rate": 0.0009897214484679666,
      "loss": 2.8351,
      "step": 370
    },
    {
      "epoch": 0.10584958217270195,
      "grad_norm": 1.1573357582092285,
      "learning_rate": 0.0009894428969359332,
      "loss": 2.8061,
      "step": 380
    },
    {
      "epoch": 0.10863509749303621,
      "grad_norm": 2.5184695720672607,
      "learning_rate": 0.0009891643454038998,
      "loss": 2.9333,
      "step": 390
    },
    {
      "epoch": 0.11142061281337047,
      "grad_norm": 1.282134771347046,
      "learning_rate": 0.0009888857938718664,
      "loss": 2.8047,
      "step": 400
    },
    {
      "epoch": 0.11420612813370473,
      "grad_norm": 1.1679853200912476,
      "learning_rate": 0.000988607242339833,
      "loss": 2.6464,
      "step": 410
    },
    {
      "epoch": 0.116991643454039,
      "grad_norm": 1.5133442878723145,
      "learning_rate": 0.0009883286908077993,
      "loss": 2.7508,
      "step": 420
    },
    {
      "epoch": 0.11977715877437325,
      "grad_norm": 1.268327236175537,
      "learning_rate": 0.0009880501392757661,
      "loss": 2.5719,
      "step": 430
    },
    {
      "epoch": 0.12256267409470752,
      "grad_norm": 1.5530740022659302,
      "learning_rate": 0.0009877715877437327,
      "loss": 2.7568,
      "step": 440
    },
    {
      "epoch": 0.12534818941504178,
      "grad_norm": 1.4323877096176147,
      "learning_rate": 0.000987493036211699,
      "loss": 2.6042,
      "step": 450
    },
    {
      "epoch": 0.12813370473537605,
      "grad_norm": 1.187488079071045,
      "learning_rate": 0.0009872144846796657,
      "loss": 2.6372,
      "step": 460
    },
    {
      "epoch": 0.1309192200557103,
      "grad_norm": 1.3312557935714722,
      "learning_rate": 0.0009869359331476323,
      "loss": 2.6408,
      "step": 470
    },
    {
      "epoch": 0.13370473537604458,
      "grad_norm": 1.446892499923706,
      "learning_rate": 0.0009866573816155989,
      "loss": 2.7481,
      "step": 480
    },
    {
      "epoch": 0.13649025069637882,
      "grad_norm": 1.278719186782837,
      "learning_rate": 0.0009863788300835655,
      "loss": 2.5203,
      "step": 490
    },
    {
      "epoch": 0.1392757660167131,
      "grad_norm": 1.296244740486145,
      "learning_rate": 0.000986100278551532,
      "loss": 2.6898,
      "step": 500
    },
    {
      "epoch": 0.14206128133704735,
      "grad_norm": 1.2635987997055054,
      "learning_rate": 0.0009858217270194986,
      "loss": 2.6824,
      "step": 510
    },
    {
      "epoch": 0.14484679665738162,
      "grad_norm": 1.5535247325897217,
      "learning_rate": 0.0009855431754874652,
      "loss": 2.9347,
      "step": 520
    },
    {
      "epoch": 0.14763231197771587,
      "grad_norm": 1.2211328744888306,
      "learning_rate": 0.0009852646239554318,
      "loss": 2.8607,
      "step": 530
    },
    {
      "epoch": 0.15041782729805014,
      "grad_norm": 1.42167067527771,
      "learning_rate": 0.0009849860724233984,
      "loss": 2.5061,
      "step": 540
    },
    {
      "epoch": 0.1532033426183844,
      "grad_norm": 1.1731417179107666,
      "learning_rate": 0.000984707520891365,
      "loss": 2.826,
      "step": 550
    },
    {
      "epoch": 0.15598885793871867,
      "grad_norm": 1.0162349939346313,
      "learning_rate": 0.0009844289693593314,
      "loss": 2.9544,
      "step": 560
    },
    {
      "epoch": 0.15877437325905291,
      "grad_norm": 1.4798792600631714,
      "learning_rate": 0.000984150417827298,
      "loss": 2.7535,
      "step": 570
    },
    {
      "epoch": 0.1615598885793872,
      "grad_norm": 1.3622181415557861,
      "learning_rate": 0.0009838718662952646,
      "loss": 2.7347,
      "step": 580
    },
    {
      "epoch": 0.16434540389972144,
      "grad_norm": 1.2378495931625366,
      "learning_rate": 0.0009835933147632312,
      "loss": 2.7594,
      "step": 590
    },
    {
      "epoch": 0.1671309192200557,
      "grad_norm": 1.5325119495391846,
      "learning_rate": 0.0009833147632311977,
      "loss": 2.9527,
      "step": 600
    },
    {
      "epoch": 0.16991643454038996,
      "grad_norm": 1.423062801361084,
      "learning_rate": 0.0009830362116991643,
      "loss": 2.6827,
      "step": 610
    },
    {
      "epoch": 0.17270194986072424,
      "grad_norm": 1.248356819152832,
      "learning_rate": 0.000982757660167131,
      "loss": 2.8528,
      "step": 620
    },
    {
      "epoch": 0.17548746518105848,
      "grad_norm": 1.1467750072479248,
      "learning_rate": 0.0009824791086350975,
      "loss": 2.7839,
      "step": 630
    },
    {
      "epoch": 0.17827298050139276,
      "grad_norm": 1.336609959602356,
      "learning_rate": 0.0009822005571030641,
      "loss": 2.5227,
      "step": 640
    },
    {
      "epoch": 0.181058495821727,
      "grad_norm": 1.247795581817627,
      "learning_rate": 0.0009819220055710307,
      "loss": 2.798,
      "step": 650
    },
    {
      "epoch": 0.18384401114206128,
      "grad_norm": 1.5382665395736694,
      "learning_rate": 0.0009816434540389973,
      "loss": 2.6285,
      "step": 660
    },
    {
      "epoch": 0.18662952646239556,
      "grad_norm": 1.3249435424804688,
      "learning_rate": 0.0009813649025069639,
      "loss": 2.7221,
      "step": 670
    },
    {
      "epoch": 0.1894150417827298,
      "grad_norm": 1.2978134155273438,
      "learning_rate": 0.0009810863509749305,
      "loss": 2.6655,
      "step": 680
    },
    {
      "epoch": 0.19220055710306408,
      "grad_norm": 1.1235485076904297,
      "learning_rate": 0.000980807799442897,
      "loss": 2.7598,
      "step": 690
    },
    {
      "epoch": 0.19498607242339833,
      "grad_norm": 1.5357059240341187,
      "learning_rate": 0.0009805292479108634,
      "loss": 2.683,
      "step": 700
    },
    {
      "epoch": 0.1977715877437326,
      "grad_norm": 1.2875046730041504,
      "learning_rate": 0.00098025069637883,
      "loss": 2.6536,
      "step": 710
    },
    {
      "epoch": 0.20055710306406685,
      "grad_norm": 1.366127848625183,
      "learning_rate": 0.0009799721448467966,
      "loss": 2.7897,
      "step": 720
    },
    {
      "epoch": 0.20334261838440112,
      "grad_norm": 1.660287618637085,
      "learning_rate": 0.0009796935933147632,
      "loss": 2.5606,
      "step": 730
    },
    {
      "epoch": 0.20612813370473537,
      "grad_norm": 1.4152723550796509,
      "learning_rate": 0.0009794150417827298,
      "loss": 2.8715,
      "step": 740
    },
    {
      "epoch": 0.20891364902506965,
      "grad_norm": 1.8255491256713867,
      "learning_rate": 0.0009791364902506964,
      "loss": 2.7588,
      "step": 750
    },
    {
      "epoch": 0.2116991643454039,
      "grad_norm": 1.6623692512512207,
      "learning_rate": 0.000978857938718663,
      "loss": 2.615,
      "step": 760
    },
    {
      "epoch": 0.21448467966573817,
      "grad_norm": 1.0077954530715942,
      "learning_rate": 0.0009785793871866296,
      "loss": 2.6124,
      "step": 770
    },
    {
      "epoch": 0.21727019498607242,
      "grad_norm": 1.1320374011993408,
      "learning_rate": 0.0009783008356545962,
      "loss": 2.4143,
      "step": 780
    },
    {
      "epoch": 0.2200557103064067,
      "grad_norm": 1.9050565958023071,
      "learning_rate": 0.0009780222841225628,
      "loss": 2.7989,
      "step": 790
    },
    {
      "epoch": 0.22284122562674094,
      "grad_norm": 1.6544328927993774,
      "learning_rate": 0.0009777437325905294,
      "loss": 2.6959,
      "step": 800
    },
    {
      "epoch": 0.22562674094707522,
      "grad_norm": 1.2990214824676514,
      "learning_rate": 0.0009774651810584957,
      "loss": 2.6594,
      "step": 810
    },
    {
      "epoch": 0.22841225626740946,
      "grad_norm": 1.2343426942825317,
      "learning_rate": 0.0009771866295264623,
      "loss": 2.6625,
      "step": 820
    },
    {
      "epoch": 0.23119777158774374,
      "grad_norm": 1.3956587314605713,
      "learning_rate": 0.000976908077994429,
      "loss": 2.8891,
      "step": 830
    },
    {
      "epoch": 0.233983286908078,
      "grad_norm": 0.9141462445259094,
      "learning_rate": 0.0009766295264623955,
      "loss": 2.5646,
      "step": 840
    },
    {
      "epoch": 0.23676880222841226,
      "grad_norm": 1.3740825653076172,
      "learning_rate": 0.0009763509749303622,
      "loss": 2.5269,
      "step": 850
    },
    {
      "epoch": 0.2395543175487465,
      "grad_norm": 1.180518388748169,
      "learning_rate": 0.0009760724233983287,
      "loss": 2.6666,
      "step": 860
    },
    {
      "epoch": 0.24233983286908078,
      "grad_norm": 1.8031930923461914,
      "learning_rate": 0.0009757938718662953,
      "loss": 2.5519,
      "step": 870
    },
    {
      "epoch": 0.24512534818941503,
      "grad_norm": 1.3670964241027832,
      "learning_rate": 0.0009755153203342619,
      "loss": 2.5203,
      "step": 880
    },
    {
      "epoch": 0.2479108635097493,
      "grad_norm": 1.359139084815979,
      "learning_rate": 0.0009752367688022285,
      "loss": 2.5205,
      "step": 890
    },
    {
      "epoch": 0.25069637883008355,
      "grad_norm": 2.213026523590088,
      "learning_rate": 0.0009749582172701949,
      "loss": 2.5453,
      "step": 900
    },
    {
      "epoch": 0.25348189415041783,
      "grad_norm": 1.5623490810394287,
      "learning_rate": 0.0009746796657381615,
      "loss": 2.4441,
      "step": 910
    },
    {
      "epoch": 0.2562674094707521,
      "grad_norm": 1.0355658531188965,
      "learning_rate": 0.0009744011142061282,
      "loss": 2.8102,
      "step": 920
    },
    {
      "epoch": 0.2590529247910863,
      "grad_norm": 1.5856624841690063,
      "learning_rate": 0.0009741225626740947,
      "loss": 2.7493,
      "step": 930
    },
    {
      "epoch": 0.2618384401114206,
      "grad_norm": 1.4202443361282349,
      "learning_rate": 0.0009738440111420613,
      "loss": 2.6921,
      "step": 940
    },
    {
      "epoch": 0.2646239554317549,
      "grad_norm": 1.4143579006195068,
      "learning_rate": 0.0009735654596100279,
      "loss": 2.8838,
      "step": 950
    },
    {
      "epoch": 0.26740947075208915,
      "grad_norm": 1.1518174409866333,
      "learning_rate": 0.0009732869080779945,
      "loss": 2.7158,
      "step": 960
    },
    {
      "epoch": 0.27019498607242337,
      "grad_norm": 1.2318888902664185,
      "learning_rate": 0.000973008356545961,
      "loss": 2.5147,
      "step": 970
    },
    {
      "epoch": 0.27298050139275765,
      "grad_norm": 1.1624425649642944,
      "learning_rate": 0.0009727298050139276,
      "loss": 2.6845,
      "step": 980
    },
    {
      "epoch": 0.2757660167130919,
      "grad_norm": 1.1771116256713867,
      "learning_rate": 0.0009724512534818942,
      "loss": 2.4437,
      "step": 990
    },
    {
      "epoch": 0.2785515320334262,
      "grad_norm": 1.4274197816848755,
      "learning_rate": 0.0009721727019498607,
      "loss": 2.6864,
      "step": 1000
    },
    {
      "epoch": 0.28133704735376047,
      "grad_norm": 2.339977741241455,
      "learning_rate": 0.0009718941504178273,
      "loss": 2.4544,
      "step": 1010
    },
    {
      "epoch": 0.2841225626740947,
      "grad_norm": 1.3057228326797485,
      "learning_rate": 0.0009716155988857938,
      "loss": 2.6233,
      "step": 1020
    },
    {
      "epoch": 0.28690807799442897,
      "grad_norm": 1.122351050376892,
      "learning_rate": 0.0009713370473537605,
      "loss": 2.8104,
      "step": 1030
    },
    {
      "epoch": 0.28969359331476324,
      "grad_norm": 1.2953671216964722,
      "learning_rate": 0.000971058495821727,
      "loss": 2.5395,
      "step": 1040
    },
    {
      "epoch": 0.2924791086350975,
      "grad_norm": 1.0992861986160278,
      "learning_rate": 0.0009707799442896936,
      "loss": 2.5608,
      "step": 1050
    },
    {
      "epoch": 0.29526462395543174,
      "grad_norm": 1.1927942037582397,
      "learning_rate": 0.0009705013927576602,
      "loss": 2.5849,
      "step": 1060
    },
    {
      "epoch": 0.298050139275766,
      "grad_norm": 1.073552131652832,
      "learning_rate": 0.0009702228412256268,
      "loss": 2.5749,
      "step": 1070
    },
    {
      "epoch": 0.3008356545961003,
      "grad_norm": 1.4066038131713867,
      "learning_rate": 0.0009699442896935934,
      "loss": 2.7315,
      "step": 1080
    },
    {
      "epoch": 0.30362116991643456,
      "grad_norm": 1.3593380451202393,
      "learning_rate": 0.0009696657381615599,
      "loss": 2.5481,
      "step": 1090
    },
    {
      "epoch": 0.3064066852367688,
      "grad_norm": 1.1767328977584839,
      "learning_rate": 0.0009693871866295266,
      "loss": 2.6792,
      "step": 1100
    },
    {
      "epoch": 0.30919220055710306,
      "grad_norm": 1.6916252374649048,
      "learning_rate": 0.000969108635097493,
      "loss": 2.7444,
      "step": 1110
    },
    {
      "epoch": 0.31197771587743733,
      "grad_norm": 1.1882399320602417,
      "learning_rate": 0.0009688300835654596,
      "loss": 2.6223,
      "step": 1120
    },
    {
      "epoch": 0.3147632311977716,
      "grad_norm": 1.5972057580947876,
      "learning_rate": 0.0009685515320334262,
      "loss": 2.4994,
      "step": 1130
    },
    {
      "epoch": 0.31754874651810583,
      "grad_norm": 1.100670576095581,
      "learning_rate": 0.0009682729805013928,
      "loss": 2.6736,
      "step": 1140
    },
    {
      "epoch": 0.3203342618384401,
      "grad_norm": 1.24209725856781,
      "learning_rate": 0.0009679944289693593,
      "loss": 2.6543,
      "step": 1150
    },
    {
      "epoch": 0.3231197771587744,
      "grad_norm": 1.3492343425750732,
      "learning_rate": 0.0009677158774373259,
      "loss": 2.4551,
      "step": 1160
    },
    {
      "epoch": 0.32590529247910865,
      "grad_norm": 1.2661808729171753,
      "learning_rate": 0.0009674373259052926,
      "loss": 2.6216,
      "step": 1170
    },
    {
      "epoch": 0.3286908077994429,
      "grad_norm": 1.2597366571426392,
      "learning_rate": 0.0009671587743732591,
      "loss": 2.7105,
      "step": 1180
    },
    {
      "epoch": 0.33147632311977715,
      "grad_norm": 1.3644498586654663,
      "learning_rate": 0.0009668802228412257,
      "loss": 2.7976,
      "step": 1190
    },
    {
      "epoch": 0.3342618384401114,
      "grad_norm": 1.5296719074249268,
      "learning_rate": 0.0009666016713091921,
      "loss": 2.6102,
      "step": 1200
    },
    {
      "epoch": 0.3370473537604457,
      "grad_norm": 1.5513615608215332,
      "learning_rate": 0.0009663231197771588,
      "loss": 2.5528,
      "step": 1210
    },
    {
      "epoch": 0.3398328690807799,
      "grad_norm": 1.5851987600326538,
      "learning_rate": 0.0009660445682451253,
      "loss": 2.5353,
      "step": 1220
    },
    {
      "epoch": 0.3426183844011142,
      "grad_norm": 1.1714510917663574,
      "learning_rate": 0.0009657660167130919,
      "loss": 2.5504,
      "step": 1230
    },
    {
      "epoch": 0.34540389972144847,
      "grad_norm": 1.2333942651748657,
      "learning_rate": 0.0009654874651810586,
      "loss": 2.5364,
      "step": 1240
    },
    {
      "epoch": 0.34818941504178275,
      "grad_norm": 1.3227509260177612,
      "learning_rate": 0.0009652089136490251,
      "loss": 2.5871,
      "step": 1250
    },
    {
      "epoch": 0.35097493036211697,
      "grad_norm": 1.330220341682434,
      "learning_rate": 0.0009649303621169917,
      "loss": 2.6502,
      "step": 1260
    },
    {
      "epoch": 0.35376044568245124,
      "grad_norm": 1.2534931898117065,
      "learning_rate": 0.0009646518105849582,
      "loss": 2.8042,
      "step": 1270
    },
    {
      "epoch": 0.3565459610027855,
      "grad_norm": 1.1250964403152466,
      "learning_rate": 0.0009643732590529249,
      "loss": 2.6739,
      "step": 1280
    },
    {
      "epoch": 0.3593314763231198,
      "grad_norm": 1.1140731573104858,
      "learning_rate": 0.0009640947075208914,
      "loss": 2.5691,
      "step": 1290
    },
    {
      "epoch": 0.362116991643454,
      "grad_norm": 1.6042699813842773,
      "learning_rate": 0.0009638161559888579,
      "loss": 2.7013,
      "step": 1300
    },
    {
      "epoch": 0.3649025069637883,
      "grad_norm": 1.444177508354187,
      "learning_rate": 0.0009635376044568245,
      "loss": 2.5751,
      "step": 1310
    },
    {
      "epoch": 0.36768802228412256,
      "grad_norm": 1.4426827430725098,
      "learning_rate": 0.0009632590529247911,
      "loss": 2.7308,
      "step": 1320
    },
    {
      "epoch": 0.37047353760445684,
      "grad_norm": 1.2747937440872192,
      "learning_rate": 0.0009629805013927577,
      "loss": 2.6738,
      "step": 1330
    },
    {
      "epoch": 0.3732590529247911,
      "grad_norm": 1.1554028987884521,
      "learning_rate": 0.0009627019498607242,
      "loss": 2.4883,
      "step": 1340
    },
    {
      "epoch": 0.37604456824512533,
      "grad_norm": 1.1143832206726074,
      "learning_rate": 0.0009624233983286909,
      "loss": 2.6659,
      "step": 1350
    },
    {
      "epoch": 0.3788300835654596,
      "grad_norm": 1.5240787267684937,
      "learning_rate": 0.0009621448467966574,
      "loss": 2.7023,
      "step": 1360
    },
    {
      "epoch": 0.3816155988857939,
      "grad_norm": 1.3093359470367432,
      "learning_rate": 0.000961866295264624,
      "loss": 2.6985,
      "step": 1370
    },
    {
      "epoch": 0.38440111420612816,
      "grad_norm": 1.2336301803588867,
      "learning_rate": 0.0009615877437325905,
      "loss": 2.5682,
      "step": 1380
    },
    {
      "epoch": 0.3871866295264624,
      "grad_norm": 1.7109999656677246,
      "learning_rate": 0.0009613091922005572,
      "loss": 2.6605,
      "step": 1390
    },
    {
      "epoch": 0.38997214484679665,
      "grad_norm": 1.04278564453125,
      "learning_rate": 0.0009610306406685237,
      "loss": 2.4804,
      "step": 1400
    },
    {
      "epoch": 0.39275766016713093,
      "grad_norm": 1.4302489757537842,
      "learning_rate": 0.0009607520891364902,
      "loss": 2.7331,
      "step": 1410
    },
    {
      "epoch": 0.3955431754874652,
      "grad_norm": 1.2373706102371216,
      "learning_rate": 0.0009604735376044569,
      "loss": 2.6105,
      "step": 1420
    },
    {
      "epoch": 0.3983286908077994,
      "grad_norm": 1.3912535905838013,
      "learning_rate": 0.0009601949860724234,
      "loss": 2.5495,
      "step": 1430
    },
    {
      "epoch": 0.4011142061281337,
      "grad_norm": 1.1748363971710205,
      "learning_rate": 0.00095991643454039,
      "loss": 2.3968,
      "step": 1440
    },
    {
      "epoch": 0.403899721448468,
      "grad_norm": 1.1870371103286743,
      "learning_rate": 0.0009596378830083565,
      "loss": 2.4305,
      "step": 1450
    },
    {
      "epoch": 0.40668523676880225,
      "grad_norm": 1.314112663269043,
      "learning_rate": 0.0009593593314763232,
      "loss": 2.5646,
      "step": 1460
    },
    {
      "epoch": 0.40947075208913647,
      "grad_norm": 1.2753387689590454,
      "learning_rate": 0.0009590807799442897,
      "loss": 2.563,
      "step": 1470
    },
    {
      "epoch": 0.41225626740947074,
      "grad_norm": 1.3219434022903442,
      "learning_rate": 0.0009588022284122563,
      "loss": 2.716,
      "step": 1480
    },
    {
      "epoch": 0.415041782729805,
      "grad_norm": 1.1183065176010132,
      "learning_rate": 0.000958523676880223,
      "loss": 2.4716,
      "step": 1490
    },
    {
      "epoch": 0.4178272980501393,
      "grad_norm": 1.319581389427185,
      "learning_rate": 0.0009582451253481894,
      "loss": 2.5921,
      "step": 1500
    },
    {
      "epoch": 0.4206128133704735,
      "grad_norm": 2.1027991771698,
      "learning_rate": 0.000957966573816156,
      "loss": 2.6737,
      "step": 1510
    },
    {
      "epoch": 0.4233983286908078,
      "grad_norm": 1.2738131284713745,
      "learning_rate": 0.0009576880222841225,
      "loss": 2.6785,
      "step": 1520
    },
    {
      "epoch": 0.42618384401114207,
      "grad_norm": 1.0660815238952637,
      "learning_rate": 0.0009574094707520892,
      "loss": 2.5633,
      "step": 1530
    },
    {
      "epoch": 0.42896935933147634,
      "grad_norm": 1.5794754028320312,
      "learning_rate": 0.0009571309192200557,
      "loss": 2.5985,
      "step": 1540
    },
    {
      "epoch": 0.43175487465181056,
      "grad_norm": 1.278288722038269,
      "learning_rate": 0.0009568523676880223,
      "loss": 2.6308,
      "step": 1550
    },
    {
      "epoch": 0.43454038997214484,
      "grad_norm": 1.8339039087295532,
      "learning_rate": 0.0009565738161559889,
      "loss": 2.5413,
      "step": 1560
    },
    {
      "epoch": 0.4373259052924791,
      "grad_norm": 1.3860092163085938,
      "learning_rate": 0.0009562952646239555,
      "loss": 2.5972,
      "step": 1570
    },
    {
      "epoch": 0.4401114206128134,
      "grad_norm": 1.3290424346923828,
      "learning_rate": 0.0009560167130919221,
      "loss": 2.7117,
      "step": 1580
    },
    {
      "epoch": 0.4428969359331476,
      "grad_norm": 1.119259238243103,
      "learning_rate": 0.0009557381615598885,
      "loss": 2.6182,
      "step": 1590
    },
    {
      "epoch": 0.4456824512534819,
      "grad_norm": 1.2380826473236084,
      "learning_rate": 0.0009554596100278552,
      "loss": 2.5878,
      "step": 1600
    },
    {
      "epoch": 0.44846796657381616,
      "grad_norm": 1.062237024307251,
      "learning_rate": 0.0009551810584958217,
      "loss": 2.4906,
      "step": 1610
    },
    {
      "epoch": 0.45125348189415043,
      "grad_norm": 1.111541986465454,
      "learning_rate": 0.0009549025069637883,
      "loss": 2.6021,
      "step": 1620
    },
    {
      "epoch": 0.45403899721448465,
      "grad_norm": 1.7508482933044434,
      "learning_rate": 0.0009546239554317548,
      "loss": 2.6359,
      "step": 1630
    },
    {
      "epoch": 0.4568245125348189,
      "grad_norm": 1.2198450565338135,
      "learning_rate": 0.0009543454038997215,
      "loss": 2.471,
      "step": 1640
    },
    {
      "epoch": 0.4596100278551532,
      "grad_norm": 1.2403230667114258,
      "learning_rate": 0.0009540668523676881,
      "loss": 2.5615,
      "step": 1650
    },
    {
      "epoch": 0.4623955431754875,
      "grad_norm": 1.550818920135498,
      "learning_rate": 0.0009537883008356546,
      "loss": 2.8137,
      "step": 1660
    },
    {
      "epoch": 0.46518105849582175,
      "grad_norm": 1.2537591457366943,
      "learning_rate": 0.0009535097493036213,
      "loss": 2.6475,
      "step": 1670
    },
    {
      "epoch": 0.467966573816156,
      "grad_norm": 1.3265122175216675,
      "learning_rate": 0.0009532311977715878,
      "loss": 2.3956,
      "step": 1680
    },
    {
      "epoch": 0.47075208913649025,
      "grad_norm": 1.6242594718933105,
      "learning_rate": 0.0009529526462395543,
      "loss": 2.7789,
      "step": 1690
    },
    {
      "epoch": 0.4735376044568245,
      "grad_norm": 1.1088627576828003,
      "learning_rate": 0.0009526740947075208,
      "loss": 2.5779,
      "step": 1700
    },
    {
      "epoch": 0.4763231197771588,
      "grad_norm": 8.648368835449219,
      "learning_rate": 0.0009523955431754875,
      "loss": 2.5963,
      "step": 1710
    },
    {
      "epoch": 0.479108635097493,
      "grad_norm": 1.5467878580093384,
      "learning_rate": 0.0009521169916434541,
      "loss": 2.4657,
      "step": 1720
    },
    {
      "epoch": 0.4818941504178273,
      "grad_norm": 1.7226130962371826,
      "learning_rate": 0.0009518384401114206,
      "loss": 2.483,
      "step": 1730
    },
    {
      "epoch": 0.48467966573816157,
      "grad_norm": 2.051547050476074,
      "learning_rate": 0.0009515598885793872,
      "loss": 2.8117,
      "step": 1740
    },
    {
      "epoch": 0.48746518105849584,
      "grad_norm": 1.0624029636383057,
      "learning_rate": 0.0009512813370473538,
      "loss": 2.6645,
      "step": 1750
    },
    {
      "epoch": 0.49025069637883006,
      "grad_norm": 1.28592050075531,
      "learning_rate": 0.0009510027855153204,
      "loss": 2.5074,
      "step": 1760
    },
    {
      "epoch": 0.49303621169916434,
      "grad_norm": 1.4625672101974487,
      "learning_rate": 0.0009507242339832869,
      "loss": 2.5313,
      "step": 1770
    },
    {
      "epoch": 0.4958217270194986,
      "grad_norm": 1.2634835243225098,
      "learning_rate": 0.0009504456824512536,
      "loss": 2.5165,
      "step": 1780
    },
    {
      "epoch": 0.4986072423398329,
      "grad_norm": 1.3684110641479492,
      "learning_rate": 0.00095016713091922,
      "loss": 2.5164,
      "step": 1790
    },
    {
      "epoch": 0.5013927576601671,
      "grad_norm": 1.130091667175293,
      "learning_rate": 0.0009498885793871866,
      "loss": 2.5757,
      "step": 1800
    },
    {
      "epoch": 0.5041782729805014,
      "grad_norm": 1.12990403175354,
      "learning_rate": 0.0009496100278551532,
      "loss": 2.1494,
      "step": 1810
    },
    {
      "epoch": 0.5069637883008357,
      "grad_norm": 1.3642442226409912,
      "learning_rate": 0.0009493314763231198,
      "loss": 2.6036,
      "step": 1820
    },
    {
      "epoch": 0.5097493036211699,
      "grad_norm": 1.393731713294983,
      "learning_rate": 0.0009490529247910864,
      "loss": 2.6385,
      "step": 1830
    },
    {
      "epoch": 0.5125348189415042,
      "grad_norm": 1.2677972316741943,
      "learning_rate": 0.0009487743732590529,
      "loss": 2.2729,
      "step": 1840
    },
    {
      "epoch": 0.5153203342618384,
      "grad_norm": 1.065980315208435,
      "learning_rate": 0.0009484958217270196,
      "loss": 2.5502,
      "step": 1850
    },
    {
      "epoch": 0.5181058495821727,
      "grad_norm": 1.183836579322815,
      "learning_rate": 0.0009482172701949861,
      "loss": 2.6661,
      "step": 1860
    },
    {
      "epoch": 0.520891364902507,
      "grad_norm": 1.207627296447754,
      "learning_rate": 0.0009479387186629527,
      "loss": 2.7478,
      "step": 1870
    },
    {
      "epoch": 0.5236768802228412,
      "grad_norm": 1.589339017868042,
      "learning_rate": 0.0009476601671309193,
      "loss": 2.6694,
      "step": 1880
    },
    {
      "epoch": 0.5264623955431755,
      "grad_norm": 1.7953917980194092,
      "learning_rate": 0.0009473816155988858,
      "loss": 2.6079,
      "step": 1890
    },
    {
      "epoch": 0.5292479108635098,
      "grad_norm": 1.3865869045257568,
      "learning_rate": 0.0009471030640668524,
      "loss": 2.3076,
      "step": 1900
    },
    {
      "epoch": 0.532033426183844,
      "grad_norm": 1.3193280696868896,
      "learning_rate": 0.0009468245125348189,
      "loss": 2.6505,
      "step": 1910
    },
    {
      "epoch": 0.5348189415041783,
      "grad_norm": 1.3402987718582153,
      "learning_rate": 0.0009465459610027855,
      "loss": 2.5501,
      "step": 1920
    },
    {
      "epoch": 0.5376044568245125,
      "grad_norm": 1.2162214517593384,
      "learning_rate": 0.0009462674094707521,
      "loss": 2.6775,
      "step": 1930
    },
    {
      "epoch": 0.5403899721448467,
      "grad_norm": 1.2824859619140625,
      "learning_rate": 0.0009459888579387187,
      "loss": 2.3733,
      "step": 1940
    },
    {
      "epoch": 0.5431754874651811,
      "grad_norm": 1.4033740758895874,
      "learning_rate": 0.0009457103064066852,
      "loss": 2.6962,
      "step": 1950
    },
    {
      "epoch": 0.5459610027855153,
      "grad_norm": 1.0902607440948486,
      "learning_rate": 0.0009454317548746519,
      "loss": 2.6807,
      "step": 1960
    },
    {
      "epoch": 0.5487465181058496,
      "grad_norm": 1.2654460668563843,
      "learning_rate": 0.0009451532033426185,
      "loss": 2.5551,
      "step": 1970
    },
    {
      "epoch": 0.5515320334261838,
      "grad_norm": 1.5638848543167114,
      "learning_rate": 0.000944874651810585,
      "loss": 2.553,
      "step": 1980
    },
    {
      "epoch": 0.5543175487465181,
      "grad_norm": 1.1250978708267212,
      "learning_rate": 0.0009445961002785515,
      "loss": 2.3587,
      "step": 1990
    },
    {
      "epoch": 0.5571030640668524,
      "grad_norm": 1.1229901313781738,
      "learning_rate": 0.0009443175487465181,
      "loss": 2.5146,
      "step": 2000
    },
    {
      "epoch": 0.5598885793871866,
      "grad_norm": 2.048711061477661,
      "learning_rate": 0.0009440389972144847,
      "loss": 2.5139,
      "step": 2010
    },
    {
      "epoch": 0.5626740947075209,
      "grad_norm": 1.0248874425888062,
      "learning_rate": 0.0009437604456824512,
      "loss": 2.5052,
      "step": 2020
    },
    {
      "epoch": 0.5654596100278552,
      "grad_norm": 1.6054816246032715,
      "learning_rate": 0.0009434818941504178,
      "loss": 2.6075,
      "step": 2030
    },
    {
      "epoch": 0.5682451253481894,
      "grad_norm": 1.7019048929214478,
      "learning_rate": 0.0009432033426183845,
      "loss": 2.5007,
      "step": 2040
    },
    {
      "epoch": 0.5710306406685237,
      "grad_norm": 1.4104639291763306,
      "learning_rate": 0.000942924791086351,
      "loss": 2.5726,
      "step": 2050
    },
    {
      "epoch": 0.5738161559888579,
      "grad_norm": 1.005989909172058,
      "learning_rate": 0.0009426462395543176,
      "loss": 2.4801,
      "step": 2060
    },
    {
      "epoch": 0.5766016713091922,
      "grad_norm": 2.079470157623291,
      "learning_rate": 0.0009423676880222842,
      "loss": 2.4045,
      "step": 2070
    },
    {
      "epoch": 0.5793871866295265,
      "grad_norm": 1.0753381252288818,
      "learning_rate": 0.0009420891364902508,
      "loss": 2.6086,
      "step": 2080
    },
    {
      "epoch": 0.5821727019498607,
      "grad_norm": 1.86472749710083,
      "learning_rate": 0.0009418105849582172,
      "loss": 2.6492,
      "step": 2090
    },
    {
      "epoch": 0.584958217270195,
      "grad_norm": 1.3462709188461304,
      "learning_rate": 0.0009415320334261838,
      "loss": 2.5506,
      "step": 2100
    },
    {
      "epoch": 0.5877437325905293,
      "grad_norm": 1.1897573471069336,
      "learning_rate": 0.0009412534818941504,
      "loss": 2.6119,
      "step": 2110
    },
    {
      "epoch": 0.5905292479108635,
      "grad_norm": 1.4608758687973022,
      "learning_rate": 0.000940974930362117,
      "loss": 2.5802,
      "step": 2120
    },
    {
      "epoch": 0.5933147632311978,
      "grad_norm": 1.1187527179718018,
      "learning_rate": 0.0009406963788300836,
      "loss": 2.6517,
      "step": 2130
    },
    {
      "epoch": 0.596100278551532,
      "grad_norm": 1.1689943075180054,
      "learning_rate": 0.0009404178272980502,
      "loss": 2.657,
      "step": 2140
    },
    {
      "epoch": 0.5988857938718662,
      "grad_norm": 1.3489910364151,
      "learning_rate": 0.0009401392757660168,
      "loss": 2.4148,
      "step": 2150
    },
    {
      "epoch": 0.6016713091922006,
      "grad_norm": 1.0046050548553467,
      "learning_rate": 0.0009398607242339833,
      "loss": 2.4339,
      "step": 2160
    },
    {
      "epoch": 0.6044568245125348,
      "grad_norm": 1.2691969871520996,
      "learning_rate": 0.0009395821727019499,
      "loss": 2.5502,
      "step": 2170
    },
    {
      "epoch": 0.6072423398328691,
      "grad_norm": 1.0142344236373901,
      "learning_rate": 0.0009393036211699164,
      "loss": 2.4946,
      "step": 2180
    },
    {
      "epoch": 0.6100278551532033,
      "grad_norm": 1.096625566482544,
      "learning_rate": 0.000939025069637883,
      "loss": 2.6312,
      "step": 2190
    },
    {
      "epoch": 0.6128133704735376,
      "grad_norm": 1.1297039985656738,
      "learning_rate": 0.0009387465181058496,
      "loss": 2.7969,
      "step": 2200
    },
    {
      "epoch": 0.6155988857938719,
      "grad_norm": 1.0699799060821533,
      "learning_rate": 0.0009384679665738161,
      "loss": 2.5823,
      "step": 2210
    },
    {
      "epoch": 0.6183844011142061,
      "grad_norm": 1.2434641122817993,
      "learning_rate": 0.0009381894150417828,
      "loss": 2.3645,
      "step": 2220
    },
    {
      "epoch": 0.6211699164345403,
      "grad_norm": 1.1526702642440796,
      "learning_rate": 0.0009379108635097493,
      "loss": 2.5647,
      "step": 2230
    },
    {
      "epoch": 0.6239554317548747,
      "grad_norm": 1.0592986345291138,
      "learning_rate": 0.0009376323119777159,
      "loss": 2.3884,
      "step": 2240
    },
    {
      "epoch": 0.6267409470752089,
      "grad_norm": 1.2500194311141968,
      "learning_rate": 0.0009373537604456825,
      "loss": 2.5747,
      "step": 2250
    },
    {
      "epoch": 0.6295264623955432,
      "grad_norm": 1.203787922859192,
      "learning_rate": 0.0009370752089136491,
      "loss": 2.7068,
      "step": 2260
    },
    {
      "epoch": 0.6323119777158774,
      "grad_norm": 1.9098879098892212,
      "learning_rate": 0.0009367966573816156,
      "loss": 2.6968,
      "step": 2270
    },
    {
      "epoch": 0.6350974930362117,
      "grad_norm": 1.539093017578125,
      "learning_rate": 0.0009365181058495821,
      "loss": 2.6002,
      "step": 2280
    },
    {
      "epoch": 0.637883008356546,
      "grad_norm": 1.3226159811019897,
      "learning_rate": 0.0009362395543175488,
      "loss": 2.5498,
      "step": 2290
    },
    {
      "epoch": 0.6406685236768802,
      "grad_norm": 1.7727137804031372,
      "learning_rate": 0.0009359610027855153,
      "loss": 2.7561,
      "step": 2300
    },
    {
      "epoch": 0.6434540389972145,
      "grad_norm": 1.203458309173584,
      "learning_rate": 0.0009356824512534819,
      "loss": 2.5337,
      "step": 2310
    },
    {
      "epoch": 0.6462395543175488,
      "grad_norm": 1.4767539501190186,
      "learning_rate": 0.0009354038997214485,
      "loss": 2.5532,
      "step": 2320
    },
    {
      "epoch": 0.649025069637883,
      "grad_norm": 1.3017712831497192,
      "learning_rate": 0.0009351253481894151,
      "loss": 2.4511,
      "step": 2330
    },
    {
      "epoch": 0.6518105849582173,
      "grad_norm": 1.0526630878448486,
      "learning_rate": 0.0009348467966573816,
      "loss": 2.341,
      "step": 2340
    },
    {
      "epoch": 0.6545961002785515,
      "grad_norm": 1.6688122749328613,
      "learning_rate": 0.0009345682451253482,
      "loss": 2.467,
      "step": 2350
    },
    {
      "epoch": 0.6573816155988857,
      "grad_norm": 1.44296395778656,
      "learning_rate": 0.0009342896935933149,
      "loss": 2.5955,
      "step": 2360
    },
    {
      "epoch": 0.6601671309192201,
      "grad_norm": 1.079331398010254,
      "learning_rate": 0.0009340111420612814,
      "loss": 2.5893,
      "step": 2370
    },
    {
      "epoch": 0.6629526462395543,
      "grad_norm": 1.214622974395752,
      "learning_rate": 0.000933732590529248,
      "loss": 2.4437,
      "step": 2380
    },
    {
      "epoch": 0.6657381615598886,
      "grad_norm": 1.0948894023895264,
      "learning_rate": 0.0009334540389972144,
      "loss": 2.5016,
      "step": 2390
    },
    {
      "epoch": 0.6685236768802229,
      "grad_norm": 1.1991463899612427,
      "learning_rate": 0.0009331754874651811,
      "loss": 2.2909,
      "step": 2400
    },
    {
      "epoch": 0.6713091922005571,
      "grad_norm": 1.2566741704940796,
      "learning_rate": 0.0009328969359331476,
      "loss": 2.4968,
      "step": 2410
    },
    {
      "epoch": 0.6740947075208914,
      "grad_norm": 1.1851484775543213,
      "learning_rate": 0.0009326183844011142,
      "loss": 2.6157,
      "step": 2420
    },
    {
      "epoch": 0.6768802228412256,
      "grad_norm": 1.1660321950912476,
      "learning_rate": 0.0009323398328690808,
      "loss": 2.4936,
      "step": 2430
    },
    {
      "epoch": 0.6796657381615598,
      "grad_norm": 1.2320165634155273,
      "learning_rate": 0.0009320612813370474,
      "loss": 2.542,
      "step": 2440
    },
    {
      "epoch": 0.6824512534818942,
      "grad_norm": 1.4994531869888306,
      "learning_rate": 0.000931782729805014,
      "loss": 2.5761,
      "step": 2450
    },
    {
      "epoch": 0.6852367688022284,
      "grad_norm": 1.663185477256775,
      "learning_rate": 0.0009315041782729805,
      "loss": 2.5274,
      "step": 2460
    },
    {
      "epoch": 0.6880222841225627,
      "grad_norm": 1.5949238538742065,
      "learning_rate": 0.0009312256267409472,
      "loss": 2.5918,
      "step": 2470
    },
    {
      "epoch": 0.6908077994428969,
      "grad_norm": 1.25506591796875,
      "learning_rate": 0.0009309470752089136,
      "loss": 2.575,
      "step": 2480
    },
    {
      "epoch": 0.6935933147632312,
      "grad_norm": 1.0503790378570557,
      "learning_rate": 0.0009306685236768802,
      "loss": 2.5169,
      "step": 2490
    },
    {
      "epoch": 0.6963788300835655,
      "grad_norm": 1.3332326412200928,
      "learning_rate": 0.0009303899721448468,
      "loss": 2.4264,
      "step": 2500
    },
    {
      "epoch": 0.6991643454038997,
      "grad_norm": 1.1266738176345825,
      "learning_rate": 0.0009301114206128134,
      "loss": 2.6411,
      "step": 2510
    },
    {
      "epoch": 0.7019498607242339,
      "grad_norm": 1.415928840637207,
      "learning_rate": 0.0009298328690807799,
      "loss": 2.7668,
      "step": 2520
    },
    {
      "epoch": 0.7047353760445683,
      "grad_norm": 1.3131725788116455,
      "learning_rate": 0.0009295543175487465,
      "loss": 2.2644,
      "step": 2530
    },
    {
      "epoch": 0.7075208913649025,
      "grad_norm": 1.0362130403518677,
      "learning_rate": 0.0009292757660167132,
      "loss": 2.6041,
      "step": 2540
    },
    {
      "epoch": 0.7103064066852368,
      "grad_norm": 1.3209302425384521,
      "learning_rate": 0.0009289972144846797,
      "loss": 2.6287,
      "step": 2550
    },
    {
      "epoch": 0.713091922005571,
      "grad_norm": 1.0345901250839233,
      "learning_rate": 0.0009287186629526463,
      "loss": 2.5093,
      "step": 2560
    },
    {
      "epoch": 0.7158774373259053,
      "grad_norm": 1.51754891872406,
      "learning_rate": 0.0009284401114206127,
      "loss": 2.5549,
      "step": 2570
    },
    {
      "epoch": 0.7186629526462396,
      "grad_norm": 1.3754395246505737,
      "learning_rate": 0.0009281615598885794,
      "loss": 2.3826,
      "step": 2580
    },
    {
      "epoch": 0.7214484679665738,
      "grad_norm": 1.5002400875091553,
      "learning_rate": 0.0009278830083565459,
      "loss": 2.6596,
      "step": 2590
    },
    {
      "epoch": 0.724233983286908,
      "grad_norm": 0.9694018959999084,
      "learning_rate": 0.0009276044568245125,
      "loss": 2.5191,
      "step": 2600
    },
    {
      "epoch": 0.7270194986072424,
      "grad_norm": 1.1042295694351196,
      "learning_rate": 0.0009273259052924792,
      "loss": 2.5094,
      "step": 2610
    },
    {
      "epoch": 0.7298050139275766,
      "grad_norm": 1.4084506034851074,
      "learning_rate": 0.0009270473537604457,
      "loss": 2.5499,
      "step": 2620
    },
    {
      "epoch": 0.7325905292479109,
      "grad_norm": 1.189603567123413,
      "learning_rate": 0.0009267688022284123,
      "loss": 2.4433,
      "step": 2630
    },
    {
      "epoch": 0.7353760445682451,
      "grad_norm": 1.1173259019851685,
      "learning_rate": 0.0009264902506963788,
      "loss": 2.4789,
      "step": 2640
    },
    {
      "epoch": 0.7381615598885793,
      "grad_norm": 1.520937204360962,
      "learning_rate": 0.0009262116991643455,
      "loss": 2.7348,
      "step": 2650
    },
    {
      "epoch": 0.7409470752089137,
      "grad_norm": 1.2023638486862183,
      "learning_rate": 0.000925933147632312,
      "loss": 2.4222,
      "step": 2660
    },
    {
      "epoch": 0.7437325905292479,
      "grad_norm": 1.3705735206604004,
      "learning_rate": 0.0009256545961002786,
      "loss": 2.5702,
      "step": 2670
    },
    {
      "epoch": 0.7465181058495822,
      "grad_norm": 1.2582453489303589,
      "learning_rate": 0.0009253760445682451,
      "loss": 2.276,
      "step": 2680
    },
    {
      "epoch": 0.7493036211699164,
      "grad_norm": 1.1726007461547852,
      "learning_rate": 0.0009250974930362117,
      "loss": 2.509,
      "step": 2690
    },
    {
      "epoch": 0.7520891364902507,
      "grad_norm": 1.6141993999481201,
      "learning_rate": 0.0009248189415041783,
      "loss": 2.4536,
      "step": 2700
    },
    {
      "epoch": 0.754874651810585,
      "grad_norm": 2.3008360862731934,
      "learning_rate": 0.0009245403899721448,
      "loss": 2.5664,
      "step": 2710
    },
    {
      "epoch": 0.7576601671309192,
      "grad_norm": 1.2353569269180298,
      "learning_rate": 0.0009242618384401115,
      "loss": 2.5003,
      "step": 2720
    },
    {
      "epoch": 0.7604456824512534,
      "grad_norm": 1.112949252128601,
      "learning_rate": 0.000923983286908078,
      "loss": 2.5316,
      "step": 2730
    },
    {
      "epoch": 0.7632311977715878,
      "grad_norm": 1.1286025047302246,
      "learning_rate": 0.0009237047353760446,
      "loss": 2.4426,
      "step": 2740
    },
    {
      "epoch": 0.766016713091922,
      "grad_norm": 1.2434155941009521,
      "learning_rate": 0.0009234261838440111,
      "loss": 2.6419,
      "step": 2750
    },
    {
      "epoch": 0.7688022284122563,
      "grad_norm": 1.7417652606964111,
      "learning_rate": 0.0009231476323119778,
      "loss": 2.312,
      "step": 2760
    },
    {
      "epoch": 0.7715877437325905,
      "grad_norm": 1.3058805465698242,
      "learning_rate": 0.0009228690807799444,
      "loss": 2.52,
      "step": 2770
    },
    {
      "epoch": 0.7743732590529248,
      "grad_norm": 1.5557122230529785,
      "learning_rate": 0.0009225905292479108,
      "loss": 2.3995,
      "step": 2780
    },
    {
      "epoch": 0.7771587743732591,
      "grad_norm": 1.3772742748260498,
      "learning_rate": 0.0009223119777158775,
      "loss": 2.7236,
      "step": 2790
    },
    {
      "epoch": 0.7799442896935933,
      "grad_norm": 1.1548471450805664,
      "learning_rate": 0.000922033426183844,
      "loss": 2.3827,
      "step": 2800
    },
    {
      "epoch": 0.7827298050139275,
      "grad_norm": 2.149418830871582,
      "learning_rate": 0.0009217548746518106,
      "loss": 2.5081,
      "step": 2810
    },
    {
      "epoch": 0.7855153203342619,
      "grad_norm": 1.384088397026062,
      "learning_rate": 0.0009214763231197771,
      "loss": 2.6378,
      "step": 2820
    },
    {
      "epoch": 0.7883008356545961,
      "grad_norm": 1.4311349391937256,
      "learning_rate": 0.0009211977715877438,
      "loss": 2.7217,
      "step": 2830
    },
    {
      "epoch": 0.7910863509749304,
      "grad_norm": 1.71019446849823,
      "learning_rate": 0.0009209192200557103,
      "loss": 2.5329,
      "step": 2840
    },
    {
      "epoch": 0.7938718662952646,
      "grad_norm": 1.2393792867660522,
      "learning_rate": 0.0009206406685236769,
      "loss": 2.403,
      "step": 2850
    },
    {
      "epoch": 0.7966573816155988,
      "grad_norm": 1.1013095378875732,
      "learning_rate": 0.0009203621169916436,
      "loss": 2.4774,
      "step": 2860
    },
    {
      "epoch": 0.7994428969359332,
      "grad_norm": 1.4328322410583496,
      "learning_rate": 0.00092008356545961,
      "loss": 2.2297,
      "step": 2870
    },
    {
      "epoch": 0.8022284122562674,
      "grad_norm": 1.6561702489852905,
      "learning_rate": 0.0009198050139275766,
      "loss": 2.4779,
      "step": 2880
    },
    {
      "epoch": 0.8050139275766016,
      "grad_norm": 1.2476462125778198,
      "learning_rate": 0.0009195264623955431,
      "loss": 2.4749,
      "step": 2890
    },
    {
      "epoch": 0.807799442896936,
      "grad_norm": 0.8828177452087402,
      "learning_rate": 0.0009192479108635098,
      "loss": 2.5131,
      "step": 2900
    },
    {
      "epoch": 0.8105849582172702,
      "grad_norm": 1.2814828157424927,
      "learning_rate": 0.0009189693593314763,
      "loss": 2.5885,
      "step": 2910
    },
    {
      "epoch": 0.8133704735376045,
      "grad_norm": 1.216457724571228,
      "learning_rate": 0.0009186908077994429,
      "loss": 2.5715,
      "step": 2920
    },
    {
      "epoch": 0.8161559888579387,
      "grad_norm": 1.5148258209228516,
      "learning_rate": 0.0009184122562674095,
      "loss": 2.4477,
      "step": 2930
    },
    {
      "epoch": 0.8189415041782729,
      "grad_norm": 1.8793452978134155,
      "learning_rate": 0.0009181337047353761,
      "loss": 2.548,
      "step": 2940
    },
    {
      "epoch": 0.8217270194986073,
      "grad_norm": 1.3416855335235596,
      "learning_rate": 0.0009178551532033427,
      "loss": 2.6586,
      "step": 2950
    },
    {
      "epoch": 0.8245125348189415,
      "grad_norm": 1.4540530443191528,
      "learning_rate": 0.0009175766016713092,
      "loss": 2.7244,
      "step": 2960
    },
    {
      "epoch": 0.8272980501392758,
      "grad_norm": 1.309016227722168,
      "learning_rate": 0.0009172980501392759,
      "loss": 2.4218,
      "step": 2970
    },
    {
      "epoch": 0.83008356545961,
      "grad_norm": 1.7011269330978394,
      "learning_rate": 0.0009170194986072423,
      "loss": 2.5071,
      "step": 2980
    },
    {
      "epoch": 0.8328690807799443,
      "grad_norm": 1.927377462387085,
      "learning_rate": 0.0009167409470752089,
      "loss": 2.4459,
      "step": 2990
    },
    {
      "epoch": 0.8356545961002786,
      "grad_norm": 1.1857326030731201,
      "learning_rate": 0.0009164623955431754,
      "loss": 2.4978,
      "step": 3000
    },
    {
      "epoch": 0.8384401114206128,
      "grad_norm": 1.2392797470092773,
      "learning_rate": 0.0009161838440111421,
      "loss": 2.5065,
      "step": 3010
    },
    {
      "epoch": 0.841225626740947,
      "grad_norm": 1.5550098419189453,
      "learning_rate": 0.0009159052924791087,
      "loss": 2.4555,
      "step": 3020
    },
    {
      "epoch": 0.8440111420612814,
      "grad_norm": 1.5402288436889648,
      "learning_rate": 0.0009156267409470752,
      "loss": 2.4129,
      "step": 3030
    },
    {
      "epoch": 0.8467966573816156,
      "grad_norm": 1.3559483289718628,
      "learning_rate": 0.0009153481894150419,
      "loss": 2.5606,
      "step": 3040
    },
    {
      "epoch": 0.8495821727019499,
      "grad_norm": 1.3642971515655518,
      "learning_rate": 0.0009150696378830084,
      "loss": 2.6879,
      "step": 3050
    },
    {
      "epoch": 0.8523676880222841,
      "grad_norm": 1.137377381324768,
      "learning_rate": 0.000914791086350975,
      "loss": 2.6803,
      "step": 3060
    },
    {
      "epoch": 0.8551532033426184,
      "grad_norm": 1.2168655395507812,
      "learning_rate": 0.0009145125348189414,
      "loss": 2.5603,
      "step": 3070
    },
    {
      "epoch": 0.8579387186629527,
      "grad_norm": 1.461601972579956,
      "learning_rate": 0.0009142339832869081,
      "loss": 2.4968,
      "step": 3080
    },
    {
      "epoch": 0.8607242339832869,
      "grad_norm": 1.6744568347930908,
      "learning_rate": 0.0009139554317548747,
      "loss": 2.4586,
      "step": 3090
    },
    {
      "epoch": 0.8635097493036211,
      "grad_norm": 1.3374288082122803,
      "learning_rate": 0.0009136768802228412,
      "loss": 2.4106,
      "step": 3100
    },
    {
      "epoch": 0.8662952646239555,
      "grad_norm": 2.210033655166626,
      "learning_rate": 0.0009133983286908078,
      "loss": 2.5373,
      "step": 3110
    },
    {
      "epoch": 0.8690807799442897,
      "grad_norm": 1.2114864587783813,
      "learning_rate": 0.0009131197771587744,
      "loss": 2.4437,
      "step": 3120
    },
    {
      "epoch": 0.871866295264624,
      "grad_norm": 1.506703495979309,
      "learning_rate": 0.000912841225626741,
      "loss": 2.6351,
      "step": 3130
    },
    {
      "epoch": 0.8746518105849582,
      "grad_norm": 1.183419942855835,
      "learning_rate": 0.0009125626740947075,
      "loss": 2.5452,
      "step": 3140
    },
    {
      "epoch": 0.8774373259052924,
      "grad_norm": 1.8683195114135742,
      "learning_rate": 0.0009122841225626742,
      "loss": 2.5449,
      "step": 3150
    },
    {
      "epoch": 0.8802228412256268,
      "grad_norm": 1.2449302673339844,
      "learning_rate": 0.0009120055710306407,
      "loss": 2.5588,
      "step": 3160
    },
    {
      "epoch": 0.883008356545961,
      "grad_norm": 1.5908424854278564,
      "learning_rate": 0.0009117270194986072,
      "loss": 2.5872,
      "step": 3170
    },
    {
      "epoch": 0.8857938718662952,
      "grad_norm": 1.3939064741134644,
      "learning_rate": 0.0009114484679665738,
      "loss": 2.3256,
      "step": 3180
    },
    {
      "epoch": 0.8885793871866295,
      "grad_norm": 1.0585473775863647,
      "learning_rate": 0.0009111699164345404,
      "loss": 2.5549,
      "step": 3190
    },
    {
      "epoch": 0.8913649025069638,
      "grad_norm": 1.27499258518219,
      "learning_rate": 0.000910891364902507,
      "loss": 2.2818,
      "step": 3200
    },
    {
      "epoch": 0.8941504178272981,
      "grad_norm": 1.499476671218872,
      "learning_rate": 0.0009106128133704735,
      "loss": 2.7133,
      "step": 3210
    },
    {
      "epoch": 0.8969359331476323,
      "grad_norm": 1.184173583984375,
      "learning_rate": 0.0009103342618384402,
      "loss": 2.554,
      "step": 3220
    },
    {
      "epoch": 0.8997214484679665,
      "grad_norm": 2.019127607345581,
      "learning_rate": 0.0009100557103064067,
      "loss": 2.4746,
      "step": 3230
    },
    {
      "epoch": 0.9025069637883009,
      "grad_norm": 1.4537019729614258,
      "learning_rate": 0.0009097771587743733,
      "loss": 2.4752,
      "step": 3240
    },
    {
      "epoch": 0.9052924791086351,
      "grad_norm": 1.377322793006897,
      "learning_rate": 0.0009094986072423399,
      "loss": 2.672,
      "step": 3250
    },
    {
      "epoch": 0.9080779944289693,
      "grad_norm": 1.1086807250976562,
      "learning_rate": 0.0009092200557103065,
      "loss": 2.5019,
      "step": 3260
    },
    {
      "epoch": 0.9108635097493036,
      "grad_norm": 1.18252694606781,
      "learning_rate": 0.000908941504178273,
      "loss": 2.5586,
      "step": 3270
    },
    {
      "epoch": 0.9136490250696379,
      "grad_norm": 1.5163630247116089,
      "learning_rate": 0.0009086629526462395,
      "loss": 2.3507,
      "step": 3280
    },
    {
      "epoch": 0.9164345403899722,
      "grad_norm": 1.8268383741378784,
      "learning_rate": 0.0009083844011142061,
      "loss": 2.4914,
      "step": 3290
    },
    {
      "epoch": 0.9192200557103064,
      "grad_norm": 1.286784291267395,
      "learning_rate": 0.0009081058495821727,
      "loss": 2.5713,
      "step": 3300
    },
    {
      "epoch": 0.9220055710306406,
      "grad_norm": 1.1048730611801147,
      "learning_rate": 0.0009078272980501393,
      "loss": 2.287,
      "step": 3310
    },
    {
      "epoch": 0.924791086350975,
      "grad_norm": 1.727943778038025,
      "learning_rate": 0.0009075487465181058,
      "loss": 2.6129,
      "step": 3320
    },
    {
      "epoch": 0.9275766016713092,
      "grad_norm": 1.3459112644195557,
      "learning_rate": 0.0009072701949860725,
      "loss": 2.5085,
      "step": 3330
    },
    {
      "epoch": 0.9303621169916435,
      "grad_norm": 1.5971940755844116,
      "learning_rate": 0.0009069916434540391,
      "loss": 2.6632,
      "step": 3340
    },
    {
      "epoch": 0.9331476323119777,
      "grad_norm": 1.2070146799087524,
      "learning_rate": 0.0009067130919220056,
      "loss": 2.4827,
      "step": 3350
    },
    {
      "epoch": 0.935933147632312,
      "grad_norm": 1.0728448629379272,
      "learning_rate": 0.0009064345403899722,
      "loss": 2.4516,
      "step": 3360
    },
    {
      "epoch": 0.9387186629526463,
      "grad_norm": 2.1424288749694824,
      "learning_rate": 0.0009061559888579387,
      "loss": 2.513,
      "step": 3370
    },
    {
      "epoch": 0.9415041782729805,
      "grad_norm": 1.658043622970581,
      "learning_rate": 0.0009058774373259053,
      "loss": 2.477,
      "step": 3380
    },
    {
      "epoch": 0.9442896935933147,
      "grad_norm": 1.8323450088500977,
      "learning_rate": 0.0009055988857938718,
      "loss": 2.5106,
      "step": 3390
    },
    {
      "epoch": 0.947075208913649,
      "grad_norm": 0.9170251488685608,
      "learning_rate": 0.0009053203342618385,
      "loss": 2.3986,
      "step": 3400
    },
    {
      "epoch": 0.9498607242339833,
      "grad_norm": 1.5963199138641357,
      "learning_rate": 0.0009050417827298051,
      "loss": 2.5241,
      "step": 3410
    },
    {
      "epoch": 0.9526462395543176,
      "grad_norm": 1.1471236944198608,
      "learning_rate": 0.0009047632311977716,
      "loss": 2.6149,
      "step": 3420
    },
    {
      "epoch": 0.9554317548746518,
      "grad_norm": 1.4615322351455688,
      "learning_rate": 0.0009044846796657382,
      "loss": 2.6316,
      "step": 3430
    },
    {
      "epoch": 0.958217270194986,
      "grad_norm": 1.2425521612167358,
      "learning_rate": 0.0009042061281337048,
      "loss": 2.5847,
      "step": 3440
    },
    {
      "epoch": 0.9610027855153204,
      "grad_norm": 1.3988072872161865,
      "learning_rate": 0.0009039275766016714,
      "loss": 2.3965,
      "step": 3450
    },
    {
      "epoch": 0.9637883008356546,
      "grad_norm": 1.579912781715393,
      "learning_rate": 0.0009036490250696378,
      "loss": 2.5922,
      "step": 3460
    },
    {
      "epoch": 0.9665738161559888,
      "grad_norm": 1.555151104927063,
      "learning_rate": 0.0009033704735376044,
      "loss": 2.8832,
      "step": 3470
    },
    {
      "epoch": 0.9693593314763231,
      "grad_norm": 1.6257613897323608,
      "learning_rate": 0.000903091922005571,
      "loss": 2.551,
      "step": 3480
    },
    {
      "epoch": 0.9721448467966574,
      "grad_norm": 1.2166742086410522,
      "learning_rate": 0.0009028133704735376,
      "loss": 2.5677,
      "step": 3490
    },
    {
      "epoch": 0.9749303621169917,
      "grad_norm": 1.5265882015228271,
      "learning_rate": 0.0009025348189415042,
      "loss": 2.4385,
      "step": 3500
    },
    {
      "epoch": 0.9777158774373259,
      "grad_norm": 1.8979231119155884,
      "learning_rate": 0.0009022562674094708,
      "loss": 2.3441,
      "step": 3510
    },
    {
      "epoch": 0.9805013927576601,
      "grad_norm": 1.2076042890548706,
      "learning_rate": 0.0009019777158774374,
      "loss": 2.5314,
      "step": 3520
    },
    {
      "epoch": 0.9832869080779945,
      "grad_norm": 1.588383674621582,
      "learning_rate": 0.0009016991643454039,
      "loss": 2.4564,
      "step": 3530
    },
    {
      "epoch": 0.9860724233983287,
      "grad_norm": 2.211045503616333,
      "learning_rate": 0.0009014206128133705,
      "loss": 2.4851,
      "step": 3540
    },
    {
      "epoch": 0.9888579387186629,
      "grad_norm": 1.1218012571334839,
      "learning_rate": 0.0009011420612813371,
      "loss": 2.4948,
      "step": 3550
    },
    {
      "epoch": 0.9916434540389972,
      "grad_norm": 1.4403835535049438,
      "learning_rate": 0.0009008635097493037,
      "loss": 2.2573,
      "step": 3560
    },
    {
      "epoch": 0.9944289693593314,
      "grad_norm": 1.191651463508606,
      "learning_rate": 0.0009005849582172702,
      "loss": 2.4785,
      "step": 3570
    },
    {
      "epoch": 0.9972144846796658,
      "grad_norm": 1.260407567024231,
      "learning_rate": 0.0009003064066852367,
      "loss": 2.6673,
      "step": 3580
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3984650373458862,
      "learning_rate": 0.0009000278551532034,
      "loss": 2.3738,
      "step": 3590
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.4115781784057617,
      "eval_runtime": 5.9989,
      "eval_samples_per_second": 532.093,
      "eval_steps_per_second": 66.512,
      "step": 3590
    },
    {
      "epoch": 1.0027855153203342,
      "grad_norm": 1.441165566444397,
      "learning_rate": 0.0008997493036211699,
      "loss": 2.4452,
      "step": 3600
    },
    {
      "epoch": 1.0055710306406684,
      "grad_norm": 1.1580885648727417,
      "learning_rate": 0.0008994707520891365,
      "loss": 2.436,
      "step": 3610
    },
    {
      "epoch": 1.0083565459610029,
      "grad_norm": 1.5686877965927124,
      "learning_rate": 0.0008991922005571031,
      "loss": 2.7319,
      "step": 3620
    },
    {
      "epoch": 1.011142061281337,
      "grad_norm": 1.2834532260894775,
      "learning_rate": 0.0008989136490250697,
      "loss": 2.5854,
      "step": 3630
    },
    {
      "epoch": 1.0139275766016713,
      "grad_norm": 1.102574110031128,
      "learning_rate": 0.0008986350974930362,
      "loss": 2.4751,
      "step": 3640
    },
    {
      "epoch": 1.0167130919220055,
      "grad_norm": 1.3247770071029663,
      "learning_rate": 0.0008983565459610028,
      "loss": 2.4198,
      "step": 3650
    },
    {
      "epoch": 1.0194986072423398,
      "grad_norm": 1.1859385967254639,
      "learning_rate": 0.0008980779944289695,
      "loss": 2.4973,
      "step": 3660
    },
    {
      "epoch": 1.0222841225626742,
      "grad_norm": 1.0926506519317627,
      "learning_rate": 0.0008977994428969359,
      "loss": 2.487,
      "step": 3670
    },
    {
      "epoch": 1.0250696378830084,
      "grad_norm": 1.2439014911651611,
      "learning_rate": 0.0008975208913649025,
      "loss": 2.3085,
      "step": 3680
    },
    {
      "epoch": 1.0278551532033426,
      "grad_norm": 1.6597952842712402,
      "learning_rate": 0.0008972423398328691,
      "loss": 2.5289,
      "step": 3690
    },
    {
      "epoch": 1.0306406685236769,
      "grad_norm": 1.4079557657241821,
      "learning_rate": 0.0008969637883008357,
      "loss": 2.5859,
      "step": 3700
    },
    {
      "epoch": 1.033426183844011,
      "grad_norm": 1.1636780500411987,
      "learning_rate": 0.0008966852367688022,
      "loss": 2.5076,
      "step": 3710
    },
    {
      "epoch": 1.0362116991643453,
      "grad_norm": 1.7774262428283691,
      "learning_rate": 0.0008964066852367688,
      "loss": 2.434,
      "step": 3720
    },
    {
      "epoch": 1.0389972144846797,
      "grad_norm": 2.285799980163574,
      "learning_rate": 0.0008961281337047355,
      "loss": 2.5291,
      "step": 3730
    },
    {
      "epoch": 1.041782729805014,
      "grad_norm": 1.266107439994812,
      "learning_rate": 0.000895849582172702,
      "loss": 2.4534,
      "step": 3740
    },
    {
      "epoch": 1.0445682451253482,
      "grad_norm": 1.65928316116333,
      "learning_rate": 0.0008955710306406686,
      "loss": 2.3141,
      "step": 3750
    },
    {
      "epoch": 1.0473537604456824,
      "grad_norm": 3.5317509174346924,
      "learning_rate": 0.000895292479108635,
      "loss": 2.4969,
      "step": 3760
    },
    {
      "epoch": 1.0501392757660166,
      "grad_norm": 1.1920363903045654,
      "learning_rate": 0.0008950139275766017,
      "loss": 2.424,
      "step": 3770
    },
    {
      "epoch": 1.052924791086351,
      "grad_norm": 1.5603898763656616,
      "learning_rate": 0.0008947353760445682,
      "loss": 2.4022,
      "step": 3780
    },
    {
      "epoch": 1.0557103064066853,
      "grad_norm": 1.4640552997589111,
      "learning_rate": 0.0008944568245125348,
      "loss": 2.6232,
      "step": 3790
    },
    {
      "epoch": 1.0584958217270195,
      "grad_norm": 1.0775586366653442,
      "learning_rate": 0.0008941782729805014,
      "loss": 2.3704,
      "step": 3800
    },
    {
      "epoch": 1.0612813370473537,
      "grad_norm": 2.666846990585327,
      "learning_rate": 0.000893899721448468,
      "loss": 2.373,
      "step": 3810
    },
    {
      "epoch": 1.064066852367688,
      "grad_norm": 1.6552537679672241,
      "learning_rate": 0.0008936211699164346,
      "loss": 2.4768,
      "step": 3820
    },
    {
      "epoch": 1.0668523676880224,
      "grad_norm": 1.1541993618011475,
      "learning_rate": 0.0008933426183844011,
      "loss": 2.6049,
      "step": 3830
    },
    {
      "epoch": 1.0696378830083566,
      "grad_norm": 1.4340739250183105,
      "learning_rate": 0.0008930640668523678,
      "loss": 2.4749,
      "step": 3840
    },
    {
      "epoch": 1.0724233983286908,
      "grad_norm": 1.119301438331604,
      "learning_rate": 0.0008927855153203343,
      "loss": 2.3993,
      "step": 3850
    },
    {
      "epoch": 1.075208913649025,
      "grad_norm": 1.3327243328094482,
      "learning_rate": 0.0008925069637883008,
      "loss": 2.593,
      "step": 3860
    },
    {
      "epoch": 1.0779944289693593,
      "grad_norm": 1.6743268966674805,
      "learning_rate": 0.0008922284122562674,
      "loss": 2.457,
      "step": 3870
    },
    {
      "epoch": 1.0807799442896937,
      "grad_norm": 1.2398854494094849,
      "learning_rate": 0.000891949860724234,
      "loss": 2.3759,
      "step": 3880
    },
    {
      "epoch": 1.083565459610028,
      "grad_norm": 1.5862865447998047,
      "learning_rate": 0.0008916713091922006,
      "loss": 2.4813,
      "step": 3890
    },
    {
      "epoch": 1.0863509749303621,
      "grad_norm": 1.331459403038025,
      "learning_rate": 0.0008913927576601671,
      "loss": 2.5459,
      "step": 3900
    },
    {
      "epoch": 1.0891364902506964,
      "grad_norm": 1.810538411140442,
      "learning_rate": 0.0008911142061281338,
      "loss": 2.4292,
      "step": 3910
    },
    {
      "epoch": 1.0919220055710306,
      "grad_norm": 1.3181859254837036,
      "learning_rate": 0.0008908356545961003,
      "loss": 2.6004,
      "step": 3920
    },
    {
      "epoch": 1.0947075208913648,
      "grad_norm": 1.4223825931549072,
      "learning_rate": 0.0008905571030640669,
      "loss": 2.3024,
      "step": 3930
    },
    {
      "epoch": 1.0974930362116992,
      "grad_norm": 1.360426425933838,
      "learning_rate": 0.0008902785515320334,
      "loss": 2.5646,
      "step": 3940
    },
    {
      "epoch": 1.1002785515320335,
      "grad_norm": 1.7869102954864502,
      "learning_rate": 0.0008900000000000001,
      "loss": 2.4077,
      "step": 3950
    },
    {
      "epoch": 1.1030640668523677,
      "grad_norm": 1.1787109375,
      "learning_rate": 0.0008897214484679665,
      "loss": 2.3381,
      "step": 3960
    },
    {
      "epoch": 1.105849582172702,
      "grad_norm": 1.3338098526000977,
      "learning_rate": 0.0008894428969359331,
      "loss": 2.3283,
      "step": 3970
    },
    {
      "epoch": 1.1086350974930361,
      "grad_norm": 1.3834604024887085,
      "learning_rate": 0.0008891643454038998,
      "loss": 2.5309,
      "step": 3980
    },
    {
      "epoch": 1.1114206128133706,
      "grad_norm": 1.460176706314087,
      "learning_rate": 0.0008888857938718663,
      "loss": 2.3667,
      "step": 3990
    },
    {
      "epoch": 1.1142061281337048,
      "grad_norm": 1.8518205881118774,
      "learning_rate": 0.0008886072423398329,
      "loss": 2.241,
      "step": 4000
    },
    {
      "epoch": 1.116991643454039,
      "grad_norm": 1.553539752960205,
      "learning_rate": 0.0008883286908077994,
      "loss": 2.6345,
      "step": 4010
    },
    {
      "epoch": 1.1197771587743732,
      "grad_norm": 1.398406982421875,
      "learning_rate": 0.0008880501392757661,
      "loss": 2.5813,
      "step": 4020
    },
    {
      "epoch": 1.1225626740947074,
      "grad_norm": 1.5909459590911865,
      "learning_rate": 0.0008877715877437326,
      "loss": 2.4838,
      "step": 4030
    },
    {
      "epoch": 1.1253481894150417,
      "grad_norm": 1.4152133464813232,
      "learning_rate": 0.0008874930362116992,
      "loss": 2.4599,
      "step": 4040
    },
    {
      "epoch": 1.128133704735376,
      "grad_norm": 1.5470185279846191,
      "learning_rate": 0.0008872144846796659,
      "loss": 2.5011,
      "step": 4050
    },
    {
      "epoch": 1.1309192200557103,
      "grad_norm": 1.5291905403137207,
      "learning_rate": 0.0008869359331476323,
      "loss": 2.5202,
      "step": 4060
    },
    {
      "epoch": 1.1337047353760445,
      "grad_norm": 1.3582431077957153,
      "learning_rate": 0.0008866573816155989,
      "loss": 2.4284,
      "step": 4070
    },
    {
      "epoch": 1.1364902506963788,
      "grad_norm": 1.4645233154296875,
      "learning_rate": 0.0008863788300835654,
      "loss": 2.5524,
      "step": 4080
    },
    {
      "epoch": 1.1392757660167132,
      "grad_norm": 1.430952548980713,
      "learning_rate": 0.0008861002785515321,
      "loss": 2.4664,
      "step": 4090
    },
    {
      "epoch": 1.1420612813370474,
      "grad_norm": 1.1082100868225098,
      "learning_rate": 0.0008858217270194986,
      "loss": 2.4975,
      "step": 4100
    },
    {
      "epoch": 1.1448467966573816,
      "grad_norm": 1.9331750869750977,
      "learning_rate": 0.0008855431754874652,
      "loss": 2.6311,
      "step": 4110
    },
    {
      "epoch": 1.1476323119777159,
      "grad_norm": 1.3669627904891968,
      "learning_rate": 0.0008852646239554317,
      "loss": 2.4317,
      "step": 4120
    },
    {
      "epoch": 1.15041782729805,
      "grad_norm": 1.1022802591323853,
      "learning_rate": 0.0008849860724233984,
      "loss": 2.3322,
      "step": 4130
    },
    {
      "epoch": 1.1532033426183843,
      "grad_norm": 2.19382643699646,
      "learning_rate": 0.000884707520891365,
      "loss": 2.4829,
      "step": 4140
    },
    {
      "epoch": 1.1559888579387188,
      "grad_norm": 1.6811145544052124,
      "learning_rate": 0.0008844289693593314,
      "loss": 2.4562,
      "step": 4150
    },
    {
      "epoch": 1.158774373259053,
      "grad_norm": 1.306420922279358,
      "learning_rate": 0.0008841504178272981,
      "loss": 2.2867,
      "step": 4160
    },
    {
      "epoch": 1.1615598885793872,
      "grad_norm": 1.4247044324874878,
      "learning_rate": 0.0008838718662952646,
      "loss": 2.387,
      "step": 4170
    },
    {
      "epoch": 1.1643454038997214,
      "grad_norm": 1.995288610458374,
      "learning_rate": 0.0008835933147632312,
      "loss": 2.5947,
      "step": 4180
    },
    {
      "epoch": 1.1671309192200556,
      "grad_norm": 1.206483244895935,
      "learning_rate": 0.0008833147632311977,
      "loss": 2.3207,
      "step": 4190
    },
    {
      "epoch": 1.16991643454039,
      "grad_norm": 1.666409969329834,
      "learning_rate": 0.0008830362116991644,
      "loss": 2.6924,
      "step": 4200
    },
    {
      "epoch": 1.1727019498607243,
      "grad_norm": 1.3321566581726074,
      "learning_rate": 0.0008827576601671309,
      "loss": 2.4377,
      "step": 4210
    },
    {
      "epoch": 1.1754874651810585,
      "grad_norm": 1.1819018125534058,
      "learning_rate": 0.0008824791086350975,
      "loss": 2.4964,
      "step": 4220
    },
    {
      "epoch": 1.1782729805013927,
      "grad_norm": 1.2121493816375732,
      "learning_rate": 0.0008822005571030642,
      "loss": 2.36,
      "step": 4230
    },
    {
      "epoch": 1.181058495821727,
      "grad_norm": 1.5262370109558105,
      "learning_rate": 0.0008819220055710307,
      "loss": 2.5002,
      "step": 4240
    },
    {
      "epoch": 1.1838440111420612,
      "grad_norm": 1.691681146621704,
      "learning_rate": 0.0008816434540389973,
      "loss": 2.4905,
      "step": 4250
    },
    {
      "epoch": 1.1866295264623956,
      "grad_norm": 1.288764476776123,
      "learning_rate": 0.0008813649025069637,
      "loss": 2.5145,
      "step": 4260
    },
    {
      "epoch": 1.1894150417827298,
      "grad_norm": 1.8767213821411133,
      "learning_rate": 0.0008810863509749304,
      "loss": 2.4486,
      "step": 4270
    },
    {
      "epoch": 1.192200557103064,
      "grad_norm": 1.7199182510375977,
      "learning_rate": 0.0008808077994428969,
      "loss": 2.6236,
      "step": 4280
    },
    {
      "epoch": 1.1949860724233983,
      "grad_norm": 1.5484123229980469,
      "learning_rate": 0.0008805292479108635,
      "loss": 2.4287,
      "step": 4290
    },
    {
      "epoch": 1.1977715877437327,
      "grad_norm": 1.8447967767715454,
      "learning_rate": 0.0008802506963788301,
      "loss": 2.3652,
      "step": 4300
    },
    {
      "epoch": 1.200557103064067,
      "grad_norm": 1.3724100589752197,
      "learning_rate": 0.0008799721448467967,
      "loss": 2.4281,
      "step": 4310
    },
    {
      "epoch": 1.2033426183844012,
      "grad_norm": 1.733373999595642,
      "learning_rate": 0.0008796935933147633,
      "loss": 2.4655,
      "step": 4320
    },
    {
      "epoch": 1.2061281337047354,
      "grad_norm": 1.606735348701477,
      "learning_rate": 0.0008794150417827298,
      "loss": 2.4621,
      "step": 4330
    },
    {
      "epoch": 1.2089136490250696,
      "grad_norm": 1.4502986669540405,
      "learning_rate": 0.0008791364902506965,
      "loss": 2.5805,
      "step": 4340
    },
    {
      "epoch": 1.2116991643454038,
      "grad_norm": 1.7336798906326294,
      "learning_rate": 0.000878857938718663,
      "loss": 2.3323,
      "step": 4350
    },
    {
      "epoch": 1.2144846796657383,
      "grad_norm": 1.6230531930923462,
      "learning_rate": 0.0008785793871866295,
      "loss": 2.4984,
      "step": 4360
    },
    {
      "epoch": 1.2172701949860725,
      "grad_norm": 1.0689170360565186,
      "learning_rate": 0.000878300835654596,
      "loss": 2.6374,
      "step": 4370
    },
    {
      "epoch": 1.2200557103064067,
      "grad_norm": 1.5728257894515991,
      "learning_rate": 0.0008780222841225627,
      "loss": 2.5693,
      "step": 4380
    },
    {
      "epoch": 1.222841225626741,
      "grad_norm": 1.8292169570922852,
      "learning_rate": 0.0008777437325905293,
      "loss": 2.4872,
      "step": 4390
    },
    {
      "epoch": 1.2256267409470751,
      "grad_norm": 1.6334279775619507,
      "learning_rate": 0.0008774651810584958,
      "loss": 2.3259,
      "step": 4400
    },
    {
      "epoch": 1.2284122562674096,
      "grad_norm": 1.3291821479797363,
      "learning_rate": 0.0008771866295264625,
      "loss": 2.3132,
      "step": 4410
    },
    {
      "epoch": 1.2311977715877438,
      "grad_norm": 1.1702544689178467,
      "learning_rate": 0.000876908077994429,
      "loss": 2.2123,
      "step": 4420
    },
    {
      "epoch": 1.233983286908078,
      "grad_norm": 2.004547119140625,
      "learning_rate": 0.0008766295264623956,
      "loss": 2.2877,
      "step": 4430
    },
    {
      "epoch": 1.2367688022284122,
      "grad_norm": 0.9480071663856506,
      "learning_rate": 0.000876350974930362,
      "loss": 2.4932,
      "step": 4440
    },
    {
      "epoch": 1.2395543175487465,
      "grad_norm": 1.5603861808776855,
      "learning_rate": 0.0008760724233983288,
      "loss": 2.4337,
      "step": 4450
    },
    {
      "epoch": 1.2423398328690807,
      "grad_norm": 1.3219623565673828,
      "learning_rate": 0.0008757938718662953,
      "loss": 2.2288,
      "step": 4460
    },
    {
      "epoch": 1.2451253481894151,
      "grad_norm": 2.0687811374664307,
      "learning_rate": 0.0008755153203342618,
      "loss": 2.5952,
      "step": 4470
    },
    {
      "epoch": 1.2479108635097493,
      "grad_norm": 1.760677456855774,
      "learning_rate": 0.0008752367688022284,
      "loss": 2.3734,
      "step": 4480
    },
    {
      "epoch": 1.2506963788300836,
      "grad_norm": 3.0500378608703613,
      "learning_rate": 0.000874958217270195,
      "loss": 2.4399,
      "step": 4490
    },
    {
      "epoch": 1.2534818941504178,
      "grad_norm": 1.3710576295852661,
      "learning_rate": 0.0008746796657381616,
      "loss": 2.6067,
      "step": 4500
    },
    {
      "epoch": 1.2562674094707522,
      "grad_norm": 1.122080683708191,
      "learning_rate": 0.0008744011142061281,
      "loss": 2.286,
      "step": 4510
    },
    {
      "epoch": 1.2590529247910864,
      "grad_norm": 1.063055396080017,
      "learning_rate": 0.0008741225626740948,
      "loss": 2.4277,
      "step": 4520
    },
    {
      "epoch": 1.2618384401114207,
      "grad_norm": 1.3381478786468506,
      "learning_rate": 0.0008738440111420613,
      "loss": 2.6703,
      "step": 4530
    },
    {
      "epoch": 1.2646239554317549,
      "grad_norm": 1.7039910554885864,
      "learning_rate": 0.0008735654596100279,
      "loss": 2.3752,
      "step": 4540
    },
    {
      "epoch": 1.267409470752089,
      "grad_norm": 1.0273371934890747,
      "learning_rate": 0.0008732869080779944,
      "loss": 2.3107,
      "step": 4550
    },
    {
      "epoch": 1.2701949860724233,
      "grad_norm": 1.6109884977340698,
      "learning_rate": 0.000873008356545961,
      "loss": 2.5529,
      "step": 4560
    },
    {
      "epoch": 1.2729805013927575,
      "grad_norm": 1.6448652744293213,
      "learning_rate": 0.0008727298050139276,
      "loss": 2.5762,
      "step": 4570
    },
    {
      "epoch": 1.275766016713092,
      "grad_norm": 1.2523846626281738,
      "learning_rate": 0.0008724512534818941,
      "loss": 2.5849,
      "step": 4580
    },
    {
      "epoch": 1.2785515320334262,
      "grad_norm": 1.7281931638717651,
      "learning_rate": 0.0008721727019498608,
      "loss": 2.438,
      "step": 4590
    },
    {
      "epoch": 1.2813370473537604,
      "grad_norm": 1.2612711191177368,
      "learning_rate": 0.0008718941504178273,
      "loss": 2.2761,
      "step": 4600
    },
    {
      "epoch": 1.2841225626740946,
      "grad_norm": 1.335732102394104,
      "learning_rate": 0.0008716155988857939,
      "loss": 2.2729,
      "step": 4610
    },
    {
      "epoch": 1.286908077994429,
      "grad_norm": 1.1917599439620972,
      "learning_rate": 0.0008713370473537605,
      "loss": 2.4013,
      "step": 4620
    },
    {
      "epoch": 1.2896935933147633,
      "grad_norm": 2.48160457611084,
      "learning_rate": 0.0008710584958217271,
      "loss": 2.4561,
      "step": 4630
    },
    {
      "epoch": 1.2924791086350975,
      "grad_norm": 1.750442385673523,
      "learning_rate": 0.0008707799442896937,
      "loss": 2.4956,
      "step": 4640
    },
    {
      "epoch": 1.2952646239554317,
      "grad_norm": 1.385128378868103,
      "learning_rate": 0.0008705013927576601,
      "loss": 2.7708,
      "step": 4650
    },
    {
      "epoch": 1.298050139275766,
      "grad_norm": 1.5476528406143188,
      "learning_rate": 0.0008702228412256267,
      "loss": 2.3172,
      "step": 4660
    },
    {
      "epoch": 1.3008356545961002,
      "grad_norm": 1.5204412937164307,
      "learning_rate": 0.0008699442896935933,
      "loss": 2.4992,
      "step": 4670
    },
    {
      "epoch": 1.3036211699164346,
      "grad_norm": 1.8563165664672852,
      "learning_rate": 0.0008696657381615599,
      "loss": 2.5331,
      "step": 4680
    },
    {
      "epoch": 1.3064066852367688,
      "grad_norm": 1.7913271188735962,
      "learning_rate": 0.0008693871866295264,
      "loss": 2.6922,
      "step": 4690
    },
    {
      "epoch": 1.309192200557103,
      "grad_norm": 1.307502269744873,
      "learning_rate": 0.0008691086350974931,
      "loss": 2.4706,
      "step": 4700
    },
    {
      "epoch": 1.3119777158774373,
      "grad_norm": 1.441011905670166,
      "learning_rate": 0.0008688300835654597,
      "loss": 2.2823,
      "step": 4710
    },
    {
      "epoch": 1.3147632311977717,
      "grad_norm": 1.485840916633606,
      "learning_rate": 0.0008685515320334262,
      "loss": 2.3346,
      "step": 4720
    },
    {
      "epoch": 1.317548746518106,
      "grad_norm": 1.225375771522522,
      "learning_rate": 0.0008682729805013928,
      "loss": 2.6006,
      "step": 4730
    },
    {
      "epoch": 1.3203342618384402,
      "grad_norm": 1.9063146114349365,
      "learning_rate": 0.0008679944289693594,
      "loss": 2.4412,
      "step": 4740
    },
    {
      "epoch": 1.3231197771587744,
      "grad_norm": 1.4216948747634888,
      "learning_rate": 0.0008677158774373259,
      "loss": 2.7766,
      "step": 4750
    },
    {
      "epoch": 1.3259052924791086,
      "grad_norm": 1.4981646537780762,
      "learning_rate": 0.0008674373259052924,
      "loss": 2.5225,
      "step": 4760
    },
    {
      "epoch": 1.3286908077994428,
      "grad_norm": 1.255902647972107,
      "learning_rate": 0.0008671587743732591,
      "loss": 2.5313,
      "step": 4770
    },
    {
      "epoch": 1.331476323119777,
      "grad_norm": 1.6333494186401367,
      "learning_rate": 0.0008668802228412257,
      "loss": 2.6593,
      "step": 4780
    },
    {
      "epoch": 1.3342618384401115,
      "grad_norm": 1.5105376243591309,
      "learning_rate": 0.0008666016713091922,
      "loss": 2.4244,
      "step": 4790
    },
    {
      "epoch": 1.3370473537604457,
      "grad_norm": 1.518783688545227,
      "learning_rate": 0.0008663231197771588,
      "loss": 2.5585,
      "step": 4800
    },
    {
      "epoch": 1.33983286908078,
      "grad_norm": 1.7885708808898926,
      "learning_rate": 0.0008660445682451254,
      "loss": 2.2863,
      "step": 4810
    },
    {
      "epoch": 1.3426183844011141,
      "grad_norm": 1.0780287981033325,
      "learning_rate": 0.000865766016713092,
      "loss": 2.1848,
      "step": 4820
    },
    {
      "epoch": 1.3454038997214486,
      "grad_norm": 1.1311479806900024,
      "learning_rate": 0.0008654874651810585,
      "loss": 2.291,
      "step": 4830
    },
    {
      "epoch": 1.3481894150417828,
      "grad_norm": 1.0257426500320435,
      "learning_rate": 0.000865208913649025,
      "loss": 2.4411,
      "step": 4840
    },
    {
      "epoch": 1.350974930362117,
      "grad_norm": 1.3569297790527344,
      "learning_rate": 0.0008649303621169916,
      "loss": 2.7234,
      "step": 4850
    },
    {
      "epoch": 1.3537604456824512,
      "grad_norm": 1.3115129470825195,
      "learning_rate": 0.0008646518105849582,
      "loss": 2.4593,
      "step": 4860
    },
    {
      "epoch": 1.3565459610027855,
      "grad_norm": 1.1604605913162231,
      "learning_rate": 0.0008643732590529248,
      "loss": 2.368,
      "step": 4870
    },
    {
      "epoch": 1.3593314763231197,
      "grad_norm": 1.4671127796173096,
      "learning_rate": 0.0008640947075208914,
      "loss": 2.3469,
      "step": 4880
    },
    {
      "epoch": 1.362116991643454,
      "grad_norm": 2.4904963970184326,
      "learning_rate": 0.000863816155988858,
      "loss": 2.1883,
      "step": 4890
    },
    {
      "epoch": 1.3649025069637883,
      "grad_norm": 1.4059557914733887,
      "learning_rate": 0.0008635376044568245,
      "loss": 2.51,
      "step": 4900
    },
    {
      "epoch": 1.3676880222841226,
      "grad_norm": 1.4377623796463013,
      "learning_rate": 0.0008632590529247911,
      "loss": 2.4393,
      "step": 4910
    },
    {
      "epoch": 1.3704735376044568,
      "grad_norm": 1.2955317497253418,
      "learning_rate": 0.0008629805013927577,
      "loss": 2.5581,
      "step": 4920
    },
    {
      "epoch": 1.3732590529247912,
      "grad_norm": 1.303709864616394,
      "learning_rate": 0.0008627019498607243,
      "loss": 2.3807,
      "step": 4930
    },
    {
      "epoch": 1.3760445682451254,
      "grad_norm": 1.2068945169448853,
      "learning_rate": 0.0008624233983286909,
      "loss": 2.54,
      "step": 4940
    },
    {
      "epoch": 1.3788300835654597,
      "grad_norm": 1.318222999572754,
      "learning_rate": 0.0008621448467966574,
      "loss": 2.3877,
      "step": 4950
    },
    {
      "epoch": 1.3816155988857939,
      "grad_norm": 1.4617475271224976,
      "learning_rate": 0.000861866295264624,
      "loss": 2.4316,
      "step": 4960
    },
    {
      "epoch": 1.384401114206128,
      "grad_norm": 1.6448726654052734,
      "learning_rate": 0.0008615877437325905,
      "loss": 2.464,
      "step": 4970
    },
    {
      "epoch": 1.3871866295264623,
      "grad_norm": 1.313822627067566,
      "learning_rate": 0.0008613091922005571,
      "loss": 2.4642,
      "step": 4980
    },
    {
      "epoch": 1.3899721448467965,
      "grad_norm": 2.091464042663574,
      "learning_rate": 0.0008610306406685237,
      "loss": 2.5476,
      "step": 4990
    },
    {
      "epoch": 1.392757660167131,
      "grad_norm": 1.8374922275543213,
      "learning_rate": 0.0008607520891364903,
      "loss": 2.3661,
      "step": 5000
    },
    {
      "epoch": 1.3955431754874652,
      "grad_norm": 1.8063920736312866,
      "learning_rate": 0.0008604735376044568,
      "loss": 2.6067,
      "step": 5010
    },
    {
      "epoch": 1.3983286908077994,
      "grad_norm": 2.4252281188964844,
      "learning_rate": 0.0008601949860724234,
      "loss": 2.4501,
      "step": 5020
    },
    {
      "epoch": 1.4011142061281336,
      "grad_norm": 1.9636483192443848,
      "learning_rate": 0.0008599164345403901,
      "loss": 2.5231,
      "step": 5030
    },
    {
      "epoch": 1.403899721448468,
      "grad_norm": 1.3639276027679443,
      "learning_rate": 0.0008596378830083565,
      "loss": 2.4074,
      "step": 5040
    },
    {
      "epoch": 1.4066852367688023,
      "grad_norm": 1.4940053224563599,
      "learning_rate": 0.0008593593314763231,
      "loss": 2.577,
      "step": 5050
    },
    {
      "epoch": 1.4094707520891365,
      "grad_norm": 1.026761770248413,
      "learning_rate": 0.0008590807799442897,
      "loss": 2.4391,
      "step": 5060
    },
    {
      "epoch": 1.4122562674094707,
      "grad_norm": 1.284701943397522,
      "learning_rate": 0.0008588022284122563,
      "loss": 2.4555,
      "step": 5070
    },
    {
      "epoch": 1.415041782729805,
      "grad_norm": 1.4414998292922974,
      "learning_rate": 0.0008585236768802228,
      "loss": 2.2344,
      "step": 5080
    },
    {
      "epoch": 1.4178272980501392,
      "grad_norm": 1.3859493732452393,
      "learning_rate": 0.0008582451253481894,
      "loss": 2.56,
      "step": 5090
    },
    {
      "epoch": 1.4206128133704734,
      "grad_norm": 1.718222975730896,
      "learning_rate": 0.0008579665738161561,
      "loss": 2.4333,
      "step": 5100
    },
    {
      "epoch": 1.4233983286908078,
      "grad_norm": 1.2416412830352783,
      "learning_rate": 0.0008576880222841226,
      "loss": 2.4226,
      "step": 5110
    },
    {
      "epoch": 1.426183844011142,
      "grad_norm": 1.2621831893920898,
      "learning_rate": 0.0008574094707520892,
      "loss": 2.5738,
      "step": 5120
    },
    {
      "epoch": 1.4289693593314763,
      "grad_norm": 1.3629213571548462,
      "learning_rate": 0.0008571309192200557,
      "loss": 2.4262,
      "step": 5130
    },
    {
      "epoch": 1.4317548746518105,
      "grad_norm": 1.8997697830200195,
      "learning_rate": 0.0008568523676880224,
      "loss": 2.2898,
      "step": 5140
    },
    {
      "epoch": 1.434540389972145,
      "grad_norm": 1.7614766359329224,
      "learning_rate": 0.0008565738161559888,
      "loss": 2.5426,
      "step": 5150
    },
    {
      "epoch": 1.4373259052924792,
      "grad_norm": 1.507944107055664,
      "learning_rate": 0.0008562952646239554,
      "loss": 2.3502,
      "step": 5160
    },
    {
      "epoch": 1.4401114206128134,
      "grad_norm": 1.0396989583969116,
      "learning_rate": 0.000856016713091922,
      "loss": 2.468,
      "step": 5170
    },
    {
      "epoch": 1.4428969359331476,
      "grad_norm": 1.4385406970977783,
      "learning_rate": 0.0008557381615598886,
      "loss": 2.7561,
      "step": 5180
    },
    {
      "epoch": 1.4456824512534818,
      "grad_norm": 1.0152101516723633,
      "learning_rate": 0.0008554596100278552,
      "loss": 2.4111,
      "step": 5190
    },
    {
      "epoch": 1.448467966573816,
      "grad_norm": 1.8350646495819092,
      "learning_rate": 0.0008551810584958217,
      "loss": 2.4587,
      "step": 5200
    },
    {
      "epoch": 1.4512534818941505,
      "grad_norm": 1.2435048818588257,
      "learning_rate": 0.0008549025069637884,
      "loss": 2.4059,
      "step": 5210
    },
    {
      "epoch": 1.4540389972144847,
      "grad_norm": 1.3931607007980347,
      "learning_rate": 0.0008546239554317549,
      "loss": 2.6544,
      "step": 5220
    },
    {
      "epoch": 1.456824512534819,
      "grad_norm": 2.287884473800659,
      "learning_rate": 0.0008543454038997215,
      "loss": 2.4621,
      "step": 5230
    },
    {
      "epoch": 1.4596100278551531,
      "grad_norm": 1.3647141456604004,
      "learning_rate": 0.000854066852367688,
      "loss": 2.4195,
      "step": 5240
    },
    {
      "epoch": 1.4623955431754876,
      "grad_norm": 1.4106786251068115,
      "learning_rate": 0.0008537883008356546,
      "loss": 2.3733,
      "step": 5250
    },
    {
      "epoch": 1.4651810584958218,
      "grad_norm": 1.4248214960098267,
      "learning_rate": 0.0008535097493036212,
      "loss": 2.4029,
      "step": 5260
    },
    {
      "epoch": 1.467966573816156,
      "grad_norm": 1.532865285873413,
      "learning_rate": 0.0008532311977715877,
      "loss": 2.4252,
      "step": 5270
    },
    {
      "epoch": 1.4707520891364902,
      "grad_norm": 1.7465779781341553,
      "learning_rate": 0.0008529526462395544,
      "loss": 2.5714,
      "step": 5280
    },
    {
      "epoch": 1.4735376044568245,
      "grad_norm": 1.4327130317687988,
      "learning_rate": 0.0008526740947075209,
      "loss": 2.4792,
      "step": 5290
    },
    {
      "epoch": 1.4763231197771587,
      "grad_norm": 1.408367395401001,
      "learning_rate": 0.0008523955431754875,
      "loss": 2.4362,
      "step": 5300
    },
    {
      "epoch": 1.479108635097493,
      "grad_norm": 1.4660944938659668,
      "learning_rate": 0.000852116991643454,
      "loss": 2.4551,
      "step": 5310
    },
    {
      "epoch": 1.4818941504178273,
      "grad_norm": 2.0292928218841553,
      "learning_rate": 0.0008518384401114207,
      "loss": 2.2631,
      "step": 5320
    },
    {
      "epoch": 1.4846796657381616,
      "grad_norm": 1.388780951499939,
      "learning_rate": 0.0008515598885793872,
      "loss": 2.3596,
      "step": 5330
    },
    {
      "epoch": 1.4874651810584958,
      "grad_norm": 1.3283652067184448,
      "learning_rate": 0.0008512813370473537,
      "loss": 2.6007,
      "step": 5340
    },
    {
      "epoch": 1.49025069637883,
      "grad_norm": 1.5305317640304565,
      "learning_rate": 0.0008510027855153204,
      "loss": 2.4974,
      "step": 5350
    },
    {
      "epoch": 1.4930362116991645,
      "grad_norm": 1.6082890033721924,
      "learning_rate": 0.0008507242339832869,
      "loss": 2.4889,
      "step": 5360
    },
    {
      "epoch": 1.4958217270194987,
      "grad_norm": 1.7259142398834229,
      "learning_rate": 0.0008504456824512535,
      "loss": 2.6698,
      "step": 5370
    },
    {
      "epoch": 1.498607242339833,
      "grad_norm": 1.184165596961975,
      "learning_rate": 0.00085016713091922,
      "loss": 2.6046,
      "step": 5380
    },
    {
      "epoch": 1.501392757660167,
      "grad_norm": 1.5260334014892578,
      "learning_rate": 0.0008498885793871867,
      "loss": 2.6029,
      "step": 5390
    },
    {
      "epoch": 1.5041782729805013,
      "grad_norm": 1.7864551544189453,
      "learning_rate": 0.0008496100278551532,
      "loss": 2.4221,
      "step": 5400
    },
    {
      "epoch": 1.5069637883008355,
      "grad_norm": 1.599749207496643,
      "learning_rate": 0.0008493314763231198,
      "loss": 2.6459,
      "step": 5410
    },
    {
      "epoch": 1.5097493036211698,
      "grad_norm": 2.37359356880188,
      "learning_rate": 0.0008490529247910865,
      "loss": 2.485,
      "step": 5420
    },
    {
      "epoch": 1.5125348189415042,
      "grad_norm": 1.466739296913147,
      "learning_rate": 0.000848774373259053,
      "loss": 2.3284,
      "step": 5430
    },
    {
      "epoch": 1.5153203342618384,
      "grad_norm": 1.339712381362915,
      "learning_rate": 0.0008484958217270195,
      "loss": 2.5763,
      "step": 5440
    },
    {
      "epoch": 1.5181058495821727,
      "grad_norm": 1.4823673963546753,
      "learning_rate": 0.000848217270194986,
      "loss": 2.5184,
      "step": 5450
    },
    {
      "epoch": 1.520891364902507,
      "grad_norm": 3.2597639560699463,
      "learning_rate": 0.0008479387186629527,
      "loss": 2.5166,
      "step": 5460
    },
    {
      "epoch": 1.5236768802228413,
      "grad_norm": 1.5194140672683716,
      "learning_rate": 0.0008476601671309192,
      "loss": 2.4913,
      "step": 5470
    },
    {
      "epoch": 1.5264623955431755,
      "grad_norm": 2.162809371948242,
      "learning_rate": 0.0008473816155988858,
      "loss": 2.6045,
      "step": 5480
    },
    {
      "epoch": 1.5292479108635098,
      "grad_norm": 1.3894062042236328,
      "learning_rate": 0.0008471030640668523,
      "loss": 2.4226,
      "step": 5490
    },
    {
      "epoch": 1.532033426183844,
      "grad_norm": 1.5354200601577759,
      "learning_rate": 0.000846824512534819,
      "loss": 2.3011,
      "step": 5500
    },
    {
      "epoch": 1.5348189415041782,
      "grad_norm": 1.7070844173431396,
      "learning_rate": 0.0008465459610027856,
      "loss": 2.4053,
      "step": 5510
    },
    {
      "epoch": 1.5376044568245124,
      "grad_norm": 1.5369572639465332,
      "learning_rate": 0.0008462674094707521,
      "loss": 2.315,
      "step": 5520
    },
    {
      "epoch": 1.5403899721448466,
      "grad_norm": 1.2126719951629639,
      "learning_rate": 0.0008459888579387188,
      "loss": 2.502,
      "step": 5530
    },
    {
      "epoch": 1.543175487465181,
      "grad_norm": 1.2831897735595703,
      "learning_rate": 0.0008457103064066852,
      "loss": 2.4008,
      "step": 5540
    },
    {
      "epoch": 1.5459610027855153,
      "grad_norm": 1.1068071126937866,
      "learning_rate": 0.0008454317548746518,
      "loss": 2.4774,
      "step": 5550
    },
    {
      "epoch": 1.5487465181058497,
      "grad_norm": 1.1794930696487427,
      "learning_rate": 0.0008451532033426183,
      "loss": 2.3532,
      "step": 5560
    },
    {
      "epoch": 1.551532033426184,
      "grad_norm": 1.8487887382507324,
      "learning_rate": 0.000844874651810585,
      "loss": 2.0861,
      "step": 5570
    },
    {
      "epoch": 1.5543175487465182,
      "grad_norm": 1.3406431674957275,
      "learning_rate": 0.0008445961002785516,
      "loss": 2.6119,
      "step": 5580
    },
    {
      "epoch": 1.5571030640668524,
      "grad_norm": 1.2652816772460938,
      "learning_rate": 0.0008443175487465181,
      "loss": 2.3681,
      "step": 5590
    },
    {
      "epoch": 1.5598885793871866,
      "grad_norm": 1.6266114711761475,
      "learning_rate": 0.0008440389972144848,
      "loss": 2.4447,
      "step": 5600
    },
    {
      "epoch": 1.5626740947075208,
      "grad_norm": 1.513831377029419,
      "learning_rate": 0.0008437604456824513,
      "loss": 2.3051,
      "step": 5610
    },
    {
      "epoch": 1.565459610027855,
      "grad_norm": 1.3262184858322144,
      "learning_rate": 0.0008434818941504179,
      "loss": 2.4774,
      "step": 5620
    },
    {
      "epoch": 1.5682451253481893,
      "grad_norm": 1.3950124979019165,
      "learning_rate": 0.0008432033426183843,
      "loss": 2.383,
      "step": 5630
    },
    {
      "epoch": 1.5710306406685237,
      "grad_norm": 1.55614173412323,
      "learning_rate": 0.000842924791086351,
      "loss": 2.3722,
      "step": 5640
    },
    {
      "epoch": 1.573816155988858,
      "grad_norm": 1.232805848121643,
      "learning_rate": 0.0008426462395543175,
      "loss": 2.3465,
      "step": 5650
    },
    {
      "epoch": 1.5766016713091922,
      "grad_norm": 1.5806206464767456,
      "learning_rate": 0.0008423676880222841,
      "loss": 2.5359,
      "step": 5660
    },
    {
      "epoch": 1.5793871866295266,
      "grad_norm": 1.264844298362732,
      "learning_rate": 0.0008420891364902507,
      "loss": 2.6688,
      "step": 5670
    },
    {
      "epoch": 1.5821727019498608,
      "grad_norm": 1.452396273612976,
      "learning_rate": 0.0008418105849582173,
      "loss": 2.64,
      "step": 5680
    },
    {
      "epoch": 1.584958217270195,
      "grad_norm": 1.2900481224060059,
      "learning_rate": 0.0008415320334261839,
      "loss": 2.3962,
      "step": 5690
    },
    {
      "epoch": 1.5877437325905293,
      "grad_norm": 1.5687118768692017,
      "learning_rate": 0.0008412534818941504,
      "loss": 2.4746,
      "step": 5700
    },
    {
      "epoch": 1.5905292479108635,
      "grad_norm": 1.3329932689666748,
      "learning_rate": 0.0008409749303621171,
      "loss": 2.3596,
      "step": 5710
    },
    {
      "epoch": 1.5933147632311977,
      "grad_norm": 1.5110876560211182,
      "learning_rate": 0.0008406963788300836,
      "loss": 2.5301,
      "step": 5720
    },
    {
      "epoch": 1.596100278551532,
      "grad_norm": 1.1892820596694946,
      "learning_rate": 0.0008404178272980501,
      "loss": 2.5159,
      "step": 5730
    },
    {
      "epoch": 1.5988857938718661,
      "grad_norm": 1.7825387716293335,
      "learning_rate": 0.0008401392757660166,
      "loss": 2.4663,
      "step": 5740
    },
    {
      "epoch": 1.6016713091922006,
      "grad_norm": 1.039727807044983,
      "learning_rate": 0.0008398607242339833,
      "loss": 2.5224,
      "step": 5750
    },
    {
      "epoch": 1.6044568245125348,
      "grad_norm": 1.0558985471725464,
      "learning_rate": 0.0008395821727019499,
      "loss": 2.3783,
      "step": 5760
    },
    {
      "epoch": 1.6072423398328692,
      "grad_norm": 2.0491676330566406,
      "learning_rate": 0.0008393036211699164,
      "loss": 2.2951,
      "step": 5770
    },
    {
      "epoch": 1.6100278551532035,
      "grad_norm": 1.3335820436477661,
      "learning_rate": 0.0008390250696378831,
      "loss": 2.2954,
      "step": 5780
    },
    {
      "epoch": 1.6128133704735377,
      "grad_norm": 1.216596245765686,
      "learning_rate": 0.0008387465181058496,
      "loss": 2.4218,
      "step": 5790
    },
    {
      "epoch": 1.615598885793872,
      "grad_norm": 1.2934861183166504,
      "learning_rate": 0.0008384679665738162,
      "loss": 2.3665,
      "step": 5800
    },
    {
      "epoch": 1.6183844011142061,
      "grad_norm": 1.508694052696228,
      "learning_rate": 0.0008381894150417827,
      "loss": 2.586,
      "step": 5810
    },
    {
      "epoch": 1.6211699164345403,
      "grad_norm": 1.2388935089111328,
      "learning_rate": 0.0008379108635097494,
      "loss": 2.6033,
      "step": 5820
    },
    {
      "epoch": 1.6239554317548746,
      "grad_norm": 1.2857038974761963,
      "learning_rate": 0.000837632311977716,
      "loss": 2.5364,
      "step": 5830
    },
    {
      "epoch": 1.6267409470752088,
      "grad_norm": 1.407636046409607,
      "learning_rate": 0.0008373537604456824,
      "loss": 2.5435,
      "step": 5840
    },
    {
      "epoch": 1.6295264623955432,
      "grad_norm": 1.4667125940322876,
      "learning_rate": 0.000837075208913649,
      "loss": 2.4616,
      "step": 5850
    },
    {
      "epoch": 1.6323119777158774,
      "grad_norm": 1.21665358543396,
      "learning_rate": 0.0008367966573816156,
      "loss": 2.3713,
      "step": 5860
    },
    {
      "epoch": 1.6350974930362117,
      "grad_norm": 1.5814863443374634,
      "learning_rate": 0.0008365181058495822,
      "loss": 2.4121,
      "step": 5870
    },
    {
      "epoch": 1.637883008356546,
      "grad_norm": 1.461838722229004,
      "learning_rate": 0.0008362395543175487,
      "loss": 2.3801,
      "step": 5880
    },
    {
      "epoch": 1.6406685236768803,
      "grad_norm": 1.0354804992675781,
      "learning_rate": 0.0008359610027855154,
      "loss": 2.348,
      "step": 5890
    },
    {
      "epoch": 1.6434540389972145,
      "grad_norm": 2.1177961826324463,
      "learning_rate": 0.0008356824512534819,
      "loss": 2.5144,
      "step": 5900
    },
    {
      "epoch": 1.6462395543175488,
      "grad_norm": 1.5350112915039062,
      "learning_rate": 0.0008354038997214485,
      "loss": 2.3225,
      "step": 5910
    },
    {
      "epoch": 1.649025069637883,
      "grad_norm": 1.349929928779602,
      "learning_rate": 0.0008351253481894151,
      "loss": 2.4819,
      "step": 5920
    },
    {
      "epoch": 1.6518105849582172,
      "grad_norm": 1.3137552738189697,
      "learning_rate": 0.0008348467966573816,
      "loss": 2.5515,
      "step": 5930
    },
    {
      "epoch": 1.6545961002785514,
      "grad_norm": 1.4264116287231445,
      "learning_rate": 0.0008345682451253482,
      "loss": 2.2946,
      "step": 5940
    },
    {
      "epoch": 1.6573816155988856,
      "grad_norm": 1.3695999383926392,
      "learning_rate": 0.0008342896935933147,
      "loss": 2.4301,
      "step": 5950
    },
    {
      "epoch": 1.66016713091922,
      "grad_norm": NaN,
      "learning_rate": 0.0008340111420612814,
      "loss": 2.4934,
      "step": 5960
    },
    {
      "epoch": 1.6629526462395543,
      "grad_norm": 1.3568916320800781,
      "learning_rate": 0.0008337325905292479,
      "loss": 2.3618,
      "step": 5970
    },
    {
      "epoch": 1.6657381615598887,
      "grad_norm": 1.402881383895874,
      "learning_rate": 0.0008334540389972145,
      "loss": 2.2482,
      "step": 5980
    },
    {
      "epoch": 1.668523676880223,
      "grad_norm": 1.9675275087356567,
      "learning_rate": 0.0008331754874651811,
      "loss": 2.5085,
      "step": 5990
    },
    {
      "epoch": 1.6713091922005572,
      "grad_norm": 1.0988937616348267,
      "learning_rate": 0.0008328969359331477,
      "loss": 2.315,
      "step": 6000
    },
    {
      "epoch": 1.6740947075208914,
      "grad_norm": 1.2606275081634521,
      "learning_rate": 0.0008326183844011143,
      "loss": 2.3307,
      "step": 6010
    },
    {
      "epoch": 1.6768802228412256,
      "grad_norm": 1.4030606746673584,
      "learning_rate": 0.0008323398328690808,
      "loss": 2.4582,
      "step": 6020
    },
    {
      "epoch": 1.6796657381615598,
      "grad_norm": 1.5187885761260986,
      "learning_rate": 0.0008320612813370473,
      "loss": 2.7329,
      "step": 6030
    },
    {
      "epoch": 1.682451253481894,
      "grad_norm": 1.3518997430801392,
      "learning_rate": 0.0008317827298050139,
      "loss": 2.3502,
      "step": 6040
    },
    {
      "epoch": 1.6852367688022283,
      "grad_norm": 1.3769503831863403,
      "learning_rate": 0.0008315041782729805,
      "loss": 2.4084,
      "step": 6050
    },
    {
      "epoch": 1.6880222841225627,
      "grad_norm": 1.2115932703018188,
      "learning_rate": 0.000831225626740947,
      "loss": 2.4209,
      "step": 6060
    },
    {
      "epoch": 1.690807799442897,
      "grad_norm": 1.2481328248977661,
      "learning_rate": 0.0008309470752089137,
      "loss": 2.5683,
      "step": 6070
    },
    {
      "epoch": 1.6935933147632312,
      "grad_norm": 1.748487114906311,
      "learning_rate": 0.0008306685236768803,
      "loss": 2.4118,
      "step": 6080
    },
    {
      "epoch": 1.6963788300835656,
      "grad_norm": 1.1350150108337402,
      "learning_rate": 0.0008303899721448468,
      "loss": 2.5221,
      "step": 6090
    },
    {
      "epoch": 1.6991643454038998,
      "grad_norm": 1.661382794380188,
      "learning_rate": 0.0008301114206128134,
      "loss": 2.648,
      "step": 6100
    },
    {
      "epoch": 1.701949860724234,
      "grad_norm": 1.7859156131744385,
      "learning_rate": 0.00082983286908078,
      "loss": 2.3054,
      "step": 6110
    },
    {
      "epoch": 1.7047353760445683,
      "grad_norm": 1.4760489463806152,
      "learning_rate": 0.0008295543175487466,
      "loss": 2.4046,
      "step": 6120
    },
    {
      "epoch": 1.7075208913649025,
      "grad_norm": 1.1018586158752441,
      "learning_rate": 0.000829275766016713,
      "loss": 2.4088,
      "step": 6130
    },
    {
      "epoch": 1.7103064066852367,
      "grad_norm": 1.4374181032180786,
      "learning_rate": 0.0008289972144846797,
      "loss": 2.2912,
      "step": 6140
    },
    {
      "epoch": 1.713091922005571,
      "grad_norm": 1.0647774934768677,
      "learning_rate": 0.0008287186629526463,
      "loss": 2.4062,
      "step": 6150
    },
    {
      "epoch": 1.7158774373259051,
      "grad_norm": 1.726162075996399,
      "learning_rate": 0.0008284401114206128,
      "loss": 2.5319,
      "step": 6160
    },
    {
      "epoch": 1.7186629526462396,
      "grad_norm": 1.6115524768829346,
      "learning_rate": 0.0008281615598885794,
      "loss": 2.3645,
      "step": 6170
    },
    {
      "epoch": 1.7214484679665738,
      "grad_norm": 1.3554410934448242,
      "learning_rate": 0.000827883008356546,
      "loss": 2.3617,
      "step": 6180
    },
    {
      "epoch": 1.724233983286908,
      "grad_norm": 1.271749496459961,
      "learning_rate": 0.0008276044568245126,
      "loss": 2.4154,
      "step": 6190
    },
    {
      "epoch": 1.7270194986072425,
      "grad_norm": 1.5138568878173828,
      "learning_rate": 0.0008273259052924791,
      "loss": 2.6407,
      "step": 6200
    },
    {
      "epoch": 1.7298050139275767,
      "grad_norm": 2.1389286518096924,
      "learning_rate": 0.0008270473537604457,
      "loss": 2.4306,
      "step": 6210
    },
    {
      "epoch": 1.732590529247911,
      "grad_norm": 1.4736361503601074,
      "learning_rate": 0.0008267688022284122,
      "loss": 2.2516,
      "step": 6220
    },
    {
      "epoch": 1.7353760445682451,
      "grad_norm": 1.264660358428955,
      "learning_rate": 0.0008264902506963788,
      "loss": 2.4906,
      "step": 6230
    },
    {
      "epoch": 1.7381615598885793,
      "grad_norm": 1.7088195085525513,
      "learning_rate": 0.0008262116991643454,
      "loss": 2.511,
      "step": 6240
    },
    {
      "epoch": 1.7409470752089136,
      "grad_norm": 1.4652342796325684,
      "learning_rate": 0.000825933147632312,
      "loss": 2.4833,
      "step": 6250
    },
    {
      "epoch": 1.7437325905292478,
      "grad_norm": 1.2397541999816895,
      "learning_rate": 0.0008256545961002786,
      "loss": 2.4826,
      "step": 6260
    },
    {
      "epoch": 1.7465181058495822,
      "grad_norm": 1.2724382877349854,
      "learning_rate": 0.0008253760445682451,
      "loss": 2.4095,
      "step": 6270
    },
    {
      "epoch": 1.7493036211699164,
      "grad_norm": 1.8516545295715332,
      "learning_rate": 0.0008250974930362117,
      "loss": 2.3379,
      "step": 6280
    },
    {
      "epoch": 1.7520891364902507,
      "grad_norm": 1.5619397163391113,
      "learning_rate": 0.0008248189415041783,
      "loss": 2.2916,
      "step": 6290
    },
    {
      "epoch": 1.754874651810585,
      "grad_norm": 1.30494225025177,
      "learning_rate": 0.0008245403899721449,
      "loss": 2.4811,
      "step": 6300
    },
    {
      "epoch": 1.7576601671309193,
      "grad_norm": 1.6142419576644897,
      "learning_rate": 0.0008242618384401115,
      "loss": 2.2726,
      "step": 6310
    },
    {
      "epoch": 1.7604456824512535,
      "grad_norm": 1.3654570579528809,
      "learning_rate": 0.000823983286908078,
      "loss": 2.5251,
      "step": 6320
    },
    {
      "epoch": 1.7632311977715878,
      "grad_norm": 1.2181755304336548,
      "learning_rate": 0.0008237047353760446,
      "loss": 2.2608,
      "step": 6330
    },
    {
      "epoch": 1.766016713091922,
      "grad_norm": 1.4678488969802856,
      "learning_rate": 0.0008234261838440111,
      "loss": 2.5252,
      "step": 6340
    },
    {
      "epoch": 1.7688022284122562,
      "grad_norm": 1.1738835573196411,
      "learning_rate": 0.0008231476323119777,
      "loss": 2.363,
      "step": 6350
    },
    {
      "epoch": 1.7715877437325904,
      "grad_norm": 1.1241730451583862,
      "learning_rate": 0.0008228690807799443,
      "loss": 2.4647,
      "step": 6360
    },
    {
      "epoch": 1.7743732590529246,
      "grad_norm": 1.2596666812896729,
      "learning_rate": 0.0008225905292479109,
      "loss": 2.295,
      "step": 6370
    },
    {
      "epoch": 1.777158774373259,
      "grad_norm": 1.3083873987197876,
      "learning_rate": 0.0008223119777158774,
      "loss": 2.3487,
      "step": 6380
    },
    {
      "epoch": 1.7799442896935933,
      "grad_norm": 1.3180644512176514,
      "learning_rate": 0.000822033426183844,
      "loss": 2.5112,
      "step": 6390
    },
    {
      "epoch": 1.7827298050139275,
      "grad_norm": 1.120100975036621,
      "learning_rate": 0.0008217548746518107,
      "loss": 2.3129,
      "step": 6400
    },
    {
      "epoch": 1.785515320334262,
      "grad_norm": 1.5552268028259277,
      "learning_rate": 0.0008214763231197772,
      "loss": 2.4366,
      "step": 6410
    },
    {
      "epoch": 1.7883008356545962,
      "grad_norm": 1.1398698091506958,
      "learning_rate": 0.0008211977715877437,
      "loss": 2.3549,
      "step": 6420
    },
    {
      "epoch": 1.7910863509749304,
      "grad_norm": 1.7812296152114868,
      "learning_rate": 0.0008209192200557103,
      "loss": 2.4522,
      "step": 6430
    },
    {
      "epoch": 1.7938718662952646,
      "grad_norm": 1.3231663703918457,
      "learning_rate": 0.0008206406685236769,
      "loss": 2.4726,
      "step": 6440
    },
    {
      "epoch": 1.7966573816155988,
      "grad_norm": 1.4612828493118286,
      "learning_rate": 0.0008203621169916434,
      "loss": 2.4064,
      "step": 6450
    },
    {
      "epoch": 1.799442896935933,
      "grad_norm": 1.8038727045059204,
      "learning_rate": 0.00082008356545961,
      "loss": 2.2981,
      "step": 6460
    },
    {
      "epoch": 1.8022284122562673,
      "grad_norm": 0.9182248711585999,
      "learning_rate": 0.0008198050139275767,
      "loss": 2.3008,
      "step": 6470
    },
    {
      "epoch": 1.8050139275766015,
      "grad_norm": 2.1826066970825195,
      "learning_rate": 0.0008195264623955432,
      "loss": 2.5542,
      "step": 6480
    },
    {
      "epoch": 1.807799442896936,
      "grad_norm": 0.9131319522857666,
      "learning_rate": 0.0008192479108635098,
      "loss": 2.5138,
      "step": 6490
    },
    {
      "epoch": 1.8105849582172702,
      "grad_norm": 1.3898106813430786,
      "learning_rate": 0.0008189693593314764,
      "loss": 2.4421,
      "step": 6500
    },
    {
      "epoch": 1.8133704735376046,
      "grad_norm": 1.3430447578430176,
      "learning_rate": 0.000818690807799443,
      "loss": 2.458,
      "step": 6510
    },
    {
      "epoch": 1.8161559888579388,
      "grad_norm": 1.6459290981292725,
      "learning_rate": 0.0008184122562674094,
      "loss": 2.2699,
      "step": 6520
    },
    {
      "epoch": 1.818941504178273,
      "grad_norm": 1.7157340049743652,
      "learning_rate": 0.000818133704735376,
      "loss": 2.5907,
      "step": 6530
    },
    {
      "epoch": 1.8217270194986073,
      "grad_norm": 1.4923583269119263,
      "learning_rate": 0.0008178551532033426,
      "loss": 2.4782,
      "step": 6540
    },
    {
      "epoch": 1.8245125348189415,
      "grad_norm": 1.201341152191162,
      "learning_rate": 0.0008175766016713092,
      "loss": 2.1809,
      "step": 6550
    },
    {
      "epoch": 1.8272980501392757,
      "grad_norm": 1.1674045324325562,
      "learning_rate": 0.0008172980501392758,
      "loss": 2.3316,
      "step": 6560
    },
    {
      "epoch": 1.83008356545961,
      "grad_norm": 2.242022752761841,
      "learning_rate": 0.0008170194986072423,
      "loss": 2.3876,
      "step": 6570
    },
    {
      "epoch": 1.8328690807799441,
      "grad_norm": 1.2739125490188599,
      "learning_rate": 0.000816740947075209,
      "loss": 2.4668,
      "step": 6580
    },
    {
      "epoch": 1.8356545961002786,
      "grad_norm": 1.297222375869751,
      "learning_rate": 0.0008164623955431755,
      "loss": 2.5254,
      "step": 6590
    },
    {
      "epoch": 1.8384401114206128,
      "grad_norm": 1.3275885581970215,
      "learning_rate": 0.0008161838440111421,
      "loss": 2.432,
      "step": 6600
    },
    {
      "epoch": 1.841225626740947,
      "grad_norm": 1.4261376857757568,
      "learning_rate": 0.0008159052924791087,
      "loss": 2.2827,
      "step": 6610
    },
    {
      "epoch": 1.8440111420612815,
      "grad_norm": 1.2774686813354492,
      "learning_rate": 0.0008156267409470752,
      "loss": 2.4545,
      "step": 6620
    },
    {
      "epoch": 1.8467966573816157,
      "grad_norm": 1.3078491687774658,
      "learning_rate": 0.0008153481894150418,
      "loss": 2.4145,
      "step": 6630
    },
    {
      "epoch": 1.84958217270195,
      "grad_norm": 1.5586998462677002,
      "learning_rate": 0.0008150696378830083,
      "loss": 2.4144,
      "step": 6640
    },
    {
      "epoch": 1.8523676880222841,
      "grad_norm": 1.1359819173812866,
      "learning_rate": 0.000814791086350975,
      "loss": 2.4509,
      "step": 6650
    },
    {
      "epoch": 1.8551532033426184,
      "grad_norm": 1.3018604516983032,
      "learning_rate": 0.0008145125348189415,
      "loss": 2.4234,
      "step": 6660
    },
    {
      "epoch": 1.8579387186629526,
      "grad_norm": 1.1655324697494507,
      "learning_rate": 0.0008142339832869081,
      "loss": 2.3,
      "step": 6670
    },
    {
      "epoch": 1.8607242339832868,
      "grad_norm": 1.7409571409225464,
      "learning_rate": 0.0008139554317548746,
      "loss": 2.4676,
      "step": 6680
    },
    {
      "epoch": 1.863509749303621,
      "grad_norm": 1.773217797279358,
      "learning_rate": 0.0008136768802228413,
      "loss": 2.332,
      "step": 6690
    },
    {
      "epoch": 1.8662952646239555,
      "grad_norm": 1.3988044261932373,
      "learning_rate": 0.0008133983286908078,
      "loss": 2.2929,
      "step": 6700
    },
    {
      "epoch": 1.8690807799442897,
      "grad_norm": 1.5889244079589844,
      "learning_rate": 0.0008131197771587744,
      "loss": 2.3606,
      "step": 6710
    },
    {
      "epoch": 1.8718662952646241,
      "grad_norm": 1.6128571033477783,
      "learning_rate": 0.000812841225626741,
      "loss": 2.5504,
      "step": 6720
    },
    {
      "epoch": 1.8746518105849583,
      "grad_norm": 1.5105658769607544,
      "learning_rate": 0.0008125626740947075,
      "loss": 2.2997,
      "step": 6730
    },
    {
      "epoch": 1.8774373259052926,
      "grad_norm": 1.2381178140640259,
      "learning_rate": 0.0008122841225626741,
      "loss": 2.3265,
      "step": 6740
    },
    {
      "epoch": 1.8802228412256268,
      "grad_norm": 1.4113962650299072,
      "learning_rate": 0.0008120055710306406,
      "loss": 2.3434,
      "step": 6750
    },
    {
      "epoch": 1.883008356545961,
      "grad_norm": 1.5418592691421509,
      "learning_rate": 0.0008117270194986073,
      "loss": 2.5387,
      "step": 6760
    },
    {
      "epoch": 1.8857938718662952,
      "grad_norm": 1.369094967842102,
      "learning_rate": 0.0008114484679665738,
      "loss": 2.2786,
      "step": 6770
    },
    {
      "epoch": 1.8885793871866294,
      "grad_norm": 2.060520887374878,
      "learning_rate": 0.0008111699164345404,
      "loss": 2.466,
      "step": 6780
    },
    {
      "epoch": 1.8913649025069637,
      "grad_norm": 1.798980474472046,
      "learning_rate": 0.0008108913649025071,
      "loss": 2.3359,
      "step": 6790
    },
    {
      "epoch": 1.894150417827298,
      "grad_norm": 1.2556724548339844,
      "learning_rate": 0.0008106128133704736,
      "loss": 2.3672,
      "step": 6800
    },
    {
      "epoch": 1.8969359331476323,
      "grad_norm": 1.7607090473175049,
      "learning_rate": 0.0008103342618384402,
      "loss": 2.3893,
      "step": 6810
    },
    {
      "epoch": 1.8997214484679665,
      "grad_norm": 1.5466045141220093,
      "learning_rate": 0.0008100557103064066,
      "loss": 2.314,
      "step": 6820
    },
    {
      "epoch": 1.902506963788301,
      "grad_norm": 1.2858881950378418,
      "learning_rate": 0.0008097771587743733,
      "loss": 2.3362,
      "step": 6830
    },
    {
      "epoch": 1.9052924791086352,
      "grad_norm": 1.2220828533172607,
      "learning_rate": 0.0008094986072423398,
      "loss": 2.5981,
      "step": 6840
    },
    {
      "epoch": 1.9080779944289694,
      "grad_norm": 1.2157270908355713,
      "learning_rate": 0.0008092200557103064,
      "loss": 2.3806,
      "step": 6850
    },
    {
      "epoch": 1.9108635097493036,
      "grad_norm": 1.5204381942749023,
      "learning_rate": 0.0008089415041782729,
      "loss": 2.3958,
      "step": 6860
    },
    {
      "epoch": 1.9136490250696379,
      "grad_norm": 1.9839922189712524,
      "learning_rate": 0.0008086629526462396,
      "loss": 2.458,
      "step": 6870
    },
    {
      "epoch": 1.916434540389972,
      "grad_norm": 1.2304649353027344,
      "learning_rate": 0.0008083844011142062,
      "loss": 2.4507,
      "step": 6880
    },
    {
      "epoch": 1.9192200557103063,
      "grad_norm": 1.514604926109314,
      "learning_rate": 0.0008081058495821727,
      "loss": 2.2823,
      "step": 6890
    },
    {
      "epoch": 1.9220055710306405,
      "grad_norm": 1.2832010984420776,
      "learning_rate": 0.0008078272980501394,
      "loss": 2.3791,
      "step": 6900
    },
    {
      "epoch": 1.924791086350975,
      "grad_norm": 2.348871946334839,
      "learning_rate": 0.0008075487465181059,
      "loss": 2.3803,
      "step": 6910
    },
    {
      "epoch": 1.9275766016713092,
      "grad_norm": 1.8840203285217285,
      "learning_rate": 0.0008072701949860724,
      "loss": 2.2485,
      "step": 6920
    },
    {
      "epoch": 1.9303621169916436,
      "grad_norm": 1.3784795999526978,
      "learning_rate": 0.0008069916434540389,
      "loss": 2.4753,
      "step": 6930
    },
    {
      "epoch": 1.9331476323119778,
      "grad_norm": 1.338172197341919,
      "learning_rate": 0.0008067130919220056,
      "loss": 2.3499,
      "step": 6940
    },
    {
      "epoch": 1.935933147632312,
      "grad_norm": 1.3773545026779175,
      "learning_rate": 0.0008064345403899722,
      "loss": 2.5121,
      "step": 6950
    },
    {
      "epoch": 1.9387186629526463,
      "grad_norm": 2.3376359939575195,
      "learning_rate": 0.0008061559888579387,
      "loss": 2.3901,
      "step": 6960
    },
    {
      "epoch": 1.9415041782729805,
      "grad_norm": 1.5331995487213135,
      "learning_rate": 0.0008058774373259054,
      "loss": 2.5428,
      "step": 6970
    },
    {
      "epoch": 1.9442896935933147,
      "grad_norm": 1.0944633483886719,
      "learning_rate": 0.0008055988857938719,
      "loss": 2.3908,
      "step": 6980
    },
    {
      "epoch": 1.947075208913649,
      "grad_norm": 1.5982229709625244,
      "learning_rate": 0.0008053203342618385,
      "loss": 2.497,
      "step": 6990
    },
    {
      "epoch": 1.9498607242339832,
      "grad_norm": 1.1102895736694336,
      "learning_rate": 0.000805041782729805,
      "loss": 2.4045,
      "step": 7000
    },
    {
      "epoch": 1.9526462395543176,
      "grad_norm": 1.5663762092590332,
      "learning_rate": 0.0008047632311977717,
      "loss": 2.5212,
      "step": 7010
    },
    {
      "epoch": 1.9554317548746518,
      "grad_norm": 1.488722324371338,
      "learning_rate": 0.0008044846796657381,
      "loss": 2.367,
      "step": 7020
    },
    {
      "epoch": 1.958217270194986,
      "grad_norm": 1.7573461532592773,
      "learning_rate": 0.0008042061281337047,
      "loss": 2.1889,
      "step": 7030
    },
    {
      "epoch": 1.9610027855153205,
      "grad_norm": 1.1590335369110107,
      "learning_rate": 0.0008039275766016713,
      "loss": 2.3877,
      "step": 7040
    },
    {
      "epoch": 1.9637883008356547,
      "grad_norm": 1.12799072265625,
      "learning_rate": 0.0008036490250696379,
      "loss": 2.5254,
      "step": 7050
    },
    {
      "epoch": 1.966573816155989,
      "grad_norm": 1.549452543258667,
      "learning_rate": 0.0008033704735376045,
      "loss": 2.4558,
      "step": 7060
    },
    {
      "epoch": 1.9693593314763231,
      "grad_norm": 1.267368197441101,
      "learning_rate": 0.000803091922005571,
      "loss": 2.4672,
      "step": 7070
    },
    {
      "epoch": 1.9721448467966574,
      "grad_norm": 1.6546114683151245,
      "learning_rate": 0.0008028133704735377,
      "loss": 2.4483,
      "step": 7080
    },
    {
      "epoch": 1.9749303621169916,
      "grad_norm": 1.3571500778198242,
      "learning_rate": 0.0008025348189415042,
      "loss": 2.353,
      "step": 7090
    },
    {
      "epoch": 1.9777158774373258,
      "grad_norm": 1.4099880456924438,
      "learning_rate": 0.0008022562674094708,
      "loss": 2.2217,
      "step": 7100
    },
    {
      "epoch": 1.98050139275766,
      "grad_norm": 1.6505297422409058,
      "learning_rate": 0.0008019777158774373,
      "loss": 2.4136,
      "step": 7110
    },
    {
      "epoch": 1.9832869080779945,
      "grad_norm": 1.3327512741088867,
      "learning_rate": 0.0008016991643454039,
      "loss": 2.4877,
      "step": 7120
    },
    {
      "epoch": 1.9860724233983287,
      "grad_norm": 1.3107550144195557,
      "learning_rate": 0.0008014206128133705,
      "loss": 2.5093,
      "step": 7130
    },
    {
      "epoch": 1.988857938718663,
      "grad_norm": 1.3169505596160889,
      "learning_rate": 0.000801142061281337,
      "loss": 2.6188,
      "step": 7140
    },
    {
      "epoch": 1.9916434540389973,
      "grad_norm": 1.4448331594467163,
      "learning_rate": 0.0008008635097493037,
      "loss": 2.594,
      "step": 7150
    },
    {
      "epoch": 1.9944289693593316,
      "grad_norm": 1.3516491651535034,
      "learning_rate": 0.0008005849582172702,
      "loss": 2.5107,
      "step": 7160
    },
    {
      "epoch": 1.9972144846796658,
      "grad_norm": 1.892090082168579,
      "learning_rate": 0.0008003064066852368,
      "loss": 2.3801,
      "step": 7170
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.3888051509857178,
      "learning_rate": 0.0008000278551532033,
      "loss": 2.462,
      "step": 7180
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.3456850051879883,
      "eval_runtime": 6.828,
      "eval_samples_per_second": 467.483,
      "eval_steps_per_second": 58.435,
      "step": 7180
    },
    {
      "epoch": 2.002785515320334,
      "grad_norm": 1.3741111755371094,
      "learning_rate": 0.00079974930362117,
      "loss": 2.6391,
      "step": 7190
    },
    {
      "epoch": 2.0055710306406684,
      "grad_norm": 1.1111980676651,
      "learning_rate": 0.0007994707520891366,
      "loss": 2.2867,
      "step": 7200
    },
    {
      "epoch": 2.0083565459610027,
      "grad_norm": 1.3236702680587769,
      "learning_rate": 0.000799192200557103,
      "loss": 2.2375,
      "step": 7210
    },
    {
      "epoch": 2.011142061281337,
      "grad_norm": 2.15309476852417,
      "learning_rate": 0.0007989136490250696,
      "loss": 2.3324,
      "step": 7220
    },
    {
      "epoch": 2.013927576601671,
      "grad_norm": 1.2238829135894775,
      "learning_rate": 0.0007986350974930362,
      "loss": 2.3968,
      "step": 7230
    },
    {
      "epoch": 2.0167130919220058,
      "grad_norm": 1.5278304815292358,
      "learning_rate": 0.0007983565459610028,
      "loss": 2.3395,
      "step": 7240
    },
    {
      "epoch": 2.01949860724234,
      "grad_norm": 1.4765621423721313,
      "learning_rate": 0.0007980779944289693,
      "loss": 2.5689,
      "step": 7250
    },
    {
      "epoch": 2.022284122562674,
      "grad_norm": 1.614445686340332,
      "learning_rate": 0.000797799442896936,
      "loss": 2.6622,
      "step": 7260
    },
    {
      "epoch": 2.0250696378830084,
      "grad_norm": 1.826708197593689,
      "learning_rate": 0.0007975208913649026,
      "loss": 2.4545,
      "step": 7270
    },
    {
      "epoch": 2.0278551532033426,
      "grad_norm": 1.4938161373138428,
      "learning_rate": 0.0007972423398328691,
      "loss": 2.3368,
      "step": 7280
    },
    {
      "epoch": 2.030640668523677,
      "grad_norm": 1.7132558822631836,
      "learning_rate": 0.0007969637883008357,
      "loss": 2.2759,
      "step": 7290
    },
    {
      "epoch": 2.033426183844011,
      "grad_norm": 1.9337042570114136,
      "learning_rate": 0.0007966852367688023,
      "loss": 2.3594,
      "step": 7300
    },
    {
      "epoch": 2.0362116991643453,
      "grad_norm": 1.3110491037368774,
      "learning_rate": 0.0007964066852367688,
      "loss": 2.3426,
      "step": 7310
    },
    {
      "epoch": 2.0389972144846795,
      "grad_norm": 1.2324280738830566,
      "learning_rate": 0.0007961281337047353,
      "loss": 2.153,
      "step": 7320
    },
    {
      "epoch": 2.0417827298050137,
      "grad_norm": 1.3598366975784302,
      "learning_rate": 0.000795849582172702,
      "loss": 2.4335,
      "step": 7330
    },
    {
      "epoch": 2.0445682451253484,
      "grad_norm": 1.179227352142334,
      "learning_rate": 0.0007955710306406685,
      "loss": 2.2965,
      "step": 7340
    },
    {
      "epoch": 2.0473537604456826,
      "grad_norm": 1.3249680995941162,
      "learning_rate": 0.0007952924791086351,
      "loss": 2.4774,
      "step": 7350
    },
    {
      "epoch": 2.050139275766017,
      "grad_norm": 1.908708095550537,
      "learning_rate": 0.0007950139275766017,
      "loss": 2.3,
      "step": 7360
    },
    {
      "epoch": 2.052924791086351,
      "grad_norm": 1.2221944332122803,
      "learning_rate": 0.0007947353760445683,
      "loss": 2.3252,
      "step": 7370
    },
    {
      "epoch": 2.0557103064066853,
      "grad_norm": 1.4600279331207275,
      "learning_rate": 0.0007944568245125349,
      "loss": 2.4568,
      "step": 7380
    },
    {
      "epoch": 2.0584958217270195,
      "grad_norm": 2.3194024562835693,
      "learning_rate": 0.0007941782729805014,
      "loss": 2.4781,
      "step": 7390
    },
    {
      "epoch": 2.0612813370473537,
      "grad_norm": 1.783575415611267,
      "learning_rate": 0.000793899721448468,
      "loss": 2.4959,
      "step": 7400
    },
    {
      "epoch": 2.064066852367688,
      "grad_norm": 1.218075156211853,
      "learning_rate": 0.0007936211699164345,
      "loss": 2.2025,
      "step": 7410
    },
    {
      "epoch": 2.066852367688022,
      "grad_norm": 1.1576820611953735,
      "learning_rate": 0.0007933426183844011,
      "loss": 2.2758,
      "step": 7420
    },
    {
      "epoch": 2.0696378830083564,
      "grad_norm": 1.493254542350769,
      "learning_rate": 0.0007930640668523676,
      "loss": 2.4124,
      "step": 7430
    },
    {
      "epoch": 2.0724233983286906,
      "grad_norm": 1.4792152643203735,
      "learning_rate": 0.0007927855153203343,
      "loss": 2.2444,
      "step": 7440
    },
    {
      "epoch": 2.0752089136490253,
      "grad_norm": 1.673569917678833,
      "learning_rate": 0.0007925069637883009,
      "loss": 2.3837,
      "step": 7450
    },
    {
      "epoch": 2.0779944289693595,
      "grad_norm": 1.2484318017959595,
      "learning_rate": 0.0007922284122562674,
      "loss": 2.3365,
      "step": 7460
    },
    {
      "epoch": 2.0807799442896937,
      "grad_norm": 1.7455631494522095,
      "learning_rate": 0.000791949860724234,
      "loss": 2.392,
      "step": 7470
    },
    {
      "epoch": 2.083565459610028,
      "grad_norm": 1.4597277641296387,
      "learning_rate": 0.0007916713091922006,
      "loss": 2.5299,
      "step": 7480
    },
    {
      "epoch": 2.086350974930362,
      "grad_norm": 1.1004345417022705,
      "learning_rate": 0.0007913927576601672,
      "loss": 2.3013,
      "step": 7490
    },
    {
      "epoch": 2.0891364902506964,
      "grad_norm": 1.6231330633163452,
      "learning_rate": 0.0007911142061281336,
      "loss": 2.3097,
      "step": 7500
    },
    {
      "epoch": 2.0919220055710306,
      "grad_norm": 1.1903562545776367,
      "learning_rate": 0.0007908356545961003,
      "loss": 2.3909,
      "step": 7510
    },
    {
      "epoch": 2.094707520891365,
      "grad_norm": 1.835054874420166,
      "learning_rate": 0.0007905571030640669,
      "loss": 2.2404,
      "step": 7520
    },
    {
      "epoch": 2.097493036211699,
      "grad_norm": 1.2016122341156006,
      "learning_rate": 0.0007902785515320334,
      "loss": 2.3772,
      "step": 7530
    },
    {
      "epoch": 2.1002785515320332,
      "grad_norm": 1.7493493556976318,
      "learning_rate": 0.00079,
      "loss": 2.7221,
      "step": 7540
    },
    {
      "epoch": 2.103064066852368,
      "grad_norm": 1.4822440147399902,
      "learning_rate": 0.0007897214484679666,
      "loss": 2.2853,
      "step": 7550
    },
    {
      "epoch": 2.105849582172702,
      "grad_norm": 1.3900059461593628,
      "learning_rate": 0.0007894428969359332,
      "loss": 2.5967,
      "step": 7560
    },
    {
      "epoch": 2.1086350974930363,
      "grad_norm": 1.1781978607177734,
      "learning_rate": 0.0007891643454038997,
      "loss": 2.2676,
      "step": 7570
    },
    {
      "epoch": 2.1114206128133706,
      "grad_norm": 1.6522743701934814,
      "learning_rate": 0.0007888857938718663,
      "loss": 2.2471,
      "step": 7580
    },
    {
      "epoch": 2.114206128133705,
      "grad_norm": 1.1930689811706543,
      "learning_rate": 0.0007886072423398329,
      "loss": 2.2829,
      "step": 7590
    },
    {
      "epoch": 2.116991643454039,
      "grad_norm": 1.2222026586532593,
      "learning_rate": 0.0007883286908077995,
      "loss": 2.3909,
      "step": 7600
    },
    {
      "epoch": 2.1197771587743732,
      "grad_norm": 1.4398586750030518,
      "learning_rate": 0.000788050139275766,
      "loss": 2.4388,
      "step": 7610
    },
    {
      "epoch": 2.1225626740947074,
      "grad_norm": 1.2346068620681763,
      "learning_rate": 0.0007877715877437326,
      "loss": 2.5127,
      "step": 7620
    },
    {
      "epoch": 2.1253481894150417,
      "grad_norm": 1.22809898853302,
      "learning_rate": 0.0007874930362116992,
      "loss": 2.3345,
      "step": 7630
    },
    {
      "epoch": 2.128133704735376,
      "grad_norm": 1.5870004892349243,
      "learning_rate": 0.0007872144846796657,
      "loss": 2.3643,
      "step": 7640
    },
    {
      "epoch": 2.13091922005571,
      "grad_norm": 1.4222264289855957,
      "learning_rate": 0.0007869359331476323,
      "loss": 2.2193,
      "step": 7650
    },
    {
      "epoch": 2.1337047353760448,
      "grad_norm": 1.1102460622787476,
      "learning_rate": 0.0007866573816155989,
      "loss": 2.406,
      "step": 7660
    },
    {
      "epoch": 2.136490250696379,
      "grad_norm": 1.5835826396942139,
      "learning_rate": 0.0007863788300835655,
      "loss": 2.4795,
      "step": 7670
    },
    {
      "epoch": 2.139275766016713,
      "grad_norm": 1.0706018209457397,
      "learning_rate": 0.0007861002785515321,
      "loss": 2.3977,
      "step": 7680
    },
    {
      "epoch": 2.1420612813370474,
      "grad_norm": 1.4004265069961548,
      "learning_rate": 0.0007858217270194987,
      "loss": 2.2036,
      "step": 7690
    },
    {
      "epoch": 2.1448467966573816,
      "grad_norm": 1.6156766414642334,
      "learning_rate": 0.0007855431754874653,
      "loss": 2.3653,
      "step": 7700
    },
    {
      "epoch": 2.147632311977716,
      "grad_norm": 1.9360074996948242,
      "learning_rate": 0.0007852646239554317,
      "loss": 2.5263,
      "step": 7710
    },
    {
      "epoch": 2.15041782729805,
      "grad_norm": 1.3958693742752075,
      "learning_rate": 0.0007849860724233983,
      "loss": 2.4723,
      "step": 7720
    },
    {
      "epoch": 2.1532033426183843,
      "grad_norm": 1.176519513130188,
      "learning_rate": 0.0007847075208913649,
      "loss": 2.3758,
      "step": 7730
    },
    {
      "epoch": 2.1559888579387185,
      "grad_norm": 1.3438526391983032,
      "learning_rate": 0.0007844289693593315,
      "loss": 2.2192,
      "step": 7740
    },
    {
      "epoch": 2.1587743732590527,
      "grad_norm": 1.5167970657348633,
      "learning_rate": 0.000784150417827298,
      "loss": 2.2404,
      "step": 7750
    },
    {
      "epoch": 2.1615598885793874,
      "grad_norm": 3.6618289947509766,
      "learning_rate": 0.0007838718662952646,
      "loss": 2.5129,
      "step": 7760
    },
    {
      "epoch": 2.1643454038997216,
      "grad_norm": 1.4332131147384644,
      "learning_rate": 0.0007835933147632313,
      "loss": 2.3242,
      "step": 7770
    },
    {
      "epoch": 2.167130919220056,
      "grad_norm": 1.8259273767471313,
      "learning_rate": 0.0007833147632311978,
      "loss": 2.4068,
      "step": 7780
    },
    {
      "epoch": 2.16991643454039,
      "grad_norm": 1.3640179634094238,
      "learning_rate": 0.0007830362116991644,
      "loss": 2.3068,
      "step": 7790
    },
    {
      "epoch": 2.1727019498607243,
      "grad_norm": 1.6360002756118774,
      "learning_rate": 0.000782757660167131,
      "loss": 2.3563,
      "step": 7800
    },
    {
      "epoch": 2.1754874651810585,
      "grad_norm": 1.6932252645492554,
      "learning_rate": 0.0007824791086350975,
      "loss": 2.2108,
      "step": 7810
    },
    {
      "epoch": 2.1782729805013927,
      "grad_norm": 1.9871102571487427,
      "learning_rate": 0.000782200557103064,
      "loss": 2.3693,
      "step": 7820
    },
    {
      "epoch": 2.181058495821727,
      "grad_norm": 1.5458319187164307,
      "learning_rate": 0.0007819220055710306,
      "loss": 2.3781,
      "step": 7830
    },
    {
      "epoch": 2.183844011142061,
      "grad_norm": 1.841949224472046,
      "learning_rate": 0.0007816434540389973,
      "loss": 2.5661,
      "step": 7840
    },
    {
      "epoch": 2.1866295264623954,
      "grad_norm": 1.9619232416152954,
      "learning_rate": 0.0007813649025069638,
      "loss": 2.3721,
      "step": 7850
    },
    {
      "epoch": 2.1894150417827296,
      "grad_norm": 1.6557899713516235,
      "learning_rate": 0.0007810863509749304,
      "loss": 2.3742,
      "step": 7860
    },
    {
      "epoch": 2.1922005571030643,
      "grad_norm": 1.2167049646377563,
      "learning_rate": 0.000780807799442897,
      "loss": 2.4909,
      "step": 7870
    },
    {
      "epoch": 2.1949860724233985,
      "grad_norm": 1.1975045204162598,
      "learning_rate": 0.0007805292479108636,
      "loss": 2.2364,
      "step": 7880
    },
    {
      "epoch": 2.1977715877437327,
      "grad_norm": 1.2464836835861206,
      "learning_rate": 0.00078025069637883,
      "loss": 2.4179,
      "step": 7890
    },
    {
      "epoch": 2.200557103064067,
      "grad_norm": 1.4668638706207275,
      "learning_rate": 0.0007799721448467966,
      "loss": 2.3313,
      "step": 7900
    },
    {
      "epoch": 2.203342618384401,
      "grad_norm": 1.2883070707321167,
      "learning_rate": 0.0007796935933147632,
      "loss": 2.3031,
      "step": 7910
    },
    {
      "epoch": 2.2061281337047354,
      "grad_norm": 1.149996280670166,
      "learning_rate": 0.0007794150417827298,
      "loss": 2.5628,
      "step": 7920
    },
    {
      "epoch": 2.2089136490250696,
      "grad_norm": 1.1854993104934692,
      "learning_rate": 0.0007791364902506964,
      "loss": 2.291,
      "step": 7930
    },
    {
      "epoch": 2.211699164345404,
      "grad_norm": 11.77219009399414,
      "learning_rate": 0.0007788579387186629,
      "loss": 2.4959,
      "step": 7940
    },
    {
      "epoch": 2.214484679665738,
      "grad_norm": 1.7004156112670898,
      "learning_rate": 0.0007785793871866296,
      "loss": 2.473,
      "step": 7950
    },
    {
      "epoch": 2.2172701949860723,
      "grad_norm": 1.5431103706359863,
      "learning_rate": 0.0007783008356545961,
      "loss": 2.4805,
      "step": 7960
    },
    {
      "epoch": 2.220055710306407,
      "grad_norm": 1.3068970441818237,
      "learning_rate": 0.0007780222841225627,
      "loss": 2.3288,
      "step": 7970
    },
    {
      "epoch": 2.222841225626741,
      "grad_norm": 1.8789465427398682,
      "learning_rate": 0.0007777437325905293,
      "loss": 2.4745,
      "step": 7980
    },
    {
      "epoch": 2.2256267409470754,
      "grad_norm": 1.2414215803146362,
      "learning_rate": 0.0007774651810584959,
      "loss": 2.3879,
      "step": 7990
    },
    {
      "epoch": 2.2284122562674096,
      "grad_norm": 1.5291800498962402,
      "learning_rate": 0.0007771866295264624,
      "loss": 2.2045,
      "step": 8000
    },
    {
      "epoch": 2.231197771587744,
      "grad_norm": 1.8238320350646973,
      "learning_rate": 0.0007769080779944289,
      "loss": 2.5326,
      "step": 8010
    },
    {
      "epoch": 2.233983286908078,
      "grad_norm": 1.659321904182434,
      "learning_rate": 0.0007766295264623956,
      "loss": 2.3201,
      "step": 8020
    },
    {
      "epoch": 2.2367688022284122,
      "grad_norm": 1.6889435052871704,
      "learning_rate": 0.0007763509749303621,
      "loss": 2.3365,
      "step": 8030
    },
    {
      "epoch": 2.2395543175487465,
      "grad_norm": 1.5800708532333374,
      "learning_rate": 0.0007760724233983287,
      "loss": 2.3371,
      "step": 8040
    },
    {
      "epoch": 2.2423398328690807,
      "grad_norm": 1.694828748703003,
      "learning_rate": 0.0007757938718662953,
      "loss": 2.3255,
      "step": 8050
    },
    {
      "epoch": 2.245125348189415,
      "grad_norm": 1.4173928499221802,
      "learning_rate": 0.0007755153203342619,
      "loss": 2.5504,
      "step": 8060
    },
    {
      "epoch": 2.247910863509749,
      "grad_norm": 1.6746065616607666,
      "learning_rate": 0.0007752367688022284,
      "loss": 2.3559,
      "step": 8070
    },
    {
      "epoch": 2.2506963788300833,
      "grad_norm": 1.6489975452423096,
      "learning_rate": 0.000774958217270195,
      "loss": 2.2128,
      "step": 8080
    },
    {
      "epoch": 2.253481894150418,
      "grad_norm": 1.6548523902893066,
      "learning_rate": 0.0007746796657381617,
      "loss": 2.1734,
      "step": 8090
    },
    {
      "epoch": 2.256267409470752,
      "grad_norm": 1.378372311592102,
      "learning_rate": 0.0007744011142061281,
      "loss": 2.409,
      "step": 8100
    },
    {
      "epoch": 2.2590529247910864,
      "grad_norm": 1.4121822118759155,
      "learning_rate": 0.0007741225626740947,
      "loss": 2.3331,
      "step": 8110
    },
    {
      "epoch": 2.2618384401114207,
      "grad_norm": 1.4129271507263184,
      "learning_rate": 0.0007738440111420612,
      "loss": 2.2123,
      "step": 8120
    },
    {
      "epoch": 2.264623955431755,
      "grad_norm": 1.301721215248108,
      "learning_rate": 0.0007735654596100279,
      "loss": 2.2118,
      "step": 8130
    },
    {
      "epoch": 2.267409470752089,
      "grad_norm": 1.4975769519805908,
      "learning_rate": 0.0007732869080779944,
      "loss": 2.3808,
      "step": 8140
    },
    {
      "epoch": 2.2701949860724233,
      "grad_norm": 1.443001389503479,
      "learning_rate": 0.000773008356545961,
      "loss": 2.325,
      "step": 8150
    },
    {
      "epoch": 2.2729805013927575,
      "grad_norm": 1.2780228853225708,
      "learning_rate": 0.0007727298050139277,
      "loss": 2.1617,
      "step": 8160
    },
    {
      "epoch": 2.2757660167130918,
      "grad_norm": 1.8316709995269775,
      "learning_rate": 0.0007724512534818942,
      "loss": 2.4177,
      "step": 8170
    },
    {
      "epoch": 2.2785515320334264,
      "grad_norm": 1.8100210428237915,
      "learning_rate": 0.0007721727019498608,
      "loss": 2.5521,
      "step": 8180
    },
    {
      "epoch": 2.2813370473537606,
      "grad_norm": 1.4639196395874023,
      "learning_rate": 0.0007718941504178272,
      "loss": 2.2824,
      "step": 8190
    },
    {
      "epoch": 2.284122562674095,
      "grad_norm": 1.6286786794662476,
      "learning_rate": 0.000771615598885794,
      "loss": 2.4665,
      "step": 8200
    },
    {
      "epoch": 2.286908077994429,
      "grad_norm": 1.4491313695907593,
      "learning_rate": 0.0007713370473537604,
      "loss": 2.3642,
      "step": 8210
    },
    {
      "epoch": 2.2896935933147633,
      "grad_norm": 1.609211802482605,
      "learning_rate": 0.000771058495821727,
      "loss": 2.3582,
      "step": 8220
    },
    {
      "epoch": 2.2924791086350975,
      "grad_norm": 1.4543278217315674,
      "learning_rate": 0.0007707799442896935,
      "loss": 2.2226,
      "step": 8230
    },
    {
      "epoch": 2.2952646239554317,
      "grad_norm": 1.779722809791565,
      "learning_rate": 0.0007705013927576602,
      "loss": 2.5744,
      "step": 8240
    },
    {
      "epoch": 2.298050139275766,
      "grad_norm": 1.5530107021331787,
      "learning_rate": 0.0007702228412256268,
      "loss": 2.1908,
      "step": 8250
    },
    {
      "epoch": 2.3008356545961,
      "grad_norm": 1.3232446908950806,
      "learning_rate": 0.0007699442896935933,
      "loss": 2.4007,
      "step": 8260
    },
    {
      "epoch": 2.3036211699164344,
      "grad_norm": 1.98631751537323,
      "learning_rate": 0.00076966573816156,
      "loss": 2.2721,
      "step": 8270
    },
    {
      "epoch": 2.3064066852367686,
      "grad_norm": 1.2082756757736206,
      "learning_rate": 0.0007693871866295265,
      "loss": 2.5193,
      "step": 8280
    },
    {
      "epoch": 2.309192200557103,
      "grad_norm": 1.429635763168335,
      "learning_rate": 0.000769108635097493,
      "loss": 2.3748,
      "step": 8290
    },
    {
      "epoch": 2.3119777158774375,
      "grad_norm": 1.3761560916900635,
      "learning_rate": 0.0007688300835654595,
      "loss": 2.2374,
      "step": 8300
    },
    {
      "epoch": 2.3147632311977717,
      "grad_norm": 1.6037163734436035,
      "learning_rate": 0.0007685515320334262,
      "loss": 2.6674,
      "step": 8310
    },
    {
      "epoch": 2.317548746518106,
      "grad_norm": 1.893540859222412,
      "learning_rate": 0.0007682729805013928,
      "loss": 2.3362,
      "step": 8320
    },
    {
      "epoch": 2.32033426183844,
      "grad_norm": 1.5208780765533447,
      "learning_rate": 0.0007679944289693593,
      "loss": 2.5034,
      "step": 8330
    },
    {
      "epoch": 2.3231197771587744,
      "grad_norm": 1.7099648714065552,
      "learning_rate": 0.000767715877437326,
      "loss": 2.2658,
      "step": 8340
    },
    {
      "epoch": 2.3259052924791086,
      "grad_norm": 1.3102055788040161,
      "learning_rate": 0.0007674373259052925,
      "loss": 2.3804,
      "step": 8350
    },
    {
      "epoch": 2.328690807799443,
      "grad_norm": 1.7335511445999146,
      "learning_rate": 0.0007671587743732591,
      "loss": 2.5237,
      "step": 8360
    },
    {
      "epoch": 2.331476323119777,
      "grad_norm": 1.2657817602157593,
      "learning_rate": 0.0007668802228412256,
      "loss": 2.3104,
      "step": 8370
    },
    {
      "epoch": 2.3342618384401113,
      "grad_norm": 1.7001936435699463,
      "learning_rate": 0.0007666016713091923,
      "loss": 2.321,
      "step": 8380
    },
    {
      "epoch": 2.337047353760446,
      "grad_norm": 1.4276800155639648,
      "learning_rate": 0.0007663231197771587,
      "loss": 2.4924,
      "step": 8390
    },
    {
      "epoch": 2.33983286908078,
      "grad_norm": 1.2855167388916016,
      "learning_rate": 0.0007660445682451253,
      "loss": 2.252,
      "step": 8400
    },
    {
      "epoch": 2.3426183844011144,
      "grad_norm": 1.7249946594238281,
      "learning_rate": 0.000765766016713092,
      "loss": 2.4354,
      "step": 8410
    },
    {
      "epoch": 2.3454038997214486,
      "grad_norm": 1.858582615852356,
      "learning_rate": 0.0007654874651810585,
      "loss": 2.2055,
      "step": 8420
    },
    {
      "epoch": 2.348189415041783,
      "grad_norm": 1.188062310218811,
      "learning_rate": 0.0007652089136490251,
      "loss": 2.3983,
      "step": 8430
    },
    {
      "epoch": 2.350974930362117,
      "grad_norm": 1.3432443141937256,
      "learning_rate": 0.0007649303621169916,
      "loss": 2.641,
      "step": 8440
    },
    {
      "epoch": 2.3537604456824512,
      "grad_norm": 1.568278431892395,
      "learning_rate": 0.0007646518105849583,
      "loss": 2.4025,
      "step": 8450
    },
    {
      "epoch": 2.3565459610027855,
      "grad_norm": 1.4579803943634033,
      "learning_rate": 0.0007643732590529248,
      "loss": 2.5027,
      "step": 8460
    },
    {
      "epoch": 2.3593314763231197,
      "grad_norm": 1.5864202976226807,
      "learning_rate": 0.0007640947075208914,
      "loss": 2.3277,
      "step": 8470
    },
    {
      "epoch": 2.362116991643454,
      "grad_norm": 1.7353154420852661,
      "learning_rate": 0.000763816155988858,
      "loss": 2.4848,
      "step": 8480
    },
    {
      "epoch": 2.364902506963788,
      "grad_norm": 1.1158220767974854,
      "learning_rate": 0.0007635376044568246,
      "loss": 2.3138,
      "step": 8490
    },
    {
      "epoch": 2.3676880222841223,
      "grad_norm": 1.649505615234375,
      "learning_rate": 0.0007632590529247911,
      "loss": 2.4609,
      "step": 8500
    },
    {
      "epoch": 2.370473537604457,
      "grad_norm": 1.8397389650344849,
      "learning_rate": 0.0007629805013927576,
      "loss": 2.2621,
      "step": 8510
    },
    {
      "epoch": 2.3732590529247912,
      "grad_norm": 2.1041419506073,
      "learning_rate": 0.0007627019498607243,
      "loss": 2.5743,
      "step": 8520
    },
    {
      "epoch": 2.3760445682451254,
      "grad_norm": 1.1643623113632202,
      "learning_rate": 0.0007624233983286908,
      "loss": 2.3382,
      "step": 8530
    },
    {
      "epoch": 2.3788300835654597,
      "grad_norm": 1.3600784540176392,
      "learning_rate": 0.0007621448467966574,
      "loss": 2.3806,
      "step": 8540
    },
    {
      "epoch": 2.381615598885794,
      "grad_norm": 1.3446297645568848,
      "learning_rate": 0.0007618662952646239,
      "loss": 2.2692,
      "step": 8550
    },
    {
      "epoch": 2.384401114206128,
      "grad_norm": 1.2936811447143555,
      "learning_rate": 0.0007615877437325906,
      "loss": 2.3165,
      "step": 8560
    },
    {
      "epoch": 2.3871866295264623,
      "grad_norm": 2.0060832500457764,
      "learning_rate": 0.0007613091922005572,
      "loss": 2.4715,
      "step": 8570
    },
    {
      "epoch": 2.3899721448467965,
      "grad_norm": 1.666473388671875,
      "learning_rate": 0.0007610306406685237,
      "loss": 2.3468,
      "step": 8580
    },
    {
      "epoch": 2.3927576601671308,
      "grad_norm": 2.040447950363159,
      "learning_rate": 0.0007607520891364902,
      "loss": 2.4058,
      "step": 8590
    },
    {
      "epoch": 2.3955431754874654,
      "grad_norm": 2.4067656993865967,
      "learning_rate": 0.0007604735376044568,
      "loss": 2.3954,
      "step": 8600
    },
    {
      "epoch": 2.3983286908077996,
      "grad_norm": 1.7033617496490479,
      "learning_rate": 0.0007601949860724234,
      "loss": 2.4581,
      "step": 8610
    },
    {
      "epoch": 2.401114206128134,
      "grad_norm": 1.7156145572662354,
      "learning_rate": 0.0007599164345403899,
      "loss": 2.4327,
      "step": 8620
    },
    {
      "epoch": 2.403899721448468,
      "grad_norm": 1.8100734949111938,
      "learning_rate": 0.0007596378830083566,
      "loss": 2.3031,
      "step": 8630
    },
    {
      "epoch": 2.4066852367688023,
      "grad_norm": 1.4880170822143555,
      "learning_rate": 0.0007593593314763232,
      "loss": 2.4834,
      "step": 8640
    },
    {
      "epoch": 2.4094707520891365,
      "grad_norm": 1.6055798530578613,
      "learning_rate": 0.0007590807799442897,
      "loss": 2.4941,
      "step": 8650
    },
    {
      "epoch": 2.4122562674094707,
      "grad_norm": 1.4801167249679565,
      "learning_rate": 0.0007588022284122563,
      "loss": 2.5211,
      "step": 8660
    },
    {
      "epoch": 2.415041782729805,
      "grad_norm": 1.358851671218872,
      "learning_rate": 0.0007585236768802229,
      "loss": 2.4684,
      "step": 8670
    },
    {
      "epoch": 2.417827298050139,
      "grad_norm": 1.5158265829086304,
      "learning_rate": 0.0007582451253481895,
      "loss": 2.3383,
      "step": 8680
    },
    {
      "epoch": 2.4206128133704734,
      "grad_norm": 1.6192301511764526,
      "learning_rate": 0.0007579665738161559,
      "loss": 2.3088,
      "step": 8690
    },
    {
      "epoch": 2.4233983286908076,
      "grad_norm": 1.941269874572754,
      "learning_rate": 0.0007576880222841226,
      "loss": 2.197,
      "step": 8700
    },
    {
      "epoch": 2.426183844011142,
      "grad_norm": 1.0830644369125366,
      "learning_rate": 0.0007574094707520891,
      "loss": 2.3111,
      "step": 8710
    },
    {
      "epoch": 2.4289693593314765,
      "grad_norm": 1.7332136631011963,
      "learning_rate": 0.0007571309192200557,
      "loss": 2.4254,
      "step": 8720
    },
    {
      "epoch": 2.4317548746518107,
      "grad_norm": 1.9213546514511108,
      "learning_rate": 0.0007568523676880223,
      "loss": 2.5167,
      "step": 8730
    },
    {
      "epoch": 2.434540389972145,
      "grad_norm": 1.5074533224105835,
      "learning_rate": 0.0007565738161559889,
      "loss": 2.3613,
      "step": 8740
    },
    {
      "epoch": 2.437325905292479,
      "grad_norm": 1.3193397521972656,
      "learning_rate": 0.0007562952646239555,
      "loss": 2.2702,
      "step": 8750
    },
    {
      "epoch": 2.4401114206128134,
      "grad_norm": 1.4146932363510132,
      "learning_rate": 0.000756016713091922,
      "loss": 2.3661,
      "step": 8760
    },
    {
      "epoch": 2.4428969359331476,
      "grad_norm": 1.4304039478302002,
      "learning_rate": 0.0007557381615598886,
      "loss": 2.3885,
      "step": 8770
    },
    {
      "epoch": 2.445682451253482,
      "grad_norm": 1.6335968971252441,
      "learning_rate": 0.0007554596100278552,
      "loss": 2.3008,
      "step": 8780
    },
    {
      "epoch": 2.448467966573816,
      "grad_norm": 1.3721731901168823,
      "learning_rate": 0.0007551810584958217,
      "loss": 2.3996,
      "step": 8790
    },
    {
      "epoch": 2.4512534818941503,
      "grad_norm": 1.8505229949951172,
      "learning_rate": 0.0007549025069637883,
      "loss": 2.57,
      "step": 8800
    },
    {
      "epoch": 2.4540389972144845,
      "grad_norm": 1.3937355279922485,
      "learning_rate": 0.0007546239554317549,
      "loss": 2.3527,
      "step": 8810
    },
    {
      "epoch": 2.456824512534819,
      "grad_norm": 1.2241661548614502,
      "learning_rate": 0.0007543454038997215,
      "loss": 2.5286,
      "step": 8820
    },
    {
      "epoch": 2.4596100278551534,
      "grad_norm": 1.6129497289657593,
      "learning_rate": 0.000754066852367688,
      "loss": 2.4911,
      "step": 8830
    },
    {
      "epoch": 2.4623955431754876,
      "grad_norm": 1.274300217628479,
      "learning_rate": 0.0007537883008356546,
      "loss": 2.3783,
      "step": 8840
    },
    {
      "epoch": 2.465181058495822,
      "grad_norm": 1.783322811126709,
      "learning_rate": 0.0007535097493036212,
      "loss": 2.4255,
      "step": 8850
    },
    {
      "epoch": 2.467966573816156,
      "grad_norm": 1.4318299293518066,
      "learning_rate": 0.0007532311977715878,
      "loss": 2.4203,
      "step": 8860
    },
    {
      "epoch": 2.4707520891364902,
      "grad_norm": 1.2320953607559204,
      "learning_rate": 0.0007529526462395543,
      "loss": 2.4384,
      "step": 8870
    },
    {
      "epoch": 2.4735376044568245,
      "grad_norm": 1.484663486480713,
      "learning_rate": 0.000752674094707521,
      "loss": 2.6078,
      "step": 8880
    },
    {
      "epoch": 2.4763231197771587,
      "grad_norm": 1.4962657690048218,
      "learning_rate": 0.0007523955431754875,
      "loss": 2.288,
      "step": 8890
    },
    {
      "epoch": 2.479108635097493,
      "grad_norm": 1.4237829446792603,
      "learning_rate": 0.000752116991643454,
      "loss": 2.2627,
      "step": 8900
    },
    {
      "epoch": 2.481894150417827,
      "grad_norm": 1.5509065389633179,
      "learning_rate": 0.0007518384401114206,
      "loss": 2.2403,
      "step": 8910
    },
    {
      "epoch": 2.4846796657381613,
      "grad_norm": 2.1390247344970703,
      "learning_rate": 0.0007515598885793872,
      "loss": 2.369,
      "step": 8920
    },
    {
      "epoch": 2.487465181058496,
      "grad_norm": 1.8018276691436768,
      "learning_rate": 0.0007512813370473538,
      "loss": 2.2902,
      "step": 8930
    },
    {
      "epoch": 2.4902506963788302,
      "grad_norm": 3.709095001220703,
      "learning_rate": 0.0007510027855153203,
      "loss": 2.371,
      "step": 8940
    },
    {
      "epoch": 2.4930362116991645,
      "grad_norm": 1.687900185585022,
      "learning_rate": 0.0007507242339832869,
      "loss": 2.3376,
      "step": 8950
    },
    {
      "epoch": 2.4958217270194987,
      "grad_norm": 1.1680067777633667,
      "learning_rate": 0.0007504456824512536,
      "loss": 2.1983,
      "step": 8960
    },
    {
      "epoch": 2.498607242339833,
      "grad_norm": 1.3442914485931396,
      "learning_rate": 0.0007501671309192201,
      "loss": 2.4378,
      "step": 8970
    },
    {
      "epoch": 2.501392757660167,
      "grad_norm": 1.4153529405593872,
      "learning_rate": 0.0007498885793871867,
      "loss": 2.2399,
      "step": 8980
    },
    {
      "epoch": 2.5041782729805013,
      "grad_norm": 1.8294589519500732,
      "learning_rate": 0.0007496100278551532,
      "loss": 2.3473,
      "step": 8990
    },
    {
      "epoch": 2.5069637883008355,
      "grad_norm": 1.570271611213684,
      "learning_rate": 0.0007493314763231198,
      "loss": 2.4066,
      "step": 9000
    },
    {
      "epoch": 2.5097493036211698,
      "grad_norm": 1.3333112001419067,
      "learning_rate": 0.0007490529247910863,
      "loss": 2.3706,
      "step": 9010
    },
    {
      "epoch": 2.5125348189415044,
      "grad_norm": 2.0062131881713867,
      "learning_rate": 0.0007487743732590529,
      "loss": 2.284,
      "step": 9020
    },
    {
      "epoch": 2.5153203342618387,
      "grad_norm": 1.5587711334228516,
      "learning_rate": 0.0007484958217270195,
      "loss": 2.3791,
      "step": 9030
    },
    {
      "epoch": 2.518105849582173,
      "grad_norm": 1.1675277948379517,
      "learning_rate": 0.0007482172701949861,
      "loss": 2.3861,
      "step": 9040
    },
    {
      "epoch": 2.520891364902507,
      "grad_norm": 1.6050050258636475,
      "learning_rate": 0.0007479387186629527,
      "loss": 2.2606,
      "step": 9050
    },
    {
      "epoch": 2.5236768802228413,
      "grad_norm": 1.2020965814590454,
      "learning_rate": 0.0007476601671309193,
      "loss": 2.3534,
      "step": 9060
    },
    {
      "epoch": 2.5264623955431755,
      "grad_norm": 1.5779386758804321,
      "learning_rate": 0.0007473816155988859,
      "loss": 2.1643,
      "step": 9070
    },
    {
      "epoch": 2.5292479108635098,
      "grad_norm": 1.533983588218689,
      "learning_rate": 0.0007471030640668523,
      "loss": 2.368,
      "step": 9080
    },
    {
      "epoch": 2.532033426183844,
      "grad_norm": 1.915905237197876,
      "learning_rate": 0.0007468245125348189,
      "loss": 2.4328,
      "step": 9090
    },
    {
      "epoch": 2.534818941504178,
      "grad_norm": 1.019263744354248,
      "learning_rate": 0.0007465459610027855,
      "loss": 2.2588,
      "step": 9100
    },
    {
      "epoch": 2.5376044568245124,
      "grad_norm": 1.7605804204940796,
      "learning_rate": 0.0007462674094707521,
      "loss": 2.2942,
      "step": 9110
    },
    {
      "epoch": 2.5403899721448466,
      "grad_norm": 1.4644399881362915,
      "learning_rate": 0.0007459888579387186,
      "loss": 2.2719,
      "step": 9120
    },
    {
      "epoch": 2.543175487465181,
      "grad_norm": 1.226865291595459,
      "learning_rate": 0.0007457103064066852,
      "loss": 2.3404,
      "step": 9130
    },
    {
      "epoch": 2.545961002785515,
      "grad_norm": 1.5778697729110718,
      "learning_rate": 0.0007454317548746519,
      "loss": 2.4342,
      "step": 9140
    },
    {
      "epoch": 2.5487465181058497,
      "grad_norm": 1.2344343662261963,
      "learning_rate": 0.0007451532033426184,
      "loss": 2.3326,
      "step": 9150
    },
    {
      "epoch": 2.551532033426184,
      "grad_norm": 1.4098068475723267,
      "learning_rate": 0.000744874651810585,
      "loss": 2.0531,
      "step": 9160
    },
    {
      "epoch": 2.554317548746518,
      "grad_norm": 1.165583610534668,
      "learning_rate": 0.0007445961002785516,
      "loss": 2.45,
      "step": 9170
    },
    {
      "epoch": 2.5571030640668524,
      "grad_norm": 1.4263547658920288,
      "learning_rate": 0.0007443175487465182,
      "loss": 2.4217,
      "step": 9180
    },
    {
      "epoch": 2.5598885793871866,
      "grad_norm": 1.4891533851623535,
      "learning_rate": 0.0007440389972144846,
      "loss": 2.3699,
      "step": 9190
    },
    {
      "epoch": 2.562674094707521,
      "grad_norm": 1.0862274169921875,
      "learning_rate": 0.0007437604456824512,
      "loss": 2.3378,
      "step": 9200
    },
    {
      "epoch": 2.565459610027855,
      "grad_norm": 1.303719401359558,
      "learning_rate": 0.0007434818941504179,
      "loss": 2.2526,
      "step": 9210
    },
    {
      "epoch": 2.5682451253481893,
      "grad_norm": 1.3900896310806274,
      "learning_rate": 0.0007432033426183844,
      "loss": 2.3013,
      "step": 9220
    },
    {
      "epoch": 2.571030640668524,
      "grad_norm": 1.7594650983810425,
      "learning_rate": 0.000742924791086351,
      "loss": 2.4646,
      "step": 9230
    },
    {
      "epoch": 2.573816155988858,
      "grad_norm": 1.7582216262817383,
      "learning_rate": 0.0007426462395543176,
      "loss": 2.213,
      "step": 9240
    },
    {
      "epoch": 2.5766016713091924,
      "grad_norm": 1.2949014902114868,
      "learning_rate": 0.0007423676880222842,
      "loss": 2.3016,
      "step": 9250
    },
    {
      "epoch": 2.5793871866295266,
      "grad_norm": 1.7097265720367432,
      "learning_rate": 0.0007420891364902507,
      "loss": 2.3744,
      "step": 9260
    },
    {
      "epoch": 2.582172701949861,
      "grad_norm": 1.9617414474487305,
      "learning_rate": 0.0007418105849582173,
      "loss": 2.2521,
      "step": 9270
    },
    {
      "epoch": 2.584958217270195,
      "grad_norm": 1.1380188465118408,
      "learning_rate": 0.0007415320334261838,
      "loss": 2.4955,
      "step": 9280
    },
    {
      "epoch": 2.5877437325905293,
      "grad_norm": 1.548980712890625,
      "learning_rate": 0.0007412534818941504,
      "loss": 2.6094,
      "step": 9290
    },
    {
      "epoch": 2.5905292479108635,
      "grad_norm": 1.1379472017288208,
      "learning_rate": 0.000740974930362117,
      "loss": 2.3356,
      "step": 9300
    },
    {
      "epoch": 2.5933147632311977,
      "grad_norm": 1.518998622894287,
      "learning_rate": 0.0007406963788300835,
      "loss": 2.4898,
      "step": 9310
    },
    {
      "epoch": 2.596100278551532,
      "grad_norm": 1.3997012376785278,
      "learning_rate": 0.0007404178272980502,
      "loss": 2.4226,
      "step": 9320
    },
    {
      "epoch": 2.598885793871866,
      "grad_norm": 1.508576512336731,
      "learning_rate": 0.0007401392757660167,
      "loss": 2.1546,
      "step": 9330
    },
    {
      "epoch": 2.6016713091922004,
      "grad_norm": 1.6012190580368042,
      "learning_rate": 0.0007398607242339833,
      "loss": 2.4713,
      "step": 9340
    },
    {
      "epoch": 2.6044568245125346,
      "grad_norm": 1.972005844116211,
      "learning_rate": 0.0007395821727019499,
      "loss": 2.3832,
      "step": 9350
    },
    {
      "epoch": 2.6072423398328692,
      "grad_norm": 1.5368471145629883,
      "learning_rate": 0.0007393036211699165,
      "loss": 2.1918,
      "step": 9360
    },
    {
      "epoch": 2.6100278551532035,
      "grad_norm": 1.718675971031189,
      "learning_rate": 0.0007390250696378831,
      "loss": 2.5025,
      "step": 9370
    },
    {
      "epoch": 2.6128133704735377,
      "grad_norm": 1.7549974918365479,
      "learning_rate": 0.0007387465181058495,
      "loss": 2.3588,
      "step": 9380
    },
    {
      "epoch": 2.615598885793872,
      "grad_norm": 1.4878015518188477,
      "learning_rate": 0.0007384679665738162,
      "loss": 2.3635,
      "step": 9390
    },
    {
      "epoch": 2.618384401114206,
      "grad_norm": 1.3782914876937866,
      "learning_rate": 0.0007381894150417827,
      "loss": 2.2169,
      "step": 9400
    },
    {
      "epoch": 2.6211699164345403,
      "grad_norm": 1.1813881397247314,
      "learning_rate": 0.0007379108635097493,
      "loss": 2.5836,
      "step": 9410
    },
    {
      "epoch": 2.6239554317548746,
      "grad_norm": 1.7750219106674194,
      "learning_rate": 0.0007376323119777159,
      "loss": 2.5136,
      "step": 9420
    },
    {
      "epoch": 2.6267409470752088,
      "grad_norm": 1.316391110420227,
      "learning_rate": 0.0007373537604456825,
      "loss": 2.4186,
      "step": 9430
    },
    {
      "epoch": 2.6295264623955434,
      "grad_norm": 1.3804583549499512,
      "learning_rate": 0.000737075208913649,
      "loss": 2.5069,
      "step": 9440
    },
    {
      "epoch": 2.6323119777158777,
      "grad_norm": 1.7381277084350586,
      "learning_rate": 0.0007367966573816156,
      "loss": 2.2635,
      "step": 9450
    },
    {
      "epoch": 2.635097493036212,
      "grad_norm": 1.3893578052520752,
      "learning_rate": 0.0007365181058495823,
      "loss": 2.3964,
      "step": 9460
    },
    {
      "epoch": 2.637883008356546,
      "grad_norm": 1.339447021484375,
      "learning_rate": 0.0007362395543175488,
      "loss": 2.1194,
      "step": 9470
    },
    {
      "epoch": 2.6406685236768803,
      "grad_norm": 2.664994716644287,
      "learning_rate": 0.0007359610027855153,
      "loss": 2.5513,
      "step": 9480
    },
    {
      "epoch": 2.6434540389972145,
      "grad_norm": 1.5504966974258423,
      "learning_rate": 0.0007356824512534818,
      "loss": 2.4332,
      "step": 9490
    },
    {
      "epoch": 2.6462395543175488,
      "grad_norm": 1.410773515701294,
      "learning_rate": 0.0007354038997214485,
      "loss": 2.5267,
      "step": 9500
    },
    {
      "epoch": 2.649025069637883,
      "grad_norm": 1.1298623085021973,
      "learning_rate": 0.000735125348189415,
      "loss": 2.2621,
      "step": 9510
    },
    {
      "epoch": 2.651810584958217,
      "grad_norm": 1.6642322540283203,
      "learning_rate": 0.0007348467966573816,
      "loss": 2.2976,
      "step": 9520
    },
    {
      "epoch": 2.6545961002785514,
      "grad_norm": 1.237148404121399,
      "learning_rate": 0.0007345682451253483,
      "loss": 2.4156,
      "step": 9530
    },
    {
      "epoch": 2.6573816155988856,
      "grad_norm": 1.7186014652252197,
      "learning_rate": 0.0007342896935933148,
      "loss": 2.4561,
      "step": 9540
    },
    {
      "epoch": 2.66016713091922,
      "grad_norm": 1.4074742794036865,
      "learning_rate": 0.0007340111420612814,
      "loss": 2.3973,
      "step": 9550
    },
    {
      "epoch": 2.662952646239554,
      "grad_norm": 1.7260712385177612,
      "learning_rate": 0.0007337325905292479,
      "loss": 2.3247,
      "step": 9560
    },
    {
      "epoch": 2.6657381615598887,
      "grad_norm": 1.402286171913147,
      "learning_rate": 0.0007334540389972146,
      "loss": 2.3292,
      "step": 9570
    },
    {
      "epoch": 2.668523676880223,
      "grad_norm": 1.8071842193603516,
      "learning_rate": 0.000733175487465181,
      "loss": 2.3509,
      "step": 9580
    },
    {
      "epoch": 2.671309192200557,
      "grad_norm": 1.2411383390426636,
      "learning_rate": 0.0007328969359331476,
      "loss": 2.4733,
      "step": 9590
    },
    {
      "epoch": 2.6740947075208914,
      "grad_norm": 1.1988577842712402,
      "learning_rate": 0.0007326183844011142,
      "loss": 2.1967,
      "step": 9600
    },
    {
      "epoch": 2.6768802228412256,
      "grad_norm": 1.3097485303878784,
      "learning_rate": 0.0007323398328690808,
      "loss": 2.2342,
      "step": 9610
    },
    {
      "epoch": 2.67966573816156,
      "grad_norm": 1.2092360258102417,
      "learning_rate": 0.0007320612813370474,
      "loss": 2.2821,
      "step": 9620
    },
    {
      "epoch": 2.682451253481894,
      "grad_norm": 1.9631106853485107,
      "learning_rate": 0.0007317827298050139,
      "loss": 2.4322,
      "step": 9630
    },
    {
      "epoch": 2.6852367688022283,
      "grad_norm": 1.7552547454833984,
      "learning_rate": 0.0007315041782729806,
      "loss": 2.3192,
      "step": 9640
    },
    {
      "epoch": 2.688022284122563,
      "grad_norm": 1.1332685947418213,
      "learning_rate": 0.0007312256267409471,
      "loss": 2.4838,
      "step": 9650
    },
    {
      "epoch": 2.690807799442897,
      "grad_norm": 1.6888245344161987,
      "learning_rate": 0.0007309470752089137,
      "loss": 2.4708,
      "step": 9660
    },
    {
      "epoch": 2.6935933147632314,
      "grad_norm": 1.396222472190857,
      "learning_rate": 0.0007306685236768801,
      "loss": 2.4466,
      "step": 9670
    },
    {
      "epoch": 2.6963788300835656,
      "grad_norm": 1.589674472808838,
      "learning_rate": 0.0007303899721448468,
      "loss": 2.3874,
      "step": 9680
    },
    {
      "epoch": 2.6991643454039,
      "grad_norm": 1.5221251249313354,
      "learning_rate": 0.0007301114206128134,
      "loss": 2.4088,
      "step": 9690
    },
    {
      "epoch": 2.701949860724234,
      "grad_norm": 1.3884721994400024,
      "learning_rate": 0.0007298328690807799,
      "loss": 2.5341,
      "step": 9700
    },
    {
      "epoch": 2.7047353760445683,
      "grad_norm": 1.3148475885391235,
      "learning_rate": 0.0007295543175487466,
      "loss": 2.3889,
      "step": 9710
    },
    {
      "epoch": 2.7075208913649025,
      "grad_norm": 2.054696559906006,
      "learning_rate": 0.0007292757660167131,
      "loss": 2.3956,
      "step": 9720
    },
    {
      "epoch": 2.7103064066852367,
      "grad_norm": 1.3521347045898438,
      "learning_rate": 0.0007289972144846797,
      "loss": 2.4478,
      "step": 9730
    },
    {
      "epoch": 2.713091922005571,
      "grad_norm": 1.382061243057251,
      "learning_rate": 0.0007287186629526462,
      "loss": 2.3275,
      "step": 9740
    },
    {
      "epoch": 2.715877437325905,
      "grad_norm": 1.512724757194519,
      "learning_rate": 0.0007284401114206129,
      "loss": 2.2149,
      "step": 9750
    },
    {
      "epoch": 2.7186629526462394,
      "grad_norm": 1.536775827407837,
      "learning_rate": 0.0007281615598885794,
      "loss": 2.1978,
      "step": 9760
    },
    {
      "epoch": 2.7214484679665736,
      "grad_norm": 1.4560428857803345,
      "learning_rate": 0.000727883008356546,
      "loss": 2.3823,
      "step": 9770
    },
    {
      "epoch": 2.724233983286908,
      "grad_norm": 1.301755428314209,
      "learning_rate": 0.0007276044568245126,
      "loss": 2.4815,
      "step": 9780
    },
    {
      "epoch": 2.7270194986072425,
      "grad_norm": 1.5249533653259277,
      "learning_rate": 0.0007273259052924791,
      "loss": 2.3098,
      "step": 9790
    },
    {
      "epoch": 2.7298050139275767,
      "grad_norm": 1.5404889583587646,
      "learning_rate": 0.0007270473537604457,
      "loss": 2.4713,
      "step": 9800
    },
    {
      "epoch": 2.732590529247911,
      "grad_norm": 1.515488624572754,
      "learning_rate": 0.0007267688022284122,
      "loss": 2.2766,
      "step": 9810
    },
    {
      "epoch": 2.735376044568245,
      "grad_norm": 1.5301793813705444,
      "learning_rate": 0.0007264902506963789,
      "loss": 2.2548,
      "step": 9820
    },
    {
      "epoch": 2.7381615598885793,
      "grad_norm": 1.6182140111923218,
      "learning_rate": 0.0007262116991643454,
      "loss": 2.3408,
      "step": 9830
    },
    {
      "epoch": 2.7409470752089136,
      "grad_norm": 1.9362878799438477,
      "learning_rate": 0.000725933147632312,
      "loss": 2.1249,
      "step": 9840
    },
    {
      "epoch": 2.743732590529248,
      "grad_norm": 1.3543976545333862,
      "learning_rate": 0.0007256545961002786,
      "loss": 2.3848,
      "step": 9850
    },
    {
      "epoch": 2.7465181058495824,
      "grad_norm": 1.181307077407837,
      "learning_rate": 0.0007253760445682452,
      "loss": 2.3297,
      "step": 9860
    },
    {
      "epoch": 2.7493036211699167,
      "grad_norm": 1.3723430633544922,
      "learning_rate": 0.0007250974930362118,
      "loss": 2.3242,
      "step": 9870
    },
    {
      "epoch": 2.752089136490251,
      "grad_norm": 1.5478616952896118,
      "learning_rate": 0.0007248189415041782,
      "loss": 2.4671,
      "step": 9880
    },
    {
      "epoch": 2.754874651810585,
      "grad_norm": 2.053260326385498,
      "learning_rate": 0.0007245403899721449,
      "loss": 2.2201,
      "step": 9890
    },
    {
      "epoch": 2.7576601671309193,
      "grad_norm": 1.4442824125289917,
      "learning_rate": 0.0007242618384401114,
      "loss": 2.2985,
      "step": 9900
    },
    {
      "epoch": 2.7604456824512535,
      "grad_norm": 1.439883828163147,
      "learning_rate": 0.000723983286908078,
      "loss": 2.4169,
      "step": 9910
    },
    {
      "epoch": 2.7632311977715878,
      "grad_norm": 1.5166566371917725,
      "learning_rate": 0.0007237047353760445,
      "loss": 2.5257,
      "step": 9920
    },
    {
      "epoch": 2.766016713091922,
      "grad_norm": 1.4951491355895996,
      "learning_rate": 0.0007234261838440112,
      "loss": 2.4755,
      "step": 9930
    },
    {
      "epoch": 2.768802228412256,
      "grad_norm": 1.2902568578720093,
      "learning_rate": 0.0007231476323119778,
      "loss": 2.2731,
      "step": 9940
    },
    {
      "epoch": 2.7715877437325904,
      "grad_norm": 1.7049360275268555,
      "learning_rate": 0.0007228690807799443,
      "loss": 2.3305,
      "step": 9950
    },
    {
      "epoch": 2.7743732590529246,
      "grad_norm": 1.5867469310760498,
      "learning_rate": 0.000722590529247911,
      "loss": 2.0308,
      "step": 9960
    },
    {
      "epoch": 2.777158774373259,
      "grad_norm": 1.3668726682662964,
      "learning_rate": 0.0007223119777158774,
      "loss": 2.36,
      "step": 9970
    },
    {
      "epoch": 2.779944289693593,
      "grad_norm": 1.441586971282959,
      "learning_rate": 0.000722033426183844,
      "loss": 2.4351,
      "step": 9980
    },
    {
      "epoch": 2.7827298050139273,
      "grad_norm": 1.3021416664123535,
      "learning_rate": 0.0007217548746518105,
      "loss": 2.4502,
      "step": 9990
    },
    {
      "epoch": 2.785515320334262,
      "grad_norm": 1.544910192489624,
      "learning_rate": 0.0007214763231197772,
      "loss": 2.3614,
      "step": 10000
    },
    {
      "epoch": 2.788300835654596,
      "grad_norm": 1.5383659601211548,
      "learning_rate": 0.0007211977715877438,
      "loss": 2.543,
      "step": 10010
    },
    {
      "epoch": 2.7910863509749304,
      "grad_norm": 1.2709949016571045,
      "learning_rate": 0.0007209192200557103,
      "loss": 2.4522,
      "step": 10020
    },
    {
      "epoch": 2.7938718662952646,
      "grad_norm": 1.2867791652679443,
      "learning_rate": 0.0007206406685236769,
      "loss": 2.4953,
      "step": 10030
    },
    {
      "epoch": 2.796657381615599,
      "grad_norm": 1.6536414623260498,
      "learning_rate": 0.0007203621169916435,
      "loss": 2.4587,
      "step": 10040
    },
    {
      "epoch": 2.799442896935933,
      "grad_norm": 1.2190901041030884,
      "learning_rate": 0.0007200835654596101,
      "loss": 2.4656,
      "step": 10050
    },
    {
      "epoch": 2.8022284122562673,
      "grad_norm": 1.4217512607574463,
      "learning_rate": 0.0007198050139275766,
      "loss": 2.521,
      "step": 10060
    },
    {
      "epoch": 2.8050139275766015,
      "grad_norm": 1.6850361824035645,
      "learning_rate": 0.0007195264623955433,
      "loss": 2.2879,
      "step": 10070
    },
    {
      "epoch": 2.807799442896936,
      "grad_norm": 1.5242919921875,
      "learning_rate": 0.0007192479108635097,
      "loss": 2.2142,
      "step": 10080
    },
    {
      "epoch": 2.8105849582172704,
      "grad_norm": 1.8045217990875244,
      "learning_rate": 0.0007189693593314763,
      "loss": 2.3522,
      "step": 10090
    },
    {
      "epoch": 2.8133704735376046,
      "grad_norm": 1.494310975074768,
      "learning_rate": 0.0007186908077994429,
      "loss": 2.5575,
      "step": 10100
    },
    {
      "epoch": 2.816155988857939,
      "grad_norm": 1.4451628923416138,
      "learning_rate": 0.0007184122562674095,
      "loss": 2.3526,
      "step": 10110
    },
    {
      "epoch": 2.818941504178273,
      "grad_norm": 2.2249255180358887,
      "learning_rate": 0.0007181337047353761,
      "loss": 2.2404,
      "step": 10120
    },
    {
      "epoch": 2.8217270194986073,
      "grad_norm": 1.606014370918274,
      "learning_rate": 0.0007178551532033426,
      "loss": 2.3376,
      "step": 10130
    },
    {
      "epoch": 2.8245125348189415,
      "grad_norm": 2.39663028717041,
      "learning_rate": 0.0007175766016713092,
      "loss": 2.3961,
      "step": 10140
    },
    {
      "epoch": 2.8272980501392757,
      "grad_norm": 1.2290235757827759,
      "learning_rate": 0.0007172980501392758,
      "loss": 2.3134,
      "step": 10150
    },
    {
      "epoch": 2.83008356545961,
      "grad_norm": 1.5230097770690918,
      "learning_rate": 0.0007170194986072424,
      "loss": 2.5553,
      "step": 10160
    },
    {
      "epoch": 2.832869080779944,
      "grad_norm": 1.5864174365997314,
      "learning_rate": 0.0007167409470752089,
      "loss": 2.4607,
      "step": 10170
    },
    {
      "epoch": 2.8356545961002784,
      "grad_norm": 1.0669023990631104,
      "learning_rate": 0.0007164623955431755,
      "loss": 2.3368,
      "step": 10180
    },
    {
      "epoch": 2.8384401114206126,
      "grad_norm": 1.299668550491333,
      "learning_rate": 0.0007161838440111421,
      "loss": 2.3722,
      "step": 10190
    },
    {
      "epoch": 2.841225626740947,
      "grad_norm": 1.2640379667282104,
      "learning_rate": 0.0007159052924791086,
      "loss": 2.4558,
      "step": 10200
    },
    {
      "epoch": 2.8440111420612815,
      "grad_norm": 1.4137020111083984,
      "learning_rate": 0.0007156267409470752,
      "loss": 2.2459,
      "step": 10210
    },
    {
      "epoch": 2.8467966573816157,
      "grad_norm": 1.0397700071334839,
      "learning_rate": 0.0007153481894150418,
      "loss": 2.3072,
      "step": 10220
    },
    {
      "epoch": 2.84958217270195,
      "grad_norm": 1.0485267639160156,
      "learning_rate": 0.0007150696378830084,
      "loss": 2.185,
      "step": 10230
    },
    {
      "epoch": 2.852367688022284,
      "grad_norm": 1.1531760692596436,
      "learning_rate": 0.0007147910863509749,
      "loss": 2.3959,
      "step": 10240
    },
    {
      "epoch": 2.8551532033426184,
      "grad_norm": 1.471862554550171,
      "learning_rate": 0.0007145125348189416,
      "loss": 2.5417,
      "step": 10250
    },
    {
      "epoch": 2.8579387186629526,
      "grad_norm": 1.3935881853103638,
      "learning_rate": 0.0007142339832869082,
      "loss": 2.533,
      "step": 10260
    },
    {
      "epoch": 2.860724233983287,
      "grad_norm": 1.1069316864013672,
      "learning_rate": 0.0007139554317548746,
      "loss": 2.4247,
      "step": 10270
    },
    {
      "epoch": 2.863509749303621,
      "grad_norm": 1.241066813468933,
      "learning_rate": 0.0007136768802228412,
      "loss": 2.3562,
      "step": 10280
    },
    {
      "epoch": 2.8662952646239557,
      "grad_norm": 1.3428879976272583,
      "learning_rate": 0.0007133983286908078,
      "loss": 2.4348,
      "step": 10290
    },
    {
      "epoch": 2.86908077994429,
      "grad_norm": 1.9236162900924683,
      "learning_rate": 0.0007131197771587744,
      "loss": 2.3383,
      "step": 10300
    },
    {
      "epoch": 2.871866295264624,
      "grad_norm": 1.576094150543213,
      "learning_rate": 0.0007128412256267409,
      "loss": 2.4974,
      "step": 10310
    },
    {
      "epoch": 2.8746518105849583,
      "grad_norm": 1.3260782957077026,
      "learning_rate": 0.0007125626740947075,
      "loss": 2.2554,
      "step": 10320
    },
    {
      "epoch": 2.8774373259052926,
      "grad_norm": 1.9240928888320923,
      "learning_rate": 0.0007122841225626742,
      "loss": 2.2778,
      "step": 10330
    },
    {
      "epoch": 2.8802228412256268,
      "grad_norm": 2.150343179702759,
      "learning_rate": 0.0007120055710306407,
      "loss": 2.1977,
      "step": 10340
    },
    {
      "epoch": 2.883008356545961,
      "grad_norm": 1.2291816473007202,
      "learning_rate": 0.0007117270194986073,
      "loss": 2.3444,
      "step": 10350
    },
    {
      "epoch": 2.885793871866295,
      "grad_norm": 1.697007179260254,
      "learning_rate": 0.0007114484679665739,
      "loss": 2.3879,
      "step": 10360
    },
    {
      "epoch": 2.8885793871866294,
      "grad_norm": 1.3901231288909912,
      "learning_rate": 0.0007111699164345404,
      "loss": 2.4012,
      "step": 10370
    },
    {
      "epoch": 2.8913649025069637,
      "grad_norm": 1.263445496559143,
      "learning_rate": 0.0007108913649025069,
      "loss": 2.5148,
      "step": 10380
    },
    {
      "epoch": 2.894150417827298,
      "grad_norm": 1.6851438283920288,
      "learning_rate": 0.0007106128133704735,
      "loss": 2.4494,
      "step": 10390
    },
    {
      "epoch": 2.896935933147632,
      "grad_norm": 1.415812611579895,
      "learning_rate": 0.0007103342618384401,
      "loss": 2.3494,
      "step": 10400
    },
    {
      "epoch": 2.8997214484679663,
      "grad_norm": 1.8114591836929321,
      "learning_rate": 0.0007100557103064067,
      "loss": 2.6285,
      "step": 10410
    },
    {
      "epoch": 2.902506963788301,
      "grad_norm": 1.2195487022399902,
      "learning_rate": 0.0007097771587743733,
      "loss": 2.4072,
      "step": 10420
    },
    {
      "epoch": 2.905292479108635,
      "grad_norm": 1.2362065315246582,
      "learning_rate": 0.0007094986072423399,
      "loss": 2.321,
      "step": 10430
    },
    {
      "epoch": 2.9080779944289694,
      "grad_norm": 1.4467740058898926,
      "learning_rate": 0.0007092200557103065,
      "loss": 2.4496,
      "step": 10440
    },
    {
      "epoch": 2.9108635097493036,
      "grad_norm": 1.351852536201477,
      "learning_rate": 0.000708941504178273,
      "loss": 2.2785,
      "step": 10450
    },
    {
      "epoch": 2.913649025069638,
      "grad_norm": 1.5142446756362915,
      "learning_rate": 0.0007086629526462395,
      "loss": 2.3368,
      "step": 10460
    },
    {
      "epoch": 2.916434540389972,
      "grad_norm": 2.5077648162841797,
      "learning_rate": 0.0007083844011142061,
      "loss": 2.3247,
      "step": 10470
    },
    {
      "epoch": 2.9192200557103063,
      "grad_norm": 1.3967523574829102,
      "learning_rate": 0.0007081058495821727,
      "loss": 2.297,
      "step": 10480
    },
    {
      "epoch": 2.9220055710306405,
      "grad_norm": 1.6617259979248047,
      "learning_rate": 0.0007078272980501393,
      "loss": 2.1763,
      "step": 10490
    },
    {
      "epoch": 2.924791086350975,
      "grad_norm": 1.4823755025863647,
      "learning_rate": 0.0007075487465181058,
      "loss": 2.4303,
      "step": 10500
    },
    {
      "epoch": 2.9275766016713094,
      "grad_norm": 1.4646761417388916,
      "learning_rate": 0.0007072701949860725,
      "loss": 2.316,
      "step": 10510
    },
    {
      "epoch": 2.9303621169916436,
      "grad_norm": 1.2804663181304932,
      "learning_rate": 0.000706991643454039,
      "loss": 2.4062,
      "step": 10520
    },
    {
      "epoch": 2.933147632311978,
      "grad_norm": 1.3429853916168213,
      "learning_rate": 0.0007067130919220056,
      "loss": 2.3971,
      "step": 10530
    },
    {
      "epoch": 2.935933147632312,
      "grad_norm": 1.0549006462097168,
      "learning_rate": 0.0007064345403899722,
      "loss": 2.1992,
      "step": 10540
    },
    {
      "epoch": 2.9387186629526463,
      "grad_norm": 1.4732582569122314,
      "learning_rate": 0.0007061559888579388,
      "loss": 2.3099,
      "step": 10550
    },
    {
      "epoch": 2.9415041782729805,
      "grad_norm": 1.7424421310424805,
      "learning_rate": 0.0007058774373259052,
      "loss": 2.3573,
      "step": 10560
    },
    {
      "epoch": 2.9442896935933147,
      "grad_norm": 1.3314623832702637,
      "learning_rate": 0.0007055988857938718,
      "loss": 2.3198,
      "step": 10570
    },
    {
      "epoch": 2.947075208913649,
      "grad_norm": 1.3312488794326782,
      "learning_rate": 0.0007053203342618385,
      "loss": 2.3546,
      "step": 10580
    },
    {
      "epoch": 2.949860724233983,
      "grad_norm": 1.424085021018982,
      "learning_rate": 0.000705041782729805,
      "loss": 2.5882,
      "step": 10590
    },
    {
      "epoch": 2.9526462395543174,
      "grad_norm": 1.3392583131790161,
      "learning_rate": 0.0007047632311977716,
      "loss": 2.5685,
      "step": 10600
    },
    {
      "epoch": 2.9554317548746516,
      "grad_norm": 1.4052537679672241,
      "learning_rate": 0.0007044846796657382,
      "loss": 2.4402,
      "step": 10610
    },
    {
      "epoch": 2.958217270194986,
      "grad_norm": 1.397567868232727,
      "learning_rate": 0.0007042061281337048,
      "loss": 2.3302,
      "step": 10620
    },
    {
      "epoch": 2.9610027855153205,
      "grad_norm": 1.3947128057479858,
      "learning_rate": 0.0007039275766016713,
      "loss": 2.4623,
      "step": 10630
    },
    {
      "epoch": 2.9637883008356547,
      "grad_norm": 1.2437934875488281,
      "learning_rate": 0.0007036490250696379,
      "loss": 2.4866,
      "step": 10640
    },
    {
      "epoch": 2.966573816155989,
      "grad_norm": 1.3195363283157349,
      "learning_rate": 0.0007033704735376045,
      "loss": 2.3566,
      "step": 10650
    },
    {
      "epoch": 2.969359331476323,
      "grad_norm": 1.7923583984375,
      "learning_rate": 0.000703091922005571,
      "loss": 2.2605,
      "step": 10660
    },
    {
      "epoch": 2.9721448467966574,
      "grad_norm": 1.4107218980789185,
      "learning_rate": 0.0007028133704735376,
      "loss": 2.2722,
      "step": 10670
    },
    {
      "epoch": 2.9749303621169916,
      "grad_norm": 1.5458372831344604,
      "learning_rate": 0.0007025348189415041,
      "loss": 2.2518,
      "step": 10680
    },
    {
      "epoch": 2.977715877437326,
      "grad_norm": 1.157973289489746,
      "learning_rate": 0.0007022562674094708,
      "loss": 2.5184,
      "step": 10690
    },
    {
      "epoch": 2.98050139275766,
      "grad_norm": 2.1085071563720703,
      "learning_rate": 0.0007019777158774373,
      "loss": 2.5823,
      "step": 10700
    },
    {
      "epoch": 2.9832869080779947,
      "grad_norm": 1.5865784883499146,
      "learning_rate": 0.0007016991643454039,
      "loss": 2.5659,
      "step": 10710
    },
    {
      "epoch": 2.986072423398329,
      "grad_norm": 2.2547357082366943,
      "learning_rate": 0.0007014206128133705,
      "loss": 2.3064,
      "step": 10720
    },
    {
      "epoch": 2.988857938718663,
      "grad_norm": 1.3991316556930542,
      "learning_rate": 0.0007011420612813371,
      "loss": 2.4115,
      "step": 10730
    },
    {
      "epoch": 2.9916434540389973,
      "grad_norm": 1.2863905429840088,
      "learning_rate": 0.0007008635097493037,
      "loss": 2.4125,
      "step": 10740
    },
    {
      "epoch": 2.9944289693593316,
      "grad_norm": 1.2823905944824219,
      "learning_rate": 0.0007005849582172702,
      "loss": 2.2049,
      "step": 10750
    },
    {
      "epoch": 2.997214484679666,
      "grad_norm": 1.4716718196868896,
      "learning_rate": 0.0007003064066852369,
      "loss": 2.3922,
      "step": 10760
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.8532835245132446,
      "learning_rate": 0.0007000278551532033,
      "loss": 2.3085,
      "step": 10770
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.312403440475464,
      "eval_runtime": 5.5281,
      "eval_samples_per_second": 577.416,
      "eval_steps_per_second": 72.177,
      "step": 10770
    },
    {
      "epoch": 3.002785515320334,
      "grad_norm": 1.386950135231018,
      "learning_rate": 0.0006997493036211699,
      "loss": 2.2894,
      "step": 10780
    },
    {
      "epoch": 3.0055710306406684,
      "grad_norm": 1.4693832397460938,
      "learning_rate": 0.0006994707520891365,
      "loss": 2.2487,
      "step": 10790
    },
    {
      "epoch": 3.0083565459610027,
      "grad_norm": 1.5077530145645142,
      "learning_rate": 0.0006991922005571031,
      "loss": 2.2473,
      "step": 10800
    },
    {
      "epoch": 3.011142061281337,
      "grad_norm": 2.041928768157959,
      "learning_rate": 0.0006989136490250696,
      "loss": 2.1555,
      "step": 10810
    },
    {
      "epoch": 3.013927576601671,
      "grad_norm": 1.2765976190567017,
      "learning_rate": 0.0006986350974930362,
      "loss": 2.3592,
      "step": 10820
    },
    {
      "epoch": 3.0167130919220058,
      "grad_norm": 1.3343368768692017,
      "learning_rate": 0.0006983565459610029,
      "loss": 2.1777,
      "step": 10830
    },
    {
      "epoch": 3.01949860724234,
      "grad_norm": 1.3319602012634277,
      "learning_rate": 0.0006980779944289694,
      "loss": 2.3562,
      "step": 10840
    },
    {
      "epoch": 3.022284122562674,
      "grad_norm": 1.8625812530517578,
      "learning_rate": 0.000697799442896936,
      "loss": 2.4238,
      "step": 10850
    },
    {
      "epoch": 3.0250696378830084,
      "grad_norm": 2.2248752117156982,
      "learning_rate": 0.0006975208913649024,
      "loss": 2.2944,
      "step": 10860
    },
    {
      "epoch": 3.0278551532033426,
      "grad_norm": 1.4366286993026733,
      "learning_rate": 0.0006972423398328691,
      "loss": 2.3852,
      "step": 10870
    },
    {
      "epoch": 3.030640668523677,
      "grad_norm": 1.4723210334777832,
      "learning_rate": 0.0006969637883008356,
      "loss": 2.4903,
      "step": 10880
    },
    {
      "epoch": 3.033426183844011,
      "grad_norm": 2.1193485260009766,
      "learning_rate": 0.0006966852367688022,
      "loss": 2.3142,
      "step": 10890
    },
    {
      "epoch": 3.0362116991643453,
      "grad_norm": 1.7772603034973145,
      "learning_rate": 0.0006964066852367689,
      "loss": 2.2299,
      "step": 10900
    },
    {
      "epoch": 3.0389972144846795,
      "grad_norm": 1.868485927581787,
      "learning_rate": 0.0006961281337047354,
      "loss": 2.2539,
      "step": 10910
    },
    {
      "epoch": 3.0417827298050137,
      "grad_norm": 1.5560569763183594,
      "learning_rate": 0.000695849582172702,
      "loss": 2.5214,
      "step": 10920
    },
    {
      "epoch": 3.0445682451253484,
      "grad_norm": 1.2463985681533813,
      "learning_rate": 0.0006955710306406685,
      "loss": 2.215,
      "step": 10930
    },
    {
      "epoch": 3.0473537604456826,
      "grad_norm": 1.6031733751296997,
      "learning_rate": 0.0006952924791086352,
      "loss": 2.444,
      "step": 10940
    },
    {
      "epoch": 3.050139275766017,
      "grad_norm": 1.1233065128326416,
      "learning_rate": 0.0006950139275766017,
      "loss": 2.2536,
      "step": 10950
    },
    {
      "epoch": 3.052924791086351,
      "grad_norm": 1.8141156435012817,
      "learning_rate": 0.0006947353760445682,
      "loss": 2.4967,
      "step": 10960
    },
    {
      "epoch": 3.0557103064066853,
      "grad_norm": 1.488006591796875,
      "learning_rate": 0.0006944568245125348,
      "loss": 2.3332,
      "step": 10970
    },
    {
      "epoch": 3.0584958217270195,
      "grad_norm": 1.1604326963424683,
      "learning_rate": 0.0006941782729805014,
      "loss": 2.3597,
      "step": 10980
    },
    {
      "epoch": 3.0612813370473537,
      "grad_norm": 1.3863896131515503,
      "learning_rate": 0.000693899721448468,
      "loss": 2.4456,
      "step": 10990
    },
    {
      "epoch": 3.064066852367688,
      "grad_norm": 3.974900245666504,
      "learning_rate": 0.0006936211699164345,
      "loss": 2.201,
      "step": 11000
    },
    {
      "epoch": 3.066852367688022,
      "grad_norm": 0.9125473499298096,
      "learning_rate": 0.0006933426183844012,
      "loss": 2.2029,
      "step": 11010
    },
    {
      "epoch": 3.0696378830083564,
      "grad_norm": 1.4147096872329712,
      "learning_rate": 0.0006930640668523677,
      "loss": 2.2867,
      "step": 11020
    },
    {
      "epoch": 3.0724233983286906,
      "grad_norm": 2.0280978679656982,
      "learning_rate": 0.0006927855153203343,
      "loss": 2.4376,
      "step": 11030
    },
    {
      "epoch": 3.0752089136490253,
      "grad_norm": 1.7635990381240845,
      "learning_rate": 0.0006925069637883008,
      "loss": 2.4386,
      "step": 11040
    },
    {
      "epoch": 3.0779944289693595,
      "grad_norm": 1.3074215650558472,
      "learning_rate": 0.0006922284122562675,
      "loss": 2.4362,
      "step": 11050
    },
    {
      "epoch": 3.0807799442896937,
      "grad_norm": 2.1583898067474365,
      "learning_rate": 0.000691949860724234,
      "loss": 2.3315,
      "step": 11060
    },
    {
      "epoch": 3.083565459610028,
      "grad_norm": 1.2717777490615845,
      "learning_rate": 0.0006916713091922005,
      "loss": 2.2521,
      "step": 11070
    },
    {
      "epoch": 3.086350974930362,
      "grad_norm": 1.8793842792510986,
      "learning_rate": 0.0006913927576601672,
      "loss": 2.2286,
      "step": 11080
    },
    {
      "epoch": 3.0891364902506964,
      "grad_norm": 1.3979418277740479,
      "learning_rate": 0.0006911142061281337,
      "loss": 2.3645,
      "step": 11090
    },
    {
      "epoch": 3.0919220055710306,
      "grad_norm": 1.7369914054870605,
      "learning_rate": 0.0006908356545961003,
      "loss": 2.2603,
      "step": 11100
    },
    {
      "epoch": 3.094707520891365,
      "grad_norm": 1.3118127584457397,
      "learning_rate": 0.0006905571030640668,
      "loss": 2.0746,
      "step": 11110
    },
    {
      "epoch": 3.097493036211699,
      "grad_norm": 1.598785400390625,
      "learning_rate": 0.0006902785515320335,
      "loss": 2.5419,
      "step": 11120
    },
    {
      "epoch": 3.1002785515320332,
      "grad_norm": 1.8013362884521484,
      "learning_rate": 0.00069,
      "loss": 2.378,
      "step": 11130
    },
    {
      "epoch": 3.103064066852368,
      "grad_norm": 1.4144867658615112,
      "learning_rate": 0.0006897214484679666,
      "loss": 2.35,
      "step": 11140
    },
    {
      "epoch": 3.105849582172702,
      "grad_norm": 1.579683542251587,
      "learning_rate": 0.0006894428969359333,
      "loss": 2.4239,
      "step": 11150
    },
    {
      "epoch": 3.1086350974930363,
      "grad_norm": 1.1938552856445312,
      "learning_rate": 0.0006891643454038997,
      "loss": 2.2677,
      "step": 11160
    },
    {
      "epoch": 3.1114206128133706,
      "grad_norm": 1.5557012557983398,
      "learning_rate": 0.0006888857938718663,
      "loss": 2.0387,
      "step": 11170
    },
    {
      "epoch": 3.114206128133705,
      "grad_norm": 1.549385666847229,
      "learning_rate": 0.0006886072423398328,
      "loss": 2.3451,
      "step": 11180
    },
    {
      "epoch": 3.116991643454039,
      "grad_norm": 1.6305795907974243,
      "learning_rate": 0.0006883286908077995,
      "loss": 2.2872,
      "step": 11190
    },
    {
      "epoch": 3.1197771587743732,
      "grad_norm": 1.2924286127090454,
      "learning_rate": 0.000688050139275766,
      "loss": 2.2577,
      "step": 11200
    },
    {
      "epoch": 3.1225626740947074,
      "grad_norm": 1.5456626415252686,
      "learning_rate": 0.0006877715877437326,
      "loss": 2.2747,
      "step": 11210
    },
    {
      "epoch": 3.1253481894150417,
      "grad_norm": 1.7712186574935913,
      "learning_rate": 0.0006874930362116992,
      "loss": 2.2232,
      "step": 11220
    },
    {
      "epoch": 3.128133704735376,
      "grad_norm": 1.3544074296951294,
      "learning_rate": 0.0006872144846796658,
      "loss": 2.6101,
      "step": 11230
    },
    {
      "epoch": 3.13091922005571,
      "grad_norm": 1.8222817182540894,
      "learning_rate": 0.0006869359331476324,
      "loss": 2.2644,
      "step": 11240
    },
    {
      "epoch": 3.1337047353760448,
      "grad_norm": 2.395026683807373,
      "learning_rate": 0.0006866573816155988,
      "loss": 2.2084,
      "step": 11250
    },
    {
      "epoch": 3.136490250696379,
      "grad_norm": 1.2060096263885498,
      "learning_rate": 0.0006863788300835655,
      "loss": 2.3224,
      "step": 11260
    },
    {
      "epoch": 3.139275766016713,
      "grad_norm": 1.6855298280715942,
      "learning_rate": 0.000686100278551532,
      "loss": 2.4005,
      "step": 11270
    },
    {
      "epoch": 3.1420612813370474,
      "grad_norm": 1.4457300901412964,
      "learning_rate": 0.0006858217270194986,
      "loss": 2.2916,
      "step": 11280
    },
    {
      "epoch": 3.1448467966573816,
      "grad_norm": 1.835447907447815,
      "learning_rate": 0.0006855431754874651,
      "loss": 2.2815,
      "step": 11290
    },
    {
      "epoch": 3.147632311977716,
      "grad_norm": 2.811699151992798,
      "learning_rate": 0.0006852646239554318,
      "loss": 2.4531,
      "step": 11300
    },
    {
      "epoch": 3.15041782729805,
      "grad_norm": 1.2420728206634521,
      "learning_rate": 0.0006849860724233984,
      "loss": 2.344,
      "step": 11310
    },
    {
      "epoch": 3.1532033426183843,
      "grad_norm": 1.5069117546081543,
      "learning_rate": 0.0006847075208913649,
      "loss": 2.4307,
      "step": 11320
    },
    {
      "epoch": 3.1559888579387185,
      "grad_norm": 1.5282071828842163,
      "learning_rate": 0.0006844289693593316,
      "loss": 2.395,
      "step": 11330
    },
    {
      "epoch": 3.1587743732590527,
      "grad_norm": 1.610898733139038,
      "learning_rate": 0.0006841504178272981,
      "loss": 2.331,
      "step": 11340
    },
    {
      "epoch": 3.1615598885793874,
      "grad_norm": 1.4591559171676636,
      "learning_rate": 0.0006838718662952646,
      "loss": 2.3664,
      "step": 11350
    },
    {
      "epoch": 3.1643454038997216,
      "grad_norm": 1.8427658081054688,
      "learning_rate": 0.0006835933147632311,
      "loss": 2.2954,
      "step": 11360
    },
    {
      "epoch": 3.167130919220056,
      "grad_norm": 1.406312108039856,
      "learning_rate": 0.0006833147632311978,
      "loss": 2.154,
      "step": 11370
    },
    {
      "epoch": 3.16991643454039,
      "grad_norm": 1.5184069871902466,
      "learning_rate": 0.0006830362116991644,
      "loss": 2.2552,
      "step": 11380
    },
    {
      "epoch": 3.1727019498607243,
      "grad_norm": 1.9669189453125,
      "learning_rate": 0.0006827576601671309,
      "loss": 2.4458,
      "step": 11390
    },
    {
      "epoch": 3.1754874651810585,
      "grad_norm": 1.3075045347213745,
      "learning_rate": 0.0006824791086350975,
      "loss": 2.4145,
      "step": 11400
    },
    {
      "epoch": 3.1782729805013927,
      "grad_norm": 1.4196548461914062,
      "learning_rate": 0.0006822005571030641,
      "loss": 2.2797,
      "step": 11410
    },
    {
      "epoch": 3.181058495821727,
      "grad_norm": 1.154481053352356,
      "learning_rate": 0.0006819220055710307,
      "loss": 2.0872,
      "step": 11420
    },
    {
      "epoch": 3.183844011142061,
      "grad_norm": 1.4140485525131226,
      "learning_rate": 0.0006816434540389972,
      "loss": 2.5408,
      "step": 11430
    },
    {
      "epoch": 3.1866295264623954,
      "grad_norm": 1.489327073097229,
      "learning_rate": 0.0006813649025069639,
      "loss": 2.1678,
      "step": 11440
    },
    {
      "epoch": 3.1894150417827296,
      "grad_norm": 1.235809087753296,
      "learning_rate": 0.0006810863509749303,
      "loss": 2.2376,
      "step": 11450
    },
    {
      "epoch": 3.1922005571030643,
      "grad_norm": 1.8783191442489624,
      "learning_rate": 0.0006808077994428969,
      "loss": 2.2397,
      "step": 11460
    },
    {
      "epoch": 3.1949860724233985,
      "grad_norm": 1.7655872106552124,
      "learning_rate": 0.0006805292479108635,
      "loss": 2.3731,
      "step": 11470
    },
    {
      "epoch": 3.1977715877437327,
      "grad_norm": 1.32347571849823,
      "learning_rate": 0.0006802506963788301,
      "loss": 2.2836,
      "step": 11480
    },
    {
      "epoch": 3.200557103064067,
      "grad_norm": 1.4737799167633057,
      "learning_rate": 0.0006799721448467967,
      "loss": 2.1902,
      "step": 11490
    },
    {
      "epoch": 3.203342618384401,
      "grad_norm": 1.36138117313385,
      "learning_rate": 0.0006796935933147632,
      "loss": 2.3573,
      "step": 11500
    },
    {
      "epoch": 3.2061281337047354,
      "grad_norm": 1.1764534711837769,
      "learning_rate": 0.0006794150417827299,
      "loss": 2.3857,
      "step": 11510
    },
    {
      "epoch": 3.2089136490250696,
      "grad_norm": 1.2769792079925537,
      "learning_rate": 0.0006791364902506964,
      "loss": 2.1933,
      "step": 11520
    },
    {
      "epoch": 3.211699164345404,
      "grad_norm": 1.6117793321609497,
      "learning_rate": 0.000678857938718663,
      "loss": 2.4443,
      "step": 11530
    },
    {
      "epoch": 3.214484679665738,
      "grad_norm": 1.313603162765503,
      "learning_rate": 0.0006785793871866296,
      "loss": 2.3974,
      "step": 11540
    },
    {
      "epoch": 3.2172701949860723,
      "grad_norm": 1.3294599056243896,
      "learning_rate": 0.0006783008356545961,
      "loss": 2.167,
      "step": 11550
    },
    {
      "epoch": 3.220055710306407,
      "grad_norm": 1.7156250476837158,
      "learning_rate": 0.0006780222841225627,
      "loss": 2.2141,
      "step": 11560
    },
    {
      "epoch": 3.222841225626741,
      "grad_norm": 2.1304304599761963,
      "learning_rate": 0.0006777437325905292,
      "loss": 2.2473,
      "step": 11570
    },
    {
      "epoch": 3.2256267409470754,
      "grad_norm": 1.4131273031234741,
      "learning_rate": 0.0006774651810584958,
      "loss": 2.2504,
      "step": 11580
    },
    {
      "epoch": 3.2284122562674096,
      "grad_norm": 1.2308204174041748,
      "learning_rate": 0.0006771866295264624,
      "loss": 2.2418,
      "step": 11590
    },
    {
      "epoch": 3.231197771587744,
      "grad_norm": 1.2790871858596802,
      "learning_rate": 0.000676908077994429,
      "loss": 2.3603,
      "step": 11600
    },
    {
      "epoch": 3.233983286908078,
      "grad_norm": 1.2928651571273804,
      "learning_rate": 0.0006766295264623955,
      "loss": 2.2675,
      "step": 11610
    },
    {
      "epoch": 3.2367688022284122,
      "grad_norm": 1.7565464973449707,
      "learning_rate": 0.0006763509749303622,
      "loss": 2.215,
      "step": 11620
    },
    {
      "epoch": 3.2395543175487465,
      "grad_norm": 3.0433757305145264,
      "learning_rate": 0.0006760724233983288,
      "loss": 2.2853,
      "step": 11630
    },
    {
      "epoch": 3.2423398328690807,
      "grad_norm": 1.702882170677185,
      "learning_rate": 0.0006757938718662953,
      "loss": 2.2582,
      "step": 11640
    },
    {
      "epoch": 3.245125348189415,
      "grad_norm": 1.3077017068862915,
      "learning_rate": 0.0006755153203342618,
      "loss": 2.367,
      "step": 11650
    },
    {
      "epoch": 3.247910863509749,
      "grad_norm": 2.8055784702301025,
      "learning_rate": 0.0006752367688022284,
      "loss": 2.3981,
      "step": 11660
    },
    {
      "epoch": 3.2506963788300833,
      "grad_norm": 1.6697369813919067,
      "learning_rate": 0.000674958217270195,
      "loss": 2.2954,
      "step": 11670
    },
    {
      "epoch": 3.253481894150418,
      "grad_norm": 1.2988882064819336,
      "learning_rate": 0.0006746796657381615,
      "loss": 2.2961,
      "step": 11680
    },
    {
      "epoch": 3.256267409470752,
      "grad_norm": 1.7483989000320435,
      "learning_rate": 0.0006744011142061281,
      "loss": 2.2054,
      "step": 11690
    },
    {
      "epoch": 3.2590529247910864,
      "grad_norm": 1.9878357648849487,
      "learning_rate": 0.0006741225626740948,
      "loss": 2.3167,
      "step": 11700
    },
    {
      "epoch": 3.2618384401114207,
      "grad_norm": 2.5278372764587402,
      "learning_rate": 0.0006738440111420613,
      "loss": 2.4374,
      "step": 11710
    },
    {
      "epoch": 3.264623955431755,
      "grad_norm": 1.321617603302002,
      "learning_rate": 0.0006735654596100279,
      "loss": 2.3838,
      "step": 11720
    },
    {
      "epoch": 3.267409470752089,
      "grad_norm": 1.0338809490203857,
      "learning_rate": 0.0006732869080779945,
      "loss": 2.4058,
      "step": 11730
    },
    {
      "epoch": 3.2701949860724233,
      "grad_norm": 1.8823890686035156,
      "learning_rate": 0.0006730083565459611,
      "loss": 2.447,
      "step": 11740
    },
    {
      "epoch": 3.2729805013927575,
      "grad_norm": 1.2797390222549438,
      "learning_rate": 0.0006727298050139275,
      "loss": 2.2547,
      "step": 11750
    },
    {
      "epoch": 3.2757660167130918,
      "grad_norm": 1.3729853630065918,
      "learning_rate": 0.0006724512534818941,
      "loss": 2.45,
      "step": 11760
    },
    {
      "epoch": 3.2785515320334264,
      "grad_norm": 1.3792301416397095,
      "learning_rate": 0.0006721727019498607,
      "loss": 2.3596,
      "step": 11770
    },
    {
      "epoch": 3.2813370473537606,
      "grad_norm": 1.6049904823303223,
      "learning_rate": 0.0006718941504178273,
      "loss": 2.275,
      "step": 11780
    },
    {
      "epoch": 3.284122562674095,
      "grad_norm": 1.68440580368042,
      "learning_rate": 0.0006716155988857939,
      "loss": 2.2095,
      "step": 11790
    },
    {
      "epoch": 3.286908077994429,
      "grad_norm": 2.462297201156616,
      "learning_rate": 0.0006713370473537605,
      "loss": 2.2955,
      "step": 11800
    },
    {
      "epoch": 3.2896935933147633,
      "grad_norm": 1.936463475227356,
      "learning_rate": 0.0006710584958217271,
      "loss": 2.2957,
      "step": 11810
    },
    {
      "epoch": 3.2924791086350975,
      "grad_norm": 1.7701616287231445,
      "learning_rate": 0.0006707799442896936,
      "loss": 2.234,
      "step": 11820
    },
    {
      "epoch": 3.2952646239554317,
      "grad_norm": 1.4379043579101562,
      "learning_rate": 0.0006705013927576602,
      "loss": 2.4968,
      "step": 11830
    },
    {
      "epoch": 3.298050139275766,
      "grad_norm": 1.6884350776672363,
      "learning_rate": 0.0006702228412256267,
      "loss": 2.5194,
      "step": 11840
    },
    {
      "epoch": 3.3008356545961,
      "grad_norm": 1.576322317123413,
      "learning_rate": 0.0006699442896935933,
      "loss": 2.3532,
      "step": 11850
    },
    {
      "epoch": 3.3036211699164344,
      "grad_norm": 1.3941923379898071,
      "learning_rate": 0.0006696657381615599,
      "loss": 2.2616,
      "step": 11860
    },
    {
      "epoch": 3.3064066852367686,
      "grad_norm": 1.9877969026565552,
      "learning_rate": 0.0006693871866295264,
      "loss": 2.2413,
      "step": 11870
    },
    {
      "epoch": 3.309192200557103,
      "grad_norm": 1.4015910625457764,
      "learning_rate": 0.0006691086350974931,
      "loss": 2.4372,
      "step": 11880
    },
    {
      "epoch": 3.3119777158774375,
      "grad_norm": 1.5869876146316528,
      "learning_rate": 0.0006688300835654596,
      "loss": 2.382,
      "step": 11890
    },
    {
      "epoch": 3.3147632311977717,
      "grad_norm": 1.292646884918213,
      "learning_rate": 0.0006685515320334262,
      "loss": 2.3437,
      "step": 11900
    },
    {
      "epoch": 3.317548746518106,
      "grad_norm": 1.5839320421218872,
      "learning_rate": 0.0006682729805013928,
      "loss": 2.237,
      "step": 11910
    },
    {
      "epoch": 3.32033426183844,
      "grad_norm": 2.070206642150879,
      "learning_rate": 0.0006679944289693594,
      "loss": 2.3827,
      "step": 11920
    },
    {
      "epoch": 3.3231197771587744,
      "grad_norm": 1.2153842449188232,
      "learning_rate": 0.0006677158774373259,
      "loss": 2.1923,
      "step": 11930
    },
    {
      "epoch": 3.3259052924791086,
      "grad_norm": 1.4731053113937378,
      "learning_rate": 0.0006674373259052924,
      "loss": 2.1103,
      "step": 11940
    },
    {
      "epoch": 3.328690807799443,
      "grad_norm": 2.609004497528076,
      "learning_rate": 0.0006671587743732591,
      "loss": 2.3488,
      "step": 11950
    },
    {
      "epoch": 3.331476323119777,
      "grad_norm": 1.6782714128494263,
      "learning_rate": 0.0006668802228412256,
      "loss": 2.4056,
      "step": 11960
    },
    {
      "epoch": 3.3342618384401113,
      "grad_norm": 1.1719131469726562,
      "learning_rate": 0.0006666016713091922,
      "loss": 2.4061,
      "step": 11970
    },
    {
      "epoch": 3.337047353760446,
      "grad_norm": 1.3973935842514038,
      "learning_rate": 0.0006663231197771588,
      "loss": 2.4012,
      "step": 11980
    },
    {
      "epoch": 3.33983286908078,
      "grad_norm": 1.0862617492675781,
      "learning_rate": 0.0006660445682451254,
      "loss": 2.17,
      "step": 11990
    },
    {
      "epoch": 3.3426183844011144,
      "grad_norm": 2.6598868370056152,
      "learning_rate": 0.0006657660167130919,
      "loss": 2.2903,
      "step": 12000
    },
    {
      "epoch": 3.3454038997214486,
      "grad_norm": 1.3091000318527222,
      "learning_rate": 0.0006654874651810585,
      "loss": 2.3808,
      "step": 12010
    },
    {
      "epoch": 3.348189415041783,
      "grad_norm": 1.7500382661819458,
      "learning_rate": 0.0006652089136490252,
      "loss": 2.298,
      "step": 12020
    },
    {
      "epoch": 3.350974930362117,
      "grad_norm": 1.4368178844451904,
      "learning_rate": 0.0006649303621169917,
      "loss": 2.3229,
      "step": 12030
    },
    {
      "epoch": 3.3537604456824512,
      "grad_norm": 1.2537524700164795,
      "learning_rate": 0.0006646518105849582,
      "loss": 2.3897,
      "step": 12040
    },
    {
      "epoch": 3.3565459610027855,
      "grad_norm": 1.4852575063705444,
      "learning_rate": 0.0006643732590529247,
      "loss": 2.3217,
      "step": 12050
    },
    {
      "epoch": 3.3593314763231197,
      "grad_norm": 2.31591796875,
      "learning_rate": 0.0006640947075208914,
      "loss": 2.2765,
      "step": 12060
    },
    {
      "epoch": 3.362116991643454,
      "grad_norm": 2.027773857116699,
      "learning_rate": 0.0006638161559888579,
      "loss": 2.2514,
      "step": 12070
    },
    {
      "epoch": 3.364902506963788,
      "grad_norm": 1.3393545150756836,
      "learning_rate": 0.0006635376044568245,
      "loss": 2.4376,
      "step": 12080
    },
    {
      "epoch": 3.3676880222841223,
      "grad_norm": 1.1981143951416016,
      "learning_rate": 0.0006632590529247911,
      "loss": 2.2388,
      "step": 12090
    },
    {
      "epoch": 3.370473537604457,
      "grad_norm": 1.140688419342041,
      "learning_rate": 0.0006629805013927577,
      "loss": 2.4635,
      "step": 12100
    },
    {
      "epoch": 3.3732590529247912,
      "grad_norm": 1.7481083869934082,
      "learning_rate": 0.0006627019498607243,
      "loss": 2.1716,
      "step": 12110
    },
    {
      "epoch": 3.3760445682451254,
      "grad_norm": 1.1800107955932617,
      "learning_rate": 0.0006624233983286908,
      "loss": 2.0566,
      "step": 12120
    },
    {
      "epoch": 3.3788300835654597,
      "grad_norm": 1.988433837890625,
      "learning_rate": 0.0006621448467966575,
      "loss": 2.1994,
      "step": 12130
    },
    {
      "epoch": 3.381615598885794,
      "grad_norm": 1.4424883127212524,
      "learning_rate": 0.0006618662952646239,
      "loss": 2.3928,
      "step": 12140
    },
    {
      "epoch": 3.384401114206128,
      "grad_norm": 2.0706470012664795,
      "learning_rate": 0.0006615877437325905,
      "loss": 2.3046,
      "step": 12150
    },
    {
      "epoch": 3.3871866295264623,
      "grad_norm": 1.4056771993637085,
      "learning_rate": 0.0006613091922005571,
      "loss": 2.2953,
      "step": 12160
    },
    {
      "epoch": 3.3899721448467965,
      "grad_norm": 1.2677068710327148,
      "learning_rate": 0.0006610306406685237,
      "loss": 2.3913,
      "step": 12170
    },
    {
      "epoch": 3.3927576601671308,
      "grad_norm": 1.5428416728973389,
      "learning_rate": 0.0006607520891364903,
      "loss": 2.4061,
      "step": 12180
    },
    {
      "epoch": 3.3955431754874654,
      "grad_norm": 1.7420330047607422,
      "learning_rate": 0.0006604735376044568,
      "loss": 2.3812,
      "step": 12190
    },
    {
      "epoch": 3.3983286908077996,
      "grad_norm": 1.4830384254455566,
      "learning_rate": 0.0006601949860724235,
      "loss": 2.2811,
      "step": 12200
    },
    {
      "epoch": 3.401114206128134,
      "grad_norm": 1.4703325033187866,
      "learning_rate": 0.00065991643454039,
      "loss": 2.3653,
      "step": 12210
    },
    {
      "epoch": 3.403899721448468,
      "grad_norm": 1.048804759979248,
      "learning_rate": 0.0006596378830083566,
      "loss": 2.2502,
      "step": 12220
    },
    {
      "epoch": 3.4066852367688023,
      "grad_norm": 1.4925798177719116,
      "learning_rate": 0.000659359331476323,
      "loss": 2.3299,
      "step": 12230
    },
    {
      "epoch": 3.4094707520891365,
      "grad_norm": 1.5886212587356567,
      "learning_rate": 0.0006590807799442897,
      "loss": 2.4509,
      "step": 12240
    },
    {
      "epoch": 3.4122562674094707,
      "grad_norm": 1.5946342945098877,
      "learning_rate": 0.0006588022284122562,
      "loss": 2.3434,
      "step": 12250
    },
    {
      "epoch": 3.415041782729805,
      "grad_norm": 1.5982081890106201,
      "learning_rate": 0.0006585236768802228,
      "loss": 2.3597,
      "step": 12260
    },
    {
      "epoch": 3.417827298050139,
      "grad_norm": 1.514117956161499,
      "learning_rate": 0.0006582451253481895,
      "loss": 2.3725,
      "step": 12270
    },
    {
      "epoch": 3.4206128133704734,
      "grad_norm": 1.6044261455535889,
      "learning_rate": 0.000657966573816156,
      "loss": 2.3247,
      "step": 12280
    },
    {
      "epoch": 3.4233983286908076,
      "grad_norm": 1.5624676942825317,
      "learning_rate": 0.0006576880222841226,
      "loss": 2.2044,
      "step": 12290
    },
    {
      "epoch": 3.426183844011142,
      "grad_norm": 2.0961341857910156,
      "learning_rate": 0.0006574094707520891,
      "loss": 2.0212,
      "step": 12300
    },
    {
      "epoch": 3.4289693593314765,
      "grad_norm": 1.6983823776245117,
      "learning_rate": 0.0006571309192200558,
      "loss": 2.5357,
      "step": 12310
    },
    {
      "epoch": 3.4317548746518107,
      "grad_norm": 1.3439903259277344,
      "learning_rate": 0.0006568523676880223,
      "loss": 2.3099,
      "step": 12320
    },
    {
      "epoch": 3.434540389972145,
      "grad_norm": 1.5233957767486572,
      "learning_rate": 0.0006565738161559889,
      "loss": 2.3458,
      "step": 12330
    },
    {
      "epoch": 3.437325905292479,
      "grad_norm": 1.139768123626709,
      "learning_rate": 0.0006562952646239554,
      "loss": 2.096,
      "step": 12340
    },
    {
      "epoch": 3.4401114206128134,
      "grad_norm": 1.2495485544204712,
      "learning_rate": 0.000656016713091922,
      "loss": 2.2312,
      "step": 12350
    },
    {
      "epoch": 3.4428969359331476,
      "grad_norm": 1.3979088068008423,
      "learning_rate": 0.0006557381615598886,
      "loss": 2.1858,
      "step": 12360
    },
    {
      "epoch": 3.445682451253482,
      "grad_norm": 2.152129888534546,
      "learning_rate": 0.0006554596100278551,
      "loss": 2.2956,
      "step": 12370
    },
    {
      "epoch": 3.448467966573816,
      "grad_norm": 1.440187931060791,
      "learning_rate": 0.0006551810584958218,
      "loss": 2.1585,
      "step": 12380
    },
    {
      "epoch": 3.4512534818941503,
      "grad_norm": 1.5514905452728271,
      "learning_rate": 0.0006549025069637883,
      "loss": 2.4096,
      "step": 12390
    },
    {
      "epoch": 3.4540389972144845,
      "grad_norm": 1.6300894021987915,
      "learning_rate": 0.0006546239554317549,
      "loss": 2.3008,
      "step": 12400
    },
    {
      "epoch": 3.456824512534819,
      "grad_norm": 1.6461085081100464,
      "learning_rate": 0.0006543454038997214,
      "loss": 2.3831,
      "step": 12410
    },
    {
      "epoch": 3.4596100278551534,
      "grad_norm": 1.264406681060791,
      "learning_rate": 0.0006540668523676881,
      "loss": 2.3263,
      "step": 12420
    },
    {
      "epoch": 3.4623955431754876,
      "grad_norm": 1.581548810005188,
      "learning_rate": 0.0006537883008356547,
      "loss": 2.4173,
      "step": 12430
    },
    {
      "epoch": 3.465181058495822,
      "grad_norm": 1.5874558687210083,
      "learning_rate": 0.0006535097493036211,
      "loss": 2.2804,
      "step": 12440
    },
    {
      "epoch": 3.467966573816156,
      "grad_norm": 1.3968000411987305,
      "learning_rate": 0.0006532311977715878,
      "loss": 2.4711,
      "step": 12450
    },
    {
      "epoch": 3.4707520891364902,
      "grad_norm": 1.4124841690063477,
      "learning_rate": 0.0006529526462395543,
      "loss": 2.3283,
      "step": 12460
    },
    {
      "epoch": 3.4735376044568245,
      "grad_norm": 1.8433033227920532,
      "learning_rate": 0.0006526740947075209,
      "loss": 2.2869,
      "step": 12470
    },
    {
      "epoch": 3.4763231197771587,
      "grad_norm": 1.7941030263900757,
      "learning_rate": 0.0006523955431754874,
      "loss": 2.5025,
      "step": 12480
    },
    {
      "epoch": 3.479108635097493,
      "grad_norm": 1.1824946403503418,
      "learning_rate": 0.0006521169916434541,
      "loss": 2.3889,
      "step": 12490
    },
    {
      "epoch": 3.481894150417827,
      "grad_norm": 1.5138256549835205,
      "learning_rate": 0.0006518384401114206,
      "loss": 2.2613,
      "step": 12500
    },
    {
      "epoch": 3.4846796657381613,
      "grad_norm": 1.5366629362106323,
      "learning_rate": 0.0006515598885793872,
      "loss": 2.243,
      "step": 12510
    },
    {
      "epoch": 3.487465181058496,
      "grad_norm": 1.9428881406784058,
      "learning_rate": 0.0006512813370473539,
      "loss": 2.2142,
      "step": 12520
    },
    {
      "epoch": 3.4902506963788302,
      "grad_norm": 1.4987980127334595,
      "learning_rate": 0.0006510027855153204,
      "loss": 2.3005,
      "step": 12530
    },
    {
      "epoch": 3.4930362116991645,
      "grad_norm": 1.2449827194213867,
      "learning_rate": 0.0006507242339832869,
      "loss": 2.2063,
      "step": 12540
    },
    {
      "epoch": 3.4958217270194987,
      "grad_norm": 1.216658353805542,
      "learning_rate": 0.0006504456824512534,
      "loss": 2.4846,
      "step": 12550
    },
    {
      "epoch": 3.498607242339833,
      "grad_norm": 1.6016367673873901,
      "learning_rate": 0.0006501671309192201,
      "loss": 2.3054,
      "step": 12560
    },
    {
      "epoch": 3.501392757660167,
      "grad_norm": 1.2292553186416626,
      "learning_rate": 0.0006498885793871866,
      "loss": 2.4074,
      "step": 12570
    },
    {
      "epoch": 3.5041782729805013,
      "grad_norm": 1.072706937789917,
      "learning_rate": 0.0006496100278551532,
      "loss": 2.392,
      "step": 12580
    },
    {
      "epoch": 3.5069637883008355,
      "grad_norm": 1.298393726348877,
      "learning_rate": 0.0006493314763231198,
      "loss": 2.4602,
      "step": 12590
    },
    {
      "epoch": 3.5097493036211698,
      "grad_norm": 1.8510762453079224,
      "learning_rate": 0.0006490529247910864,
      "loss": 2.3641,
      "step": 12600
    },
    {
      "epoch": 3.5125348189415044,
      "grad_norm": 1.7840064764022827,
      "learning_rate": 0.000648774373259053,
      "loss": 2.3544,
      "step": 12610
    },
    {
      "epoch": 3.5153203342618387,
      "grad_norm": 1.7667157649993896,
      "learning_rate": 0.0006484958217270195,
      "loss": 2.1753,
      "step": 12620
    },
    {
      "epoch": 3.518105849582173,
      "grad_norm": 1.64847993850708,
      "learning_rate": 0.0006482172701949862,
      "loss": 2.4313,
      "step": 12630
    },
    {
      "epoch": 3.520891364902507,
      "grad_norm": 1.9822548627853394,
      "learning_rate": 0.0006479387186629526,
      "loss": 2.1662,
      "step": 12640
    },
    {
      "epoch": 3.5236768802228413,
      "grad_norm": 1.3804535865783691,
      "learning_rate": 0.0006476601671309192,
      "loss": 2.4812,
      "step": 12650
    },
    {
      "epoch": 3.5264623955431755,
      "grad_norm": 1.110731840133667,
      "learning_rate": 0.0006473816155988857,
      "loss": 2.3455,
      "step": 12660
    },
    {
      "epoch": 3.5292479108635098,
      "grad_norm": 1.3322674036026,
      "learning_rate": 0.0006471030640668524,
      "loss": 2.4239,
      "step": 12670
    },
    {
      "epoch": 3.532033426183844,
      "grad_norm": 1.3214213848114014,
      "learning_rate": 0.000646824512534819,
      "loss": 2.2379,
      "step": 12680
    },
    {
      "epoch": 3.534818941504178,
      "grad_norm": 1.492768406867981,
      "learning_rate": 0.0006465459610027855,
      "loss": 2.3355,
      "step": 12690
    },
    {
      "epoch": 3.5376044568245124,
      "grad_norm": 1.7679088115692139,
      "learning_rate": 0.0006462674094707522,
      "loss": 2.0464,
      "step": 12700
    },
    {
      "epoch": 3.5403899721448466,
      "grad_norm": 1.7919611930847168,
      "learning_rate": 0.0006459888579387187,
      "loss": 2.3948,
      "step": 12710
    },
    {
      "epoch": 3.543175487465181,
      "grad_norm": 1.5885462760925293,
      "learning_rate": 0.0006457103064066853,
      "loss": 2.2518,
      "step": 12720
    },
    {
      "epoch": 3.545961002785515,
      "grad_norm": 1.3037611246109009,
      "learning_rate": 0.0006454317548746517,
      "loss": 2.4279,
      "step": 12730
    },
    {
      "epoch": 3.5487465181058497,
      "grad_norm": 1.112847924232483,
      "learning_rate": 0.0006451532033426184,
      "loss": 2.3766,
      "step": 12740
    },
    {
      "epoch": 3.551532033426184,
      "grad_norm": 0.8563538789749146,
      "learning_rate": 0.000644874651810585,
      "loss": 2.4574,
      "step": 12750
    },
    {
      "epoch": 3.554317548746518,
      "grad_norm": 1.329334020614624,
      "learning_rate": 0.0006445961002785515,
      "loss": 2.3416,
      "step": 12760
    },
    {
      "epoch": 3.5571030640668524,
      "grad_norm": 1.4369100332260132,
      "learning_rate": 0.0006443175487465181,
      "loss": 2.1711,
      "step": 12770
    },
    {
      "epoch": 3.5598885793871866,
      "grad_norm": 2.3404994010925293,
      "learning_rate": 0.0006440389972144847,
      "loss": 2.167,
      "step": 12780
    },
    {
      "epoch": 3.562674094707521,
      "grad_norm": 1.5837877988815308,
      "learning_rate": 0.0006437604456824513,
      "loss": 2.2482,
      "step": 12790
    },
    {
      "epoch": 3.565459610027855,
      "grad_norm": 1.3232747316360474,
      "learning_rate": 0.0006434818941504178,
      "loss": 2.5357,
      "step": 12800
    },
    {
      "epoch": 3.5682451253481893,
      "grad_norm": 1.3227343559265137,
      "learning_rate": 0.0006432033426183845,
      "loss": 2.21,
      "step": 12810
    },
    {
      "epoch": 3.571030640668524,
      "grad_norm": 1.337367057800293,
      "learning_rate": 0.000642924791086351,
      "loss": 2.4813,
      "step": 12820
    },
    {
      "epoch": 3.573816155988858,
      "grad_norm": 1.2933659553527832,
      "learning_rate": 0.0006426462395543175,
      "loss": 2.3996,
      "step": 12830
    },
    {
      "epoch": 3.5766016713091924,
      "grad_norm": 1.4521788358688354,
      "learning_rate": 0.0006423676880222841,
      "loss": 2.4413,
      "step": 12840
    },
    {
      "epoch": 3.5793871866295266,
      "grad_norm": 1.3915497064590454,
      "learning_rate": 0.0006420891364902507,
      "loss": 2.2959,
      "step": 12850
    },
    {
      "epoch": 3.582172701949861,
      "grad_norm": 1.6604337692260742,
      "learning_rate": 0.0006418105849582173,
      "loss": 2.1366,
      "step": 12860
    },
    {
      "epoch": 3.584958217270195,
      "grad_norm": 1.5483143329620361,
      "learning_rate": 0.0006415320334261838,
      "loss": 2.2948,
      "step": 12870
    },
    {
      "epoch": 3.5877437325905293,
      "grad_norm": 1.2211140394210815,
      "learning_rate": 0.0006412534818941505,
      "loss": 2.1678,
      "step": 12880
    },
    {
      "epoch": 3.5905292479108635,
      "grad_norm": 2.3549163341522217,
      "learning_rate": 0.000640974930362117,
      "loss": 2.2198,
      "step": 12890
    },
    {
      "epoch": 3.5933147632311977,
      "grad_norm": 1.74261474609375,
      "learning_rate": 0.0006406963788300836,
      "loss": 2.247,
      "step": 12900
    },
    {
      "epoch": 3.596100278551532,
      "grad_norm": 1.344321608543396,
      "learning_rate": 0.0006404178272980502,
      "loss": 2.2355,
      "step": 12910
    },
    {
      "epoch": 3.598885793871866,
      "grad_norm": 1.3407344818115234,
      "learning_rate": 0.0006401392757660168,
      "loss": 2.5162,
      "step": 12920
    },
    {
      "epoch": 3.6016713091922004,
      "grad_norm": 1.4165371656417847,
      "learning_rate": 0.0006398607242339833,
      "loss": 2.2288,
      "step": 12930
    },
    {
      "epoch": 3.6044568245125346,
      "grad_norm": 1.2794291973114014,
      "learning_rate": 0.0006395821727019498,
      "loss": 2.3632,
      "step": 12940
    },
    {
      "epoch": 3.6072423398328692,
      "grad_norm": 1.520396113395691,
      "learning_rate": 0.0006393036211699164,
      "loss": 2.1669,
      "step": 12950
    },
    {
      "epoch": 3.6100278551532035,
      "grad_norm": 2.7111475467681885,
      "learning_rate": 0.000639025069637883,
      "loss": 2.2749,
      "step": 12960
    },
    {
      "epoch": 3.6128133704735377,
      "grad_norm": 1.1265215873718262,
      "learning_rate": 0.0006387465181058496,
      "loss": 2.2248,
      "step": 12970
    },
    {
      "epoch": 3.615598885793872,
      "grad_norm": 1.9359190464019775,
      "learning_rate": 0.0006384679665738161,
      "loss": 2.4698,
      "step": 12980
    },
    {
      "epoch": 3.618384401114206,
      "grad_norm": 1.592952013015747,
      "learning_rate": 0.0006381894150417828,
      "loss": 2.2712,
      "step": 12990
    },
    {
      "epoch": 3.6211699164345403,
      "grad_norm": 1.2103365659713745,
      "learning_rate": 0.0006379108635097494,
      "loss": 2.3495,
      "step": 13000
    },
    {
      "epoch": 3.6239554317548746,
      "grad_norm": 1.5439467430114746,
      "learning_rate": 0.0006376323119777159,
      "loss": 2.3718,
      "step": 13010
    },
    {
      "epoch": 3.6267409470752088,
      "grad_norm": 1.5840152502059937,
      "learning_rate": 0.0006373537604456825,
      "loss": 2.5994,
      "step": 13020
    },
    {
      "epoch": 3.6295264623955434,
      "grad_norm": 1.6175898313522339,
      "learning_rate": 0.000637075208913649,
      "loss": 2.4174,
      "step": 13030
    },
    {
      "epoch": 3.6323119777158777,
      "grad_norm": 1.2764261960983276,
      "learning_rate": 0.0006367966573816156,
      "loss": 2.3954,
      "step": 13040
    },
    {
      "epoch": 3.635097493036212,
      "grad_norm": 1.4969353675842285,
      "learning_rate": 0.0006365181058495821,
      "loss": 2.3248,
      "step": 13050
    },
    {
      "epoch": 3.637883008356546,
      "grad_norm": 1.2243307828903198,
      "learning_rate": 0.0006362395543175488,
      "loss": 2.3951,
      "step": 13060
    },
    {
      "epoch": 3.6406685236768803,
      "grad_norm": 1.6999108791351318,
      "learning_rate": 0.0006359610027855154,
      "loss": 2.2447,
      "step": 13070
    },
    {
      "epoch": 3.6434540389972145,
      "grad_norm": 1.348879098892212,
      "learning_rate": 0.0006356824512534819,
      "loss": 2.3915,
      "step": 13080
    },
    {
      "epoch": 3.6462395543175488,
      "grad_norm": 1.7670551538467407,
      "learning_rate": 0.0006354038997214485,
      "loss": 2.2877,
      "step": 13090
    },
    {
      "epoch": 3.649025069637883,
      "grad_norm": 1.442944884300232,
      "learning_rate": 0.0006351253481894151,
      "loss": 2.2949,
      "step": 13100
    },
    {
      "epoch": 3.651810584958217,
      "grad_norm": 1.7100521326065063,
      "learning_rate": 0.0006348467966573817,
      "loss": 2.4161,
      "step": 13110
    },
    {
      "epoch": 3.6545961002785514,
      "grad_norm": 1.2628357410430908,
      "learning_rate": 0.0006345682451253481,
      "loss": 2.4302,
      "step": 13120
    },
    {
      "epoch": 3.6573816155988856,
      "grad_norm": 1.5802732706069946,
      "learning_rate": 0.0006342896935933147,
      "loss": 2.3779,
      "step": 13130
    },
    {
      "epoch": 3.66016713091922,
      "grad_norm": 1.5282628536224365,
      "learning_rate": 0.0006340111420612813,
      "loss": 2.2619,
      "step": 13140
    },
    {
      "epoch": 3.662952646239554,
      "grad_norm": 1.3802465200424194,
      "learning_rate": 0.0006337325905292479,
      "loss": 2.3245,
      "step": 13150
    },
    {
      "epoch": 3.6657381615598887,
      "grad_norm": 1.5182214975357056,
      "learning_rate": 0.0006334540389972145,
      "loss": 2.1118,
      "step": 13160
    },
    {
      "epoch": 3.668523676880223,
      "grad_norm": 2.080096960067749,
      "learning_rate": 0.0006331754874651811,
      "loss": 2.5412,
      "step": 13170
    },
    {
      "epoch": 3.671309192200557,
      "grad_norm": 1.615185260772705,
      "learning_rate": 0.0006328969359331477,
      "loss": 2.2165,
      "step": 13180
    },
    {
      "epoch": 3.6740947075208914,
      "grad_norm": 1.6928261518478394,
      "learning_rate": 0.0006326183844011142,
      "loss": 1.9706,
      "step": 13190
    },
    {
      "epoch": 3.6768802228412256,
      "grad_norm": 2.0479063987731934,
      "learning_rate": 0.0006323398328690808,
      "loss": 2.3821,
      "step": 13200
    },
    {
      "epoch": 3.67966573816156,
      "grad_norm": 1.6208504438400269,
      "learning_rate": 0.0006320612813370474,
      "loss": 2.3457,
      "step": 13210
    },
    {
      "epoch": 3.682451253481894,
      "grad_norm": 1.399108648300171,
      "learning_rate": 0.000631782729805014,
      "loss": 2.2955,
      "step": 13220
    },
    {
      "epoch": 3.6852367688022283,
      "grad_norm": 1.7483665943145752,
      "learning_rate": 0.0006315041782729805,
      "loss": 2.1829,
      "step": 13230
    },
    {
      "epoch": 3.688022284122563,
      "grad_norm": 1.3567827939987183,
      "learning_rate": 0.000631225626740947,
      "loss": 2.4275,
      "step": 13240
    },
    {
      "epoch": 3.690807799442897,
      "grad_norm": 1.2226344347000122,
      "learning_rate": 0.0006309470752089137,
      "loss": 2.3362,
      "step": 13250
    },
    {
      "epoch": 3.6935933147632314,
      "grad_norm": 1.2998751401901245,
      "learning_rate": 0.0006306685236768802,
      "loss": 2.3183,
      "step": 13260
    },
    {
      "epoch": 3.6963788300835656,
      "grad_norm": 1.7455302476882935,
      "learning_rate": 0.0006303899721448468,
      "loss": 2.3862,
      "step": 13270
    },
    {
      "epoch": 3.6991643454039,
      "grad_norm": 1.2507836818695068,
      "learning_rate": 0.0006301114206128134,
      "loss": 2.3482,
      "step": 13280
    },
    {
      "epoch": 3.701949860724234,
      "grad_norm": 1.46087646484375,
      "learning_rate": 0.00062983286908078,
      "loss": 2.3329,
      "step": 13290
    },
    {
      "epoch": 3.7047353760445683,
      "grad_norm": 1.5997000932693481,
      "learning_rate": 0.0006295543175487465,
      "loss": 2.2397,
      "step": 13300
    },
    {
      "epoch": 3.7075208913649025,
      "grad_norm": 1.2824490070343018,
      "learning_rate": 0.0006292757660167131,
      "loss": 2.2623,
      "step": 13310
    },
    {
      "epoch": 3.7103064066852367,
      "grad_norm": 1.3035646677017212,
      "learning_rate": 0.0006289972144846798,
      "loss": 2.3264,
      "step": 13320
    },
    {
      "epoch": 3.713091922005571,
      "grad_norm": 1.277483582496643,
      "learning_rate": 0.0006287186629526462,
      "loss": 2.275,
      "step": 13330
    },
    {
      "epoch": 3.715877437325905,
      "grad_norm": 2.160543918609619,
      "learning_rate": 0.0006284401114206128,
      "loss": 2.1657,
      "step": 13340
    },
    {
      "epoch": 3.7186629526462394,
      "grad_norm": 1.7614634037017822,
      "learning_rate": 0.0006281615598885794,
      "loss": 2.2941,
      "step": 13350
    },
    {
      "epoch": 3.7214484679665736,
      "grad_norm": 1.6878989934921265,
      "learning_rate": 0.000627883008356546,
      "loss": 2.4177,
      "step": 13360
    },
    {
      "epoch": 3.724233983286908,
      "grad_norm": 1.8711540699005127,
      "learning_rate": 0.0006276044568245125,
      "loss": 2.2979,
      "step": 13370
    },
    {
      "epoch": 3.7270194986072425,
      "grad_norm": 1.2737147808074951,
      "learning_rate": 0.0006273259052924791,
      "loss": 2.149,
      "step": 13380
    },
    {
      "epoch": 3.7298050139275767,
      "grad_norm": 1.7210054397583008,
      "learning_rate": 0.0006270473537604458,
      "loss": 2.4381,
      "step": 13390
    },
    {
      "epoch": 3.732590529247911,
      "grad_norm": 1.1948802471160889,
      "learning_rate": 0.0006267688022284123,
      "loss": 2.4793,
      "step": 13400
    },
    {
      "epoch": 3.735376044568245,
      "grad_norm": 1.7165642976760864,
      "learning_rate": 0.0006264902506963789,
      "loss": 2.5901,
      "step": 13410
    },
    {
      "epoch": 3.7381615598885793,
      "grad_norm": 1.5551687479019165,
      "learning_rate": 0.0006262116991643453,
      "loss": 2.421,
      "step": 13420
    },
    {
      "epoch": 3.7409470752089136,
      "grad_norm": 1.1057573556900024,
      "learning_rate": 0.000625933147632312,
      "loss": 2.4497,
      "step": 13430
    },
    {
      "epoch": 3.743732590529248,
      "grad_norm": 1.527305245399475,
      "learning_rate": 0.0006256545961002785,
      "loss": 2.2007,
      "step": 13440
    },
    {
      "epoch": 3.7465181058495824,
      "grad_norm": 1.5145385265350342,
      "learning_rate": 0.0006253760445682451,
      "loss": 2.3646,
      "step": 13450
    },
    {
      "epoch": 3.7493036211699167,
      "grad_norm": 1.6140410900115967,
      "learning_rate": 0.0006250974930362117,
      "loss": 2.3158,
      "step": 13460
    },
    {
      "epoch": 3.752089136490251,
      "grad_norm": 1.2814449071884155,
      "learning_rate": 0.0006248189415041783,
      "loss": 2.2165,
      "step": 13470
    },
    {
      "epoch": 3.754874651810585,
      "grad_norm": 1.4274603128433228,
      "learning_rate": 0.0006245403899721449,
      "loss": 2.2022,
      "step": 13480
    },
    {
      "epoch": 3.7576601671309193,
      "grad_norm": 1.3688075542449951,
      "learning_rate": 0.0006242618384401114,
      "loss": 2.5891,
      "step": 13490
    },
    {
      "epoch": 3.7604456824512535,
      "grad_norm": 1.4082837104797363,
      "learning_rate": 0.0006239832869080781,
      "loss": 2.3546,
      "step": 13500
    },
    {
      "epoch": 3.7632311977715878,
      "grad_norm": 1.384764552116394,
      "learning_rate": 0.0006237047353760446,
      "loss": 2.3098,
      "step": 13510
    },
    {
      "epoch": 3.766016713091922,
      "grad_norm": 1.3979073762893677,
      "learning_rate": 0.0006234261838440111,
      "loss": 2.147,
      "step": 13520
    },
    {
      "epoch": 3.768802228412256,
      "grad_norm": 1.7133673429489136,
      "learning_rate": 0.0006231476323119777,
      "loss": 2.45,
      "step": 13530
    },
    {
      "epoch": 3.7715877437325904,
      "grad_norm": 1.7536202669143677,
      "learning_rate": 0.0006228690807799443,
      "loss": 2.3077,
      "step": 13540
    },
    {
      "epoch": 3.7743732590529246,
      "grad_norm": 1.2039637565612793,
      "learning_rate": 0.0006225905292479109,
      "loss": 2.2834,
      "step": 13550
    },
    {
      "epoch": 3.777158774373259,
      "grad_norm": 1.912584900856018,
      "learning_rate": 0.0006223119777158774,
      "loss": 2.2003,
      "step": 13560
    },
    {
      "epoch": 3.779944289693593,
      "grad_norm": 1.2611923217773438,
      "learning_rate": 0.0006220334261838441,
      "loss": 2.1937,
      "step": 13570
    },
    {
      "epoch": 3.7827298050139273,
      "grad_norm": 1.7379077672958374,
      "learning_rate": 0.0006217548746518106,
      "loss": 2.4368,
      "step": 13580
    },
    {
      "epoch": 3.785515320334262,
      "grad_norm": 1.4824559688568115,
      "learning_rate": 0.0006214763231197772,
      "loss": 2.2337,
      "step": 13590
    },
    {
      "epoch": 3.788300835654596,
      "grad_norm": 1.458328127861023,
      "learning_rate": 0.0006211977715877437,
      "loss": 2.1565,
      "step": 13600
    },
    {
      "epoch": 3.7910863509749304,
      "grad_norm": 1.653336763381958,
      "learning_rate": 0.0006209192200557104,
      "loss": 2.45,
      "step": 13610
    },
    {
      "epoch": 3.7938718662952646,
      "grad_norm": 1.7908991575241089,
      "learning_rate": 0.0006206406685236768,
      "loss": 2.2978,
      "step": 13620
    },
    {
      "epoch": 3.796657381615599,
      "grad_norm": 1.38297438621521,
      "learning_rate": 0.0006203621169916434,
      "loss": 2.456,
      "step": 13630
    },
    {
      "epoch": 3.799442896935933,
      "grad_norm": 2.262619972229004,
      "learning_rate": 0.0006200835654596101,
      "loss": 2.1419,
      "step": 13640
    },
    {
      "epoch": 3.8022284122562673,
      "grad_norm": 1.680673599243164,
      "learning_rate": 0.0006198050139275766,
      "loss": 2.1976,
      "step": 13650
    },
    {
      "epoch": 3.8050139275766015,
      "grad_norm": 1.227888584136963,
      "learning_rate": 0.0006195264623955432,
      "loss": 2.4351,
      "step": 13660
    },
    {
      "epoch": 3.807799442896936,
      "grad_norm": 1.6858214139938354,
      "learning_rate": 0.0006192479108635097,
      "loss": 2.2053,
      "step": 13670
    },
    {
      "epoch": 3.8105849582172704,
      "grad_norm": 1.1789467334747314,
      "learning_rate": 0.0006189693593314764,
      "loss": 2.3229,
      "step": 13680
    },
    {
      "epoch": 3.8133704735376046,
      "grad_norm": 1.3728806972503662,
      "learning_rate": 0.0006186908077994429,
      "loss": 2.3169,
      "step": 13690
    },
    {
      "epoch": 3.816155988857939,
      "grad_norm": 2.1094112396240234,
      "learning_rate": 0.0006184122562674095,
      "loss": 2.2951,
      "step": 13700
    },
    {
      "epoch": 3.818941504178273,
      "grad_norm": 3.7139194011688232,
      "learning_rate": 0.0006181337047353762,
      "loss": 2.2572,
      "step": 13710
    },
    {
      "epoch": 3.8217270194986073,
      "grad_norm": 1.4526629447937012,
      "learning_rate": 0.0006178551532033426,
      "loss": 2.4283,
      "step": 13720
    },
    {
      "epoch": 3.8245125348189415,
      "grad_norm": 1.2047464847564697,
      "learning_rate": 0.0006175766016713092,
      "loss": 2.3383,
      "step": 13730
    },
    {
      "epoch": 3.8272980501392757,
      "grad_norm": 1.4190878868103027,
      "learning_rate": 0.0006172980501392757,
      "loss": 2.3929,
      "step": 13740
    },
    {
      "epoch": 3.83008356545961,
      "grad_norm": 1.558292269706726,
      "learning_rate": 0.0006170194986072424,
      "loss": 2.3621,
      "step": 13750
    },
    {
      "epoch": 3.832869080779944,
      "grad_norm": 1.3910553455352783,
      "learning_rate": 0.0006167409470752089,
      "loss": 2.161,
      "step": 13760
    },
    {
      "epoch": 3.8356545961002784,
      "grad_norm": 1.2965446710586548,
      "learning_rate": 0.0006164623955431755,
      "loss": 2.4087,
      "step": 13770
    },
    {
      "epoch": 3.8384401114206126,
      "grad_norm": 1.3610602617263794,
      "learning_rate": 0.000616183844011142,
      "loss": 2.3928,
      "step": 13780
    },
    {
      "epoch": 3.841225626740947,
      "grad_norm": 1.6701500415802002,
      "learning_rate": 0.0006159052924791087,
      "loss": 2.3799,
      "step": 13790
    },
    {
      "epoch": 3.8440111420612815,
      "grad_norm": 1.8001396656036377,
      "learning_rate": 0.0006156267409470753,
      "loss": 2.5155,
      "step": 13800
    },
    {
      "epoch": 3.8467966573816157,
      "grad_norm": 1.7388993501663208,
      "learning_rate": 0.0006153481894150417,
      "loss": 2.2451,
      "step": 13810
    },
    {
      "epoch": 3.84958217270195,
      "grad_norm": 1.3166495561599731,
      "learning_rate": 0.0006150696378830084,
      "loss": 2.2235,
      "step": 13820
    },
    {
      "epoch": 3.852367688022284,
      "grad_norm": 1.2238574028015137,
      "learning_rate": 0.0006147910863509749,
      "loss": 2.4466,
      "step": 13830
    },
    {
      "epoch": 3.8551532033426184,
      "grad_norm": 1.4573776721954346,
      "learning_rate": 0.0006145125348189415,
      "loss": 2.149,
      "step": 13840
    },
    {
      "epoch": 3.8579387186629526,
      "grad_norm": 1.6888271570205688,
      "learning_rate": 0.000614233983286908,
      "loss": 2.2482,
      "step": 13850
    },
    {
      "epoch": 3.860724233983287,
      "grad_norm": 1.4359431266784668,
      "learning_rate": 0.0006139554317548747,
      "loss": 2.3151,
      "step": 13860
    },
    {
      "epoch": 3.863509749303621,
      "grad_norm": 6.025242805480957,
      "learning_rate": 0.0006136768802228413,
      "loss": 2.2494,
      "step": 13870
    },
    {
      "epoch": 3.8662952646239557,
      "grad_norm": 3.073169231414795,
      "learning_rate": 0.0006133983286908078,
      "loss": 2.5698,
      "step": 13880
    },
    {
      "epoch": 3.86908077994429,
      "grad_norm": 2.069894552230835,
      "learning_rate": 0.0006131197771587745,
      "loss": 2.3903,
      "step": 13890
    },
    {
      "epoch": 3.871866295264624,
      "grad_norm": 1.587308406829834,
      "learning_rate": 0.000612841225626741,
      "loss": 2.5106,
      "step": 13900
    },
    {
      "epoch": 3.8746518105849583,
      "grad_norm": 1.1215622425079346,
      "learning_rate": 0.0006125626740947076,
      "loss": 2.2304,
      "step": 13910
    },
    {
      "epoch": 3.8774373259052926,
      "grad_norm": 1.2467210292816162,
      "learning_rate": 0.000612284122562674,
      "loss": 2.3306,
      "step": 13920
    },
    {
      "epoch": 3.8802228412256268,
      "grad_norm": 1.6534005403518677,
      "learning_rate": 0.0006120055710306407,
      "loss": 2.2411,
      "step": 13930
    },
    {
      "epoch": 3.883008356545961,
      "grad_norm": 1.3607392311096191,
      "learning_rate": 0.0006117270194986072,
      "loss": 2.2792,
      "step": 13940
    },
    {
      "epoch": 3.885793871866295,
      "grad_norm": 2.243166208267212,
      "learning_rate": 0.0006114484679665738,
      "loss": 2.4046,
      "step": 13950
    },
    {
      "epoch": 3.8885793871866294,
      "grad_norm": 2.5822432041168213,
      "learning_rate": 0.0006111699164345404,
      "loss": 2.3736,
      "step": 13960
    },
    {
      "epoch": 3.8913649025069637,
      "grad_norm": 1.1679655313491821,
      "learning_rate": 0.000610891364902507,
      "loss": 2.1926,
      "step": 13970
    },
    {
      "epoch": 3.894150417827298,
      "grad_norm": 1.1927286386489868,
      "learning_rate": 0.0006106128133704736,
      "loss": 2.406,
      "step": 13980
    },
    {
      "epoch": 3.896935933147632,
      "grad_norm": 2.11562180519104,
      "learning_rate": 0.0006103342618384401,
      "loss": 2.3136,
      "step": 13990
    },
    {
      "epoch": 3.8997214484679663,
      "grad_norm": 1.1998339891433716,
      "learning_rate": 0.0006100557103064068,
      "loss": 2.3461,
      "step": 14000
    },
    {
      "epoch": 3.902506963788301,
      "grad_norm": 1.5190181732177734,
      "learning_rate": 0.0006097771587743732,
      "loss": 2.3186,
      "step": 14010
    },
    {
      "epoch": 3.905292479108635,
      "grad_norm": 1.7949798107147217,
      "learning_rate": 0.0006094986072423398,
      "loss": 2.3551,
      "step": 14020
    },
    {
      "epoch": 3.9080779944289694,
      "grad_norm": 2.0643231868743896,
      "learning_rate": 0.0006092200557103063,
      "loss": 2.391,
      "step": 14030
    },
    {
      "epoch": 3.9108635097493036,
      "grad_norm": 1.2505639791488647,
      "learning_rate": 0.000608941504178273,
      "loss": 2.2288,
      "step": 14040
    },
    {
      "epoch": 3.913649025069638,
      "grad_norm": 1.2173726558685303,
      "learning_rate": 0.0006086629526462396,
      "loss": 2.2265,
      "step": 14050
    },
    {
      "epoch": 3.916434540389972,
      "grad_norm": 1.3302245140075684,
      "learning_rate": 0.0006083844011142061,
      "loss": 2.4306,
      "step": 14060
    },
    {
      "epoch": 3.9192200557103063,
      "grad_norm": 1.3645262718200684,
      "learning_rate": 0.0006081058495821728,
      "loss": 2.1743,
      "step": 14070
    },
    {
      "epoch": 3.9220055710306405,
      "grad_norm": 1.4118432998657227,
      "learning_rate": 0.0006078272980501393,
      "loss": 2.2185,
      "step": 14080
    },
    {
      "epoch": 3.924791086350975,
      "grad_norm": 1.6188522577285767,
      "learning_rate": 0.0006075487465181059,
      "loss": 2.3102,
      "step": 14090
    },
    {
      "epoch": 3.9275766016713094,
      "grad_norm": 1.3667428493499756,
      "learning_rate": 0.0006072701949860724,
      "loss": 2.5612,
      "step": 14100
    },
    {
      "epoch": 3.9303621169916436,
      "grad_norm": 1.6217498779296875,
      "learning_rate": 0.000606991643454039,
      "loss": 2.3656,
      "step": 14110
    },
    {
      "epoch": 3.933147632311978,
      "grad_norm": 0.9873479604721069,
      "learning_rate": 0.0006067130919220056,
      "loss": 2.2936,
      "step": 14120
    },
    {
      "epoch": 3.935933147632312,
      "grad_norm": 1.796876072883606,
      "learning_rate": 0.0006064345403899721,
      "loss": 2.335,
      "step": 14130
    },
    {
      "epoch": 3.9387186629526463,
      "grad_norm": 1.3321703672409058,
      "learning_rate": 0.0006061559888579387,
      "loss": 2.4552,
      "step": 14140
    },
    {
      "epoch": 3.9415041782729805,
      "grad_norm": 1.7799837589263916,
      "learning_rate": 0.0006058774373259053,
      "loss": 2.5033,
      "step": 14150
    },
    {
      "epoch": 3.9442896935933147,
      "grad_norm": 1.3360286951065063,
      "learning_rate": 0.0006055988857938719,
      "loss": 2.4805,
      "step": 14160
    },
    {
      "epoch": 3.947075208913649,
      "grad_norm": 1.6407709121704102,
      "learning_rate": 0.0006053203342618384,
      "loss": 2.1038,
      "step": 14170
    },
    {
      "epoch": 3.949860724233983,
      "grad_norm": 3.189438581466675,
      "learning_rate": 0.0006050417827298051,
      "loss": 2.2337,
      "step": 14180
    },
    {
      "epoch": 3.9526462395543174,
      "grad_norm": 1.6908210515975952,
      "learning_rate": 0.0006047632311977716,
      "loss": 2.3892,
      "step": 14190
    },
    {
      "epoch": 3.9554317548746516,
      "grad_norm": 2.019535541534424,
      "learning_rate": 0.0006044846796657382,
      "loss": 2.3097,
      "step": 14200
    },
    {
      "epoch": 3.958217270194986,
      "grad_norm": 1.193924069404602,
      "learning_rate": 0.0006042061281337047,
      "loss": 2.346,
      "step": 14210
    },
    {
      "epoch": 3.9610027855153205,
      "grad_norm": 1.0123779773712158,
      "learning_rate": 0.0006039275766016713,
      "loss": 2.1806,
      "step": 14220
    },
    {
      "epoch": 3.9637883008356547,
      "grad_norm": 1.855773687362671,
      "learning_rate": 0.0006036490250696379,
      "loss": 2.3554,
      "step": 14230
    },
    {
      "epoch": 3.966573816155989,
      "grad_norm": 1.5394079685211182,
      "learning_rate": 0.0006033704735376044,
      "loss": 2.2263,
      "step": 14240
    },
    {
      "epoch": 3.969359331476323,
      "grad_norm": 1.3309495449066162,
      "learning_rate": 0.0006030919220055711,
      "loss": 2.4517,
      "step": 14250
    },
    {
      "epoch": 3.9721448467966574,
      "grad_norm": 2.5343358516693115,
      "learning_rate": 0.0006028133704735376,
      "loss": 2.1338,
      "step": 14260
    },
    {
      "epoch": 3.9749303621169916,
      "grad_norm": 1.8830889463424683,
      "learning_rate": 0.0006025348189415042,
      "loss": 2.3768,
      "step": 14270
    },
    {
      "epoch": 3.977715877437326,
      "grad_norm": 1.6405713558197021,
      "learning_rate": 0.0006022562674094708,
      "loss": 2.3242,
      "step": 14280
    },
    {
      "epoch": 3.98050139275766,
      "grad_norm": 1.3107353448867798,
      "learning_rate": 0.0006019777158774374,
      "loss": 2.1452,
      "step": 14290
    },
    {
      "epoch": 3.9832869080779947,
      "grad_norm": 1.6839147806167603,
      "learning_rate": 0.000601699164345404,
      "loss": 2.1762,
      "step": 14300
    },
    {
      "epoch": 3.986072423398329,
      "grad_norm": 1.4004991054534912,
      "learning_rate": 0.0006014206128133704,
      "loss": 2.5207,
      "step": 14310
    },
    {
      "epoch": 3.988857938718663,
      "grad_norm": 1.4352954626083374,
      "learning_rate": 0.000601142061281337,
      "loss": 2.2929,
      "step": 14320
    },
    {
      "epoch": 3.9916434540389973,
      "grad_norm": 1.0374358892440796,
      "learning_rate": 0.0006008635097493036,
      "loss": 2.2666,
      "step": 14330
    },
    {
      "epoch": 3.9944289693593316,
      "grad_norm": 1.7030855417251587,
      "learning_rate": 0.0006005849582172702,
      "loss": 2.2062,
      "step": 14340
    },
    {
      "epoch": 3.997214484679666,
      "grad_norm": 1.4634010791778564,
      "learning_rate": 0.0006003064066852367,
      "loss": 2.1871,
      "step": 14350
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.4374189376831055,
      "learning_rate": 0.0006000278551532034,
      "loss": 2.1557,
      "step": 14360
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.2973484992980957,
      "eval_runtime": 6.4366,
      "eval_samples_per_second": 495.91,
      "eval_steps_per_second": 61.989,
      "step": 14360
    },
    {
      "epoch": 4.002785515320334,
      "grad_norm": 2.0360937118530273,
      "learning_rate": 0.00059974930362117,
      "loss": 2.1416,
      "step": 14370
    },
    {
      "epoch": 4.005571030640668,
      "grad_norm": 1.8102110624313354,
      "learning_rate": 0.0005994707520891365,
      "loss": 2.2923,
      "step": 14380
    },
    {
      "epoch": 4.008356545961003,
      "grad_norm": 1.3762331008911133,
      "learning_rate": 0.0005991922005571031,
      "loss": 2.2526,
      "step": 14390
    },
    {
      "epoch": 4.011142061281337,
      "grad_norm": 2.005733013153076,
      "learning_rate": 0.0005989136490250697,
      "loss": 2.4382,
      "step": 14400
    },
    {
      "epoch": 4.013927576601671,
      "grad_norm": 1.4826385974884033,
      "learning_rate": 0.0005986350974930362,
      "loss": 2.2217,
      "step": 14410
    },
    {
      "epoch": 4.016713091922005,
      "grad_norm": 1.3500720262527466,
      "learning_rate": 0.0005983565459610027,
      "loss": 2.3309,
      "step": 14420
    },
    {
      "epoch": 4.0194986072423395,
      "grad_norm": 1.2640655040740967,
      "learning_rate": 0.0005980779944289694,
      "loss": 2.1705,
      "step": 14430
    },
    {
      "epoch": 4.022284122562674,
      "grad_norm": 1.41635262966156,
      "learning_rate": 0.000597799442896936,
      "loss": 2.1922,
      "step": 14440
    },
    {
      "epoch": 4.025069637883008,
      "grad_norm": 1.3955096006393433,
      "learning_rate": 0.0005975208913649025,
      "loss": 2.1993,
      "step": 14450
    },
    {
      "epoch": 4.027855153203342,
      "grad_norm": 1.3384681940078735,
      "learning_rate": 0.0005972423398328691,
      "loss": 2.0238,
      "step": 14460
    },
    {
      "epoch": 4.030640668523677,
      "grad_norm": 1.7167984247207642,
      "learning_rate": 0.0005969637883008357,
      "loss": 2.2141,
      "step": 14470
    },
    {
      "epoch": 4.0334261838440115,
      "grad_norm": 1.495998501777649,
      "learning_rate": 0.0005966852367688023,
      "loss": 2.3749,
      "step": 14480
    },
    {
      "epoch": 4.036211699164346,
      "grad_norm": 1.4264177083969116,
      "learning_rate": 0.0005964066852367688,
      "loss": 2.1992,
      "step": 14490
    },
    {
      "epoch": 4.03899721448468,
      "grad_norm": 1.8571105003356934,
      "learning_rate": 0.0005961281337047353,
      "loss": 2.2462,
      "step": 14500
    },
    {
      "epoch": 4.041782729805014,
      "grad_norm": 2.22822642326355,
      "learning_rate": 0.0005958495821727019,
      "loss": 2.478,
      "step": 14510
    },
    {
      "epoch": 4.044568245125348,
      "grad_norm": 1.3806140422821045,
      "learning_rate": 0.0005955710306406685,
      "loss": 2.3171,
      "step": 14520
    },
    {
      "epoch": 4.047353760445683,
      "grad_norm": 1.579259991645813,
      "learning_rate": 0.0005952924791086351,
      "loss": 2.305,
      "step": 14530
    },
    {
      "epoch": 4.050139275766017,
      "grad_norm": 1.6080167293548584,
      "learning_rate": 0.0005950139275766017,
      "loss": 2.2763,
      "step": 14540
    },
    {
      "epoch": 4.052924791086351,
      "grad_norm": 2.4508509635925293,
      "learning_rate": 0.0005947353760445683,
      "loss": 2.2928,
      "step": 14550
    },
    {
      "epoch": 4.055710306406685,
      "grad_norm": 1.3869081735610962,
      "learning_rate": 0.0005944568245125348,
      "loss": 2.1104,
      "step": 14560
    },
    {
      "epoch": 4.0584958217270195,
      "grad_norm": 1.9487316608428955,
      "learning_rate": 0.0005941782729805014,
      "loss": 2.255,
      "step": 14570
    },
    {
      "epoch": 4.061281337047354,
      "grad_norm": 1.4858343601226807,
      "learning_rate": 0.000593899721448468,
      "loss": 2.3094,
      "step": 14580
    },
    {
      "epoch": 4.064066852367688,
      "grad_norm": 1.6465998888015747,
      "learning_rate": 0.0005936211699164346,
      "loss": 2.4228,
      "step": 14590
    },
    {
      "epoch": 4.066852367688022,
      "grad_norm": 1.239040493965149,
      "learning_rate": 0.0005933426183844012,
      "loss": 2.0611,
      "step": 14600
    },
    {
      "epoch": 4.069637883008356,
      "grad_norm": 1.8274582624435425,
      "learning_rate": 0.0005930640668523677,
      "loss": 2.3788,
      "step": 14610
    },
    {
      "epoch": 4.072423398328691,
      "grad_norm": 1.6512389183044434,
      "learning_rate": 0.0005927855153203343,
      "loss": 2.2509,
      "step": 14620
    },
    {
      "epoch": 4.075208913649025,
      "grad_norm": 2.010730504989624,
      "learning_rate": 0.0005925069637883008,
      "loss": 2.3429,
      "step": 14630
    },
    {
      "epoch": 4.077994428969359,
      "grad_norm": 1.5680612325668335,
      "learning_rate": 0.0005922284122562674,
      "loss": 2.2668,
      "step": 14640
    },
    {
      "epoch": 4.080779944289693,
      "grad_norm": 2.0549559593200684,
      "learning_rate": 0.000591949860724234,
      "loss": 2.2345,
      "step": 14650
    },
    {
      "epoch": 4.0835654596100275,
      "grad_norm": 1.460972547531128,
      "learning_rate": 0.0005916713091922006,
      "loss": 2.183,
      "step": 14660
    },
    {
      "epoch": 4.086350974930362,
      "grad_norm": 1.6095952987670898,
      "learning_rate": 0.0005913927576601671,
      "loss": 2.2562,
      "step": 14670
    },
    {
      "epoch": 4.089136490250697,
      "grad_norm": 1.5512384176254272,
      "learning_rate": 0.0005911142061281337,
      "loss": 2.2013,
      "step": 14680
    },
    {
      "epoch": 4.091922005571031,
      "grad_norm": 1.6830017566680908,
      "learning_rate": 0.0005908356545961004,
      "loss": 2.212,
      "step": 14690
    },
    {
      "epoch": 4.094707520891365,
      "grad_norm": 1.6106349229812622,
      "learning_rate": 0.0005905571030640668,
      "loss": 2.3027,
      "step": 14700
    },
    {
      "epoch": 4.0974930362116995,
      "grad_norm": 1.0851926803588867,
      "learning_rate": 0.0005902785515320334,
      "loss": 2.3326,
      "step": 14710
    },
    {
      "epoch": 4.100278551532034,
      "grad_norm": 1.3003994226455688,
      "learning_rate": 0.00059,
      "loss": 2.1016,
      "step": 14720
    },
    {
      "epoch": 4.103064066852368,
      "grad_norm": 2.0965077877044678,
      "learning_rate": 0.0005897214484679666,
      "loss": 2.2317,
      "step": 14730
    },
    {
      "epoch": 4.105849582172702,
      "grad_norm": 1.4770190715789795,
      "learning_rate": 0.0005894428969359331,
      "loss": 2.1883,
      "step": 14740
    },
    {
      "epoch": 4.108635097493036,
      "grad_norm": 1.4317492246627808,
      "learning_rate": 0.0005891643454038997,
      "loss": 2.4513,
      "step": 14750
    },
    {
      "epoch": 4.111420612813371,
      "grad_norm": 1.358065128326416,
      "learning_rate": 0.0005888857938718664,
      "loss": 2.1916,
      "step": 14760
    },
    {
      "epoch": 4.114206128133705,
      "grad_norm": 1.6423113346099854,
      "learning_rate": 0.0005886072423398329,
      "loss": 2.0488,
      "step": 14770
    },
    {
      "epoch": 4.116991643454039,
      "grad_norm": 1.8894460201263428,
      "learning_rate": 0.0005883286908077995,
      "loss": 2.2755,
      "step": 14780
    },
    {
      "epoch": 4.119777158774373,
      "grad_norm": 1.298604965209961,
      "learning_rate": 0.000588050139275766,
      "loss": 2.4174,
      "step": 14790
    },
    {
      "epoch": 4.1225626740947074,
      "grad_norm": 1.6297407150268555,
      "learning_rate": 0.0005877715877437327,
      "loss": 2.302,
      "step": 14800
    },
    {
      "epoch": 4.125348189415042,
      "grad_norm": 1.3700745105743408,
      "learning_rate": 0.0005874930362116991,
      "loss": 2.3637,
      "step": 14810
    },
    {
      "epoch": 4.128133704735376,
      "grad_norm": 1.297002911567688,
      "learning_rate": 0.0005872144846796657,
      "loss": 2.2518,
      "step": 14820
    },
    {
      "epoch": 4.13091922005571,
      "grad_norm": 2.0868654251098633,
      "learning_rate": 0.0005869359331476323,
      "loss": 2.3742,
      "step": 14830
    },
    {
      "epoch": 4.133704735376044,
      "grad_norm": 1.6286650896072388,
      "learning_rate": 0.0005866573816155989,
      "loss": 2.216,
      "step": 14840
    },
    {
      "epoch": 4.1364902506963785,
      "grad_norm": 1.5017366409301758,
      "learning_rate": 0.0005863788300835655,
      "loss": 2.1553,
      "step": 14850
    },
    {
      "epoch": 4.139275766016713,
      "grad_norm": 1.5166455507278442,
      "learning_rate": 0.000586100278551532,
      "loss": 2.3135,
      "step": 14860
    },
    {
      "epoch": 4.142061281337047,
      "grad_norm": 1.870684266090393,
      "learning_rate": 0.0005858217270194987,
      "loss": 2.27,
      "step": 14870
    },
    {
      "epoch": 4.144846796657381,
      "grad_norm": 1.4969291687011719,
      "learning_rate": 0.0005855431754874652,
      "loss": 2.3462,
      "step": 14880
    },
    {
      "epoch": 4.147632311977716,
      "grad_norm": 1.296578288078308,
      "learning_rate": 0.0005852646239554318,
      "loss": 2.1611,
      "step": 14890
    },
    {
      "epoch": 4.1504178272980505,
      "grad_norm": 2.5750794410705566,
      "learning_rate": 0.0005849860724233983,
      "loss": 2.5712,
      "step": 14900
    },
    {
      "epoch": 4.153203342618385,
      "grad_norm": 1.5424673557281494,
      "learning_rate": 0.0005847075208913649,
      "loss": 2.3024,
      "step": 14910
    },
    {
      "epoch": 4.155988857938719,
      "grad_norm": 2.925360679626465,
      "learning_rate": 0.0005844289693593315,
      "loss": 2.3798,
      "step": 14920
    },
    {
      "epoch": 4.158774373259053,
      "grad_norm": 1.4289361238479614,
      "learning_rate": 0.000584150417827298,
      "loss": 2.3005,
      "step": 14930
    },
    {
      "epoch": 4.161559888579387,
      "grad_norm": 1.865227460861206,
      "learning_rate": 0.0005838718662952647,
      "loss": 2.3139,
      "step": 14940
    },
    {
      "epoch": 4.164345403899722,
      "grad_norm": 2.095386505126953,
      "learning_rate": 0.0005835933147632312,
      "loss": 2.3546,
      "step": 14950
    },
    {
      "epoch": 4.167130919220056,
      "grad_norm": 1.4315598011016846,
      "learning_rate": 0.0005833147632311978,
      "loss": 2.392,
      "step": 14960
    },
    {
      "epoch": 4.16991643454039,
      "grad_norm": 1.417771816253662,
      "learning_rate": 0.0005830362116991643,
      "loss": 2.3018,
      "step": 14970
    },
    {
      "epoch": 4.172701949860724,
      "grad_norm": 1.7993704080581665,
      "learning_rate": 0.000582757660167131,
      "loss": 2.204,
      "step": 14980
    },
    {
      "epoch": 4.1754874651810585,
      "grad_norm": 2.559723377227783,
      "learning_rate": 0.0005824791086350975,
      "loss": 2.2503,
      "step": 14990
    },
    {
      "epoch": 4.178272980501393,
      "grad_norm": 2.5463738441467285,
      "learning_rate": 0.000582200557103064,
      "loss": 2.2932,
      "step": 15000
    },
    {
      "epoch": 4.181058495821727,
      "grad_norm": 1.3811120986938477,
      "learning_rate": 0.0005819220055710307,
      "loss": 2.2724,
      "step": 15010
    },
    {
      "epoch": 4.183844011142061,
      "grad_norm": 1.801372766494751,
      "learning_rate": 0.0005816434540389972,
      "loss": 2.1991,
      "step": 15020
    },
    {
      "epoch": 4.186629526462395,
      "grad_norm": 1.370427131652832,
      "learning_rate": 0.0005813649025069638,
      "loss": 2.2596,
      "step": 15030
    },
    {
      "epoch": 4.18941504178273,
      "grad_norm": 1.441612958908081,
      "learning_rate": 0.0005810863509749303,
      "loss": 2.3803,
      "step": 15040
    },
    {
      "epoch": 4.192200557103064,
      "grad_norm": 1.465203046798706,
      "learning_rate": 0.000580807799442897,
      "loss": 2.4767,
      "step": 15050
    },
    {
      "epoch": 4.194986072423398,
      "grad_norm": 1.2184219360351562,
      "learning_rate": 0.0005805292479108635,
      "loss": 2.2513,
      "step": 15060
    },
    {
      "epoch": 4.197771587743732,
      "grad_norm": 1.640784502029419,
      "learning_rate": 0.0005802506963788301,
      "loss": 2.1977,
      "step": 15070
    },
    {
      "epoch": 4.2005571030640665,
      "grad_norm": 1.401044249534607,
      "learning_rate": 0.0005799721448467968,
      "loss": 2.2538,
      "step": 15080
    },
    {
      "epoch": 4.203342618384401,
      "grad_norm": 1.5254902839660645,
      "learning_rate": 0.0005796935933147633,
      "loss": 2.3149,
      "step": 15090
    },
    {
      "epoch": 4.206128133704736,
      "grad_norm": 2.107022285461426,
      "learning_rate": 0.0005794150417827298,
      "loss": 2.2607,
      "step": 15100
    },
    {
      "epoch": 4.20891364902507,
      "grad_norm": 1.3245548009872437,
      "learning_rate": 0.0005791364902506963,
      "loss": 2.3025,
      "step": 15110
    },
    {
      "epoch": 4.211699164345404,
      "grad_norm": 1.7014973163604736,
      "learning_rate": 0.000578857938718663,
      "loss": 2.3637,
      "step": 15120
    },
    {
      "epoch": 4.2144846796657385,
      "grad_norm": 1.117921233177185,
      "learning_rate": 0.0005785793871866295,
      "loss": 2.0905,
      "step": 15130
    },
    {
      "epoch": 4.217270194986073,
      "grad_norm": 1.3539248704910278,
      "learning_rate": 0.0005783008356545961,
      "loss": 2.2317,
      "step": 15140
    },
    {
      "epoch": 4.220055710306407,
      "grad_norm": 1.183687686920166,
      "learning_rate": 0.0005780222841225626,
      "loss": 2.3598,
      "step": 15150
    },
    {
      "epoch": 4.222841225626741,
      "grad_norm": 1.9197256565093994,
      "learning_rate": 0.0005777437325905293,
      "loss": 2.3248,
      "step": 15160
    },
    {
      "epoch": 4.225626740947075,
      "grad_norm": 1.6556881666183472,
      "learning_rate": 0.0005774651810584959,
      "loss": 2.3098,
      "step": 15170
    },
    {
      "epoch": 4.22841225626741,
      "grad_norm": 1.3151664733886719,
      "learning_rate": 0.0005771866295264624,
      "loss": 2.2205,
      "step": 15180
    },
    {
      "epoch": 4.231197771587744,
      "grad_norm": 2.5781543254852295,
      "learning_rate": 0.0005769080779944291,
      "loss": 2.3634,
      "step": 15190
    },
    {
      "epoch": 4.233983286908078,
      "grad_norm": 1.6633973121643066,
      "learning_rate": 0.0005766295264623955,
      "loss": 2.1442,
      "step": 15200
    },
    {
      "epoch": 4.236768802228412,
      "grad_norm": 1.6118977069854736,
      "learning_rate": 0.0005763509749303621,
      "loss": 2.3357,
      "step": 15210
    },
    {
      "epoch": 4.2395543175487465,
      "grad_norm": 1.5229166746139526,
      "learning_rate": 0.0005760724233983286,
      "loss": 2.1706,
      "step": 15220
    },
    {
      "epoch": 4.242339832869081,
      "grad_norm": 1.5232617855072021,
      "learning_rate": 0.0005757938718662953,
      "loss": 2.3708,
      "step": 15230
    },
    {
      "epoch": 4.245125348189415,
      "grad_norm": 1.6790465116500854,
      "learning_rate": 0.0005755153203342619,
      "loss": 2.2842,
      "step": 15240
    },
    {
      "epoch": 4.247910863509749,
      "grad_norm": 1.5185904502868652,
      "learning_rate": 0.0005752367688022284,
      "loss": 2.3205,
      "step": 15250
    },
    {
      "epoch": 4.250696378830083,
      "grad_norm": 1.4643218517303467,
      "learning_rate": 0.0005749582172701951,
      "loss": 2.1956,
      "step": 15260
    },
    {
      "epoch": 4.2534818941504176,
      "grad_norm": 1.3411837816238403,
      "learning_rate": 0.0005746796657381616,
      "loss": 2.4737,
      "step": 15270
    },
    {
      "epoch": 4.256267409470752,
      "grad_norm": 1.4123855829238892,
      "learning_rate": 0.0005744011142061282,
      "loss": 2.1749,
      "step": 15280
    },
    {
      "epoch": 4.259052924791086,
      "grad_norm": 1.9098623991012573,
      "learning_rate": 0.0005741225626740946,
      "loss": 2.3757,
      "step": 15290
    },
    {
      "epoch": 4.26183844011142,
      "grad_norm": 1.7643256187438965,
      "learning_rate": 0.0005738440111420613,
      "loss": 2.1857,
      "step": 15300
    },
    {
      "epoch": 4.264623955431755,
      "grad_norm": 1.2040835618972778,
      "learning_rate": 0.0005735654596100278,
      "loss": 2.2424,
      "step": 15310
    },
    {
      "epoch": 4.2674094707520895,
      "grad_norm": 1.2833579778671265,
      "learning_rate": 0.0005732869080779944,
      "loss": 2.139,
      "step": 15320
    },
    {
      "epoch": 4.270194986072424,
      "grad_norm": 1.3987234830856323,
      "learning_rate": 0.000573008356545961,
      "loss": 2.3178,
      "step": 15330
    },
    {
      "epoch": 4.272980501392758,
      "grad_norm": 1.8178486824035645,
      "learning_rate": 0.0005727298050139276,
      "loss": 2.0977,
      "step": 15340
    },
    {
      "epoch": 4.275766016713092,
      "grad_norm": 1.5040615797042847,
      "learning_rate": 0.0005724512534818942,
      "loss": 2.18,
      "step": 15350
    },
    {
      "epoch": 4.278551532033426,
      "grad_norm": 1.717322587966919,
      "learning_rate": 0.0005721727019498607,
      "loss": 2.2635,
      "step": 15360
    },
    {
      "epoch": 4.281337047353761,
      "grad_norm": 1.3062556982040405,
      "learning_rate": 0.0005718941504178274,
      "loss": 2.2213,
      "step": 15370
    },
    {
      "epoch": 4.284122562674095,
      "grad_norm": 1.8128899335861206,
      "learning_rate": 0.0005716155988857939,
      "loss": 2.1004,
      "step": 15380
    },
    {
      "epoch": 4.286908077994429,
      "grad_norm": 1.2150332927703857,
      "learning_rate": 0.0005713370473537604,
      "loss": 2.111,
      "step": 15390
    },
    {
      "epoch": 4.289693593314763,
      "grad_norm": 1.8370839357376099,
      "learning_rate": 0.000571058495821727,
      "loss": 2.261,
      "step": 15400
    },
    {
      "epoch": 4.2924791086350975,
      "grad_norm": 1.690324068069458,
      "learning_rate": 0.0005707799442896936,
      "loss": 2.1956,
      "step": 15410
    },
    {
      "epoch": 4.295264623955432,
      "grad_norm": 1.4479166269302368,
      "learning_rate": 0.0005705013927576602,
      "loss": 2.2069,
      "step": 15420
    },
    {
      "epoch": 4.298050139275766,
      "grad_norm": 1.2721556425094604,
      "learning_rate": 0.0005702228412256267,
      "loss": 2.2999,
      "step": 15430
    },
    {
      "epoch": 4.3008356545961,
      "grad_norm": 1.5766477584838867,
      "learning_rate": 0.0005699442896935934,
      "loss": 2.2178,
      "step": 15440
    },
    {
      "epoch": 4.303621169916434,
      "grad_norm": 1.280797004699707,
      "learning_rate": 0.0005696657381615599,
      "loss": 2.1381,
      "step": 15450
    },
    {
      "epoch": 4.306406685236769,
      "grad_norm": 1.3170640468597412,
      "learning_rate": 0.0005693871866295265,
      "loss": 2.1952,
      "step": 15460
    },
    {
      "epoch": 4.309192200557103,
      "grad_norm": 1.159849762916565,
      "learning_rate": 0.000569108635097493,
      "loss": 2.1183,
      "step": 15470
    },
    {
      "epoch": 4.311977715877437,
      "grad_norm": 1.4106264114379883,
      "learning_rate": 0.0005688300835654597,
      "loss": 2.2935,
      "step": 15480
    },
    {
      "epoch": 4.314763231197771,
      "grad_norm": 1.3840694427490234,
      "learning_rate": 0.0005685515320334263,
      "loss": 2.1886,
      "step": 15490
    },
    {
      "epoch": 4.3175487465181055,
      "grad_norm": 1.561401605606079,
      "learning_rate": 0.0005682729805013927,
      "loss": 2.3567,
      "step": 15500
    },
    {
      "epoch": 4.32033426183844,
      "grad_norm": 2.1924421787261963,
      "learning_rate": 0.0005679944289693593,
      "loss": 2.1859,
      "step": 15510
    },
    {
      "epoch": 4.323119777158775,
      "grad_norm": 1.5631184577941895,
      "learning_rate": 0.0005677158774373259,
      "loss": 2.3207,
      "step": 15520
    },
    {
      "epoch": 4.325905292479109,
      "grad_norm": 1.381013035774231,
      "learning_rate": 0.0005674373259052925,
      "loss": 2.2384,
      "step": 15530
    },
    {
      "epoch": 4.328690807799443,
      "grad_norm": 1.3240058422088623,
      "learning_rate": 0.000567158774373259,
      "loss": 2.1282,
      "step": 15540
    },
    {
      "epoch": 4.3314763231197775,
      "grad_norm": 1.4956963062286377,
      "learning_rate": 0.0005668802228412257,
      "loss": 2.4587,
      "step": 15550
    },
    {
      "epoch": 4.334261838440112,
      "grad_norm": 1.2007981538772583,
      "learning_rate": 0.0005666016713091922,
      "loss": 2.3636,
      "step": 15560
    },
    {
      "epoch": 4.337047353760446,
      "grad_norm": 1.2129684686660767,
      "learning_rate": 0.0005663231197771588,
      "loss": 2.3074,
      "step": 15570
    },
    {
      "epoch": 4.33983286908078,
      "grad_norm": 1.4774694442749023,
      "learning_rate": 0.0005660445682451254,
      "loss": 1.9292,
      "step": 15580
    },
    {
      "epoch": 4.342618384401114,
      "grad_norm": 1.646934986114502,
      "learning_rate": 0.000565766016713092,
      "loss": 2.3572,
      "step": 15590
    },
    {
      "epoch": 4.345403899721449,
      "grad_norm": 1.5079293251037598,
      "learning_rate": 0.0005654874651810585,
      "loss": 2.2065,
      "step": 15600
    },
    {
      "epoch": 4.348189415041783,
      "grad_norm": 1.253960371017456,
      "learning_rate": 0.000565208913649025,
      "loss": 2.2064,
      "step": 15610
    },
    {
      "epoch": 4.350974930362117,
      "grad_norm": 1.789851427078247,
      "learning_rate": 0.0005649303621169917,
      "loss": 2.3293,
      "step": 15620
    },
    {
      "epoch": 4.353760445682451,
      "grad_norm": 1.794514536857605,
      "learning_rate": 0.0005646518105849582,
      "loss": 2.332,
      "step": 15630
    },
    {
      "epoch": 4.3565459610027855,
      "grad_norm": 1.5129202604293823,
      "learning_rate": 0.0005643732590529248,
      "loss": 2.373,
      "step": 15640
    },
    {
      "epoch": 4.35933147632312,
      "grad_norm": 1.5168125629425049,
      "learning_rate": 0.0005640947075208914,
      "loss": 2.2836,
      "step": 15650
    },
    {
      "epoch": 4.362116991643454,
      "grad_norm": 2.1647772789001465,
      "learning_rate": 0.000563816155988858,
      "loss": 2.3194,
      "step": 15660
    },
    {
      "epoch": 4.364902506963788,
      "grad_norm": 1.3328505754470825,
      "learning_rate": 0.0005635376044568246,
      "loss": 2.15,
      "step": 15670
    },
    {
      "epoch": 4.367688022284122,
      "grad_norm": 1.2876603603363037,
      "learning_rate": 0.000563259052924791,
      "loss": 2.3862,
      "step": 15680
    },
    {
      "epoch": 4.370473537604457,
      "grad_norm": 1.9776959419250488,
      "learning_rate": 0.0005629805013927576,
      "loss": 2.1821,
      "step": 15690
    },
    {
      "epoch": 4.373259052924791,
      "grad_norm": 1.6575382947921753,
      "learning_rate": 0.0005627019498607242,
      "loss": 2.2074,
      "step": 15700
    },
    {
      "epoch": 4.376044568245125,
      "grad_norm": 1.8612887859344482,
      "learning_rate": 0.0005624233983286908,
      "loss": 2.1472,
      "step": 15710
    },
    {
      "epoch": 4.378830083565459,
      "grad_norm": 1.9858790636062622,
      "learning_rate": 0.0005621448467966573,
      "loss": 2.3143,
      "step": 15720
    },
    {
      "epoch": 4.381615598885794,
      "grad_norm": 1.858015537261963,
      "learning_rate": 0.000561866295264624,
      "loss": 2.2484,
      "step": 15730
    },
    {
      "epoch": 4.3844011142061285,
      "grad_norm": 1.4164525270462036,
      "learning_rate": 0.0005615877437325906,
      "loss": 2.174,
      "step": 15740
    },
    {
      "epoch": 4.387186629526463,
      "grad_norm": 2.0316476821899414,
      "learning_rate": 0.0005613091922005571,
      "loss": 2.5098,
      "step": 15750
    },
    {
      "epoch": 4.389972144846797,
      "grad_norm": 1.3795833587646484,
      "learning_rate": 0.0005610306406685237,
      "loss": 2.3486,
      "step": 15760
    },
    {
      "epoch": 4.392757660167131,
      "grad_norm": 1.2722967863082886,
      "learning_rate": 0.0005607520891364903,
      "loss": 2.0444,
      "step": 15770
    },
    {
      "epoch": 4.395543175487465,
      "grad_norm": 1.393162488937378,
      "learning_rate": 0.0005604735376044569,
      "loss": 2.3657,
      "step": 15780
    },
    {
      "epoch": 4.3983286908078,
      "grad_norm": 1.906105637550354,
      "learning_rate": 0.0005601949860724233,
      "loss": 2.3321,
      "step": 15790
    },
    {
      "epoch": 4.401114206128134,
      "grad_norm": 1.9262198209762573,
      "learning_rate": 0.00055991643454039,
      "loss": 2.1693,
      "step": 15800
    },
    {
      "epoch": 4.403899721448468,
      "grad_norm": 1.5891270637512207,
      "learning_rate": 0.0005596378830083566,
      "loss": 2.5285,
      "step": 15810
    },
    {
      "epoch": 4.406685236768802,
      "grad_norm": 1.3921881914138794,
      "learning_rate": 0.0005593593314763231,
      "loss": 2.4325,
      "step": 15820
    },
    {
      "epoch": 4.4094707520891365,
      "grad_norm": 1.679152011871338,
      "learning_rate": 0.0005590807799442897,
      "loss": 2.3982,
      "step": 15830
    },
    {
      "epoch": 4.412256267409471,
      "grad_norm": 1.7745466232299805,
      "learning_rate": 0.0005588022284122563,
      "loss": 2.2373,
      "step": 15840
    },
    {
      "epoch": 4.415041782729805,
      "grad_norm": 2.596088171005249,
      "learning_rate": 0.0005585236768802229,
      "loss": 2.2784,
      "step": 15850
    },
    {
      "epoch": 4.417827298050139,
      "grad_norm": 1.3227458000183105,
      "learning_rate": 0.0005582451253481894,
      "loss": 2.2395,
      "step": 15860
    },
    {
      "epoch": 4.420612813370473,
      "grad_norm": 1.5703375339508057,
      "learning_rate": 0.000557966573816156,
      "loss": 2.2469,
      "step": 15870
    },
    {
      "epoch": 4.423398328690808,
      "grad_norm": 1.9340840578079224,
      "learning_rate": 0.0005576880222841225,
      "loss": 2.4705,
      "step": 15880
    },
    {
      "epoch": 4.426183844011142,
      "grad_norm": 1.3810302019119263,
      "learning_rate": 0.0005574094707520891,
      "loss": 2.3225,
      "step": 15890
    },
    {
      "epoch": 4.428969359331476,
      "grad_norm": 1.9542077779769897,
      "learning_rate": 0.0005571309192200557,
      "loss": 2.3561,
      "step": 15900
    },
    {
      "epoch": 4.43175487465181,
      "grad_norm": 1.78766667842865,
      "learning_rate": 0.0005568523676880223,
      "loss": 2.1983,
      "step": 15910
    },
    {
      "epoch": 4.4345403899721445,
      "grad_norm": 1.3523175716400146,
      "learning_rate": 0.0005565738161559889,
      "loss": 2.3746,
      "step": 15920
    },
    {
      "epoch": 4.437325905292479,
      "grad_norm": 1.4821974039077759,
      "learning_rate": 0.0005562952646239554,
      "loss": 2.2486,
      "step": 15930
    },
    {
      "epoch": 4.440111420612814,
      "grad_norm": 1.4232267141342163,
      "learning_rate": 0.000556016713091922,
      "loss": 2.1762,
      "step": 15940
    },
    {
      "epoch": 4.442896935933147,
      "grad_norm": 1.622207522392273,
      "learning_rate": 0.0005557381615598886,
      "loss": 2.2658,
      "step": 15950
    },
    {
      "epoch": 4.445682451253482,
      "grad_norm": 1.8587249517440796,
      "learning_rate": 0.0005554596100278552,
      "loss": 2.2575,
      "step": 15960
    },
    {
      "epoch": 4.4484679665738165,
      "grad_norm": 1.9293287992477417,
      "learning_rate": 0.0005551810584958218,
      "loss": 2.3856,
      "step": 15970
    },
    {
      "epoch": 4.451253481894151,
      "grad_norm": 1.2628570795059204,
      "learning_rate": 0.0005549025069637884,
      "loss": 2.2241,
      "step": 15980
    },
    {
      "epoch": 4.454038997214485,
      "grad_norm": 1.1475825309753418,
      "learning_rate": 0.0005546239554317549,
      "loss": 2.2195,
      "step": 15990
    },
    {
      "epoch": 4.456824512534819,
      "grad_norm": 1.8680592775344849,
      "learning_rate": 0.0005543454038997214,
      "loss": 2.3876,
      "step": 16000
    },
    {
      "epoch": 4.459610027855153,
      "grad_norm": 1.6336191892623901,
      "learning_rate": 0.000554066852367688,
      "loss": 1.9763,
      "step": 16010
    },
    {
      "epoch": 4.462395543175488,
      "grad_norm": 1.5156480073928833,
      "learning_rate": 0.0005537883008356546,
      "loss": 2.3169,
      "step": 16020
    },
    {
      "epoch": 4.465181058495822,
      "grad_norm": 1.4387495517730713,
      "learning_rate": 0.0005535097493036212,
      "loss": 2.1975,
      "step": 16030
    },
    {
      "epoch": 4.467966573816156,
      "grad_norm": 1.8215675354003906,
      "learning_rate": 0.0005532311977715877,
      "loss": 2.1839,
      "step": 16040
    },
    {
      "epoch": 4.47075208913649,
      "grad_norm": 1.9898614883422852,
      "learning_rate": 0.0005529526462395543,
      "loss": 2.4426,
      "step": 16050
    },
    {
      "epoch": 4.4735376044568245,
      "grad_norm": 1.4325311183929443,
      "learning_rate": 0.000552674094707521,
      "loss": 1.9454,
      "step": 16060
    },
    {
      "epoch": 4.476323119777159,
      "grad_norm": 1.8608039617538452,
      "learning_rate": 0.0005523955431754875,
      "loss": 2.1487,
      "step": 16070
    },
    {
      "epoch": 4.479108635097493,
      "grad_norm": 1.4331871271133423,
      "learning_rate": 0.000552116991643454,
      "loss": 2.3335,
      "step": 16080
    },
    {
      "epoch": 4.481894150417827,
      "grad_norm": 1.5322517156600952,
      "learning_rate": 0.0005518384401114206,
      "loss": 2.3613,
      "step": 16090
    },
    {
      "epoch": 4.484679665738161,
      "grad_norm": 1.941576361656189,
      "learning_rate": 0.0005515598885793872,
      "loss": 2.1817,
      "step": 16100
    },
    {
      "epoch": 4.487465181058496,
      "grad_norm": 1.4993246793746948,
      "learning_rate": 0.0005512813370473537,
      "loss": 2.5148,
      "step": 16110
    },
    {
      "epoch": 4.49025069637883,
      "grad_norm": 1.7511115074157715,
      "learning_rate": 0.0005510027855153203,
      "loss": 2.3459,
      "step": 16120
    },
    {
      "epoch": 4.493036211699164,
      "grad_norm": 0.9630801677703857,
      "learning_rate": 0.000550724233983287,
      "loss": 2.2292,
      "step": 16130
    },
    {
      "epoch": 4.495821727019498,
      "grad_norm": 1.831324577331543,
      "learning_rate": 0.0005504456824512535,
      "loss": 2.1494,
      "step": 16140
    },
    {
      "epoch": 4.498607242339833,
      "grad_norm": 1.6534597873687744,
      "learning_rate": 0.0005501671309192201,
      "loss": 2.3167,
      "step": 16150
    },
    {
      "epoch": 4.501392757660167,
      "grad_norm": 2.0068514347076416,
      "learning_rate": 0.0005498885793871867,
      "loss": 2.4001,
      "step": 16160
    },
    {
      "epoch": 4.504178272980502,
      "grad_norm": 1.8478058576583862,
      "learning_rate": 0.0005496100278551533,
      "loss": 2.2774,
      "step": 16170
    },
    {
      "epoch": 4.506963788300836,
      "grad_norm": 1.8266569375991821,
      "learning_rate": 0.0005493314763231197,
      "loss": 2.352,
      "step": 16180
    },
    {
      "epoch": 4.50974930362117,
      "grad_norm": 1.7823731899261475,
      "learning_rate": 0.0005490529247910863,
      "loss": 2.4886,
      "step": 16190
    },
    {
      "epoch": 4.512534818941504,
      "grad_norm": 1.292259693145752,
      "learning_rate": 0.0005487743732590529,
      "loss": 2.2601,
      "step": 16200
    },
    {
      "epoch": 4.515320334261839,
      "grad_norm": 1.2497751712799072,
      "learning_rate": 0.0005484958217270195,
      "loss": 2.1602,
      "step": 16210
    },
    {
      "epoch": 4.518105849582173,
      "grad_norm": 1.5017327070236206,
      "learning_rate": 0.0005482172701949861,
      "loss": 2.1827,
      "step": 16220
    },
    {
      "epoch": 4.520891364902507,
      "grad_norm": 1.5205382108688354,
      "learning_rate": 0.0005479387186629526,
      "loss": 2.2885,
      "step": 16230
    },
    {
      "epoch": 4.523676880222841,
      "grad_norm": 1.3314756155014038,
      "learning_rate": 0.0005476601671309193,
      "loss": 2.356,
      "step": 16240
    },
    {
      "epoch": 4.5264623955431755,
      "grad_norm": 1.8690762519836426,
      "learning_rate": 0.0005473816155988858,
      "loss": 2.2706,
      "step": 16250
    },
    {
      "epoch": 4.52924791086351,
      "grad_norm": 1.5421088933944702,
      "learning_rate": 0.0005471030640668524,
      "loss": 2.2133,
      "step": 16260
    },
    {
      "epoch": 4.532033426183844,
      "grad_norm": 1.7878080606460571,
      "learning_rate": 0.000546824512534819,
      "loss": 2.0063,
      "step": 16270
    },
    {
      "epoch": 4.534818941504178,
      "grad_norm": 1.3376930952072144,
      "learning_rate": 0.0005465459610027855,
      "loss": 2.3328,
      "step": 16280
    },
    {
      "epoch": 4.537604456824512,
      "grad_norm": 1.4826444387435913,
      "learning_rate": 0.0005462674094707521,
      "loss": 2.2015,
      "step": 16290
    },
    {
      "epoch": 4.540389972144847,
      "grad_norm": 3.734140396118164,
      "learning_rate": 0.0005459888579387186,
      "loss": 2.3152,
      "step": 16300
    },
    {
      "epoch": 4.543175487465181,
      "grad_norm": 0.9821690320968628,
      "learning_rate": 0.0005457103064066853,
      "loss": 2.2772,
      "step": 16310
    },
    {
      "epoch": 4.545961002785515,
      "grad_norm": 1.2790207862854004,
      "learning_rate": 0.0005454317548746518,
      "loss": 2.191,
      "step": 16320
    },
    {
      "epoch": 4.548746518105849,
      "grad_norm": 1.5667293071746826,
      "learning_rate": 0.0005451532033426184,
      "loss": 2.1348,
      "step": 16330
    },
    {
      "epoch": 4.5515320334261835,
      "grad_norm": 1.3806856870651245,
      "learning_rate": 0.0005448746518105849,
      "loss": 2.1575,
      "step": 16340
    },
    {
      "epoch": 4.554317548746518,
      "grad_norm": 1.5149239301681519,
      "learning_rate": 0.0005445961002785516,
      "loss": 2.1444,
      "step": 16350
    },
    {
      "epoch": 4.557103064066853,
      "grad_norm": 2.1412861347198486,
      "learning_rate": 0.0005443175487465181,
      "loss": 2.3,
      "step": 16360
    },
    {
      "epoch": 4.559888579387186,
      "grad_norm": 1.1943260431289673,
      "learning_rate": 0.0005440389972144847,
      "loss": 2.1921,
      "step": 16370
    },
    {
      "epoch": 4.562674094707521,
      "grad_norm": 1.3089715242385864,
      "learning_rate": 0.0005437604456824514,
      "loss": 2.2561,
      "step": 16380
    },
    {
      "epoch": 4.5654596100278555,
      "grad_norm": 1.5351771116256714,
      "learning_rate": 0.0005434818941504178,
      "loss": 2.1929,
      "step": 16390
    },
    {
      "epoch": 4.56824512534819,
      "grad_norm": 1.8000266551971436,
      "learning_rate": 0.0005432033426183844,
      "loss": 2.2982,
      "step": 16400
    },
    {
      "epoch": 4.571030640668524,
      "grad_norm": 1.5873949527740479,
      "learning_rate": 0.0005429247910863509,
      "loss": 2.1632,
      "step": 16410
    },
    {
      "epoch": 4.573816155988858,
      "grad_norm": 1.375633716583252,
      "learning_rate": 0.0005426462395543176,
      "loss": 2.4978,
      "step": 16420
    },
    {
      "epoch": 4.576601671309192,
      "grad_norm": 1.2919456958770752,
      "learning_rate": 0.0005423676880222841,
      "loss": 2.1541,
      "step": 16430
    },
    {
      "epoch": 4.579387186629527,
      "grad_norm": 1.318093180656433,
      "learning_rate": 0.0005420891364902507,
      "loss": 2.234,
      "step": 16440
    },
    {
      "epoch": 4.582172701949861,
      "grad_norm": 1.4195215702056885,
      "learning_rate": 0.0005418105849582174,
      "loss": 2.3464,
      "step": 16450
    },
    {
      "epoch": 4.584958217270195,
      "grad_norm": 1.2426780462265015,
      "learning_rate": 0.0005415320334261839,
      "loss": 2.281,
      "step": 16460
    },
    {
      "epoch": 4.587743732590529,
      "grad_norm": 1.6538323163986206,
      "learning_rate": 0.0005412534818941505,
      "loss": 2.31,
      "step": 16470
    },
    {
      "epoch": 4.5905292479108635,
      "grad_norm": 1.4615976810455322,
      "learning_rate": 0.0005409749303621169,
      "loss": 2.2304,
      "step": 16480
    },
    {
      "epoch": 4.593314763231198,
      "grad_norm": 1.388168215751648,
      "learning_rate": 0.0005406963788300836,
      "loss": 2.3758,
      "step": 16490
    },
    {
      "epoch": 4.596100278551532,
      "grad_norm": 1.5128377676010132,
      "learning_rate": 0.0005404178272980501,
      "loss": 2.1081,
      "step": 16500
    },
    {
      "epoch": 4.598885793871866,
      "grad_norm": 1.2069073915481567,
      "learning_rate": 0.0005401392757660167,
      "loss": 2.1294,
      "step": 16510
    },
    {
      "epoch": 4.6016713091922,
      "grad_norm": 1.9881315231323242,
      "learning_rate": 0.0005398607242339832,
      "loss": 2.3327,
      "step": 16520
    },
    {
      "epoch": 4.604456824512535,
      "grad_norm": 1.4203182458877563,
      "learning_rate": 0.0005395821727019499,
      "loss": 2.2529,
      "step": 16530
    },
    {
      "epoch": 4.607242339832869,
      "grad_norm": 1.8557389974594116,
      "learning_rate": 0.0005393036211699165,
      "loss": 2.3923,
      "step": 16540
    },
    {
      "epoch": 4.610027855153203,
      "grad_norm": 1.8854354619979858,
      "learning_rate": 0.000539025069637883,
      "loss": 2.4045,
      "step": 16550
    },
    {
      "epoch": 4.612813370473537,
      "grad_norm": 1.3739370107650757,
      "learning_rate": 0.0005387465181058497,
      "loss": 2.2498,
      "step": 16560
    },
    {
      "epoch": 4.615598885793872,
      "grad_norm": 1.0533350706100464,
      "learning_rate": 0.0005384679665738162,
      "loss": 2.2642,
      "step": 16570
    },
    {
      "epoch": 4.618384401114206,
      "grad_norm": 1.4463253021240234,
      "learning_rate": 0.0005381894150417827,
      "loss": 2.4523,
      "step": 16580
    },
    {
      "epoch": 4.621169916434541,
      "grad_norm": 1.4398044347763062,
      "learning_rate": 0.0005379108635097492,
      "loss": 2.2924,
      "step": 16590
    },
    {
      "epoch": 4.623955431754875,
      "grad_norm": 1.7498183250427246,
      "learning_rate": 0.0005376323119777159,
      "loss": 2.3326,
      "step": 16600
    },
    {
      "epoch": 4.626740947075209,
      "grad_norm": 1.2686421871185303,
      "learning_rate": 0.0005373537604456825,
      "loss": 2.1246,
      "step": 16610
    },
    {
      "epoch": 4.629526462395543,
      "grad_norm": 1.3826086521148682,
      "learning_rate": 0.000537075208913649,
      "loss": 2.373,
      "step": 16620
    },
    {
      "epoch": 4.632311977715878,
      "grad_norm": 1.6111911535263062,
      "learning_rate": 0.0005367966573816157,
      "loss": 2.2065,
      "step": 16630
    },
    {
      "epoch": 4.635097493036212,
      "grad_norm": 1.827287197113037,
      "learning_rate": 0.0005365181058495822,
      "loss": 2.1392,
      "step": 16640
    },
    {
      "epoch": 4.637883008356546,
      "grad_norm": 1.6120771169662476,
      "learning_rate": 0.0005362395543175488,
      "loss": 2.3125,
      "step": 16650
    },
    {
      "epoch": 4.64066852367688,
      "grad_norm": 1.7510769367218018,
      "learning_rate": 0.0005359610027855153,
      "loss": 2.2381,
      "step": 16660
    },
    {
      "epoch": 4.6434540389972145,
      "grad_norm": 1.398735523223877,
      "learning_rate": 0.000535682451253482,
      "loss": 2.2159,
      "step": 16670
    },
    {
      "epoch": 4.646239554317549,
      "grad_norm": 1.9172762632369995,
      "learning_rate": 0.0005354038997214484,
      "loss": 2.5636,
      "step": 16680
    },
    {
      "epoch": 4.649025069637883,
      "grad_norm": 1.3437453508377075,
      "learning_rate": 0.000535125348189415,
      "loss": 2.2124,
      "step": 16690
    },
    {
      "epoch": 4.651810584958217,
      "grad_norm": 2.118659734725952,
      "learning_rate": 0.0005348467966573817,
      "loss": 2.2796,
      "step": 16700
    },
    {
      "epoch": 4.654596100278551,
      "grad_norm": 1.510791301727295,
      "learning_rate": 0.0005345682451253482,
      "loss": 2.3755,
      "step": 16710
    },
    {
      "epoch": 4.657381615598886,
      "grad_norm": 1.2669790983200073,
      "learning_rate": 0.0005342896935933148,
      "loss": 2.3147,
      "step": 16720
    },
    {
      "epoch": 4.66016713091922,
      "grad_norm": 1.819203495979309,
      "learning_rate": 0.0005340111420612813,
      "loss": 2.1786,
      "step": 16730
    },
    {
      "epoch": 4.662952646239554,
      "grad_norm": 2.1055490970611572,
      "learning_rate": 0.000533732590529248,
      "loss": 2.2636,
      "step": 16740
    },
    {
      "epoch": 4.665738161559888,
      "grad_norm": 1.7905651330947876,
      "learning_rate": 0.0005334540389972145,
      "loss": 2.2588,
      "step": 16750
    },
    {
      "epoch": 4.6685236768802225,
      "grad_norm": 1.8527495861053467,
      "learning_rate": 0.0005331754874651811,
      "loss": 2.3444,
      "step": 16760
    },
    {
      "epoch": 4.671309192200557,
      "grad_norm": 1.5505859851837158,
      "learning_rate": 0.0005328969359331476,
      "loss": 2.2664,
      "step": 16770
    },
    {
      "epoch": 4.674094707520892,
      "grad_norm": 1.5466021299362183,
      "learning_rate": 0.0005326183844011142,
      "loss": 2.2521,
      "step": 16780
    },
    {
      "epoch": 4.676880222841225,
      "grad_norm": 1.1949156522750854,
      "learning_rate": 0.0005323398328690808,
      "loss": 2.2729,
      "step": 16790
    },
    {
      "epoch": 4.67966573816156,
      "grad_norm": 1.2795556783676147,
      "learning_rate": 0.0005320612813370473,
      "loss": 2.0613,
      "step": 16800
    },
    {
      "epoch": 4.6824512534818945,
      "grad_norm": 1.7697001695632935,
      "learning_rate": 0.000531782729805014,
      "loss": 2.24,
      "step": 16810
    },
    {
      "epoch": 4.685236768802229,
      "grad_norm": 1.91380774974823,
      "learning_rate": 0.0005315041782729805,
      "loss": 2.2337,
      "step": 16820
    },
    {
      "epoch": 4.688022284122563,
      "grad_norm": 1.400400996208191,
      "learning_rate": 0.0005312256267409471,
      "loss": 2.2462,
      "step": 16830
    },
    {
      "epoch": 4.690807799442897,
      "grad_norm": 1.5263745784759521,
      "learning_rate": 0.0005309470752089136,
      "loss": 2.2616,
      "step": 16840
    },
    {
      "epoch": 4.693593314763231,
      "grad_norm": 1.8499573469161987,
      "learning_rate": 0.0005306685236768803,
      "loss": 2.3075,
      "step": 16850
    },
    {
      "epoch": 4.696378830083566,
      "grad_norm": 1.077271819114685,
      "learning_rate": 0.0005303899721448469,
      "loss": 2.1378,
      "step": 16860
    },
    {
      "epoch": 4.6991643454039,
      "grad_norm": 1.4132205247879028,
      "learning_rate": 0.0005301114206128133,
      "loss": 2.4971,
      "step": 16870
    },
    {
      "epoch": 4.701949860724234,
      "grad_norm": 1.6449542045593262,
      "learning_rate": 0.0005298328690807799,
      "loss": 2.1074,
      "step": 16880
    },
    {
      "epoch": 4.704735376044568,
      "grad_norm": 2.1623451709747314,
      "learning_rate": 0.0005295543175487465,
      "loss": 2.2117,
      "step": 16890
    },
    {
      "epoch": 4.7075208913649025,
      "grad_norm": 1.5595866441726685,
      "learning_rate": 0.0005292757660167131,
      "loss": 2.2223,
      "step": 16900
    },
    {
      "epoch": 4.710306406685237,
      "grad_norm": 1.508374571800232,
      "learning_rate": 0.0005289972144846796,
      "loss": 2.3684,
      "step": 16910
    },
    {
      "epoch": 4.713091922005571,
      "grad_norm": 2.1505985260009766,
      "learning_rate": 0.0005287186629526463,
      "loss": 2.4337,
      "step": 16920
    },
    {
      "epoch": 4.715877437325905,
      "grad_norm": 1.5136798620224,
      "learning_rate": 0.0005284401114206129,
      "loss": 2.2538,
      "step": 16930
    },
    {
      "epoch": 4.718662952646239,
      "grad_norm": 1.4048041105270386,
      "learning_rate": 0.0005281615598885794,
      "loss": 2.3826,
      "step": 16940
    },
    {
      "epoch": 4.721448467966574,
      "grad_norm": 1.4442652463912964,
      "learning_rate": 0.000527883008356546,
      "loss": 2.169,
      "step": 16950
    },
    {
      "epoch": 4.724233983286908,
      "grad_norm": 1.7887468338012695,
      "learning_rate": 0.0005276044568245126,
      "loss": 2.278,
      "step": 16960
    },
    {
      "epoch": 4.727019498607242,
      "grad_norm": 1.214422345161438,
      "learning_rate": 0.0005273259052924791,
      "loss": 2.274,
      "step": 16970
    },
    {
      "epoch": 4.729805013927576,
      "grad_norm": 1.1686300039291382,
      "learning_rate": 0.0005270473537604456,
      "loss": 2.2311,
      "step": 16980
    },
    {
      "epoch": 4.732590529247911,
      "grad_norm": 1.5581644773483276,
      "learning_rate": 0.0005267688022284123,
      "loss": 2.2891,
      "step": 16990
    },
    {
      "epoch": 4.735376044568245,
      "grad_norm": 1.2387582063674927,
      "learning_rate": 0.0005264902506963788,
      "loss": 2.1691,
      "step": 17000
    },
    {
      "epoch": 4.73816155988858,
      "grad_norm": 1.3435754776000977,
      "learning_rate": 0.0005262116991643454,
      "loss": 2.2612,
      "step": 17010
    },
    {
      "epoch": 4.740947075208914,
      "grad_norm": 1.7086259126663208,
      "learning_rate": 0.000525933147632312,
      "loss": 2.1038,
      "step": 17020
    },
    {
      "epoch": 4.743732590529248,
      "grad_norm": 1.0975199937820435,
      "learning_rate": 0.0005256545961002786,
      "loss": 2.1987,
      "step": 17030
    },
    {
      "epoch": 4.7465181058495824,
      "grad_norm": 1.7963478565216064,
      "learning_rate": 0.0005253760445682452,
      "loss": 2.4115,
      "step": 17040
    },
    {
      "epoch": 4.749303621169917,
      "grad_norm": 1.400903582572937,
      "learning_rate": 0.0005250974930362117,
      "loss": 2.3928,
      "step": 17050
    },
    {
      "epoch": 4.752089136490251,
      "grad_norm": 1.3704239130020142,
      "learning_rate": 0.0005248189415041783,
      "loss": 2.3917,
      "step": 17060
    },
    {
      "epoch": 4.754874651810585,
      "grad_norm": 1.7662814855575562,
      "learning_rate": 0.0005245403899721448,
      "loss": 2.362,
      "step": 17070
    },
    {
      "epoch": 4.757660167130919,
      "grad_norm": 1.3974976539611816,
      "learning_rate": 0.0005242618384401114,
      "loss": 2.1334,
      "step": 17080
    },
    {
      "epoch": 4.7604456824512535,
      "grad_norm": 1.9481440782546997,
      "learning_rate": 0.000523983286908078,
      "loss": 2.2994,
      "step": 17090
    },
    {
      "epoch": 4.763231197771588,
      "grad_norm": 2.1949026584625244,
      "learning_rate": 0.0005237047353760446,
      "loss": 2.3977,
      "step": 17100
    },
    {
      "epoch": 4.766016713091922,
      "grad_norm": 1.120580792427063,
      "learning_rate": 0.0005234261838440112,
      "loss": 2.225,
      "step": 17110
    },
    {
      "epoch": 4.768802228412256,
      "grad_norm": 1.3658796548843384,
      "learning_rate": 0.0005231476323119777,
      "loss": 2.2078,
      "step": 17120
    },
    {
      "epoch": 4.77158774373259,
      "grad_norm": 1.4813350439071655,
      "learning_rate": 0.0005228690807799443,
      "loss": 2.4448,
      "step": 17130
    },
    {
      "epoch": 4.774373259052925,
      "grad_norm": 1.6112667322158813,
      "learning_rate": 0.0005225905292479109,
      "loss": 2.3396,
      "step": 17140
    },
    {
      "epoch": 4.777158774373259,
      "grad_norm": 1.5406101942062378,
      "learning_rate": 0.0005223119777158775,
      "loss": 2.4002,
      "step": 17150
    },
    {
      "epoch": 4.779944289693593,
      "grad_norm": 1.2960151433944702,
      "learning_rate": 0.000522033426183844,
      "loss": 2.3603,
      "step": 17160
    },
    {
      "epoch": 4.782729805013927,
      "grad_norm": 1.7158252000808716,
      "learning_rate": 0.0005217548746518106,
      "loss": 2.4496,
      "step": 17170
    },
    {
      "epoch": 4.7855153203342615,
      "grad_norm": 1.6819685697555542,
      "learning_rate": 0.0005214763231197772,
      "loss": 2.1283,
      "step": 17180
    },
    {
      "epoch": 4.788300835654596,
      "grad_norm": 1.7647844552993774,
      "learning_rate": 0.0005211977715877437,
      "loss": 2.4082,
      "step": 17190
    },
    {
      "epoch": 4.791086350974931,
      "grad_norm": 1.0899072885513306,
      "learning_rate": 0.0005209192200557103,
      "loss": 2.1653,
      "step": 17200
    },
    {
      "epoch": 4.793871866295264,
      "grad_norm": 1.7174124717712402,
      "learning_rate": 0.0005206406685236769,
      "loss": 2.5028,
      "step": 17210
    },
    {
      "epoch": 4.796657381615599,
      "grad_norm": 2.0509450435638428,
      "learning_rate": 0.0005203621169916435,
      "loss": 2.3702,
      "step": 17220
    },
    {
      "epoch": 4.7994428969359335,
      "grad_norm": 2.0655276775360107,
      "learning_rate": 0.00052008356545961,
      "loss": 2.2034,
      "step": 17230
    },
    {
      "epoch": 4.802228412256268,
      "grad_norm": 1.6905007362365723,
      "learning_rate": 0.0005198050139275766,
      "loss": 2.2712,
      "step": 17240
    },
    {
      "epoch": 4.805013927576602,
      "grad_norm": 1.1532684564590454,
      "learning_rate": 0.0005195264623955432,
      "loss": 2.1521,
      "step": 17250
    },
    {
      "epoch": 4.807799442896936,
      "grad_norm": 1.4681037664413452,
      "learning_rate": 0.0005192479108635098,
      "loss": 2.3293,
      "step": 17260
    },
    {
      "epoch": 4.81058495821727,
      "grad_norm": 1.350407600402832,
      "learning_rate": 0.0005189693593314763,
      "loss": 2.3143,
      "step": 17270
    },
    {
      "epoch": 4.813370473537605,
      "grad_norm": 1.1333814859390259,
      "learning_rate": 0.0005186908077994429,
      "loss": 2.1493,
      "step": 17280
    },
    {
      "epoch": 4.816155988857939,
      "grad_norm": 1.852159857749939,
      "learning_rate": 0.0005184122562674095,
      "loss": 2.1682,
      "step": 17290
    },
    {
      "epoch": 4.818941504178273,
      "grad_norm": 1.7846760749816895,
      "learning_rate": 0.000518133704735376,
      "loss": 2.3134,
      "step": 17300
    },
    {
      "epoch": 4.821727019498607,
      "grad_norm": 1.6371181011199951,
      "learning_rate": 0.0005178551532033426,
      "loss": 2.3159,
      "step": 17310
    },
    {
      "epoch": 4.8245125348189415,
      "grad_norm": 1.3620012998580933,
      "learning_rate": 0.0005175766016713092,
      "loss": 2.2717,
      "step": 17320
    },
    {
      "epoch": 4.827298050139276,
      "grad_norm": 1.9625442028045654,
      "learning_rate": 0.0005172980501392758,
      "loss": 2.2932,
      "step": 17330
    },
    {
      "epoch": 4.83008356545961,
      "grad_norm": 1.9197413921356201,
      "learning_rate": 0.0005170194986072424,
      "loss": 2.3641,
      "step": 17340
    },
    {
      "epoch": 4.832869080779944,
      "grad_norm": 1.3924272060394287,
      "learning_rate": 0.000516740947075209,
      "loss": 2.1287,
      "step": 17350
    },
    {
      "epoch": 4.835654596100278,
      "grad_norm": 1.263311505317688,
      "learning_rate": 0.0005164623955431756,
      "loss": 2.3463,
      "step": 17360
    },
    {
      "epoch": 4.838440111420613,
      "grad_norm": 1.4884523153305054,
      "learning_rate": 0.000516183844011142,
      "loss": 2.3691,
      "step": 17370
    },
    {
      "epoch": 4.841225626740947,
      "grad_norm": 1.3292697668075562,
      "learning_rate": 0.0005159052924791086,
      "loss": 2.3675,
      "step": 17380
    },
    {
      "epoch": 4.844011142061281,
      "grad_norm": 1.7660025358200073,
      "learning_rate": 0.0005156267409470752,
      "loss": 2.3231,
      "step": 17390
    },
    {
      "epoch": 4.846796657381615,
      "grad_norm": 1.7802445888519287,
      "learning_rate": 0.0005153481894150418,
      "loss": 2.2859,
      "step": 17400
    },
    {
      "epoch": 4.84958217270195,
      "grad_norm": 1.2365936040878296,
      "learning_rate": 0.0005150696378830083,
      "loss": 2.336,
      "step": 17410
    },
    {
      "epoch": 4.852367688022284,
      "grad_norm": 2.568358898162842,
      "learning_rate": 0.0005147910863509749,
      "loss": 2.2356,
      "step": 17420
    },
    {
      "epoch": 4.855153203342619,
      "grad_norm": 1.6255303621292114,
      "learning_rate": 0.0005145125348189416,
      "loss": 2.2584,
      "step": 17430
    },
    {
      "epoch": 4.857938718662953,
      "grad_norm": 1.2990511655807495,
      "learning_rate": 0.0005142339832869081,
      "loss": 2.0963,
      "step": 17440
    },
    {
      "epoch": 4.860724233983287,
      "grad_norm": 1.484284520149231,
      "learning_rate": 0.0005139554317548747,
      "loss": 2.2364,
      "step": 17450
    },
    {
      "epoch": 4.8635097493036215,
      "grad_norm": 1.622471570968628,
      "learning_rate": 0.0005136768802228412,
      "loss": 2.1604,
      "step": 17460
    },
    {
      "epoch": 4.866295264623956,
      "grad_norm": 1.6503283977508545,
      "learning_rate": 0.0005133983286908078,
      "loss": 2.3216,
      "step": 17470
    },
    {
      "epoch": 4.86908077994429,
      "grad_norm": 1.2006393671035767,
      "learning_rate": 0.0005131197771587743,
      "loss": 2.1331,
      "step": 17480
    },
    {
      "epoch": 4.871866295264624,
      "grad_norm": 2.10343599319458,
      "learning_rate": 0.0005128412256267409,
      "loss": 2.2878,
      "step": 17490
    },
    {
      "epoch": 4.874651810584958,
      "grad_norm": 1.4017956256866455,
      "learning_rate": 0.0005125626740947076,
      "loss": 2.3554,
      "step": 17500
    },
    {
      "epoch": 4.8774373259052926,
      "grad_norm": 1.586753249168396,
      "learning_rate": 0.0005122841225626741,
      "loss": 2.2887,
      "step": 17510
    },
    {
      "epoch": 4.880222841225627,
      "grad_norm": 1.014612078666687,
      "learning_rate": 0.0005120055710306407,
      "loss": 2.1795,
      "step": 17520
    },
    {
      "epoch": 4.883008356545961,
      "grad_norm": 1.3806190490722656,
      "learning_rate": 0.0005117270194986073,
      "loss": 2.3633,
      "step": 17530
    },
    {
      "epoch": 4.885793871866295,
      "grad_norm": 1.3563506603240967,
      "learning_rate": 0.0005114484679665739,
      "loss": 2.3499,
      "step": 17540
    },
    {
      "epoch": 4.888579387186629,
      "grad_norm": 2.2778000831604004,
      "learning_rate": 0.0005111699164345404,
      "loss": 2.2016,
      "step": 17550
    },
    {
      "epoch": 4.891364902506964,
      "grad_norm": 1.734432339668274,
      "learning_rate": 0.0005108913649025069,
      "loss": 2.2971,
      "step": 17560
    },
    {
      "epoch": 4.894150417827298,
      "grad_norm": 1.3789068460464478,
      "learning_rate": 0.0005106128133704735,
      "loss": 2.2384,
      "step": 17570
    },
    {
      "epoch": 4.896935933147632,
      "grad_norm": 2.23869252204895,
      "learning_rate": 0.0005103342618384401,
      "loss": 2.4385,
      "step": 17580
    },
    {
      "epoch": 4.899721448467966,
      "grad_norm": 1.460012435913086,
      "learning_rate": 0.0005100557103064067,
      "loss": 2.3365,
      "step": 17590
    },
    {
      "epoch": 4.9025069637883005,
      "grad_norm": 1.549118161201477,
      "learning_rate": 0.0005097771587743732,
      "loss": 2.3742,
      "step": 17600
    },
    {
      "epoch": 4.905292479108635,
      "grad_norm": 1.688839077949524,
      "learning_rate": 0.0005094986072423399,
      "loss": 2.3302,
      "step": 17610
    },
    {
      "epoch": 4.908077994428969,
      "grad_norm": 1.4364171028137207,
      "learning_rate": 0.0005092200557103064,
      "loss": 2.2801,
      "step": 17620
    },
    {
      "epoch": 4.910863509749303,
      "grad_norm": 1.365646481513977,
      "learning_rate": 0.000508941504178273,
      "loss": 2.2407,
      "step": 17630
    },
    {
      "epoch": 4.913649025069638,
      "grad_norm": 1.7742363214492798,
      "learning_rate": 0.0005086629526462396,
      "loss": 2.159,
      "step": 17640
    },
    {
      "epoch": 4.9164345403899725,
      "grad_norm": 1.4561606645584106,
      "learning_rate": 0.0005083844011142062,
      "loss": 2.3259,
      "step": 17650
    },
    {
      "epoch": 4.919220055710307,
      "grad_norm": 2.2626285552978516,
      "learning_rate": 0.0005081058495821727,
      "loss": 2.3002,
      "step": 17660
    },
    {
      "epoch": 4.922005571030641,
      "grad_norm": 1.4384413957595825,
      "learning_rate": 0.0005078272980501392,
      "loss": 2.1189,
      "step": 17670
    },
    {
      "epoch": 4.924791086350975,
      "grad_norm": 1.5097267627716064,
      "learning_rate": 0.0005075487465181059,
      "loss": 2.2021,
      "step": 17680
    },
    {
      "epoch": 4.927576601671309,
      "grad_norm": 1.7363449335098267,
      "learning_rate": 0.0005072701949860724,
      "loss": 2.2373,
      "step": 17690
    },
    {
      "epoch": 4.930362116991644,
      "grad_norm": 2.172394037246704,
      "learning_rate": 0.000506991643454039,
      "loss": 2.3077,
      "step": 17700
    },
    {
      "epoch": 4.933147632311978,
      "grad_norm": 1.583793044090271,
      "learning_rate": 0.0005067130919220056,
      "loss": 2.3935,
      "step": 17710
    },
    {
      "epoch": 4.935933147632312,
      "grad_norm": 1.411789894104004,
      "learning_rate": 0.0005064345403899722,
      "loss": 2.1928,
      "step": 17720
    },
    {
      "epoch": 4.938718662952646,
      "grad_norm": 1.3762073516845703,
      "learning_rate": 0.0005061559888579387,
      "loss": 2.2147,
      "step": 17730
    },
    {
      "epoch": 4.9415041782729805,
      "grad_norm": 1.589349627494812,
      "learning_rate": 0.0005058774373259053,
      "loss": 2.1761,
      "step": 17740
    },
    {
      "epoch": 4.944289693593315,
      "grad_norm": 1.4325511455535889,
      "learning_rate": 0.000505598885793872,
      "loss": 2.2004,
      "step": 17750
    },
    {
      "epoch": 4.947075208913649,
      "grad_norm": 1.58247709274292,
      "learning_rate": 0.0005053203342618384,
      "loss": 2.3715,
      "step": 17760
    },
    {
      "epoch": 4.949860724233983,
      "grad_norm": 1.4332849979400635,
      "learning_rate": 0.000505041782729805,
      "loss": 2.1615,
      "step": 17770
    },
    {
      "epoch": 4.952646239554317,
      "grad_norm": 1.1200100183486938,
      "learning_rate": 0.0005047632311977715,
      "loss": 2.2886,
      "step": 17780
    },
    {
      "epoch": 4.955431754874652,
      "grad_norm": 1.9460400342941284,
      "learning_rate": 0.0005044846796657382,
      "loss": 2.302,
      "step": 17790
    },
    {
      "epoch": 4.958217270194986,
      "grad_norm": 1.5732263326644897,
      "learning_rate": 0.0005042061281337047,
      "loss": 2.3844,
      "step": 17800
    },
    {
      "epoch": 4.96100278551532,
      "grad_norm": 1.9945358037948608,
      "learning_rate": 0.0005039275766016713,
      "loss": 2.271,
      "step": 17810
    },
    {
      "epoch": 4.963788300835654,
      "grad_norm": 1.6514583826065063,
      "learning_rate": 0.000503649025069638,
      "loss": 2.1845,
      "step": 17820
    },
    {
      "epoch": 4.9665738161559885,
      "grad_norm": 2.2557642459869385,
      "learning_rate": 0.0005033704735376045,
      "loss": 2.2586,
      "step": 17830
    },
    {
      "epoch": 4.969359331476323,
      "grad_norm": 1.3795697689056396,
      "learning_rate": 0.0005030919220055711,
      "loss": 2.4009,
      "step": 17840
    },
    {
      "epoch": 4.972144846796658,
      "grad_norm": 2.323438882827759,
      "learning_rate": 0.0005028133704735375,
      "loss": 2.4503,
      "step": 17850
    },
    {
      "epoch": 4.974930362116992,
      "grad_norm": 2.2982447147369385,
      "learning_rate": 0.0005025348189415042,
      "loss": 2.3359,
      "step": 17860
    },
    {
      "epoch": 4.977715877437326,
      "grad_norm": 1.5544393062591553,
      "learning_rate": 0.0005022562674094707,
      "loss": 2.2331,
      "step": 17870
    },
    {
      "epoch": 4.9805013927576605,
      "grad_norm": 1.513898491859436,
      "learning_rate": 0.0005019777158774373,
      "loss": 2.2968,
      "step": 17880
    },
    {
      "epoch": 4.983286908077995,
      "grad_norm": 1.92188560962677,
      "learning_rate": 0.0005016991643454038,
      "loss": 2.5146,
      "step": 17890
    },
    {
      "epoch": 4.986072423398329,
      "grad_norm": 1.2432576417922974,
      "learning_rate": 0.0005014206128133705,
      "loss": 2.3064,
      "step": 17900
    },
    {
      "epoch": 4.988857938718663,
      "grad_norm": 1.1418895721435547,
      "learning_rate": 0.0005011420612813371,
      "loss": 2.2008,
      "step": 17910
    },
    {
      "epoch": 4.991643454038997,
      "grad_norm": 1.5717062950134277,
      "learning_rate": 0.0005008635097493036,
      "loss": 2.1464,
      "step": 17920
    },
    {
      "epoch": 4.994428969359332,
      "grad_norm": 1.5916938781738281,
      "learning_rate": 0.0005005849582172703,
      "loss": 2.2765,
      "step": 17930
    },
    {
      "epoch": 4.997214484679666,
      "grad_norm": 1.683239221572876,
      "learning_rate": 0.0005003064066852368,
      "loss": 2.1627,
      "step": 17940
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.8535270690917969,
      "learning_rate": 0.0005000278551532034,
      "loss": 2.335,
      "step": 17950
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.2959256172180176,
      "eval_runtime": 6.857,
      "eval_samples_per_second": 465.51,
      "eval_steps_per_second": 58.189,
      "step": 17950
    },
    {
      "epoch": 5.002785515320334,
      "grad_norm": 1.3381078243255615,
      "learning_rate": 0.0004997493036211699,
      "loss": 2.147,
      "step": 17960
    },
    {
      "epoch": 5.005571030640668,
      "grad_norm": 1.6841529607772827,
      "learning_rate": 0.0004994707520891365,
      "loss": 2.164,
      "step": 17970
    },
    {
      "epoch": 5.008356545961003,
      "grad_norm": 1.4474705457687378,
      "learning_rate": 0.0004991922005571031,
      "loss": 2.2153,
      "step": 17980
    },
    {
      "epoch": 5.011142061281337,
      "grad_norm": 1.1544286012649536,
      "learning_rate": 0.0004989136490250696,
      "loss": 2.1641,
      "step": 17990
    },
    {
      "epoch": 5.013927576601671,
      "grad_norm": 1.8832765817642212,
      "learning_rate": 0.0004986350974930362,
      "loss": 2.1156,
      "step": 18000
    },
    {
      "epoch": 5.016713091922005,
      "grad_norm": 1.5829887390136719,
      "learning_rate": 0.0004983565459610028,
      "loss": 2.3064,
      "step": 18010
    },
    {
      "epoch": 5.0194986072423395,
      "grad_norm": 2.2269792556762695,
      "learning_rate": 0.0004980779944289694,
      "loss": 2.1257,
      "step": 18020
    },
    {
      "epoch": 5.022284122562674,
      "grad_norm": 5.041595935821533,
      "learning_rate": 0.000497799442896936,
      "loss": 2.2811,
      "step": 18030
    },
    {
      "epoch": 5.025069637883008,
      "grad_norm": 1.1714308261871338,
      "learning_rate": 0.0004975208913649026,
      "loss": 1.9885,
      "step": 18040
    },
    {
      "epoch": 5.027855153203342,
      "grad_norm": 1.772690773010254,
      "learning_rate": 0.000497242339832869,
      "loss": 2.2502,
      "step": 18050
    },
    {
      "epoch": 5.030640668523677,
      "grad_norm": 2.1245169639587402,
      "learning_rate": 0.0004969637883008356,
      "loss": 2.3893,
      "step": 18060
    },
    {
      "epoch": 5.0334261838440115,
      "grad_norm": 1.54508638381958,
      "learning_rate": 0.0004966852367688022,
      "loss": 2.1578,
      "step": 18070
    },
    {
      "epoch": 5.036211699164346,
      "grad_norm": 1.4725834131240845,
      "learning_rate": 0.0004964066852367688,
      "loss": 2.1237,
      "step": 18080
    },
    {
      "epoch": 5.03899721448468,
      "grad_norm": 2.124929428100586,
      "learning_rate": 0.0004961281337047354,
      "loss": 2.1872,
      "step": 18090
    },
    {
      "epoch": 5.041782729805014,
      "grad_norm": 1.3178839683532715,
      "learning_rate": 0.000495849582172702,
      "loss": 2.1692,
      "step": 18100
    },
    {
      "epoch": 5.044568245125348,
      "grad_norm": 1.3653144836425781,
      "learning_rate": 0.0004955710306406686,
      "loss": 2.2416,
      "step": 18110
    },
    {
      "epoch": 5.047353760445683,
      "grad_norm": 1.7470675706863403,
      "learning_rate": 0.0004952924791086351,
      "loss": 2.1407,
      "step": 18120
    },
    {
      "epoch": 5.050139275766017,
      "grad_norm": 1.6896196603775024,
      "learning_rate": 0.0004950139275766017,
      "loss": 2.3599,
      "step": 18130
    },
    {
      "epoch": 5.052924791086351,
      "grad_norm": 1.6077886819839478,
      "learning_rate": 0.0004947353760445683,
      "loss": 2.2207,
      "step": 18140
    },
    {
      "epoch": 5.055710306406685,
      "grad_norm": 2.1643779277801514,
      "learning_rate": 0.0004944568245125349,
      "loss": 2.2831,
      "step": 18150
    },
    {
      "epoch": 5.0584958217270195,
      "grad_norm": 1.5503252744674683,
      "learning_rate": 0.0004941782729805013,
      "loss": 2.1822,
      "step": 18160
    },
    {
      "epoch": 5.061281337047354,
      "grad_norm": 1.9334731101989746,
      "learning_rate": 0.0004938997214484679,
      "loss": 2.3928,
      "step": 18170
    },
    {
      "epoch": 5.064066852367688,
      "grad_norm": 1.4174041748046875,
      "learning_rate": 0.0004936211699164346,
      "loss": 2.0814,
      "step": 18180
    },
    {
      "epoch": 5.066852367688022,
      "grad_norm": 1.740034818649292,
      "learning_rate": 0.0004933426183844011,
      "loss": 2.2106,
      "step": 18190
    },
    {
      "epoch": 5.069637883008356,
      "grad_norm": 1.962109923362732,
      "learning_rate": 0.0004930640668523677,
      "loss": 2.4771,
      "step": 18200
    },
    {
      "epoch": 5.072423398328691,
      "grad_norm": 1.7054177522659302,
      "learning_rate": 0.0004927855153203343,
      "loss": 2.2219,
      "step": 18210
    },
    {
      "epoch": 5.075208913649025,
      "grad_norm": 1.6300880908966064,
      "learning_rate": 0.0004925069637883009,
      "loss": 2.2686,
      "step": 18220
    },
    {
      "epoch": 5.077994428969359,
      "grad_norm": 1.3231334686279297,
      "learning_rate": 0.0004922284122562674,
      "loss": 2.1241,
      "step": 18230
    },
    {
      "epoch": 5.080779944289693,
      "grad_norm": 2.038482189178467,
      "learning_rate": 0.000491949860724234,
      "loss": 2.2619,
      "step": 18240
    },
    {
      "epoch": 5.0835654596100275,
      "grad_norm": 1.6999772787094116,
      "learning_rate": 0.0004916713091922005,
      "loss": 2.1765,
      "step": 18250
    },
    {
      "epoch": 5.086350974930362,
      "grad_norm": 1.2466356754302979,
      "learning_rate": 0.0004913927576601671,
      "loss": 2.0187,
      "step": 18260
    },
    {
      "epoch": 5.089136490250697,
      "grad_norm": 1.2844958305358887,
      "learning_rate": 0.0004911142061281337,
      "loss": 2.1313,
      "step": 18270
    },
    {
      "epoch": 5.091922005571031,
      "grad_norm": 1.9945650100708008,
      "learning_rate": 0.0004908356545961003,
      "loss": 2.3731,
      "step": 18280
    },
    {
      "epoch": 5.094707520891365,
      "grad_norm": 1.2252507209777832,
      "learning_rate": 0.0004905571030640669,
      "loss": 2.0691,
      "step": 18290
    },
    {
      "epoch": 5.0974930362116995,
      "grad_norm": 1.389404058456421,
      "learning_rate": 0.0004902785515320334,
      "loss": 2.0617,
      "step": 18300
    },
    {
      "epoch": 5.100278551532034,
      "grad_norm": 1.7327152490615845,
      "learning_rate": 0.00049,
      "loss": 2.2948,
      "step": 18310
    },
    {
      "epoch": 5.103064066852368,
      "grad_norm": 1.5096731185913086,
      "learning_rate": 0.0004897214484679666,
      "loss": 2.2388,
      "step": 18320
    },
    {
      "epoch": 5.105849582172702,
      "grad_norm": 1.5156420469284058,
      "learning_rate": 0.0004894428969359332,
      "loss": 2.1706,
      "step": 18330
    },
    {
      "epoch": 5.108635097493036,
      "grad_norm": 1.9509059190750122,
      "learning_rate": 0.0004891643454038998,
      "loss": 2.0714,
      "step": 18340
    },
    {
      "epoch": 5.111420612813371,
      "grad_norm": 1.360628604888916,
      "learning_rate": 0.0004888857938718663,
      "loss": 2.3016,
      "step": 18350
    },
    {
      "epoch": 5.114206128133705,
      "grad_norm": 1.4822907447814941,
      "learning_rate": 0.0004886072423398329,
      "loss": 2.1399,
      "step": 18360
    },
    {
      "epoch": 5.116991643454039,
      "grad_norm": 1.5111901760101318,
      "learning_rate": 0.0004883286908077994,
      "loss": 2.1384,
      "step": 18370
    },
    {
      "epoch": 5.119777158774373,
      "grad_norm": 2.136406421661377,
      "learning_rate": 0.000488050139275766,
      "loss": 2.2623,
      "step": 18380
    },
    {
      "epoch": 5.1225626740947074,
      "grad_norm": 1.736799716949463,
      "learning_rate": 0.0004877715877437326,
      "loss": 2.2429,
      "step": 18390
    },
    {
      "epoch": 5.125348189415042,
      "grad_norm": 1.7484387159347534,
      "learning_rate": 0.00048749303621169914,
      "loss": 2.2866,
      "step": 18400
    },
    {
      "epoch": 5.128133704735376,
      "grad_norm": 1.6712507009506226,
      "learning_rate": 0.00048721448467966573,
      "loss": 2.2244,
      "step": 18410
    },
    {
      "epoch": 5.13091922005571,
      "grad_norm": 1.5160573720932007,
      "learning_rate": 0.0004869359331476323,
      "loss": 2.2417,
      "step": 18420
    },
    {
      "epoch": 5.133704735376044,
      "grad_norm": 1.9271525144577026,
      "learning_rate": 0.0004866573816155989,
      "loss": 2.1863,
      "step": 18430
    },
    {
      "epoch": 5.1364902506963785,
      "grad_norm": 1.9178043603897095,
      "learning_rate": 0.0004863788300835655,
      "loss": 2.1524,
      "step": 18440
    },
    {
      "epoch": 5.139275766016713,
      "grad_norm": 1.7071527242660522,
      "learning_rate": 0.00048610027855153204,
      "loss": 2.4437,
      "step": 18450
    },
    {
      "epoch": 5.142061281337047,
      "grad_norm": 1.9780274629592896,
      "learning_rate": 0.00048582172701949864,
      "loss": 2.1954,
      "step": 18460
    },
    {
      "epoch": 5.144846796657381,
      "grad_norm": 1.8924436569213867,
      "learning_rate": 0.00048554317548746517,
      "loss": 2.1977,
      "step": 18470
    },
    {
      "epoch": 5.147632311977716,
      "grad_norm": 1.792840838432312,
      "learning_rate": 0.00048526462395543176,
      "loss": 2.1179,
      "step": 18480
    },
    {
      "epoch": 5.1504178272980505,
      "grad_norm": 1.4243800640106201,
      "learning_rate": 0.0004849860724233983,
      "loss": 2.1303,
      "step": 18490
    },
    {
      "epoch": 5.153203342618385,
      "grad_norm": 1.9199374914169312,
      "learning_rate": 0.00048470752089136495,
      "loss": 2.1154,
      "step": 18500
    },
    {
      "epoch": 5.155988857938719,
      "grad_norm": 1.594406008720398,
      "learning_rate": 0.0004844289693593315,
      "loss": 2.2026,
      "step": 18510
    },
    {
      "epoch": 5.158774373259053,
      "grad_norm": 1.3985140323638916,
      "learning_rate": 0.0004841504178272981,
      "loss": 2.2709,
      "step": 18520
    },
    {
      "epoch": 5.161559888579387,
      "grad_norm": 1.4555511474609375,
      "learning_rate": 0.00048387186629526467,
      "loss": 2.1863,
      "step": 18530
    },
    {
      "epoch": 5.164345403899722,
      "grad_norm": 1.5439140796661377,
      "learning_rate": 0.0004835933147632312,
      "loss": 2.3231,
      "step": 18540
    },
    {
      "epoch": 5.167130919220056,
      "grad_norm": 1.9832366704940796,
      "learning_rate": 0.0004833147632311978,
      "loss": 2.1672,
      "step": 18550
    },
    {
      "epoch": 5.16991643454039,
      "grad_norm": 1.6448156833648682,
      "learning_rate": 0.00048303621169916433,
      "loss": 2.2112,
      "step": 18560
    },
    {
      "epoch": 5.172701949860724,
      "grad_norm": 1.404781699180603,
      "learning_rate": 0.0004827576601671309,
      "loss": 2.1671,
      "step": 18570
    },
    {
      "epoch": 5.1754874651810585,
      "grad_norm": 1.4265761375427246,
      "learning_rate": 0.0004824791086350975,
      "loss": 2.1676,
      "step": 18580
    },
    {
      "epoch": 5.178272980501393,
      "grad_norm": 1.3472068309783936,
      "learning_rate": 0.0004822005571030641,
      "loss": 2.2449,
      "step": 18590
    },
    {
      "epoch": 5.181058495821727,
      "grad_norm": 1.2942640781402588,
      "learning_rate": 0.00048192200557103064,
      "loss": 2.128,
      "step": 18600
    },
    {
      "epoch": 5.183844011142061,
      "grad_norm": 3.2851784229278564,
      "learning_rate": 0.00048164345403899723,
      "loss": 2.2401,
      "step": 18610
    },
    {
      "epoch": 5.186629526462395,
      "grad_norm": 1.7680189609527588,
      "learning_rate": 0.0004813649025069638,
      "loss": 2.3624,
      "step": 18620
    },
    {
      "epoch": 5.18941504178273,
      "grad_norm": 1.3827248811721802,
      "learning_rate": 0.00048108635097493036,
      "loss": 1.971,
      "step": 18630
    },
    {
      "epoch": 5.192200557103064,
      "grad_norm": 1.5169317722320557,
      "learning_rate": 0.00048080779944289695,
      "loss": 1.9981,
      "step": 18640
    },
    {
      "epoch": 5.194986072423398,
      "grad_norm": 1.9806874990463257,
      "learning_rate": 0.0004805292479108635,
      "loss": 2.3973,
      "step": 18650
    },
    {
      "epoch": 5.197771587743732,
      "grad_norm": 1.6296511888504028,
      "learning_rate": 0.00048025069637883013,
      "loss": 2.0782,
      "step": 18660
    },
    {
      "epoch": 5.2005571030640665,
      "grad_norm": 2.548590898513794,
      "learning_rate": 0.00047997214484679667,
      "loss": 2.2101,
      "step": 18670
    },
    {
      "epoch": 5.203342618384401,
      "grad_norm": 1.6630667448043823,
      "learning_rate": 0.00047969359331476326,
      "loss": 2.257,
      "step": 18680
    },
    {
      "epoch": 5.206128133704736,
      "grad_norm": 1.3564822673797607,
      "learning_rate": 0.0004794150417827298,
      "loss": 2.3564,
      "step": 18690
    },
    {
      "epoch": 5.20891364902507,
      "grad_norm": 1.694446086883545,
      "learning_rate": 0.0004791364902506964,
      "loss": 2.2985,
      "step": 18700
    },
    {
      "epoch": 5.211699164345404,
      "grad_norm": 1.3766119480133057,
      "learning_rate": 0.000478857938718663,
      "loss": 2.3124,
      "step": 18710
    },
    {
      "epoch": 5.2144846796657385,
      "grad_norm": 1.8141539096832275,
      "learning_rate": 0.0004785793871866295,
      "loss": 2.172,
      "step": 18720
    },
    {
      "epoch": 5.217270194986073,
      "grad_norm": 1.7221976518630981,
      "learning_rate": 0.0004783008356545961,
      "loss": 2.2138,
      "step": 18730
    },
    {
      "epoch": 5.220055710306407,
      "grad_norm": 1.371199369430542,
      "learning_rate": 0.0004780222841225627,
      "loss": 2.0917,
      "step": 18740
    },
    {
      "epoch": 5.222841225626741,
      "grad_norm": 1.8473366498947144,
      "learning_rate": 0.0004777437325905293,
      "loss": 2.1421,
      "step": 18750
    },
    {
      "epoch": 5.225626740947075,
      "grad_norm": 1.967871904373169,
      "learning_rate": 0.00047746518105849583,
      "loss": 2.3454,
      "step": 18760
    },
    {
      "epoch": 5.22841225626741,
      "grad_norm": 1.5745099782943726,
      "learning_rate": 0.0004771866295264624,
      "loss": 2.2447,
      "step": 18770
    },
    {
      "epoch": 5.231197771587744,
      "grad_norm": 1.3939107656478882,
      "learning_rate": 0.00047690807799442896,
      "loss": 2.3531,
      "step": 18780
    },
    {
      "epoch": 5.233983286908078,
      "grad_norm": 1.6705759763717651,
      "learning_rate": 0.00047662952646239555,
      "loss": 2.3589,
      "step": 18790
    },
    {
      "epoch": 5.236768802228412,
      "grad_norm": 1.2868919372558594,
      "learning_rate": 0.00047635097493036214,
      "loss": 2.1668,
      "step": 18800
    },
    {
      "epoch": 5.2395543175487465,
      "grad_norm": 2.327629327774048,
      "learning_rate": 0.0004760724233983287,
      "loss": 2.3479,
      "step": 18810
    },
    {
      "epoch": 5.242339832869081,
      "grad_norm": 1.3490031957626343,
      "learning_rate": 0.00047579387186629527,
      "loss": 2.2266,
      "step": 18820
    },
    {
      "epoch": 5.245125348189415,
      "grad_norm": 1.7897921800613403,
      "learning_rate": 0.00047551532033426186,
      "loss": 2.2859,
      "step": 18830
    },
    {
      "epoch": 5.247910863509749,
      "grad_norm": 1.3070383071899414,
      "learning_rate": 0.00047523676880222845,
      "loss": 2.2487,
      "step": 18840
    },
    {
      "epoch": 5.250696378830083,
      "grad_norm": 1.7119463682174683,
      "learning_rate": 0.000474958217270195,
      "loss": 2.1902,
      "step": 18850
    },
    {
      "epoch": 5.2534818941504176,
      "grad_norm": 1.8666547536849976,
      "learning_rate": 0.0004746796657381616,
      "loss": 2.2844,
      "step": 18860
    },
    {
      "epoch": 5.256267409470752,
      "grad_norm": 2.2174298763275146,
      "learning_rate": 0.0004744011142061281,
      "loss": 2.2467,
      "step": 18870
    },
    {
      "epoch": 5.259052924791086,
      "grad_norm": 1.6630616188049316,
      "learning_rate": 0.0004741225626740947,
      "loss": 2.2357,
      "step": 18880
    },
    {
      "epoch": 5.26183844011142,
      "grad_norm": 1.827979564666748,
      "learning_rate": 0.00047384401114206124,
      "loss": 2.3063,
      "step": 18890
    },
    {
      "epoch": 5.264623955431755,
      "grad_norm": 1.6833958625793457,
      "learning_rate": 0.00047356545961002784,
      "loss": 2.1521,
      "step": 18900
    },
    {
      "epoch": 5.2674094707520895,
      "grad_norm": 1.1358453035354614,
      "learning_rate": 0.0004732869080779945,
      "loss": 2.149,
      "step": 18910
    },
    {
      "epoch": 5.270194986072424,
      "grad_norm": 1.8818775415420532,
      "learning_rate": 0.000473008356545961,
      "loss": 2.0342,
      "step": 18920
    },
    {
      "epoch": 5.272980501392758,
      "grad_norm": 1.4332035779953003,
      "learning_rate": 0.0004727298050139276,
      "loss": 2.1347,
      "step": 18930
    },
    {
      "epoch": 5.275766016713092,
      "grad_norm": 1.6057214736938477,
      "learning_rate": 0.00047245125348189415,
      "loss": 2.3017,
      "step": 18940
    },
    {
      "epoch": 5.278551532033426,
      "grad_norm": 1.7648417949676514,
      "learning_rate": 0.00047217270194986074,
      "loss": 2.0917,
      "step": 18950
    },
    {
      "epoch": 5.281337047353761,
      "grad_norm": 1.9228756427764893,
      "learning_rate": 0.0004718941504178273,
      "loss": 2.1228,
      "step": 18960
    },
    {
      "epoch": 5.284122562674095,
      "grad_norm": 1.7293620109558105,
      "learning_rate": 0.00047161559888579387,
      "loss": 2.3368,
      "step": 18970
    },
    {
      "epoch": 5.286908077994429,
      "grad_norm": 1.294724941253662,
      "learning_rate": 0.0004713370473537604,
      "loss": 2.2432,
      "step": 18980
    },
    {
      "epoch": 5.289693593314763,
      "grad_norm": 1.8346599340438843,
      "learning_rate": 0.00047105849582172705,
      "loss": 2.2046,
      "step": 18990
    },
    {
      "epoch": 5.2924791086350975,
      "grad_norm": 1.448853850364685,
      "learning_rate": 0.00047077994428969364,
      "loss": 2.3663,
      "step": 19000
    },
    {
      "epoch": 5.295264623955432,
      "grad_norm": 1.3374208211898804,
      "learning_rate": 0.0004705013927576602,
      "loss": 2.3717,
      "step": 19010
    },
    {
      "epoch": 5.298050139275766,
      "grad_norm": 1.2829504013061523,
      "learning_rate": 0.00047022284122562677,
      "loss": 2.2273,
      "step": 19020
    },
    {
      "epoch": 5.3008356545961,
      "grad_norm": 1.2362042665481567,
      "learning_rate": 0.0004699442896935933,
      "loss": 2.2423,
      "step": 19030
    },
    {
      "epoch": 5.303621169916434,
      "grad_norm": 1.5583637952804565,
      "learning_rate": 0.0004696657381615599,
      "loss": 2.302,
      "step": 19040
    },
    {
      "epoch": 5.306406685236769,
      "grad_norm": 1.0178786516189575,
      "learning_rate": 0.00046938718662952643,
      "loss": 2.0591,
      "step": 19050
    },
    {
      "epoch": 5.309192200557103,
      "grad_norm": 1.5955305099487305,
      "learning_rate": 0.000469108635097493,
      "loss": 2.1199,
      "step": 19060
    },
    {
      "epoch": 5.311977715877437,
      "grad_norm": 1.4660289287567139,
      "learning_rate": 0.00046883008356545967,
      "loss": 2.2156,
      "step": 19070
    },
    {
      "epoch": 5.314763231197771,
      "grad_norm": 1.4056878089904785,
      "learning_rate": 0.0004685515320334262,
      "loss": 2.3853,
      "step": 19080
    },
    {
      "epoch": 5.3175487465181055,
      "grad_norm": 1.246835708618164,
      "learning_rate": 0.0004682729805013928,
      "loss": 2.229,
      "step": 19090
    },
    {
      "epoch": 5.32033426183844,
      "grad_norm": 1.4602806568145752,
      "learning_rate": 0.00046799442896935933,
      "loss": 2.0817,
      "step": 19100
    },
    {
      "epoch": 5.323119777158775,
      "grad_norm": 1.8224445581436157,
      "learning_rate": 0.0004677158774373259,
      "loss": 2.4433,
      "step": 19110
    },
    {
      "epoch": 5.325905292479109,
      "grad_norm": 1.5615023374557495,
      "learning_rate": 0.00046743732590529246,
      "loss": 2.1864,
      "step": 19120
    },
    {
      "epoch": 5.328690807799443,
      "grad_norm": 1.5483372211456299,
      "learning_rate": 0.00046715877437325905,
      "loss": 2.3909,
      "step": 19130
    },
    {
      "epoch": 5.3314763231197775,
      "grad_norm": 1.8230384588241577,
      "learning_rate": 0.0004668802228412256,
      "loss": 2.2955,
      "step": 19140
    },
    {
      "epoch": 5.334261838440112,
      "grad_norm": 1.861721396446228,
      "learning_rate": 0.00046660167130919224,
      "loss": 2.3298,
      "step": 19150
    },
    {
      "epoch": 5.337047353760446,
      "grad_norm": 1.447053074836731,
      "learning_rate": 0.0004663231197771588,
      "loss": 2.2201,
      "step": 19160
    },
    {
      "epoch": 5.33983286908078,
      "grad_norm": 1.6939719915390015,
      "learning_rate": 0.00046604456824512536,
      "loss": 2.3094,
      "step": 19170
    },
    {
      "epoch": 5.342618384401114,
      "grad_norm": 1.7479568719863892,
      "learning_rate": 0.00046576601671309196,
      "loss": 2.1424,
      "step": 19180
    },
    {
      "epoch": 5.345403899721449,
      "grad_norm": 1.5526976585388184,
      "learning_rate": 0.0004654874651810585,
      "loss": 2.0843,
      "step": 19190
    },
    {
      "epoch": 5.348189415041783,
      "grad_norm": 1.446273684501648,
      "learning_rate": 0.0004652089136490251,
      "loss": 2.2729,
      "step": 19200
    },
    {
      "epoch": 5.350974930362117,
      "grad_norm": 1.6502904891967773,
      "learning_rate": 0.0004649303621169916,
      "loss": 2.321,
      "step": 19210
    },
    {
      "epoch": 5.353760445682451,
      "grad_norm": 1.6873090267181396,
      "learning_rate": 0.0004646518105849582,
      "loss": 2.2219,
      "step": 19220
    },
    {
      "epoch": 5.3565459610027855,
      "grad_norm": 2.128873586654663,
      "learning_rate": 0.0004643732590529248,
      "loss": 2.2284,
      "step": 19230
    },
    {
      "epoch": 5.35933147632312,
      "grad_norm": 1.442097783088684,
      "learning_rate": 0.0004640947075208914,
      "loss": 2.2755,
      "step": 19240
    },
    {
      "epoch": 5.362116991643454,
      "grad_norm": 1.5712956190109253,
      "learning_rate": 0.00046381615598885793,
      "loss": 2.2048,
      "step": 19250
    },
    {
      "epoch": 5.364902506963788,
      "grad_norm": 1.66533625125885,
      "learning_rate": 0.0004635376044568245,
      "loss": 2.3051,
      "step": 19260
    },
    {
      "epoch": 5.367688022284122,
      "grad_norm": 1.4305816888809204,
      "learning_rate": 0.0004632590529247911,
      "loss": 2.1586,
      "step": 19270
    },
    {
      "epoch": 5.370473537604457,
      "grad_norm": 1.396772027015686,
      "learning_rate": 0.00046298050139275765,
      "loss": 2.332,
      "step": 19280
    },
    {
      "epoch": 5.373259052924791,
      "grad_norm": 2.8832848072052,
      "learning_rate": 0.00046270194986072424,
      "loss": 2.242,
      "step": 19290
    },
    {
      "epoch": 5.376044568245125,
      "grad_norm": 1.5391851663589478,
      "learning_rate": 0.0004624233983286908,
      "loss": 2.1159,
      "step": 19300
    },
    {
      "epoch": 5.378830083565459,
      "grad_norm": 1.6187878847122192,
      "learning_rate": 0.0004621448467966574,
      "loss": 2.2566,
      "step": 19310
    },
    {
      "epoch": 5.381615598885794,
      "grad_norm": 1.4830570220947266,
      "learning_rate": 0.00046186629526462396,
      "loss": 2.1003,
      "step": 19320
    },
    {
      "epoch": 5.3844011142061285,
      "grad_norm": 1.7766599655151367,
      "learning_rate": 0.00046158774373259055,
      "loss": 2.279,
      "step": 19330
    },
    {
      "epoch": 5.387186629526463,
      "grad_norm": 1.4936529397964478,
      "learning_rate": 0.0004613091922005571,
      "loss": 2.1352,
      "step": 19340
    },
    {
      "epoch": 5.389972144846797,
      "grad_norm": 1.3977837562561035,
      "learning_rate": 0.0004610306406685237,
      "loss": 2.2039,
      "step": 19350
    },
    {
      "epoch": 5.392757660167131,
      "grad_norm": 1.5085147619247437,
      "learning_rate": 0.00046075208913649027,
      "loss": 2.2568,
      "step": 19360
    },
    {
      "epoch": 5.395543175487465,
      "grad_norm": 2.0036160945892334,
      "learning_rate": 0.0004604735376044568,
      "loss": 2.183,
      "step": 19370
    },
    {
      "epoch": 5.3983286908078,
      "grad_norm": 1.336430311203003,
      "learning_rate": 0.0004601949860724234,
      "loss": 2.2821,
      "step": 19380
    },
    {
      "epoch": 5.401114206128134,
      "grad_norm": 1.6139357089996338,
      "learning_rate": 0.00045991643454039,
      "loss": 2.2233,
      "step": 19390
    },
    {
      "epoch": 5.403899721448468,
      "grad_norm": 1.1692321300506592,
      "learning_rate": 0.0004596378830083566,
      "loss": 2.1172,
      "step": 19400
    },
    {
      "epoch": 5.406685236768802,
      "grad_norm": 2.428711175918579,
      "learning_rate": 0.0004593593314763231,
      "loss": 2.2718,
      "step": 19410
    },
    {
      "epoch": 5.4094707520891365,
      "grad_norm": 1.746713638305664,
      "learning_rate": 0.0004590807799442897,
      "loss": 2.0901,
      "step": 19420
    },
    {
      "epoch": 5.412256267409471,
      "grad_norm": 1.384931206703186,
      "learning_rate": 0.00045880222841225625,
      "loss": 2.2257,
      "step": 19430
    },
    {
      "epoch": 5.415041782729805,
      "grad_norm": 1.3563412427902222,
      "learning_rate": 0.00045852367688022284,
      "loss": 2.1929,
      "step": 19440
    },
    {
      "epoch": 5.417827298050139,
      "grad_norm": 1.182746171951294,
      "learning_rate": 0.00045824512534818943,
      "loss": 2.1621,
      "step": 19450
    },
    {
      "epoch": 5.420612813370473,
      "grad_norm": 1.3382043838500977,
      "learning_rate": 0.00045796657381615597,
      "loss": 2.2128,
      "step": 19460
    },
    {
      "epoch": 5.423398328690808,
      "grad_norm": 1.5881357192993164,
      "learning_rate": 0.0004576880222841226,
      "loss": 2.2199,
      "step": 19470
    },
    {
      "epoch": 5.426183844011142,
      "grad_norm": 1.5202885866165161,
      "learning_rate": 0.00045740947075208915,
      "loss": 2.1329,
      "step": 19480
    },
    {
      "epoch": 5.428969359331476,
      "grad_norm": 1.6654052734375,
      "learning_rate": 0.00045713091922005574,
      "loss": 2.1032,
      "step": 19490
    },
    {
      "epoch": 5.43175487465181,
      "grad_norm": 2.1992063522338867,
      "learning_rate": 0.0004568523676880223,
      "loss": 2.2111,
      "step": 19500
    },
    {
      "epoch": 5.4345403899721445,
      "grad_norm": 1.6233025789260864,
      "learning_rate": 0.00045657381615598887,
      "loss": 2.3111,
      "step": 19510
    },
    {
      "epoch": 5.437325905292479,
      "grad_norm": 1.539028525352478,
      "learning_rate": 0.0004562952646239554,
      "loss": 2.0539,
      "step": 19520
    },
    {
      "epoch": 5.440111420612814,
      "grad_norm": 1.5918506383895874,
      "learning_rate": 0.000456016713091922,
      "loss": 2.2256,
      "step": 19530
    },
    {
      "epoch": 5.442896935933147,
      "grad_norm": 1.7789249420166016,
      "learning_rate": 0.0004557381615598886,
      "loss": 2.2377,
      "step": 19540
    },
    {
      "epoch": 5.445682451253482,
      "grad_norm": 1.326906681060791,
      "learning_rate": 0.0004554596100278552,
      "loss": 2.3134,
      "step": 19550
    },
    {
      "epoch": 5.4484679665738165,
      "grad_norm": 0.9380319714546204,
      "learning_rate": 0.00045518105849582177,
      "loss": 2.1613,
      "step": 19560
    },
    {
      "epoch": 5.451253481894151,
      "grad_norm": 1.7879674434661865,
      "learning_rate": 0.0004549025069637883,
      "loss": 2.2865,
      "step": 19570
    },
    {
      "epoch": 5.454038997214485,
      "grad_norm": 1.2514142990112305,
      "learning_rate": 0.0004546239554317549,
      "loss": 2.2315,
      "step": 19580
    },
    {
      "epoch": 5.456824512534819,
      "grad_norm": 1.3225533962249756,
      "learning_rate": 0.00045434540389972144,
      "loss": 2.1594,
      "step": 19590
    },
    {
      "epoch": 5.459610027855153,
      "grad_norm": 2.0378260612487793,
      "learning_rate": 0.00045406685236768803,
      "loss": 2.1787,
      "step": 19600
    },
    {
      "epoch": 5.462395543175488,
      "grad_norm": 1.850873589515686,
      "learning_rate": 0.00045378830083565456,
      "loss": 2.337,
      "step": 19610
    },
    {
      "epoch": 5.465181058495822,
      "grad_norm": 1.485687494277954,
      "learning_rate": 0.00045350974930362116,
      "loss": 2.2015,
      "step": 19620
    },
    {
      "epoch": 5.467966573816156,
      "grad_norm": 1.2345770597457886,
      "learning_rate": 0.0004532311977715878,
      "loss": 2.1615,
      "step": 19630
    },
    {
      "epoch": 5.47075208913649,
      "grad_norm": 2.652094841003418,
      "learning_rate": 0.00045295264623955434,
      "loss": 2.2614,
      "step": 19640
    },
    {
      "epoch": 5.4735376044568245,
      "grad_norm": 1.6690058708190918,
      "learning_rate": 0.00045267409470752093,
      "loss": 2.3051,
      "step": 19650
    },
    {
      "epoch": 5.476323119777159,
      "grad_norm": 1.374998927116394,
      "learning_rate": 0.00045239554317548747,
      "loss": 2.2962,
      "step": 19660
    },
    {
      "epoch": 5.479108635097493,
      "grad_norm": 1.338588833808899,
      "learning_rate": 0.00045211699164345406,
      "loss": 2.2835,
      "step": 19670
    },
    {
      "epoch": 5.481894150417827,
      "grad_norm": 1.4047173261642456,
      "learning_rate": 0.0004518384401114206,
      "loss": 2.274,
      "step": 19680
    },
    {
      "epoch": 5.484679665738161,
      "grad_norm": 1.806997537612915,
      "learning_rate": 0.0004515598885793872,
      "loss": 2.0502,
      "step": 19690
    },
    {
      "epoch": 5.487465181058496,
      "grad_norm": 1.639145851135254,
      "learning_rate": 0.0004512813370473537,
      "loss": 2.4364,
      "step": 19700
    },
    {
      "epoch": 5.49025069637883,
      "grad_norm": 1.6400398015975952,
      "learning_rate": 0.00045100278551532037,
      "loss": 2.2849,
      "step": 19710
    },
    {
      "epoch": 5.493036211699164,
      "grad_norm": 1.5354911088943481,
      "learning_rate": 0.00045072423398328696,
      "loss": 2.4197,
      "step": 19720
    },
    {
      "epoch": 5.495821727019498,
      "grad_norm": 1.22732412815094,
      "learning_rate": 0.0004504456824512535,
      "loss": 2.1164,
      "step": 19730
    },
    {
      "epoch": 5.498607242339833,
      "grad_norm": 1.4744718074798584,
      "learning_rate": 0.0004501671309192201,
      "loss": 2.0963,
      "step": 19740
    },
    {
      "epoch": 5.501392757660167,
      "grad_norm": 1.6476502418518066,
      "learning_rate": 0.0004498885793871866,
      "loss": 2.299,
      "step": 19750
    },
    {
      "epoch": 5.504178272980502,
      "grad_norm": 1.711768627166748,
      "learning_rate": 0.0004496100278551532,
      "loss": 2.3956,
      "step": 19760
    },
    {
      "epoch": 5.506963788300836,
      "grad_norm": 1.3191171884536743,
      "learning_rate": 0.00044933147632311975,
      "loss": 2.2498,
      "step": 19770
    },
    {
      "epoch": 5.50974930362117,
      "grad_norm": 1.9242795705795288,
      "learning_rate": 0.00044905292479108634,
      "loss": 2.1823,
      "step": 19780
    },
    {
      "epoch": 5.512534818941504,
      "grad_norm": 1.4327095746994019,
      "learning_rate": 0.00044877437325905293,
      "loss": 2.2336,
      "step": 19790
    },
    {
      "epoch": 5.515320334261839,
      "grad_norm": 1.9810035228729248,
      "learning_rate": 0.0004484958217270195,
      "loss": 2.1455,
      "step": 19800
    },
    {
      "epoch": 5.518105849582173,
      "grad_norm": 1.3234001398086548,
      "learning_rate": 0.0004482172701949861,
      "loss": 2.2408,
      "step": 19810
    },
    {
      "epoch": 5.520891364902507,
      "grad_norm": 1.389622449874878,
      "learning_rate": 0.00044793871866295265,
      "loss": 2.2447,
      "step": 19820
    },
    {
      "epoch": 5.523676880222841,
      "grad_norm": 1.4304497241973877,
      "learning_rate": 0.00044766016713091925,
      "loss": 2.3176,
      "step": 19830
    },
    {
      "epoch": 5.5264623955431755,
      "grad_norm": 1.522248387336731,
      "learning_rate": 0.0004473816155988858,
      "loss": 2.2691,
      "step": 19840
    },
    {
      "epoch": 5.52924791086351,
      "grad_norm": 1.7766368389129639,
      "learning_rate": 0.0004471030640668524,
      "loss": 2.4689,
      "step": 19850
    },
    {
      "epoch": 5.532033426183844,
      "grad_norm": 1.299258828163147,
      "learning_rate": 0.0004468245125348189,
      "loss": 2.2079,
      "step": 19860
    },
    {
      "epoch": 5.534818941504178,
      "grad_norm": 1.264396071434021,
      "learning_rate": 0.00044654596100278556,
      "loss": 2.1107,
      "step": 19870
    },
    {
      "epoch": 5.537604456824512,
      "grad_norm": 1.8179502487182617,
      "learning_rate": 0.0004462674094707521,
      "loss": 2.1538,
      "step": 19880
    },
    {
      "epoch": 5.540389972144847,
      "grad_norm": 1.8476440906524658,
      "learning_rate": 0.0004459888579387187,
      "loss": 2.251,
      "step": 19890
    },
    {
      "epoch": 5.543175487465181,
      "grad_norm": 1.9847172498703003,
      "learning_rate": 0.0004457103064066853,
      "loss": 2.298,
      "step": 19900
    },
    {
      "epoch": 5.545961002785515,
      "grad_norm": 2.080392360687256,
      "learning_rate": 0.0004454317548746518,
      "loss": 2.3846,
      "step": 19910
    },
    {
      "epoch": 5.548746518105849,
      "grad_norm": 1.4296202659606934,
      "learning_rate": 0.0004451532033426184,
      "loss": 2.1346,
      "step": 19920
    },
    {
      "epoch": 5.5515320334261835,
      "grad_norm": 1.4241966009140015,
      "learning_rate": 0.00044487465181058494,
      "loss": 2.4818,
      "step": 19930
    },
    {
      "epoch": 5.554317548746518,
      "grad_norm": 1.6026116609573364,
      "learning_rate": 0.00044459610027855153,
      "loss": 2.0641,
      "step": 19940
    },
    {
      "epoch": 5.557103064066853,
      "grad_norm": 1.0575096607208252,
      "learning_rate": 0.0004443175487465181,
      "loss": 2.1148,
      "step": 19950
    },
    {
      "epoch": 5.559888579387186,
      "grad_norm": 1.4124538898468018,
      "learning_rate": 0.0004440389972144847,
      "loss": 2.2203,
      "step": 19960
    },
    {
      "epoch": 5.562674094707521,
      "grad_norm": 1.943956971168518,
      "learning_rate": 0.00044376044568245125,
      "loss": 2.3334,
      "step": 19970
    },
    {
      "epoch": 5.5654596100278555,
      "grad_norm": 2.389329433441162,
      "learning_rate": 0.00044348189415041784,
      "loss": 2.0207,
      "step": 19980
    },
    {
      "epoch": 5.56824512534819,
      "grad_norm": 2.074756383895874,
      "learning_rate": 0.00044320334261838443,
      "loss": 2.181,
      "step": 19990
    },
    {
      "epoch": 5.571030640668524,
      "grad_norm": 1.1621001958847046,
      "learning_rate": 0.00044292479108635097,
      "loss": 2.2346,
      "step": 20000
    },
    {
      "epoch": 5.573816155988858,
      "grad_norm": 1.7198083400726318,
      "learning_rate": 0.00044264623955431756,
      "loss": 2.2827,
      "step": 20010
    },
    {
      "epoch": 5.576601671309192,
      "grad_norm": 1.5475705862045288,
      "learning_rate": 0.0004423676880222841,
      "loss": 2.357,
      "step": 20020
    },
    {
      "epoch": 5.579387186629527,
      "grad_norm": 1.7048410177230835,
      "learning_rate": 0.00044208913649025074,
      "loss": 2.1434,
      "step": 20030
    },
    {
      "epoch": 5.582172701949861,
      "grad_norm": 1.6678929328918457,
      "learning_rate": 0.0004418105849582173,
      "loss": 2.2858,
      "step": 20040
    },
    {
      "epoch": 5.584958217270195,
      "grad_norm": 1.5983819961547852,
      "learning_rate": 0.00044153203342618387,
      "loss": 2.1473,
      "step": 20050
    },
    {
      "epoch": 5.587743732590529,
      "grad_norm": 1.3963831663131714,
      "learning_rate": 0.0004412534818941504,
      "loss": 2.2029,
      "step": 20060
    },
    {
      "epoch": 5.5905292479108635,
      "grad_norm": 1.4212545156478882,
      "learning_rate": 0.000440974930362117,
      "loss": 2.1744,
      "step": 20070
    },
    {
      "epoch": 5.593314763231198,
      "grad_norm": 1.6707369089126587,
      "learning_rate": 0.0004406963788300836,
      "loss": 2.4745,
      "step": 20080
    },
    {
      "epoch": 5.596100278551532,
      "grad_norm": 1.104815125465393,
      "learning_rate": 0.00044041782729805013,
      "loss": 2.2661,
      "step": 20090
    },
    {
      "epoch": 5.598885793871866,
      "grad_norm": 1.6566486358642578,
      "learning_rate": 0.0004401392757660167,
      "loss": 2.1843,
      "step": 20100
    },
    {
      "epoch": 5.6016713091922,
      "grad_norm": 2.387361526489258,
      "learning_rate": 0.0004398607242339833,
      "loss": 2.1654,
      "step": 20110
    },
    {
      "epoch": 5.604456824512535,
      "grad_norm": 1.4254937171936035,
      "learning_rate": 0.0004395821727019499,
      "loss": 2.2538,
      "step": 20120
    },
    {
      "epoch": 5.607242339832869,
      "grad_norm": 1.6260178089141846,
      "learning_rate": 0.00043930362116991644,
      "loss": 2.1927,
      "step": 20130
    },
    {
      "epoch": 5.610027855153203,
      "grad_norm": 2.3413450717926025,
      "learning_rate": 0.00043902506963788303,
      "loss": 2.1253,
      "step": 20140
    },
    {
      "epoch": 5.612813370473537,
      "grad_norm": 1.4090816974639893,
      "learning_rate": 0.00043874651810584957,
      "loss": 2.2059,
      "step": 20150
    },
    {
      "epoch": 5.615598885793872,
      "grad_norm": 1.7132647037506104,
      "learning_rate": 0.00043846796657381616,
      "loss": 2.2687,
      "step": 20160
    },
    {
      "epoch": 5.618384401114206,
      "grad_norm": 3.030515670776367,
      "learning_rate": 0.00043818941504178275,
      "loss": 2.2767,
      "step": 20170
    },
    {
      "epoch": 5.621169916434541,
      "grad_norm": 1.4950686693191528,
      "learning_rate": 0.0004379108635097493,
      "loss": 2.1858,
      "step": 20180
    },
    {
      "epoch": 5.623955431754875,
      "grad_norm": 1.7161682844161987,
      "learning_rate": 0.00043763231197771593,
      "loss": 2.4172,
      "step": 20190
    },
    {
      "epoch": 5.626740947075209,
      "grad_norm": 1.9249348640441895,
      "learning_rate": 0.00043735376044568247,
      "loss": 2.2104,
      "step": 20200
    },
    {
      "epoch": 5.629526462395543,
      "grad_norm": 2.538459062576294,
      "learning_rate": 0.00043707520891364906,
      "loss": 2.3271,
      "step": 20210
    },
    {
      "epoch": 5.632311977715878,
      "grad_norm": 1.993645429611206,
      "learning_rate": 0.0004367966573816156,
      "loss": 2.0448,
      "step": 20220
    },
    {
      "epoch": 5.635097493036212,
      "grad_norm": 1.8434115648269653,
      "learning_rate": 0.0004365181058495822,
      "loss": 2.3363,
      "step": 20230
    },
    {
      "epoch": 5.637883008356546,
      "grad_norm": 1.8356283903121948,
      "learning_rate": 0.0004362395543175487,
      "loss": 2.1381,
      "step": 20240
    },
    {
      "epoch": 5.64066852367688,
      "grad_norm": 1.5542672872543335,
      "learning_rate": 0.0004359610027855153,
      "loss": 2.2033,
      "step": 20250
    },
    {
      "epoch": 5.6434540389972145,
      "grad_norm": 1.1904897689819336,
      "learning_rate": 0.0004356824512534819,
      "loss": 2.0738,
      "step": 20260
    },
    {
      "epoch": 5.646239554317549,
      "grad_norm": 1.2676881551742554,
      "learning_rate": 0.0004354038997214485,
      "loss": 2.1932,
      "step": 20270
    },
    {
      "epoch": 5.649025069637883,
      "grad_norm": 1.8012757301330566,
      "learning_rate": 0.0004351253481894151,
      "loss": 2.2067,
      "step": 20280
    },
    {
      "epoch": 5.651810584958217,
      "grad_norm": 2.123906135559082,
      "learning_rate": 0.00043484679665738163,
      "loss": 2.5119,
      "step": 20290
    },
    {
      "epoch": 5.654596100278551,
      "grad_norm": 1.8631075620651245,
      "learning_rate": 0.0004345682451253482,
      "loss": 2.2677,
      "step": 20300
    },
    {
      "epoch": 5.657381615598886,
      "grad_norm": 1.4740486145019531,
      "learning_rate": 0.00043428969359331476,
      "loss": 2.2157,
      "step": 20310
    },
    {
      "epoch": 5.66016713091922,
      "grad_norm": 1.760536789894104,
      "learning_rate": 0.00043401114206128135,
      "loss": 2.1844,
      "step": 20320
    },
    {
      "epoch": 5.662952646239554,
      "grad_norm": 1.8552584648132324,
      "learning_rate": 0.0004337325905292479,
      "loss": 2.2759,
      "step": 20330
    },
    {
      "epoch": 5.665738161559888,
      "grad_norm": 1.9191184043884277,
      "learning_rate": 0.0004334540389972145,
      "loss": 2.2723,
      "step": 20340
    },
    {
      "epoch": 5.6685236768802225,
      "grad_norm": 1.3265821933746338,
      "learning_rate": 0.0004331754874651811,
      "loss": 2.1694,
      "step": 20350
    },
    {
      "epoch": 5.671309192200557,
      "grad_norm": 1.9205894470214844,
      "learning_rate": 0.00043289693593314766,
      "loss": 2.2825,
      "step": 20360
    },
    {
      "epoch": 5.674094707520892,
      "grad_norm": 2.085502862930298,
      "learning_rate": 0.00043261838440111425,
      "loss": 2.1695,
      "step": 20370
    },
    {
      "epoch": 5.676880222841225,
      "grad_norm": 1.2922866344451904,
      "learning_rate": 0.0004323398328690808,
      "loss": 2.1794,
      "step": 20380
    },
    {
      "epoch": 5.67966573816156,
      "grad_norm": 1.7075539827346802,
      "learning_rate": 0.0004320612813370474,
      "loss": 2.1881,
      "step": 20390
    },
    {
      "epoch": 5.6824512534818945,
      "grad_norm": 1.7007273435592651,
      "learning_rate": 0.0004317827298050139,
      "loss": 2.1065,
      "step": 20400
    },
    {
      "epoch": 5.685236768802229,
      "grad_norm": 1.5860799551010132,
      "learning_rate": 0.0004315041782729805,
      "loss": 2.2411,
      "step": 20410
    },
    {
      "epoch": 5.688022284122563,
      "grad_norm": 1.4522314071655273,
      "learning_rate": 0.00043122562674094704,
      "loss": 2.34,
      "step": 20420
    },
    {
      "epoch": 5.690807799442897,
      "grad_norm": 1.304071068763733,
      "learning_rate": 0.00043094707520891363,
      "loss": 2.1438,
      "step": 20430
    },
    {
      "epoch": 5.693593314763231,
      "grad_norm": 1.6152211427688599,
      "learning_rate": 0.0004306685236768803,
      "loss": 2.3457,
      "step": 20440
    },
    {
      "epoch": 5.696378830083566,
      "grad_norm": 1.808473825454712,
      "learning_rate": 0.0004303899721448468,
      "loss": 2.2781,
      "step": 20450
    },
    {
      "epoch": 5.6991643454039,
      "grad_norm": 1.3810430765151978,
      "learning_rate": 0.0004301114206128134,
      "loss": 2.2058,
      "step": 20460
    },
    {
      "epoch": 5.701949860724234,
      "grad_norm": 1.8035955429077148,
      "learning_rate": 0.00042983286908077994,
      "loss": 2.306,
      "step": 20470
    },
    {
      "epoch": 5.704735376044568,
      "grad_norm": 1.569966435432434,
      "learning_rate": 0.00042955431754874654,
      "loss": 2.1606,
      "step": 20480
    },
    {
      "epoch": 5.7075208913649025,
      "grad_norm": 1.8869704008102417,
      "learning_rate": 0.00042927576601671307,
      "loss": 2.2176,
      "step": 20490
    },
    {
      "epoch": 5.710306406685237,
      "grad_norm": 1.6565067768096924,
      "learning_rate": 0.00042899721448467966,
      "loss": 2.2965,
      "step": 20500
    },
    {
      "epoch": 5.713091922005571,
      "grad_norm": 1.3921862840652466,
      "learning_rate": 0.0004287186629526462,
      "loss": 2.2562,
      "step": 20510
    },
    {
      "epoch": 5.715877437325905,
      "grad_norm": 1.525402307510376,
      "learning_rate": 0.00042844011142061285,
      "loss": 2.214,
      "step": 20520
    },
    {
      "epoch": 5.718662952646239,
      "grad_norm": 1.6946420669555664,
      "learning_rate": 0.00042816155988857944,
      "loss": 2.1829,
      "step": 20530
    },
    {
      "epoch": 5.721448467966574,
      "grad_norm": 2.0281898975372314,
      "learning_rate": 0.000427883008356546,
      "loss": 2.2184,
      "step": 20540
    },
    {
      "epoch": 5.724233983286908,
      "grad_norm": 1.4067010879516602,
      "learning_rate": 0.00042760445682451257,
      "loss": 2.267,
      "step": 20550
    },
    {
      "epoch": 5.727019498607242,
      "grad_norm": 1.1821078062057495,
      "learning_rate": 0.0004273259052924791,
      "loss": 2.1562,
      "step": 20560
    },
    {
      "epoch": 5.729805013927576,
      "grad_norm": 1.8211628198623657,
      "learning_rate": 0.0004270473537604457,
      "loss": 2.2222,
      "step": 20570
    },
    {
      "epoch": 5.732590529247911,
      "grad_norm": 1.4366978406906128,
      "learning_rate": 0.00042676880222841223,
      "loss": 2.2531,
      "step": 20580
    },
    {
      "epoch": 5.735376044568245,
      "grad_norm": 1.6488661766052246,
      "learning_rate": 0.0004264902506963788,
      "loss": 2.3231,
      "step": 20590
    },
    {
      "epoch": 5.73816155988858,
      "grad_norm": 2.066453218460083,
      "learning_rate": 0.0004262116991643454,
      "loss": 2.2028,
      "step": 20600
    },
    {
      "epoch": 5.740947075208914,
      "grad_norm": 1.518815279006958,
      "learning_rate": 0.000425933147632312,
      "loss": 2.3275,
      "step": 20610
    },
    {
      "epoch": 5.743732590529248,
      "grad_norm": 1.7905758619308472,
      "learning_rate": 0.00042565459610027854,
      "loss": 2.3211,
      "step": 20620
    },
    {
      "epoch": 5.7465181058495824,
      "grad_norm": 1.673006534576416,
      "learning_rate": 0.00042537604456824513,
      "loss": 2.1271,
      "step": 20630
    },
    {
      "epoch": 5.749303621169917,
      "grad_norm": 1.2260342836380005,
      "learning_rate": 0.0004250974930362117,
      "loss": 2.1393,
      "step": 20640
    },
    {
      "epoch": 5.752089136490251,
      "grad_norm": 1.3961138725280762,
      "learning_rate": 0.00042481894150417826,
      "loss": 2.1665,
      "step": 20650
    },
    {
      "epoch": 5.754874651810585,
      "grad_norm": 1.0093979835510254,
      "learning_rate": 0.00042454038997214485,
      "loss": 2.3108,
      "step": 20660
    },
    {
      "epoch": 5.757660167130919,
      "grad_norm": 1.5526796579360962,
      "learning_rate": 0.0004242618384401114,
      "loss": 2.2429,
      "step": 20670
    },
    {
      "epoch": 5.7604456824512535,
      "grad_norm": 1.3783187866210938,
      "learning_rate": 0.00042398328690807803,
      "loss": 2.1278,
      "step": 20680
    },
    {
      "epoch": 5.763231197771588,
      "grad_norm": 1.286515712738037,
      "learning_rate": 0.00042370473537604457,
      "loss": 2.1544,
      "step": 20690
    },
    {
      "epoch": 5.766016713091922,
      "grad_norm": 1.168590784072876,
      "learning_rate": 0.00042342618384401116,
      "loss": 2.1492,
      "step": 20700
    },
    {
      "epoch": 5.768802228412256,
      "grad_norm": 1.1137404441833496,
      "learning_rate": 0.0004231476323119777,
      "loss": 2.3324,
      "step": 20710
    },
    {
      "epoch": 5.77158774373259,
      "grad_norm": 1.4439700841903687,
      "learning_rate": 0.0004228690807799443,
      "loss": 2.13,
      "step": 20720
    },
    {
      "epoch": 5.774373259052925,
      "grad_norm": 1.3689237833023071,
      "learning_rate": 0.0004225905292479109,
      "loss": 2.3111,
      "step": 20730
    },
    {
      "epoch": 5.777158774373259,
      "grad_norm": 1.2233220338821411,
      "learning_rate": 0.0004223119777158774,
      "loss": 2.1476,
      "step": 20740
    },
    {
      "epoch": 5.779944289693593,
      "grad_norm": 2.339949607849121,
      "learning_rate": 0.000422033426183844,
      "loss": 2.2677,
      "step": 20750
    },
    {
      "epoch": 5.782729805013927,
      "grad_norm": 2.1753852367401123,
      "learning_rate": 0.0004217548746518106,
      "loss": 2.111,
      "step": 20760
    },
    {
      "epoch": 5.7855153203342615,
      "grad_norm": 1.5551302433013916,
      "learning_rate": 0.0004214763231197772,
      "loss": 2.2348,
      "step": 20770
    },
    {
      "epoch": 5.788300835654596,
      "grad_norm": 1.6791037321090698,
      "learning_rate": 0.00042119777158774373,
      "loss": 2.0675,
      "step": 20780
    },
    {
      "epoch": 5.791086350974931,
      "grad_norm": 1.7496356964111328,
      "learning_rate": 0.0004209192200557103,
      "loss": 2.2589,
      "step": 20790
    },
    {
      "epoch": 5.793871866295264,
      "grad_norm": 1.8066508769989014,
      "learning_rate": 0.00042064066852367686,
      "loss": 2.2612,
      "step": 20800
    },
    {
      "epoch": 5.796657381615599,
      "grad_norm": 1.4485628604888916,
      "learning_rate": 0.00042036211699164345,
      "loss": 2.2452,
      "step": 20810
    },
    {
      "epoch": 5.7994428969359335,
      "grad_norm": 2.4361159801483154,
      "learning_rate": 0.00042008356545961004,
      "loss": 2.1687,
      "step": 20820
    },
    {
      "epoch": 5.802228412256268,
      "grad_norm": 1.6261863708496094,
      "learning_rate": 0.0004198050139275766,
      "loss": 2.3446,
      "step": 20830
    },
    {
      "epoch": 5.805013927576602,
      "grad_norm": 1.4903826713562012,
      "learning_rate": 0.0004195264623955432,
      "loss": 2.1461,
      "step": 20840
    },
    {
      "epoch": 5.807799442896936,
      "grad_norm": 1.6312068700790405,
      "learning_rate": 0.00041924791086350976,
      "loss": 2.5141,
      "step": 20850
    },
    {
      "epoch": 5.81058495821727,
      "grad_norm": 1.5026384592056274,
      "learning_rate": 0.00041896935933147635,
      "loss": 2.1684,
      "step": 20860
    },
    {
      "epoch": 5.813370473537605,
      "grad_norm": 2.4665467739105225,
      "learning_rate": 0.0004186908077994429,
      "loss": 2.2707,
      "step": 20870
    },
    {
      "epoch": 5.816155988857939,
      "grad_norm": 1.2930384874343872,
      "learning_rate": 0.0004184122562674095,
      "loss": 2.2603,
      "step": 20880
    },
    {
      "epoch": 5.818941504178273,
      "grad_norm": 2.1512091159820557,
      "learning_rate": 0.000418133704735376,
      "loss": 2.1446,
      "step": 20890
    },
    {
      "epoch": 5.821727019498607,
      "grad_norm": 1.5767900943756104,
      "learning_rate": 0.0004178551532033426,
      "loss": 2.4077,
      "step": 20900
    },
    {
      "epoch": 5.8245125348189415,
      "grad_norm": 1.3259581327438354,
      "learning_rate": 0.0004175766016713092,
      "loss": 2.1092,
      "step": 20910
    },
    {
      "epoch": 5.827298050139276,
      "grad_norm": 1.8464325666427612,
      "learning_rate": 0.0004172980501392758,
      "loss": 2.1831,
      "step": 20920
    },
    {
      "epoch": 5.83008356545961,
      "grad_norm": 2.4440524578094482,
      "learning_rate": 0.0004170194986072424,
      "loss": 2.2537,
      "step": 20930
    },
    {
      "epoch": 5.832869080779944,
      "grad_norm": 1.8187845945358276,
      "learning_rate": 0.0004167409470752089,
      "loss": 2.1519,
      "step": 20940
    },
    {
      "epoch": 5.835654596100278,
      "grad_norm": 1.576941728591919,
      "learning_rate": 0.0004164623955431755,
      "loss": 2.2981,
      "step": 20950
    },
    {
      "epoch": 5.838440111420613,
      "grad_norm": 2.808652877807617,
      "learning_rate": 0.00041618384401114205,
      "loss": 2.1876,
      "step": 20960
    },
    {
      "epoch": 5.841225626740947,
      "grad_norm": 1.529213786125183,
      "learning_rate": 0.00041590529247910864,
      "loss": 2.2485,
      "step": 20970
    },
    {
      "epoch": 5.844011142061281,
      "grad_norm": 1.3554935455322266,
      "learning_rate": 0.0004156267409470752,
      "loss": 2.3652,
      "step": 20980
    },
    {
      "epoch": 5.846796657381615,
      "grad_norm": 1.794484257698059,
      "learning_rate": 0.00041534818941504177,
      "loss": 2.2318,
      "step": 20990
    },
    {
      "epoch": 5.84958217270195,
      "grad_norm": 1.806983232498169,
      "learning_rate": 0.0004150696378830084,
      "loss": 2.2546,
      "step": 21000
    }
  ],
  "logging_steps": 10,
  "max_steps": 35900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2781534275665920.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
