{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8020532563362207,
  "eval_steps": 500,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003208213025344883,
      "grad_norm": 0.9929807782173157,
      "learning_rate": 0.00019980750721847933,
      "loss": 6.6175,
      "step": 10
    },
    {
      "epoch": 0.006416426050689766,
      "grad_norm": 1.3569743633270264,
      "learning_rate": 0.000199593626350123,
      "loss": 6.4576,
      "step": 20
    },
    {
      "epoch": 0.009624639076034648,
      "grad_norm": 1.9002569913864136,
      "learning_rate": 0.00019937974548176667,
      "loss": 5.9434,
      "step": 30
    },
    {
      "epoch": 0.012832852101379532,
      "grad_norm": 2.187473773956299,
      "learning_rate": 0.00019916586461341034,
      "loss": 4.9691,
      "step": 40
    },
    {
      "epoch": 0.016041065126724416,
      "grad_norm": 1.5395301580429077,
      "learning_rate": 0.000198951983745054,
      "loss": 5.1283,
      "step": 50
    },
    {
      "epoch": 0.019249278152069296,
      "grad_norm": 1.0944130420684814,
      "learning_rate": 0.0001987381028766977,
      "loss": 5.014,
      "step": 60
    },
    {
      "epoch": 0.02245749117741418,
      "grad_norm": 1.463235855102539,
      "learning_rate": 0.00019852422200834134,
      "loss": 4.8141,
      "step": 70
    },
    {
      "epoch": 0.025665704202759064,
      "grad_norm": 1.5792930126190186,
      "learning_rate": 0.00019831034113998504,
      "loss": 4.8278,
      "step": 80
    },
    {
      "epoch": 0.028873917228103944,
      "grad_norm": 1.4631986618041992,
      "learning_rate": 0.0001980964602716287,
      "loss": 4.6946,
      "step": 90
    },
    {
      "epoch": 0.03208213025344883,
      "grad_norm": 1.7708677053451538,
      "learning_rate": 0.0001978825794032724,
      "loss": 4.5593,
      "step": 100
    },
    {
      "epoch": 0.03529034327879371,
      "grad_norm": 1.3442966938018799,
      "learning_rate": 0.00019766869853491605,
      "loss": 4.5715,
      "step": 110
    },
    {
      "epoch": 0.03849855630413859,
      "grad_norm": 3.7381999492645264,
      "learning_rate": 0.00019745481766655974,
      "loss": 4.361,
      "step": 120
    },
    {
      "epoch": 0.04170676932948348,
      "grad_norm": 1.6169660091400146,
      "learning_rate": 0.0001972409367982034,
      "loss": 4.3992,
      "step": 130
    },
    {
      "epoch": 0.04491498235482836,
      "grad_norm": 1.7813392877578735,
      "learning_rate": 0.00019702705592984708,
      "loss": 4.5034,
      "step": 140
    },
    {
      "epoch": 0.04812319538017324,
      "grad_norm": 1.304078221321106,
      "learning_rate": 0.00019681317506149075,
      "loss": 4.6195,
      "step": 150
    },
    {
      "epoch": 0.05133140840551813,
      "grad_norm": 1.8750945329666138,
      "learning_rate": 0.00019659929419313442,
      "loss": 4.2862,
      "step": 160
    },
    {
      "epoch": 0.05453962143086301,
      "grad_norm": 1.4983075857162476,
      "learning_rate": 0.00019638541332477811,
      "loss": 4.3601,
      "step": 170
    },
    {
      "epoch": 0.05774783445620789,
      "grad_norm": 1.5268137454986572,
      "learning_rate": 0.00019617153245642178,
      "loss": 4.443,
      "step": 180
    },
    {
      "epoch": 0.060956047481552776,
      "grad_norm": 1.523414134979248,
      "learning_rate": 0.00019595765158806545,
      "loss": 4.4724,
      "step": 190
    },
    {
      "epoch": 0.06416426050689766,
      "grad_norm": 1.7114750146865845,
      "learning_rate": 0.00019574377071970912,
      "loss": 4.4758,
      "step": 200
    },
    {
      "epoch": 0.06737247353224254,
      "grad_norm": 2.3549723625183105,
      "learning_rate": 0.00019552988985135282,
      "loss": 4.4372,
      "step": 210
    },
    {
      "epoch": 0.07058068655758742,
      "grad_norm": 2.8332765102386475,
      "learning_rate": 0.0001953160089829965,
      "loss": 4.4495,
      "step": 220
    },
    {
      "epoch": 0.0737888995829323,
      "grad_norm": 2.200261354446411,
      "learning_rate": 0.00019510212811464016,
      "loss": 4.3283,
      "step": 230
    },
    {
      "epoch": 0.07699711260827719,
      "grad_norm": 1.7542457580566406,
      "learning_rate": 0.00019488824724628383,
      "loss": 4.3585,
      "step": 240
    },
    {
      "epoch": 0.08020532563362208,
      "grad_norm": 2.2906546592712402,
      "learning_rate": 0.00019467436637792752,
      "loss": 4.3102,
      "step": 250
    },
    {
      "epoch": 0.08341353865896696,
      "grad_norm": 3.6942086219787598,
      "learning_rate": 0.0001944604855095712,
      "loss": 4.515,
      "step": 260
    },
    {
      "epoch": 0.08662175168431184,
      "grad_norm": 1.9311494827270508,
      "learning_rate": 0.00019424660464121486,
      "loss": 4.0416,
      "step": 270
    },
    {
      "epoch": 0.08982996470965672,
      "grad_norm": 3.2551963329315186,
      "learning_rate": 0.00019403272377285853,
      "loss": 4.2073,
      "step": 280
    },
    {
      "epoch": 0.0930381777350016,
      "grad_norm": 2.0594606399536133,
      "learning_rate": 0.0001938188429045022,
      "loss": 4.0984,
      "step": 290
    },
    {
      "epoch": 0.09624639076034648,
      "grad_norm": 2.2760660648345947,
      "learning_rate": 0.0001936049620361459,
      "loss": 4.291,
      "step": 300
    },
    {
      "epoch": 0.09945460378569138,
      "grad_norm": 1.9606152772903442,
      "learning_rate": 0.00019339108116778954,
      "loss": 4.0656,
      "step": 310
    },
    {
      "epoch": 0.10266281681103626,
      "grad_norm": 1.33408522605896,
      "learning_rate": 0.00019317720029943323,
      "loss": 4.2605,
      "step": 320
    },
    {
      "epoch": 0.10587102983638114,
      "grad_norm": 3.5272436141967773,
      "learning_rate": 0.0001929633194310769,
      "loss": 4.0691,
      "step": 330
    },
    {
      "epoch": 0.10907924286172602,
      "grad_norm": 3.050316333770752,
      "learning_rate": 0.00019274943856272057,
      "loss": 4.1776,
      "step": 340
    },
    {
      "epoch": 0.1122874558870709,
      "grad_norm": 1.6468826532363892,
      "learning_rate": 0.00019253555769436424,
      "loss": 4.405,
      "step": 350
    },
    {
      "epoch": 0.11549566891241578,
      "grad_norm": 1.9157220125198364,
      "learning_rate": 0.00019232167682600794,
      "loss": 4.1456,
      "step": 360
    },
    {
      "epoch": 0.11870388193776067,
      "grad_norm": 3.1583054065704346,
      "learning_rate": 0.0001921077959576516,
      "loss": 4.1256,
      "step": 370
    },
    {
      "epoch": 0.12191209496310555,
      "grad_norm": 1.8012769222259521,
      "learning_rate": 0.00019189391508929527,
      "loss": 4.156,
      "step": 380
    },
    {
      "epoch": 0.12512030798845045,
      "grad_norm": 2.0717804431915283,
      "learning_rate": 0.00019168003422093894,
      "loss": 4.397,
      "step": 390
    },
    {
      "epoch": 0.12832852101379533,
      "grad_norm": 2.4557945728302,
      "learning_rate": 0.0001914661533525826,
      "loss": 4.2566,
      "step": 400
    },
    {
      "epoch": 0.1315367340391402,
      "grad_norm": 1.787262201309204,
      "learning_rate": 0.0001912522724842263,
      "loss": 4.38,
      "step": 410
    },
    {
      "epoch": 0.1347449470644851,
      "grad_norm": 1.9285707473754883,
      "learning_rate": 0.00019103839161586995,
      "loss": 4.0746,
      "step": 420
    },
    {
      "epoch": 0.13795316008982997,
      "grad_norm": 2.7452640533447266,
      "learning_rate": 0.00019082451074751365,
      "loss": 4.2652,
      "step": 430
    },
    {
      "epoch": 0.14116137311517485,
      "grad_norm": 2.027345895767212,
      "learning_rate": 0.00019061062987915732,
      "loss": 4.1917,
      "step": 440
    },
    {
      "epoch": 0.14436958614051973,
      "grad_norm": 1.647667407989502,
      "learning_rate": 0.000190396749010801,
      "loss": 4.2686,
      "step": 450
    },
    {
      "epoch": 0.1475777991658646,
      "grad_norm": 2.308013916015625,
      "learning_rate": 0.00019018286814244465,
      "loss": 4.2013,
      "step": 460
    },
    {
      "epoch": 0.1507860121912095,
      "grad_norm": 2.484921932220459,
      "learning_rate": 0.00018996898727408835,
      "loss": 4.2281,
      "step": 470
    },
    {
      "epoch": 0.15399422521655437,
      "grad_norm": 1.358372688293457,
      "learning_rate": 0.00018975510640573202,
      "loss": 3.7731,
      "step": 480
    },
    {
      "epoch": 0.15720243824189925,
      "grad_norm": 1.484513521194458,
      "learning_rate": 0.0001895412255373757,
      "loss": 4.2264,
      "step": 490
    },
    {
      "epoch": 0.16041065126724416,
      "grad_norm": 1.9827213287353516,
      "learning_rate": 0.00018932734466901936,
      "loss": 4.2347,
      "step": 500
    },
    {
      "epoch": 0.16361886429258904,
      "grad_norm": 1.3327556848526,
      "learning_rate": 0.00018911346380066303,
      "loss": 4.0443,
      "step": 510
    },
    {
      "epoch": 0.16682707731793392,
      "grad_norm": 1.8973983526229858,
      "learning_rate": 0.00018889958293230672,
      "loss": 4.2888,
      "step": 520
    },
    {
      "epoch": 0.1700352903432788,
      "grad_norm": 1.7959173917770386,
      "learning_rate": 0.0001886857020639504,
      "loss": 4.1918,
      "step": 530
    },
    {
      "epoch": 0.17324350336862368,
      "grad_norm": 1.7852979898452759,
      "learning_rate": 0.00018847182119559406,
      "loss": 4.0564,
      "step": 540
    },
    {
      "epoch": 0.17645171639396856,
      "grad_norm": 1.680551528930664,
      "learning_rate": 0.00018825794032723773,
      "loss": 4.3001,
      "step": 550
    },
    {
      "epoch": 0.17965992941931344,
      "grad_norm": 2.1219851970672607,
      "learning_rate": 0.00018804405945888143,
      "loss": 4.013,
      "step": 560
    },
    {
      "epoch": 0.18286814244465832,
      "grad_norm": 2.999102830886841,
      "learning_rate": 0.0001878301785905251,
      "loss": 3.9916,
      "step": 570
    },
    {
      "epoch": 0.1860763554700032,
      "grad_norm": 3.4120051860809326,
      "learning_rate": 0.00018761629772216876,
      "loss": 3.9925,
      "step": 580
    },
    {
      "epoch": 0.18928456849534808,
      "grad_norm": 3.4790403842926025,
      "learning_rate": 0.00018740241685381243,
      "loss": 4.0963,
      "step": 590
    },
    {
      "epoch": 0.19249278152069296,
      "grad_norm": 2.6449272632598877,
      "learning_rate": 0.0001871885359854561,
      "loss": 3.8605,
      "step": 600
    },
    {
      "epoch": 0.19570099454603784,
      "grad_norm": 1.7391000986099243,
      "learning_rate": 0.0001869746551170998,
      "loss": 4.0297,
      "step": 610
    },
    {
      "epoch": 0.19890920757138275,
      "grad_norm": 2.405965566635132,
      "learning_rate": 0.00018676077424874344,
      "loss": 4.1504,
      "step": 620
    },
    {
      "epoch": 0.20211742059672763,
      "grad_norm": 2.2231500148773193,
      "learning_rate": 0.00018654689338038714,
      "loss": 4.1188,
      "step": 630
    },
    {
      "epoch": 0.2053256336220725,
      "grad_norm": 2.632821798324585,
      "learning_rate": 0.0001863330125120308,
      "loss": 4.0629,
      "step": 640
    },
    {
      "epoch": 0.2085338466474174,
      "grad_norm": 2.695380449295044,
      "learning_rate": 0.00018611913164367447,
      "loss": 4.0123,
      "step": 650
    },
    {
      "epoch": 0.21174205967276227,
      "grad_norm": 2.8645012378692627,
      "learning_rate": 0.00018590525077531814,
      "loss": 3.9132,
      "step": 660
    },
    {
      "epoch": 0.21495027269810715,
      "grad_norm": 1.7544795274734497,
      "learning_rate": 0.00018569136990696184,
      "loss": 3.9614,
      "step": 670
    },
    {
      "epoch": 0.21815848572345203,
      "grad_norm": 1.7987943887710571,
      "learning_rate": 0.0001854774890386055,
      "loss": 4.0961,
      "step": 680
    },
    {
      "epoch": 0.22136669874879691,
      "grad_norm": 2.089115619659424,
      "learning_rate": 0.00018526360817024918,
      "loss": 4.1038,
      "step": 690
    },
    {
      "epoch": 0.2245749117741418,
      "grad_norm": 2.333409309387207,
      "learning_rate": 0.00018504972730189285,
      "loss": 4.2228,
      "step": 700
    },
    {
      "epoch": 0.22778312479948667,
      "grad_norm": 2.236550807952881,
      "learning_rate": 0.00018483584643353654,
      "loss": 4.2367,
      "step": 710
    },
    {
      "epoch": 0.23099133782483156,
      "grad_norm": 2.5641939640045166,
      "learning_rate": 0.0001846219655651802,
      "loss": 3.8369,
      "step": 720
    },
    {
      "epoch": 0.23419955085017646,
      "grad_norm": 1.9960426092147827,
      "learning_rate": 0.00018440808469682388,
      "loss": 3.979,
      "step": 730
    },
    {
      "epoch": 0.23740776387552134,
      "grad_norm": 1.7743498086929321,
      "learning_rate": 0.00018419420382846755,
      "loss": 3.8836,
      "step": 740
    },
    {
      "epoch": 0.24061597690086622,
      "grad_norm": 1.9773372411727905,
      "learning_rate": 0.00018398032296011122,
      "loss": 4.2858,
      "step": 750
    },
    {
      "epoch": 0.2438241899262111,
      "grad_norm": 2.5570192337036133,
      "learning_rate": 0.00018376644209175492,
      "loss": 4.1066,
      "step": 760
    },
    {
      "epoch": 0.24703240295155598,
      "grad_norm": 3.1494789123535156,
      "learning_rate": 0.00018355256122339856,
      "loss": 3.9116,
      "step": 770
    },
    {
      "epoch": 0.2502406159769009,
      "grad_norm": 2.2536778450012207,
      "learning_rate": 0.00018333868035504225,
      "loss": 4.0007,
      "step": 780
    },
    {
      "epoch": 0.25344882900224575,
      "grad_norm": 2.004897356033325,
      "learning_rate": 0.00018312479948668592,
      "loss": 4.0483,
      "step": 790
    },
    {
      "epoch": 0.25665704202759065,
      "grad_norm": 1.5317282676696777,
      "learning_rate": 0.00018291091861832962,
      "loss": 4.1112,
      "step": 800
    },
    {
      "epoch": 0.2598652550529355,
      "grad_norm": 1.8230574131011963,
      "learning_rate": 0.00018269703774997326,
      "loss": 4.295,
      "step": 810
    },
    {
      "epoch": 0.2630734680782804,
      "grad_norm": 1.616417646408081,
      "learning_rate": 0.00018248315688161696,
      "loss": 4.0235,
      "step": 820
    },
    {
      "epoch": 0.26628168110362527,
      "grad_norm": 1.9636763334274292,
      "learning_rate": 0.00018226927601326063,
      "loss": 4.2052,
      "step": 830
    },
    {
      "epoch": 0.2694898941289702,
      "grad_norm": 1.9898827075958252,
      "learning_rate": 0.0001820553951449043,
      "loss": 3.8299,
      "step": 840
    },
    {
      "epoch": 0.27269810715431503,
      "grad_norm": 2.130124807357788,
      "learning_rate": 0.00018184151427654797,
      "loss": 4.2331,
      "step": 850
    },
    {
      "epoch": 0.27590632017965994,
      "grad_norm": 1.859649896621704,
      "learning_rate": 0.00018162763340819163,
      "loss": 4.0141,
      "step": 860
    },
    {
      "epoch": 0.2791145332050048,
      "grad_norm": 2.182663917541504,
      "learning_rate": 0.00018141375253983533,
      "loss": 3.878,
      "step": 870
    },
    {
      "epoch": 0.2823227462303497,
      "grad_norm": 2.1161508560180664,
      "learning_rate": 0.000181199871671479,
      "loss": 4.1835,
      "step": 880
    },
    {
      "epoch": 0.2855309592556946,
      "grad_norm": 2.5770223140716553,
      "learning_rate": 0.00018098599080312267,
      "loss": 3.8088,
      "step": 890
    },
    {
      "epoch": 0.28873917228103946,
      "grad_norm": 2.4349141120910645,
      "learning_rate": 0.00018077210993476634,
      "loss": 3.9945,
      "step": 900
    },
    {
      "epoch": 0.29194738530638437,
      "grad_norm": 1.6255314350128174,
      "learning_rate": 0.00018055822906641003,
      "loss": 3.8995,
      "step": 910
    },
    {
      "epoch": 0.2951555983317292,
      "grad_norm": 3.37870192527771,
      "learning_rate": 0.0001803443481980537,
      "loss": 4.0921,
      "step": 920
    },
    {
      "epoch": 0.2983638113570741,
      "grad_norm": 2.148959159851074,
      "learning_rate": 0.00018013046732969737,
      "loss": 3.8036,
      "step": 930
    },
    {
      "epoch": 0.301572024382419,
      "grad_norm": 2.0911977291107178,
      "learning_rate": 0.00017991658646134104,
      "loss": 4.0936,
      "step": 940
    },
    {
      "epoch": 0.3047802374077639,
      "grad_norm": 1.8594186305999756,
      "learning_rate": 0.0001797027055929847,
      "loss": 4.1379,
      "step": 950
    },
    {
      "epoch": 0.30798845043310874,
      "grad_norm": 2.280639886856079,
      "learning_rate": 0.0001794888247246284,
      "loss": 4.1401,
      "step": 960
    },
    {
      "epoch": 0.31119666345845365,
      "grad_norm": 2.317460775375366,
      "learning_rate": 0.00017927494385627205,
      "loss": 3.874,
      "step": 970
    },
    {
      "epoch": 0.3144048764837985,
      "grad_norm": 1.4703642129898071,
      "learning_rate": 0.00017906106298791574,
      "loss": 4.2674,
      "step": 980
    },
    {
      "epoch": 0.3176130895091434,
      "grad_norm": 1.6096246242523193,
      "learning_rate": 0.00017884718211955941,
      "loss": 4.159,
      "step": 990
    },
    {
      "epoch": 0.3208213025344883,
      "grad_norm": 1.5120058059692383,
      "learning_rate": 0.00017863330125120308,
      "loss": 3.9474,
      "step": 1000
    },
    {
      "epoch": 0.32402951555983317,
      "grad_norm": 1.886333703994751,
      "learning_rate": 0.00017841942038284675,
      "loss": 3.9808,
      "step": 1010
    },
    {
      "epoch": 0.3272377285851781,
      "grad_norm": 2.2596702575683594,
      "learning_rate": 0.00017820553951449045,
      "loss": 3.9915,
      "step": 1020
    },
    {
      "epoch": 0.33044594161052293,
      "grad_norm": 2.04266619682312,
      "learning_rate": 0.00017799165864613412,
      "loss": 4.0146,
      "step": 1030
    },
    {
      "epoch": 0.33365415463586784,
      "grad_norm": 1.6656841039657593,
      "learning_rate": 0.00017777777777777779,
      "loss": 4.0215,
      "step": 1040
    },
    {
      "epoch": 0.3368623676612127,
      "grad_norm": 2.7082571983337402,
      "learning_rate": 0.00017756389690942146,
      "loss": 3.9371,
      "step": 1050
    },
    {
      "epoch": 0.3400705806865576,
      "grad_norm": 1.9280714988708496,
      "learning_rate": 0.00017735001604106512,
      "loss": 4.0654,
      "step": 1060
    },
    {
      "epoch": 0.34327879371190245,
      "grad_norm": 1.6905943155288696,
      "learning_rate": 0.00017713613517270882,
      "loss": 4.061,
      "step": 1070
    },
    {
      "epoch": 0.34648700673724736,
      "grad_norm": 2.4728879928588867,
      "learning_rate": 0.00017692225430435246,
      "loss": 3.8914,
      "step": 1080
    },
    {
      "epoch": 0.3496952197625922,
      "grad_norm": 1.6264522075653076,
      "learning_rate": 0.00017670837343599616,
      "loss": 3.9604,
      "step": 1090
    },
    {
      "epoch": 0.3529034327879371,
      "grad_norm": 1.7835686206817627,
      "learning_rate": 0.00017649449256763983,
      "loss": 4.0851,
      "step": 1100
    },
    {
      "epoch": 0.35611164581328203,
      "grad_norm": 1.9933665990829468,
      "learning_rate": 0.00017628061169928352,
      "loss": 3.9079,
      "step": 1110
    },
    {
      "epoch": 0.3593198588386269,
      "grad_norm": 4.560632228851318,
      "learning_rate": 0.00017606673083092717,
      "loss": 4.0444,
      "step": 1120
    },
    {
      "epoch": 0.3625280718639718,
      "grad_norm": 2.053833246231079,
      "learning_rate": 0.00017585284996257086,
      "loss": 4.1077,
      "step": 1130
    },
    {
      "epoch": 0.36573628488931664,
      "grad_norm": 2.3089566230773926,
      "learning_rate": 0.00017563896909421453,
      "loss": 3.8746,
      "step": 1140
    },
    {
      "epoch": 0.36894449791466155,
      "grad_norm": 2.273993492126465,
      "learning_rate": 0.0001754250882258582,
      "loss": 3.9762,
      "step": 1150
    },
    {
      "epoch": 0.3721527109400064,
      "grad_norm": 2.6280791759490967,
      "learning_rate": 0.00017521120735750187,
      "loss": 4.1242,
      "step": 1160
    },
    {
      "epoch": 0.3753609239653513,
      "grad_norm": 3.4105989933013916,
      "learning_rate": 0.00017499732648914554,
      "loss": 3.9282,
      "step": 1170
    },
    {
      "epoch": 0.37856913699069616,
      "grad_norm": 1.7122840881347656,
      "learning_rate": 0.00017478344562078923,
      "loss": 3.9074,
      "step": 1180
    },
    {
      "epoch": 0.3817773500160411,
      "grad_norm": 1.8528074026107788,
      "learning_rate": 0.0001745695647524329,
      "loss": 4.1092,
      "step": 1190
    },
    {
      "epoch": 0.3849855630413859,
      "grad_norm": 1.9337636232376099,
      "learning_rate": 0.00017435568388407657,
      "loss": 4.0473,
      "step": 1200
    },
    {
      "epoch": 0.38819377606673083,
      "grad_norm": 1.9283006191253662,
      "learning_rate": 0.00017414180301572024,
      "loss": 4.1426,
      "step": 1210
    },
    {
      "epoch": 0.3914019890920757,
      "grad_norm": 2.136915445327759,
      "learning_rate": 0.00017392792214736394,
      "loss": 4.118,
      "step": 1220
    },
    {
      "epoch": 0.3946102021174206,
      "grad_norm": 3.090832233428955,
      "learning_rate": 0.0001737140412790076,
      "loss": 4.0838,
      "step": 1230
    },
    {
      "epoch": 0.3978184151427655,
      "grad_norm": 1.7367016077041626,
      "learning_rate": 0.00017350016041065128,
      "loss": 4.0498,
      "step": 1240
    },
    {
      "epoch": 0.40102662816811036,
      "grad_norm": 2.549715280532837,
      "learning_rate": 0.00017328627954229495,
      "loss": 4.0492,
      "step": 1250
    },
    {
      "epoch": 0.40423484119345526,
      "grad_norm": 1.7982631921768188,
      "learning_rate": 0.00017307239867393864,
      "loss": 4.0505,
      "step": 1260
    },
    {
      "epoch": 0.4074430542188001,
      "grad_norm": 2.527186632156372,
      "learning_rate": 0.0001728585178055823,
      "loss": 3.9963,
      "step": 1270
    },
    {
      "epoch": 0.410651267244145,
      "grad_norm": 2.139296293258667,
      "learning_rate": 0.00017264463693722598,
      "loss": 4.026,
      "step": 1280
    },
    {
      "epoch": 0.4138594802694899,
      "grad_norm": 3.607391119003296,
      "learning_rate": 0.00017243075606886965,
      "loss": 3.716,
      "step": 1290
    },
    {
      "epoch": 0.4170676932948348,
      "grad_norm": 2.2469518184661865,
      "learning_rate": 0.00017221687520051332,
      "loss": 3.9968,
      "step": 1300
    },
    {
      "epoch": 0.42027590632017964,
      "grad_norm": 2.578824043273926,
      "learning_rate": 0.000172002994332157,
      "loss": 3.8834,
      "step": 1310
    },
    {
      "epoch": 0.42348411934552455,
      "grad_norm": 1.9329564571380615,
      "learning_rate": 0.00017178911346380066,
      "loss": 4.0908,
      "step": 1320
    },
    {
      "epoch": 0.4266923323708694,
      "grad_norm": 3.4855501651763916,
      "learning_rate": 0.00017157523259544435,
      "loss": 4.0754,
      "step": 1330
    },
    {
      "epoch": 0.4299005453962143,
      "grad_norm": 1.592391848564148,
      "learning_rate": 0.00017136135172708802,
      "loss": 4.0788,
      "step": 1340
    },
    {
      "epoch": 0.4331087584215592,
      "grad_norm": 1.7768322229385376,
      "learning_rate": 0.0001711474708587317,
      "loss": 4.167,
      "step": 1350
    },
    {
      "epoch": 0.43631697144690407,
      "grad_norm": 1.798975944519043,
      "learning_rate": 0.00017093358999037536,
      "loss": 3.9586,
      "step": 1360
    },
    {
      "epoch": 0.439525184472249,
      "grad_norm": 2.1416187286376953,
      "learning_rate": 0.00017071970912201906,
      "loss": 3.855,
      "step": 1370
    },
    {
      "epoch": 0.44273339749759383,
      "grad_norm": 2.7391726970672607,
      "learning_rate": 0.00017050582825366273,
      "loss": 3.8697,
      "step": 1380
    },
    {
      "epoch": 0.44594161052293874,
      "grad_norm": 1.9695415496826172,
      "learning_rate": 0.0001702919473853064,
      "loss": 3.6625,
      "step": 1390
    },
    {
      "epoch": 0.4491498235482836,
      "grad_norm": 2.2329742908477783,
      "learning_rate": 0.00017007806651695006,
      "loss": 4.2323,
      "step": 1400
    },
    {
      "epoch": 0.4523580365736285,
      "grad_norm": 2.7690107822418213,
      "learning_rate": 0.00016986418564859373,
      "loss": 3.8557,
      "step": 1410
    },
    {
      "epoch": 0.45556624959897335,
      "grad_norm": 2.4744131565093994,
      "learning_rate": 0.00016965030478023743,
      "loss": 3.9899,
      "step": 1420
    },
    {
      "epoch": 0.45877446262431826,
      "grad_norm": 2.181079149246216,
      "learning_rate": 0.00016943642391188107,
      "loss": 4.1196,
      "step": 1430
    },
    {
      "epoch": 0.4619826756496631,
      "grad_norm": 3.137943983078003,
      "learning_rate": 0.00016922254304352477,
      "loss": 4.1276,
      "step": 1440
    },
    {
      "epoch": 0.465190888675008,
      "grad_norm": 1.8179845809936523,
      "learning_rate": 0.00016900866217516844,
      "loss": 3.888,
      "step": 1450
    },
    {
      "epoch": 0.4683991017003529,
      "grad_norm": 2.0989930629730225,
      "learning_rate": 0.00016879478130681213,
      "loss": 3.8297,
      "step": 1460
    },
    {
      "epoch": 0.4716073147256978,
      "grad_norm": 2.2679786682128906,
      "learning_rate": 0.00016858090043845577,
      "loss": 3.7198,
      "step": 1470
    },
    {
      "epoch": 0.4748155277510427,
      "grad_norm": 2.0550575256347656,
      "learning_rate": 0.00016836701957009947,
      "loss": 4.1815,
      "step": 1480
    },
    {
      "epoch": 0.47802374077638754,
      "grad_norm": 3.2452001571655273,
      "learning_rate": 0.00016815313870174314,
      "loss": 4.0687,
      "step": 1490
    },
    {
      "epoch": 0.48123195380173245,
      "grad_norm": 2.349343776702881,
      "learning_rate": 0.0001679392578333868,
      "loss": 3.8802,
      "step": 1500
    },
    {
      "epoch": 0.4844401668270773,
      "grad_norm": 3.36200213432312,
      "learning_rate": 0.00016772537696503048,
      "loss": 4.1034,
      "step": 1510
    },
    {
      "epoch": 0.4876483798524222,
      "grad_norm": 2.2511489391326904,
      "learning_rate": 0.00016751149609667415,
      "loss": 3.9717,
      "step": 1520
    },
    {
      "epoch": 0.49085659287776706,
      "grad_norm": 2.5120437145233154,
      "learning_rate": 0.00016729761522831784,
      "loss": 4.1176,
      "step": 1530
    },
    {
      "epoch": 0.49406480590311197,
      "grad_norm": 2.5072154998779297,
      "learning_rate": 0.0001670837343599615,
      "loss": 3.8791,
      "step": 1540
    },
    {
      "epoch": 0.4972730189284568,
      "grad_norm": 1.8302147388458252,
      "learning_rate": 0.00016686985349160518,
      "loss": 4.0217,
      "step": 1550
    },
    {
      "epoch": 0.5004812319538018,
      "grad_norm": 1.7831693887710571,
      "learning_rate": 0.00016665597262324885,
      "loss": 3.6792,
      "step": 1560
    },
    {
      "epoch": 0.5036894449791466,
      "grad_norm": 2.4997243881225586,
      "learning_rate": 0.00016644209175489255,
      "loss": 4.2341,
      "step": 1570
    },
    {
      "epoch": 0.5068976580044915,
      "grad_norm": 2.099461793899536,
      "learning_rate": 0.00016622821088653622,
      "loss": 3.9337,
      "step": 1580
    },
    {
      "epoch": 0.5101058710298364,
      "grad_norm": 2.021148920059204,
      "learning_rate": 0.00016601433001817988,
      "loss": 4.0557,
      "step": 1590
    },
    {
      "epoch": 0.5133140840551813,
      "grad_norm": 2.2476346492767334,
      "learning_rate": 0.00016580044914982355,
      "loss": 3.923,
      "step": 1600
    },
    {
      "epoch": 0.5165222970805261,
      "grad_norm": 1.8008980751037598,
      "learning_rate": 0.00016558656828146722,
      "loss": 4.0472,
      "step": 1610
    },
    {
      "epoch": 0.519730510105871,
      "grad_norm": 1.378929615020752,
      "learning_rate": 0.00016537268741311092,
      "loss": 3.8261,
      "step": 1620
    },
    {
      "epoch": 0.5229387231312159,
      "grad_norm": 3.3628547191619873,
      "learning_rate": 0.00016515880654475456,
      "loss": 3.8221,
      "step": 1630
    },
    {
      "epoch": 0.5261469361565608,
      "grad_norm": 1.7510969638824463,
      "learning_rate": 0.00016494492567639826,
      "loss": 3.7721,
      "step": 1640
    },
    {
      "epoch": 0.5293551491819056,
      "grad_norm": 2.7384185791015625,
      "learning_rate": 0.00016473104480804193,
      "loss": 4.1257,
      "step": 1650
    },
    {
      "epoch": 0.5325633622072505,
      "grad_norm": 2.424494743347168,
      "learning_rate": 0.0001645171639396856,
      "loss": 3.8065,
      "step": 1660
    },
    {
      "epoch": 0.5357715752325954,
      "grad_norm": 2.2401938438415527,
      "learning_rate": 0.00016430328307132926,
      "loss": 3.8567,
      "step": 1670
    },
    {
      "epoch": 0.5389797882579404,
      "grad_norm": 3.1622910499572754,
      "learning_rate": 0.00016408940220297296,
      "loss": 3.6804,
      "step": 1680
    },
    {
      "epoch": 0.5421880012832853,
      "grad_norm": 2.1364176273345947,
      "learning_rate": 0.00016387552133461663,
      "loss": 3.9392,
      "step": 1690
    },
    {
      "epoch": 0.5453962143086301,
      "grad_norm": 1.8318675756454468,
      "learning_rate": 0.0001636616404662603,
      "loss": 3.7562,
      "step": 1700
    },
    {
      "epoch": 0.548604427333975,
      "grad_norm": 2.373115062713623,
      "learning_rate": 0.00016344775959790397,
      "loss": 3.781,
      "step": 1710
    },
    {
      "epoch": 0.5518126403593199,
      "grad_norm": 2.475308418273926,
      "learning_rate": 0.00016323387872954766,
      "loss": 4.0115,
      "step": 1720
    },
    {
      "epoch": 0.5550208533846648,
      "grad_norm": 5.762182712554932,
      "learning_rate": 0.00016301999786119133,
      "loss": 3.9147,
      "step": 1730
    },
    {
      "epoch": 0.5582290664100096,
      "grad_norm": 1.8071191310882568,
      "learning_rate": 0.000162806116992835,
      "loss": 4.1507,
      "step": 1740
    },
    {
      "epoch": 0.5614372794353545,
      "grad_norm": 1.9019110202789307,
      "learning_rate": 0.00016259223612447867,
      "loss": 4.1418,
      "step": 1750
    },
    {
      "epoch": 0.5646454924606994,
      "grad_norm": 1.995972752571106,
      "learning_rate": 0.00016237835525612234,
      "loss": 3.8719,
      "step": 1760
    },
    {
      "epoch": 0.5678537054860443,
      "grad_norm": 2.027113437652588,
      "learning_rate": 0.00016216447438776604,
      "loss": 3.9047,
      "step": 1770
    },
    {
      "epoch": 0.5710619185113892,
      "grad_norm": 1.954819679260254,
      "learning_rate": 0.00016195059351940968,
      "loss": 4.1468,
      "step": 1780
    },
    {
      "epoch": 0.574270131536734,
      "grad_norm": 2.091275930404663,
      "learning_rate": 0.00016173671265105337,
      "loss": 3.9813,
      "step": 1790
    },
    {
      "epoch": 0.5774783445620789,
      "grad_norm": 1.6633645296096802,
      "learning_rate": 0.00016152283178269704,
      "loss": 3.777,
      "step": 1800
    },
    {
      "epoch": 0.5806865575874238,
      "grad_norm": 2.522005796432495,
      "learning_rate": 0.00016130895091434074,
      "loss": 3.9359,
      "step": 1810
    },
    {
      "epoch": 0.5838947706127687,
      "grad_norm": 2.0835964679718018,
      "learning_rate": 0.00016109507004598438,
      "loss": 3.9168,
      "step": 1820
    },
    {
      "epoch": 0.5871029836381135,
      "grad_norm": 2.7929747104644775,
      "learning_rate": 0.00016088118917762808,
      "loss": 4.0131,
      "step": 1830
    },
    {
      "epoch": 0.5903111966634584,
      "grad_norm": 1.7257986068725586,
      "learning_rate": 0.00016066730830927175,
      "loss": 3.9439,
      "step": 1840
    },
    {
      "epoch": 0.5935194096888033,
      "grad_norm": 2.0654265880584717,
      "learning_rate": 0.00016045342744091542,
      "loss": 4.1118,
      "step": 1850
    },
    {
      "epoch": 0.5967276227141483,
      "grad_norm": 2.8161866664886475,
      "learning_rate": 0.00016023954657255909,
      "loss": 3.9887,
      "step": 1860
    },
    {
      "epoch": 0.599935835739493,
      "grad_norm": 2.5455033779144287,
      "learning_rate": 0.00016002566570420275,
      "loss": 4.0024,
      "step": 1870
    },
    {
      "epoch": 0.603144048764838,
      "grad_norm": 1.7840386629104614,
      "learning_rate": 0.00015981178483584645,
      "loss": 4.0116,
      "step": 1880
    },
    {
      "epoch": 0.6063522617901829,
      "grad_norm": 3.0625340938568115,
      "learning_rate": 0.00015959790396749012,
      "loss": 3.964,
      "step": 1890
    },
    {
      "epoch": 0.6095604748155278,
      "grad_norm": 2.045647144317627,
      "learning_rate": 0.0001593840230991338,
      "loss": 3.8874,
      "step": 1900
    },
    {
      "epoch": 0.6127686878408727,
      "grad_norm": 1.9196821451187134,
      "learning_rate": 0.00015917014223077746,
      "loss": 3.7512,
      "step": 1910
    },
    {
      "epoch": 0.6159769008662175,
      "grad_norm": 3.1822710037231445,
      "learning_rate": 0.00015895626136242115,
      "loss": 3.853,
      "step": 1920
    },
    {
      "epoch": 0.6191851138915624,
      "grad_norm": 2.67498779296875,
      "learning_rate": 0.00015874238049406482,
      "loss": 3.8842,
      "step": 1930
    },
    {
      "epoch": 0.6223933269169073,
      "grad_norm": 1.5589439868927002,
      "learning_rate": 0.0001585284996257085,
      "loss": 4.1475,
      "step": 1940
    },
    {
      "epoch": 0.6256015399422522,
      "grad_norm": 1.8868211507797241,
      "learning_rate": 0.00015831461875735216,
      "loss": 4.0337,
      "step": 1950
    },
    {
      "epoch": 0.628809752967597,
      "grad_norm": 2.9136695861816406,
      "learning_rate": 0.00015810073788899583,
      "loss": 3.7854,
      "step": 1960
    },
    {
      "epoch": 0.6320179659929419,
      "grad_norm": 2.1526665687561035,
      "learning_rate": 0.0001578868570206395,
      "loss": 4.027,
      "step": 1970
    },
    {
      "epoch": 0.6352261790182868,
      "grad_norm": 2.4602677822113037,
      "learning_rate": 0.00015767297615228317,
      "loss": 3.9006,
      "step": 1980
    },
    {
      "epoch": 0.6384343920436317,
      "grad_norm": 2.325958728790283,
      "learning_rate": 0.00015745909528392686,
      "loss": 3.9056,
      "step": 1990
    },
    {
      "epoch": 0.6416426050689766,
      "grad_norm": 1.8653922080993652,
      "learning_rate": 0.00015724521441557053,
      "loss": 4.0351,
      "step": 2000
    },
    {
      "epoch": 0.6448508180943214,
      "grad_norm": 2.088768243789673,
      "learning_rate": 0.0001570313335472142,
      "loss": 3.7137,
      "step": 2010
    },
    {
      "epoch": 0.6480590311196663,
      "grad_norm": 1.9593974351882935,
      "learning_rate": 0.00015681745267885787,
      "loss": 3.7914,
      "step": 2020
    },
    {
      "epoch": 0.6512672441450112,
      "grad_norm": 2.5103952884674072,
      "learning_rate": 0.00015660357181050157,
      "loss": 3.891,
      "step": 2030
    },
    {
      "epoch": 0.6544754571703562,
      "grad_norm": 1.492457628250122,
      "learning_rate": 0.00015638969094214524,
      "loss": 3.9842,
      "step": 2040
    },
    {
      "epoch": 0.657683670195701,
      "grad_norm": 2.515587091445923,
      "learning_rate": 0.0001561758100737889,
      "loss": 4.0309,
      "step": 2050
    },
    {
      "epoch": 0.6608918832210459,
      "grad_norm": 2.2142090797424316,
      "learning_rate": 0.00015596192920543258,
      "loss": 4.1022,
      "step": 2060
    },
    {
      "epoch": 0.6641000962463908,
      "grad_norm": 2.2114717960357666,
      "learning_rate": 0.00015574804833707624,
      "loss": 3.9963,
      "step": 2070
    },
    {
      "epoch": 0.6673083092717357,
      "grad_norm": 2.033418655395508,
      "learning_rate": 0.00015553416746871994,
      "loss": 4.1751,
      "step": 2080
    },
    {
      "epoch": 0.6705165222970805,
      "grad_norm": 1.9952499866485596,
      "learning_rate": 0.00015532028660036358,
      "loss": 3.8765,
      "step": 2090
    },
    {
      "epoch": 0.6737247353224254,
      "grad_norm": 3.195073366165161,
      "learning_rate": 0.00015510640573200728,
      "loss": 3.7897,
      "step": 2100
    },
    {
      "epoch": 0.6769329483477703,
      "grad_norm": 1.7830086946487427,
      "learning_rate": 0.00015489252486365095,
      "loss": 3.8283,
      "step": 2110
    },
    {
      "epoch": 0.6801411613731152,
      "grad_norm": 1.6678766012191772,
      "learning_rate": 0.00015467864399529464,
      "loss": 4.0029,
      "step": 2120
    },
    {
      "epoch": 0.6833493743984601,
      "grad_norm": 3.0071027278900146,
      "learning_rate": 0.00015446476312693829,
      "loss": 3.8203,
      "step": 2130
    },
    {
      "epoch": 0.6865575874238049,
      "grad_norm": 1.9064921140670776,
      "learning_rate": 0.00015425088225858198,
      "loss": 3.7773,
      "step": 2140
    },
    {
      "epoch": 0.6897658004491498,
      "grad_norm": 1.9289793968200684,
      "learning_rate": 0.00015403700139022565,
      "loss": 3.7336,
      "step": 2150
    },
    {
      "epoch": 0.6929740134744947,
      "grad_norm": 2.277463436126709,
      "learning_rate": 0.00015382312052186935,
      "loss": 3.9426,
      "step": 2160
    },
    {
      "epoch": 0.6961822264998396,
      "grad_norm": 2.0057101249694824,
      "learning_rate": 0.000153609239653513,
      "loss": 4.0449,
      "step": 2170
    },
    {
      "epoch": 0.6993904395251844,
      "grad_norm": 2.138139009475708,
      "learning_rate": 0.00015339535878515669,
      "loss": 3.973,
      "step": 2180
    },
    {
      "epoch": 0.7025986525505293,
      "grad_norm": 2.481306791305542,
      "learning_rate": 0.00015318147791680035,
      "loss": 3.8689,
      "step": 2190
    },
    {
      "epoch": 0.7058068655758742,
      "grad_norm": 1.8604938983917236,
      "learning_rate": 0.00015296759704844402,
      "loss": 3.8348,
      "step": 2200
    },
    {
      "epoch": 0.7090150786012192,
      "grad_norm": 3.08248233795166,
      "learning_rate": 0.0001527537161800877,
      "loss": 3.9962,
      "step": 2210
    },
    {
      "epoch": 0.7122232916265641,
      "grad_norm": 3.3053228855133057,
      "learning_rate": 0.00015253983531173136,
      "loss": 4.1355,
      "step": 2220
    },
    {
      "epoch": 0.7154315046519089,
      "grad_norm": 1.9488270282745361,
      "learning_rate": 0.00015232595444337506,
      "loss": 4.2517,
      "step": 2230
    },
    {
      "epoch": 0.7186397176772538,
      "grad_norm": 1.8827457427978516,
      "learning_rate": 0.00015211207357501873,
      "loss": 3.9881,
      "step": 2240
    },
    {
      "epoch": 0.7218479307025987,
      "grad_norm": 1.871275782585144,
      "learning_rate": 0.0001518981927066624,
      "loss": 3.9882,
      "step": 2250
    },
    {
      "epoch": 0.7250561437279436,
      "grad_norm": 3.001356840133667,
      "learning_rate": 0.00015168431183830607,
      "loss": 3.9887,
      "step": 2260
    },
    {
      "epoch": 0.7282643567532884,
      "grad_norm": 2.8097801208496094,
      "learning_rate": 0.00015147043096994976,
      "loss": 3.9143,
      "step": 2270
    },
    {
      "epoch": 0.7314725697786333,
      "grad_norm": 1.9800089597702026,
      "learning_rate": 0.00015125655010159343,
      "loss": 4.1458,
      "step": 2280
    },
    {
      "epoch": 0.7346807828039782,
      "grad_norm": 1.96299409866333,
      "learning_rate": 0.0001510426692332371,
      "loss": 4.0853,
      "step": 2290
    },
    {
      "epoch": 0.7378889958293231,
      "grad_norm": 2.354498863220215,
      "learning_rate": 0.00015082878836488077,
      "loss": 3.8417,
      "step": 2300
    },
    {
      "epoch": 0.7410972088546679,
      "grad_norm": 2.176910877227783,
      "learning_rate": 0.00015061490749652444,
      "loss": 3.8521,
      "step": 2310
    },
    {
      "epoch": 0.7443054218800128,
      "grad_norm": 1.8055734634399414,
      "learning_rate": 0.0001504010266281681,
      "loss": 3.9303,
      "step": 2320
    },
    {
      "epoch": 0.7475136349053577,
      "grad_norm": 1.7917792797088623,
      "learning_rate": 0.00015018714575981178,
      "loss": 3.738,
      "step": 2330
    },
    {
      "epoch": 0.7507218479307026,
      "grad_norm": 2.117816686630249,
      "learning_rate": 0.00014997326489145547,
      "loss": 4.048,
      "step": 2340
    },
    {
      "epoch": 0.7539300609560475,
      "grad_norm": 1.8879636526107788,
      "learning_rate": 0.00014975938402309914,
      "loss": 3.6788,
      "step": 2350
    },
    {
      "epoch": 0.7571382739813923,
      "grad_norm": 3.0844225883483887,
      "learning_rate": 0.0001495455031547428,
      "loss": 3.7891,
      "step": 2360
    },
    {
      "epoch": 0.7603464870067372,
      "grad_norm": 2.077103853225708,
      "learning_rate": 0.00014933162228638648,
      "loss": 3.9988,
      "step": 2370
    },
    {
      "epoch": 0.7635547000320821,
      "grad_norm": 2.6136064529418945,
      "learning_rate": 0.00014911774141803018,
      "loss": 3.7669,
      "step": 2380
    },
    {
      "epoch": 0.766762913057427,
      "grad_norm": 1.6826047897338867,
      "learning_rate": 0.00014890386054967385,
      "loss": 3.8013,
      "step": 2390
    },
    {
      "epoch": 0.7699711260827719,
      "grad_norm": 2.2695109844207764,
      "learning_rate": 0.00014868997968131751,
      "loss": 4.1352,
      "step": 2400
    },
    {
      "epoch": 0.7731793391081168,
      "grad_norm": 1.9605375528335571,
      "learning_rate": 0.00014847609881296118,
      "loss": 3.945,
      "step": 2410
    },
    {
      "epoch": 0.7763875521334617,
      "grad_norm": 1.8378612995147705,
      "learning_rate": 0.00014826221794460485,
      "loss": 4.0383,
      "step": 2420
    },
    {
      "epoch": 0.7795957651588066,
      "grad_norm": 2.031574010848999,
      "learning_rate": 0.00014804833707624855,
      "loss": 3.8376,
      "step": 2430
    },
    {
      "epoch": 0.7828039781841514,
      "grad_norm": 1.9898104667663574,
      "learning_rate": 0.0001478344562078922,
      "loss": 3.8931,
      "step": 2440
    },
    {
      "epoch": 0.7860121912094963,
      "grad_norm": 2.859266996383667,
      "learning_rate": 0.0001476205753395359,
      "loss": 4.0024,
      "step": 2450
    },
    {
      "epoch": 0.7892204042348412,
      "grad_norm": 2.4702200889587402,
      "learning_rate": 0.00014740669447117956,
      "loss": 3.7418,
      "step": 2460
    },
    {
      "epoch": 0.7924286172601861,
      "grad_norm": 1.877955675125122,
      "learning_rate": 0.00014719281360282325,
      "loss": 4.0296,
      "step": 2470
    },
    {
      "epoch": 0.795636830285531,
      "grad_norm": 2.0829124450683594,
      "learning_rate": 0.0001469789327344669,
      "loss": 3.919,
      "step": 2480
    },
    {
      "epoch": 0.7988450433108758,
      "grad_norm": 3.3244564533233643,
      "learning_rate": 0.0001467650518661106,
      "loss": 4.045,
      "step": 2490
    },
    {
      "epoch": 0.8020532563362207,
      "grad_norm": 2.064141035079956,
      "learning_rate": 0.00014655117099775426,
      "loss": 3.9802,
      "step": 2500
    }
  ],
  "logging_steps": 10,
  "max_steps": 9351,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 655495004160000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
