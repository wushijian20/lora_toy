{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.122874558870709,
  "eval_steps": 500,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003208213025344883,
      "grad_norm": 1.1983283758163452,
      "learning_rate": 0.00019980750721847933,
      "loss": 6.5784,
      "step": 10
    },
    {
      "epoch": 0.006416426050689766,
      "grad_norm": 2.383748769760132,
      "learning_rate": 0.000199593626350123,
      "loss": 6.5244,
      "step": 20
    },
    {
      "epoch": 0.009624639076034648,
      "grad_norm": 1.4066181182861328,
      "learning_rate": 0.00019937974548176667,
      "loss": 6.1228,
      "step": 30
    },
    {
      "epoch": 0.012832852101379532,
      "grad_norm": 1.604810118675232,
      "learning_rate": 0.00019916586461341034,
      "loss": 5.4314,
      "step": 40
    },
    {
      "epoch": 0.016041065126724416,
      "grad_norm": 1.9869211912155151,
      "learning_rate": 0.000198951983745054,
      "loss": 5.0872,
      "step": 50
    },
    {
      "epoch": 0.019249278152069296,
      "grad_norm": 2.957611083984375,
      "learning_rate": 0.0001987381028766977,
      "loss": 4.9137,
      "step": 60
    },
    {
      "epoch": 0.02245749117741418,
      "grad_norm": 1.4865000247955322,
      "learning_rate": 0.00019852422200834134,
      "loss": 5.0434,
      "step": 70
    },
    {
      "epoch": 0.025665704202759064,
      "grad_norm": 2.3109586238861084,
      "learning_rate": 0.00019831034113998504,
      "loss": 4.7802,
      "step": 80
    },
    {
      "epoch": 0.028873917228103944,
      "grad_norm": 1.243062138557434,
      "learning_rate": 0.0001980964602716287,
      "loss": 4.663,
      "step": 90
    },
    {
      "epoch": 0.03208213025344883,
      "grad_norm": 1.3511428833007812,
      "learning_rate": 0.0001978825794032724,
      "loss": 4.6094,
      "step": 100
    },
    {
      "epoch": 0.03529034327879371,
      "grad_norm": 1.9429733753204346,
      "learning_rate": 0.00019766869853491605,
      "loss": 4.5744,
      "step": 110
    },
    {
      "epoch": 0.03849855630413859,
      "grad_norm": 1.392471432685852,
      "learning_rate": 0.00019745481766655974,
      "loss": 4.3532,
      "step": 120
    },
    {
      "epoch": 0.04170676932948348,
      "grad_norm": 1.6916451454162598,
      "learning_rate": 0.0001972409367982034,
      "loss": 4.4809,
      "step": 130
    },
    {
      "epoch": 0.04491498235482836,
      "grad_norm": 1.2407877445220947,
      "learning_rate": 0.00019702705592984708,
      "loss": 4.2253,
      "step": 140
    },
    {
      "epoch": 0.04812319538017324,
      "grad_norm": 1.8410860300064087,
      "learning_rate": 0.00019681317506149075,
      "loss": 4.466,
      "step": 150
    },
    {
      "epoch": 0.05133140840551813,
      "grad_norm": 1.4824504852294922,
      "learning_rate": 0.00019659929419313442,
      "loss": 4.556,
      "step": 160
    },
    {
      "epoch": 0.05453962143086301,
      "grad_norm": 1.5274494886398315,
      "learning_rate": 0.00019638541332477811,
      "loss": 4.3291,
      "step": 170
    },
    {
      "epoch": 0.05774783445620789,
      "grad_norm": 1.708851933479309,
      "learning_rate": 0.00019617153245642178,
      "loss": 4.2644,
      "step": 180
    },
    {
      "epoch": 0.060956047481552776,
      "grad_norm": 1.5556572675704956,
      "learning_rate": 0.00019595765158806545,
      "loss": 4.2656,
      "step": 190
    },
    {
      "epoch": 0.06416426050689766,
      "grad_norm": 1.3052595853805542,
      "learning_rate": 0.00019574377071970912,
      "loss": 4.515,
      "step": 200
    },
    {
      "epoch": 0.06737247353224254,
      "grad_norm": 2.4464001655578613,
      "learning_rate": 0.00019552988985135282,
      "loss": 4.4356,
      "step": 210
    },
    {
      "epoch": 0.07058068655758742,
      "grad_norm": 1.5678684711456299,
      "learning_rate": 0.0001953160089829965,
      "loss": 4.4698,
      "step": 220
    },
    {
      "epoch": 0.0737888995829323,
      "grad_norm": 1.6955628395080566,
      "learning_rate": 0.00019510212811464016,
      "loss": 4.3303,
      "step": 230
    },
    {
      "epoch": 0.07699711260827719,
      "grad_norm": 3.168471336364746,
      "learning_rate": 0.00019488824724628383,
      "loss": 4.3839,
      "step": 240
    },
    {
      "epoch": 0.08020532563362208,
      "grad_norm": 2.1055126190185547,
      "learning_rate": 0.00019467436637792752,
      "loss": 4.1951,
      "step": 250
    },
    {
      "epoch": 0.08341353865896696,
      "grad_norm": 2.154214859008789,
      "learning_rate": 0.0001944604855095712,
      "loss": 4.2911,
      "step": 260
    },
    {
      "epoch": 0.08662175168431184,
      "grad_norm": 2.1030399799346924,
      "learning_rate": 0.00019424660464121486,
      "loss": 4.1543,
      "step": 270
    },
    {
      "epoch": 0.08982996470965672,
      "grad_norm": 2.408141851425171,
      "learning_rate": 0.00019403272377285853,
      "loss": 4.1835,
      "step": 280
    },
    {
      "epoch": 0.0930381777350016,
      "grad_norm": 1.7365314960479736,
      "learning_rate": 0.0001938188429045022,
      "loss": 4.356,
      "step": 290
    },
    {
      "epoch": 0.09624639076034648,
      "grad_norm": 2.310044765472412,
      "learning_rate": 0.0001936049620361459,
      "loss": 4.3094,
      "step": 300
    },
    {
      "epoch": 0.09945460378569138,
      "grad_norm": 2.2792177200317383,
      "learning_rate": 0.00019339108116778954,
      "loss": 4.344,
      "step": 310
    },
    {
      "epoch": 0.10266281681103626,
      "grad_norm": 2.3968708515167236,
      "learning_rate": 0.00019317720029943323,
      "loss": 4.1977,
      "step": 320
    },
    {
      "epoch": 0.10587102983638114,
      "grad_norm": 1.9320766925811768,
      "learning_rate": 0.0001929633194310769,
      "loss": 4.228,
      "step": 330
    },
    {
      "epoch": 0.10907924286172602,
      "grad_norm": 1.6803889274597168,
      "learning_rate": 0.00019274943856272057,
      "loss": 4.05,
      "step": 340
    },
    {
      "epoch": 0.1122874558870709,
      "grad_norm": 1.8266921043395996,
      "learning_rate": 0.00019253555769436424,
      "loss": 4.1899,
      "step": 350
    },
    {
      "epoch": 0.11549566891241578,
      "grad_norm": 1.4962135553359985,
      "learning_rate": 0.00019232167682600794,
      "loss": 4.3269,
      "step": 360
    },
    {
      "epoch": 0.11870388193776067,
      "grad_norm": 2.7197048664093018,
      "learning_rate": 0.0001921077959576516,
      "loss": 4.1859,
      "step": 370
    },
    {
      "epoch": 0.12191209496310555,
      "grad_norm": 3.336015462875366,
      "learning_rate": 0.00019189391508929527,
      "loss": 4.1157,
      "step": 380
    },
    {
      "epoch": 0.12512030798845045,
      "grad_norm": 2.937793254852295,
      "learning_rate": 0.00019168003422093894,
      "loss": 4.2637,
      "step": 390
    },
    {
      "epoch": 0.12832852101379533,
      "grad_norm": 1.4884883165359497,
      "learning_rate": 0.0001914661533525826,
      "loss": 4.1511,
      "step": 400
    },
    {
      "epoch": 0.1315367340391402,
      "grad_norm": 1.9539333581924438,
      "learning_rate": 0.0001912522724842263,
      "loss": 3.9707,
      "step": 410
    },
    {
      "epoch": 0.1347449470644851,
      "grad_norm": 1.5094072818756104,
      "learning_rate": 0.00019103839161586995,
      "loss": 4.145,
      "step": 420
    },
    {
      "epoch": 0.13795316008982997,
      "grad_norm": 2.652632236480713,
      "learning_rate": 0.00019082451074751365,
      "loss": 4.1095,
      "step": 430
    },
    {
      "epoch": 0.14116137311517485,
      "grad_norm": 2.1157455444335938,
      "learning_rate": 0.00019061062987915732,
      "loss": 4.0097,
      "step": 440
    },
    {
      "epoch": 0.14436958614051973,
      "grad_norm": 2.671804189682007,
      "learning_rate": 0.000190396749010801,
      "loss": 4.2516,
      "step": 450
    },
    {
      "epoch": 0.1475777991658646,
      "grad_norm": 2.2349512577056885,
      "learning_rate": 0.00019018286814244465,
      "loss": 4.1692,
      "step": 460
    },
    {
      "epoch": 0.1507860121912095,
      "grad_norm": 1.4282678365707397,
      "learning_rate": 0.00018996898727408835,
      "loss": 4.1818,
      "step": 470
    },
    {
      "epoch": 0.15399422521655437,
      "grad_norm": 2.2927565574645996,
      "learning_rate": 0.00018975510640573202,
      "loss": 4.1962,
      "step": 480
    },
    {
      "epoch": 0.15720243824189925,
      "grad_norm": 2.240100622177124,
      "learning_rate": 0.0001895412255373757,
      "loss": 4.1023,
      "step": 490
    },
    {
      "epoch": 0.16041065126724416,
      "grad_norm": 1.7559913396835327,
      "learning_rate": 0.00018932734466901936,
      "loss": 4.2296,
      "step": 500
    },
    {
      "epoch": 0.16361886429258904,
      "grad_norm": 2.80961275100708,
      "learning_rate": 0.00018911346380066303,
      "loss": 4.2073,
      "step": 510
    },
    {
      "epoch": 0.16682707731793392,
      "grad_norm": 1.9017000198364258,
      "learning_rate": 0.00018889958293230672,
      "loss": 3.943,
      "step": 520
    },
    {
      "epoch": 0.1700352903432788,
      "grad_norm": 3.002676010131836,
      "learning_rate": 0.0001886857020639504,
      "loss": 4.1292,
      "step": 530
    },
    {
      "epoch": 0.17324350336862368,
      "grad_norm": 1.7197189331054688,
      "learning_rate": 0.00018847182119559406,
      "loss": 4.1951,
      "step": 540
    },
    {
      "epoch": 0.17645171639396856,
      "grad_norm": 1.8173481225967407,
      "learning_rate": 0.00018825794032723773,
      "loss": 4.2885,
      "step": 550
    },
    {
      "epoch": 0.17965992941931344,
      "grad_norm": 2.5451200008392334,
      "learning_rate": 0.00018804405945888143,
      "loss": 3.8283,
      "step": 560
    },
    {
      "epoch": 0.18286814244465832,
      "grad_norm": 4.116666316986084,
      "learning_rate": 0.0001878301785905251,
      "loss": 4.3159,
      "step": 570
    },
    {
      "epoch": 0.1860763554700032,
      "grad_norm": 2.4113287925720215,
      "learning_rate": 0.00018761629772216876,
      "loss": 4.1235,
      "step": 580
    },
    {
      "epoch": 0.18928456849534808,
      "grad_norm": 1.7203558683395386,
      "learning_rate": 0.00018740241685381243,
      "loss": 4.209,
      "step": 590
    },
    {
      "epoch": 0.19249278152069296,
      "grad_norm": 2.0295886993408203,
      "learning_rate": 0.0001871885359854561,
      "loss": 4.1689,
      "step": 600
    },
    {
      "epoch": 0.19570099454603784,
      "grad_norm": 2.5288290977478027,
      "learning_rate": 0.0001869746551170998,
      "loss": 3.7552,
      "step": 610
    },
    {
      "epoch": 0.19890920757138275,
      "grad_norm": 1.8706116676330566,
      "learning_rate": 0.00018676077424874344,
      "loss": 4.1645,
      "step": 620
    },
    {
      "epoch": 0.20211742059672763,
      "grad_norm": 2.057385206222534,
      "learning_rate": 0.00018654689338038714,
      "loss": 4.2275,
      "step": 630
    },
    {
      "epoch": 0.2053256336220725,
      "grad_norm": 2.0618972778320312,
      "learning_rate": 0.0001863330125120308,
      "loss": 4.325,
      "step": 640
    },
    {
      "epoch": 0.2085338466474174,
      "grad_norm": 1.9901732206344604,
      "learning_rate": 0.00018611913164367447,
      "loss": 3.9644,
      "step": 650
    },
    {
      "epoch": 0.21174205967276227,
      "grad_norm": 2.8751800060272217,
      "learning_rate": 0.00018590525077531814,
      "loss": 4.0262,
      "step": 660
    },
    {
      "epoch": 0.21495027269810715,
      "grad_norm": 2.0578691959381104,
      "learning_rate": 0.00018569136990696184,
      "loss": 4.1838,
      "step": 670
    },
    {
      "epoch": 0.21815848572345203,
      "grad_norm": 1.968585729598999,
      "learning_rate": 0.0001854774890386055,
      "loss": 3.9027,
      "step": 680
    },
    {
      "epoch": 0.22136669874879691,
      "grad_norm": 2.03605318069458,
      "learning_rate": 0.00018526360817024918,
      "loss": 4.0758,
      "step": 690
    },
    {
      "epoch": 0.2245749117741418,
      "grad_norm": 2.7659213542938232,
      "learning_rate": 0.00018504972730189285,
      "loss": 4.0117,
      "step": 700
    },
    {
      "epoch": 0.22778312479948667,
      "grad_norm": 1.7381958961486816,
      "learning_rate": 0.00018483584643353654,
      "loss": 3.7897,
      "step": 710
    },
    {
      "epoch": 0.23099133782483156,
      "grad_norm": 1.9161226749420166,
      "learning_rate": 0.0001846219655651802,
      "loss": 4.1692,
      "step": 720
    },
    {
      "epoch": 0.23419955085017646,
      "grad_norm": 1.9479641914367676,
      "learning_rate": 0.00018440808469682388,
      "loss": 3.9752,
      "step": 730
    },
    {
      "epoch": 0.23740776387552134,
      "grad_norm": 2.3413400650024414,
      "learning_rate": 0.00018419420382846755,
      "loss": 4.2622,
      "step": 740
    },
    {
      "epoch": 0.24061597690086622,
      "grad_norm": 1.9952948093414307,
      "learning_rate": 0.00018398032296011122,
      "loss": 4.1897,
      "step": 750
    },
    {
      "epoch": 0.2438241899262111,
      "grad_norm": 1.6951522827148438,
      "learning_rate": 0.00018376644209175492,
      "loss": 3.965,
      "step": 760
    },
    {
      "epoch": 0.24703240295155598,
      "grad_norm": 2.83673357963562,
      "learning_rate": 0.00018355256122339856,
      "loss": 4.2604,
      "step": 770
    },
    {
      "epoch": 0.2502406159769009,
      "grad_norm": 1.7921984195709229,
      "learning_rate": 0.00018333868035504225,
      "loss": 4.0102,
      "step": 780
    },
    {
      "epoch": 0.25344882900224575,
      "grad_norm": 3.1436421871185303,
      "learning_rate": 0.00018312479948668592,
      "loss": 4.1704,
      "step": 790
    },
    {
      "epoch": 0.25665704202759065,
      "grad_norm": 2.363903045654297,
      "learning_rate": 0.00018291091861832962,
      "loss": 4.0511,
      "step": 800
    },
    {
      "epoch": 0.2598652550529355,
      "grad_norm": 1.81502366065979,
      "learning_rate": 0.00018269703774997326,
      "loss": 3.9104,
      "step": 810
    },
    {
      "epoch": 0.2630734680782804,
      "grad_norm": 1.801508903503418,
      "learning_rate": 0.00018248315688161696,
      "loss": 3.996,
      "step": 820
    },
    {
      "epoch": 0.26628168110362527,
      "grad_norm": 2.337092399597168,
      "learning_rate": 0.00018226927601326063,
      "loss": 3.7473,
      "step": 830
    },
    {
      "epoch": 0.2694898941289702,
      "grad_norm": 1.9983611106872559,
      "learning_rate": 0.0001820553951449043,
      "loss": 4.2882,
      "step": 840
    },
    {
      "epoch": 0.27269810715431503,
      "grad_norm": 2.027909278869629,
      "learning_rate": 0.00018184151427654797,
      "loss": 4.2433,
      "step": 850
    },
    {
      "epoch": 0.27590632017965994,
      "grad_norm": 1.712869644165039,
      "learning_rate": 0.00018162763340819163,
      "loss": 3.8283,
      "step": 860
    },
    {
      "epoch": 0.2791145332050048,
      "grad_norm": 1.7736915349960327,
      "learning_rate": 0.00018141375253983533,
      "loss": 4.0457,
      "step": 870
    },
    {
      "epoch": 0.2823227462303497,
      "grad_norm": 2.2658841609954834,
      "learning_rate": 0.000181199871671479,
      "loss": 4.1728,
      "step": 880
    },
    {
      "epoch": 0.2855309592556946,
      "grad_norm": 2.3572957515716553,
      "learning_rate": 0.00018098599080312267,
      "loss": 4.005,
      "step": 890
    },
    {
      "epoch": 0.28873917228103946,
      "grad_norm": 4.014227390289307,
      "learning_rate": 0.00018077210993476634,
      "loss": 4.2235,
      "step": 900
    },
    {
      "epoch": 0.29194738530638437,
      "grad_norm": 2.143670082092285,
      "learning_rate": 0.00018055822906641003,
      "loss": 3.9217,
      "step": 910
    },
    {
      "epoch": 0.2951555983317292,
      "grad_norm": 2.538867950439453,
      "learning_rate": 0.0001803443481980537,
      "loss": 4.1954,
      "step": 920
    },
    {
      "epoch": 0.2983638113570741,
      "grad_norm": 1.6266535520553589,
      "learning_rate": 0.00018013046732969737,
      "loss": 4.242,
      "step": 930
    },
    {
      "epoch": 0.301572024382419,
      "grad_norm": 2.407832622528076,
      "learning_rate": 0.00017991658646134104,
      "loss": 3.8215,
      "step": 940
    },
    {
      "epoch": 0.3047802374077639,
      "grad_norm": 2.9120304584503174,
      "learning_rate": 0.0001797027055929847,
      "loss": 3.9595,
      "step": 950
    },
    {
      "epoch": 0.30798845043310874,
      "grad_norm": 2.097067356109619,
      "learning_rate": 0.0001794888247246284,
      "loss": 3.6518,
      "step": 960
    },
    {
      "epoch": 0.31119666345845365,
      "grad_norm": 1.8193588256835938,
      "learning_rate": 0.00017927494385627205,
      "loss": 3.8854,
      "step": 970
    },
    {
      "epoch": 0.3144048764837985,
      "grad_norm": 2.4384777545928955,
      "learning_rate": 0.00017906106298791574,
      "loss": 3.9523,
      "step": 980
    },
    {
      "epoch": 0.3176130895091434,
      "grad_norm": 2.7034900188446045,
      "learning_rate": 0.00017884718211955941,
      "loss": 4.1155,
      "step": 990
    },
    {
      "epoch": 0.3208213025344883,
      "grad_norm": 1.479897379875183,
      "learning_rate": 0.00017863330125120308,
      "loss": 4.0588,
      "step": 1000
    },
    {
      "epoch": 0.32402951555983317,
      "grad_norm": 1.9492558240890503,
      "learning_rate": 0.00017841942038284675,
      "loss": 4.0175,
      "step": 1010
    },
    {
      "epoch": 0.3272377285851781,
      "grad_norm": 1.9467204809188843,
      "learning_rate": 0.00017820553951449045,
      "loss": 4.1938,
      "step": 1020
    },
    {
      "epoch": 0.33044594161052293,
      "grad_norm": 2.906378984451294,
      "learning_rate": 0.00017799165864613412,
      "loss": 4.0278,
      "step": 1030
    },
    {
      "epoch": 0.33365415463586784,
      "grad_norm": 2.1511802673339844,
      "learning_rate": 0.00017777777777777779,
      "loss": 4.0039,
      "step": 1040
    },
    {
      "epoch": 0.3368623676612127,
      "grad_norm": 1.5978970527648926,
      "learning_rate": 0.00017756389690942146,
      "loss": 3.98,
      "step": 1050
    },
    {
      "epoch": 0.3400705806865576,
      "grad_norm": 1.5500731468200684,
      "learning_rate": 0.00017735001604106512,
      "loss": 4.0384,
      "step": 1060
    },
    {
      "epoch": 0.34327879371190245,
      "grad_norm": 2.749234676361084,
      "learning_rate": 0.00017713613517270882,
      "loss": 3.9761,
      "step": 1070
    },
    {
      "epoch": 0.34648700673724736,
      "grad_norm": 1.9572138786315918,
      "learning_rate": 0.00017692225430435246,
      "loss": 3.8539,
      "step": 1080
    },
    {
      "epoch": 0.3496952197625922,
      "grad_norm": 2.164808750152588,
      "learning_rate": 0.00017670837343599616,
      "loss": 4.0017,
      "step": 1090
    },
    {
      "epoch": 0.3529034327879371,
      "grad_norm": 1.5234178304672241,
      "learning_rate": 0.00017649449256763983,
      "loss": 4.0751,
      "step": 1100
    },
    {
      "epoch": 0.35611164581328203,
      "grad_norm": 2.439108371734619,
      "learning_rate": 0.00017628061169928352,
      "loss": 3.9935,
      "step": 1110
    },
    {
      "epoch": 0.3593198588386269,
      "grad_norm": 1.7081377506256104,
      "learning_rate": 0.00017606673083092717,
      "loss": 3.9497,
      "step": 1120
    },
    {
      "epoch": 0.3625280718639718,
      "grad_norm": 2.918872356414795,
      "learning_rate": 0.00017585284996257086,
      "loss": 3.8335,
      "step": 1130
    },
    {
      "epoch": 0.36573628488931664,
      "grad_norm": 2.6193735599517822,
      "learning_rate": 0.00017563896909421453,
      "loss": 3.9514,
      "step": 1140
    },
    {
      "epoch": 0.36894449791466155,
      "grad_norm": 2.0638182163238525,
      "learning_rate": 0.0001754250882258582,
      "loss": 4.0007,
      "step": 1150
    },
    {
      "epoch": 0.3721527109400064,
      "grad_norm": 2.0979418754577637,
      "learning_rate": 0.00017521120735750187,
      "loss": 4.2207,
      "step": 1160
    },
    {
      "epoch": 0.3753609239653513,
      "grad_norm": 1.6236650943756104,
      "learning_rate": 0.00017499732648914554,
      "loss": 4.1278,
      "step": 1170
    },
    {
      "epoch": 0.37856913699069616,
      "grad_norm": 1.4541728496551514,
      "learning_rate": 0.00017478344562078923,
      "loss": 4.0222,
      "step": 1180
    },
    {
      "epoch": 0.3817773500160411,
      "grad_norm": 2.231041193008423,
      "learning_rate": 0.0001745695647524329,
      "loss": 3.7468,
      "step": 1190
    },
    {
      "epoch": 0.3849855630413859,
      "grad_norm": 2.645289659500122,
      "learning_rate": 0.00017435568388407657,
      "loss": 3.7373,
      "step": 1200
    },
    {
      "epoch": 0.38819377606673083,
      "grad_norm": 2.5603702068328857,
      "learning_rate": 0.00017414180301572024,
      "loss": 3.8212,
      "step": 1210
    },
    {
      "epoch": 0.3914019890920757,
      "grad_norm": 2.0443761348724365,
      "learning_rate": 0.00017392792214736394,
      "loss": 4.0386,
      "step": 1220
    },
    {
      "epoch": 0.3946102021174206,
      "grad_norm": 1.8223211765289307,
      "learning_rate": 0.0001737140412790076,
      "loss": 4.0416,
      "step": 1230
    },
    {
      "epoch": 0.3978184151427655,
      "grad_norm": 3.0214056968688965,
      "learning_rate": 0.00017350016041065128,
      "loss": 3.937,
      "step": 1240
    },
    {
      "epoch": 0.40102662816811036,
      "grad_norm": 1.8569966554641724,
      "learning_rate": 0.00017328627954229495,
      "loss": 4.1451,
      "step": 1250
    },
    {
      "epoch": 0.40423484119345526,
      "grad_norm": 2.539517879486084,
      "learning_rate": 0.00017307239867393864,
      "loss": 4.0165,
      "step": 1260
    },
    {
      "epoch": 0.4074430542188001,
      "grad_norm": 2.017457962036133,
      "learning_rate": 0.0001728585178055823,
      "loss": 3.8963,
      "step": 1270
    },
    {
      "epoch": 0.410651267244145,
      "grad_norm": 2.2230031490325928,
      "learning_rate": 0.00017264463693722598,
      "loss": 3.8815,
      "step": 1280
    },
    {
      "epoch": 0.4138594802694899,
      "grad_norm": 1.9695563316345215,
      "learning_rate": 0.00017243075606886965,
      "loss": 3.8648,
      "step": 1290
    },
    {
      "epoch": 0.4170676932948348,
      "grad_norm": 1.8194799423217773,
      "learning_rate": 0.00017221687520051332,
      "loss": 4.145,
      "step": 1300
    },
    {
      "epoch": 0.42027590632017964,
      "grad_norm": 2.072035074234009,
      "learning_rate": 0.000172002994332157,
      "loss": 3.8141,
      "step": 1310
    },
    {
      "epoch": 0.42348411934552455,
      "grad_norm": 1.9647324085235596,
      "learning_rate": 0.00017178911346380066,
      "loss": 4.1425,
      "step": 1320
    },
    {
      "epoch": 0.4266923323708694,
      "grad_norm": 2.0801777839660645,
      "learning_rate": 0.00017157523259544435,
      "loss": 4.0883,
      "step": 1330
    },
    {
      "epoch": 0.4299005453962143,
      "grad_norm": 1.7603702545166016,
      "learning_rate": 0.00017136135172708802,
      "loss": 3.8815,
      "step": 1340
    },
    {
      "epoch": 0.4331087584215592,
      "grad_norm": 2.200935125350952,
      "learning_rate": 0.0001711474708587317,
      "loss": 3.7951,
      "step": 1350
    },
    {
      "epoch": 0.43631697144690407,
      "grad_norm": 2.3795740604400635,
      "learning_rate": 0.00017093358999037536,
      "loss": 4.031,
      "step": 1360
    },
    {
      "epoch": 0.439525184472249,
      "grad_norm": 3.0328521728515625,
      "learning_rate": 0.00017071970912201906,
      "loss": 3.9536,
      "step": 1370
    },
    {
      "epoch": 0.44273339749759383,
      "grad_norm": 1.859718680381775,
      "learning_rate": 0.00017050582825366273,
      "loss": 4.1404,
      "step": 1380
    },
    {
      "epoch": 0.44594161052293874,
      "grad_norm": 1.8813097476959229,
      "learning_rate": 0.0001702919473853064,
      "loss": 4.068,
      "step": 1390
    },
    {
      "epoch": 0.4491498235482836,
      "grad_norm": 1.587493658065796,
      "learning_rate": 0.00017007806651695006,
      "loss": 3.7291,
      "step": 1400
    },
    {
      "epoch": 0.4523580365736285,
      "grad_norm": 2.6667652130126953,
      "learning_rate": 0.00016986418564859373,
      "loss": 4.1571,
      "step": 1410
    },
    {
      "epoch": 0.45556624959897335,
      "grad_norm": 2.054722785949707,
      "learning_rate": 0.00016965030478023743,
      "loss": 3.5774,
      "step": 1420
    },
    {
      "epoch": 0.45877446262431826,
      "grad_norm": 2.0262043476104736,
      "learning_rate": 0.00016943642391188107,
      "loss": 3.9615,
      "step": 1430
    },
    {
      "epoch": 0.4619826756496631,
      "grad_norm": 2.4865529537200928,
      "learning_rate": 0.00016922254304352477,
      "loss": 3.8782,
      "step": 1440
    },
    {
      "epoch": 0.465190888675008,
      "grad_norm": 1.8818535804748535,
      "learning_rate": 0.00016900866217516844,
      "loss": 4.1086,
      "step": 1450
    },
    {
      "epoch": 0.4683991017003529,
      "grad_norm": 2.109370470046997,
      "learning_rate": 0.00016879478130681213,
      "loss": 3.9845,
      "step": 1460
    },
    {
      "epoch": 0.4716073147256978,
      "grad_norm": 1.8926252126693726,
      "learning_rate": 0.00016858090043845577,
      "loss": 4.1483,
      "step": 1470
    },
    {
      "epoch": 0.4748155277510427,
      "grad_norm": 1.878896951675415,
      "learning_rate": 0.00016836701957009947,
      "loss": 4.0156,
      "step": 1480
    },
    {
      "epoch": 0.47802374077638754,
      "grad_norm": 1.9727569818496704,
      "learning_rate": 0.00016815313870174314,
      "loss": 4.0899,
      "step": 1490
    },
    {
      "epoch": 0.48123195380173245,
      "grad_norm": 2.6462152004241943,
      "learning_rate": 0.0001679392578333868,
      "loss": 4.0509,
      "step": 1500
    },
    {
      "epoch": 0.4844401668270773,
      "grad_norm": 2.851273536682129,
      "learning_rate": 0.00016772537696503048,
      "loss": 3.8336,
      "step": 1510
    },
    {
      "epoch": 0.4876483798524222,
      "grad_norm": 2.1313538551330566,
      "learning_rate": 0.00016751149609667415,
      "loss": 4.0849,
      "step": 1520
    },
    {
      "epoch": 0.49085659287776706,
      "grad_norm": 1.9736741781234741,
      "learning_rate": 0.00016729761522831784,
      "loss": 3.7586,
      "step": 1530
    },
    {
      "epoch": 0.49406480590311197,
      "grad_norm": 1.7775824069976807,
      "learning_rate": 0.0001670837343599615,
      "loss": 3.7514,
      "step": 1540
    },
    {
      "epoch": 0.4972730189284568,
      "grad_norm": 2.7223961353302,
      "learning_rate": 0.00016686985349160518,
      "loss": 3.8098,
      "step": 1550
    },
    {
      "epoch": 0.5004812319538018,
      "grad_norm": 2.2440030574798584,
      "learning_rate": 0.00016665597262324885,
      "loss": 4.2045,
      "step": 1560
    },
    {
      "epoch": 0.5036894449791466,
      "grad_norm": 2.0957484245300293,
      "learning_rate": 0.00016644209175489255,
      "loss": 4.0704,
      "step": 1570
    },
    {
      "epoch": 0.5068976580044915,
      "grad_norm": 2.3906004428863525,
      "learning_rate": 0.00016622821088653622,
      "loss": 4.0016,
      "step": 1580
    },
    {
      "epoch": 0.5101058710298364,
      "grad_norm": 2.331468105316162,
      "learning_rate": 0.00016601433001817988,
      "loss": 3.9734,
      "step": 1590
    },
    {
      "epoch": 0.5133140840551813,
      "grad_norm": 2.96667742729187,
      "learning_rate": 0.00016580044914982355,
      "loss": 4.0892,
      "step": 1600
    },
    {
      "epoch": 0.5165222970805261,
      "grad_norm": 2.109192371368408,
      "learning_rate": 0.00016558656828146722,
      "loss": 4.2457,
      "step": 1610
    },
    {
      "epoch": 0.519730510105871,
      "grad_norm": 2.1242024898529053,
      "learning_rate": 0.00016537268741311092,
      "loss": 4.0753,
      "step": 1620
    },
    {
      "epoch": 0.5229387231312159,
      "grad_norm": 3.969597101211548,
      "learning_rate": 0.00016515880654475456,
      "loss": 3.7931,
      "step": 1630
    },
    {
      "epoch": 0.5261469361565608,
      "grad_norm": 2.2417430877685547,
      "learning_rate": 0.00016494492567639826,
      "loss": 3.7502,
      "step": 1640
    },
    {
      "epoch": 0.5293551491819056,
      "grad_norm": 1.6081113815307617,
      "learning_rate": 0.00016473104480804193,
      "loss": 3.938,
      "step": 1650
    },
    {
      "epoch": 0.5325633622072505,
      "grad_norm": 1.715523362159729,
      "learning_rate": 0.0001645171639396856,
      "loss": 4.0128,
      "step": 1660
    },
    {
      "epoch": 0.5357715752325954,
      "grad_norm": 1.8839601278305054,
      "learning_rate": 0.00016430328307132926,
      "loss": 4.1002,
      "step": 1670
    },
    {
      "epoch": 0.5389797882579404,
      "grad_norm": 2.2105612754821777,
      "learning_rate": 0.00016408940220297296,
      "loss": 3.9223,
      "step": 1680
    },
    {
      "epoch": 0.5421880012832853,
      "grad_norm": 1.932026743888855,
      "learning_rate": 0.00016387552133461663,
      "loss": 3.9165,
      "step": 1690
    },
    {
      "epoch": 0.5453962143086301,
      "grad_norm": 2.1850948333740234,
      "learning_rate": 0.0001636616404662603,
      "loss": 4.0381,
      "step": 1700
    },
    {
      "epoch": 0.548604427333975,
      "grad_norm": 1.916129469871521,
      "learning_rate": 0.00016344775959790397,
      "loss": 3.8648,
      "step": 1710
    },
    {
      "epoch": 0.5518126403593199,
      "grad_norm": 2.288389205932617,
      "learning_rate": 0.00016323387872954766,
      "loss": 3.7637,
      "step": 1720
    },
    {
      "epoch": 0.5550208533846648,
      "grad_norm": 2.792354106903076,
      "learning_rate": 0.00016301999786119133,
      "loss": 3.9609,
      "step": 1730
    },
    {
      "epoch": 0.5582290664100096,
      "grad_norm": 2.032458543777466,
      "learning_rate": 0.000162806116992835,
      "loss": 3.9977,
      "step": 1740
    },
    {
      "epoch": 0.5614372794353545,
      "grad_norm": 2.5949947834014893,
      "learning_rate": 0.00016259223612447867,
      "loss": 4.0316,
      "step": 1750
    },
    {
      "epoch": 0.5646454924606994,
      "grad_norm": 2.6265125274658203,
      "learning_rate": 0.00016237835525612234,
      "loss": 4.035,
      "step": 1760
    },
    {
      "epoch": 0.5678537054860443,
      "grad_norm": 1.8597300052642822,
      "learning_rate": 0.00016216447438776604,
      "loss": 4.1642,
      "step": 1770
    },
    {
      "epoch": 0.5710619185113892,
      "grad_norm": 1.9310953617095947,
      "learning_rate": 0.00016195059351940968,
      "loss": 4.1972,
      "step": 1780
    },
    {
      "epoch": 0.574270131536734,
      "grad_norm": 2.0915284156799316,
      "learning_rate": 0.00016173671265105337,
      "loss": 3.9685,
      "step": 1790
    },
    {
      "epoch": 0.5774783445620789,
      "grad_norm": 2.076540946960449,
      "learning_rate": 0.00016152283178269704,
      "loss": 3.8043,
      "step": 1800
    },
    {
      "epoch": 0.5806865575874238,
      "grad_norm": 1.8908146619796753,
      "learning_rate": 0.00016130895091434074,
      "loss": 3.9737,
      "step": 1810
    },
    {
      "epoch": 0.5838947706127687,
      "grad_norm": 2.06009840965271,
      "learning_rate": 0.00016109507004598438,
      "loss": 3.9032,
      "step": 1820
    },
    {
      "epoch": 0.5871029836381135,
      "grad_norm": 2.1291112899780273,
      "learning_rate": 0.00016088118917762808,
      "loss": 3.5698,
      "step": 1830
    },
    {
      "epoch": 0.5903111966634584,
      "grad_norm": 2.436479091644287,
      "learning_rate": 0.00016066730830927175,
      "loss": 4.0396,
      "step": 1840
    },
    {
      "epoch": 0.5935194096888033,
      "grad_norm": 2.3087265491485596,
      "learning_rate": 0.00016045342744091542,
      "loss": 3.9245,
      "step": 1850
    },
    {
      "epoch": 0.5967276227141483,
      "grad_norm": 2.6207966804504395,
      "learning_rate": 0.00016023954657255909,
      "loss": 3.7199,
      "step": 1860
    },
    {
      "epoch": 0.599935835739493,
      "grad_norm": 1.785598635673523,
      "learning_rate": 0.00016002566570420275,
      "loss": 3.9345,
      "step": 1870
    },
    {
      "epoch": 0.603144048764838,
      "grad_norm": 2.423572301864624,
      "learning_rate": 0.00015981178483584645,
      "loss": 4.0457,
      "step": 1880
    },
    {
      "epoch": 0.6063522617901829,
      "grad_norm": 1.5407596826553345,
      "learning_rate": 0.00015959790396749012,
      "loss": 3.9038,
      "step": 1890
    },
    {
      "epoch": 0.6095604748155278,
      "grad_norm": 2.3923301696777344,
      "learning_rate": 0.0001593840230991338,
      "loss": 3.9484,
      "step": 1900
    },
    {
      "epoch": 0.6127686878408727,
      "grad_norm": 2.6278083324432373,
      "learning_rate": 0.00015917014223077746,
      "loss": 4.0543,
      "step": 1910
    },
    {
      "epoch": 0.6159769008662175,
      "grad_norm": 1.6903777122497559,
      "learning_rate": 0.00015895626136242115,
      "loss": 3.8655,
      "step": 1920
    },
    {
      "epoch": 0.6191851138915624,
      "grad_norm": 1.7126209735870361,
      "learning_rate": 0.00015874238049406482,
      "loss": 3.7907,
      "step": 1930
    },
    {
      "epoch": 0.6223933269169073,
      "grad_norm": 2.361161231994629,
      "learning_rate": 0.0001585284996257085,
      "loss": 4.2125,
      "step": 1940
    },
    {
      "epoch": 0.6256015399422522,
      "grad_norm": 2.215440034866333,
      "learning_rate": 0.00015831461875735216,
      "loss": 4.3307,
      "step": 1950
    },
    {
      "epoch": 0.628809752967597,
      "grad_norm": 2.700606107711792,
      "learning_rate": 0.00015810073788899583,
      "loss": 3.9673,
      "step": 1960
    },
    {
      "epoch": 0.6320179659929419,
      "grad_norm": 1.6652956008911133,
      "learning_rate": 0.0001578868570206395,
      "loss": 4.021,
      "step": 1970
    },
    {
      "epoch": 0.6352261790182868,
      "grad_norm": 2.1564223766326904,
      "learning_rate": 0.00015767297615228317,
      "loss": 3.9198,
      "step": 1980
    },
    {
      "epoch": 0.6384343920436317,
      "grad_norm": 3.2227423191070557,
      "learning_rate": 0.00015745909528392686,
      "loss": 3.9762,
      "step": 1990
    },
    {
      "epoch": 0.6416426050689766,
      "grad_norm": 2.7817091941833496,
      "learning_rate": 0.00015724521441557053,
      "loss": 3.9866,
      "step": 2000
    },
    {
      "epoch": 0.6448508180943214,
      "grad_norm": 1.99258291721344,
      "learning_rate": 0.0001570313335472142,
      "loss": 3.9344,
      "step": 2010
    },
    {
      "epoch": 0.6480590311196663,
      "grad_norm": 2.568507671356201,
      "learning_rate": 0.00015681745267885787,
      "loss": 4.1798,
      "step": 2020
    },
    {
      "epoch": 0.6512672441450112,
      "grad_norm": 2.0561373233795166,
      "learning_rate": 0.00015660357181050157,
      "loss": 4.152,
      "step": 2030
    },
    {
      "epoch": 0.6544754571703562,
      "grad_norm": 2.0449752807617188,
      "learning_rate": 0.00015638969094214524,
      "loss": 4.0113,
      "step": 2040
    },
    {
      "epoch": 0.657683670195701,
      "grad_norm": 2.040909767150879,
      "learning_rate": 0.0001561758100737889,
      "loss": 3.8777,
      "step": 2050
    },
    {
      "epoch": 0.6608918832210459,
      "grad_norm": 2.25301194190979,
      "learning_rate": 0.00015596192920543258,
      "loss": 3.9385,
      "step": 2060
    },
    {
      "epoch": 0.6641000962463908,
      "grad_norm": 2.4239563941955566,
      "learning_rate": 0.00015574804833707624,
      "loss": 3.8616,
      "step": 2070
    },
    {
      "epoch": 0.6673083092717357,
      "grad_norm": 2.8954689502716064,
      "learning_rate": 0.00015553416746871994,
      "loss": 4.0633,
      "step": 2080
    },
    {
      "epoch": 0.6705165222970805,
      "grad_norm": 2.503237724304199,
      "learning_rate": 0.00015532028660036358,
      "loss": 3.9436,
      "step": 2090
    },
    {
      "epoch": 0.6737247353224254,
      "grad_norm": 2.9667651653289795,
      "learning_rate": 0.00015510640573200728,
      "loss": 3.5962,
      "step": 2100
    },
    {
      "epoch": 0.6769329483477703,
      "grad_norm": 2.0553531646728516,
      "learning_rate": 0.00015489252486365095,
      "loss": 3.9837,
      "step": 2110
    },
    {
      "epoch": 0.6801411613731152,
      "grad_norm": 2.6143360137939453,
      "learning_rate": 0.00015467864399529464,
      "loss": 3.6193,
      "step": 2120
    },
    {
      "epoch": 0.6833493743984601,
      "grad_norm": 2.3962907791137695,
      "learning_rate": 0.00015446476312693829,
      "loss": 4.0623,
      "step": 2130
    },
    {
      "epoch": 0.6865575874238049,
      "grad_norm": 1.393470287322998,
      "learning_rate": 0.00015425088225858198,
      "loss": 4.3141,
      "step": 2140
    },
    {
      "epoch": 0.6897658004491498,
      "grad_norm": 1.611470341682434,
      "learning_rate": 0.00015403700139022565,
      "loss": 3.8358,
      "step": 2150
    },
    {
      "epoch": 0.6929740134744947,
      "grad_norm": 1.9299007654190063,
      "learning_rate": 0.00015382312052186935,
      "loss": 3.9701,
      "step": 2160
    },
    {
      "epoch": 0.6961822264998396,
      "grad_norm": 1.542120337486267,
      "learning_rate": 0.000153609239653513,
      "loss": 3.6599,
      "step": 2170
    },
    {
      "epoch": 0.6993904395251844,
      "grad_norm": 2.880634069442749,
      "learning_rate": 0.00015339535878515669,
      "loss": 3.9574,
      "step": 2180
    },
    {
      "epoch": 0.7025986525505293,
      "grad_norm": 2.8203744888305664,
      "learning_rate": 0.00015318147791680035,
      "loss": 3.8562,
      "step": 2190
    },
    {
      "epoch": 0.7058068655758742,
      "grad_norm": 2.0851359367370605,
      "learning_rate": 0.00015296759704844402,
      "loss": 3.8291,
      "step": 2200
    },
    {
      "epoch": 0.7090150786012192,
      "grad_norm": 2.4497275352478027,
      "learning_rate": 0.0001527537161800877,
      "loss": 4.0756,
      "step": 2210
    },
    {
      "epoch": 0.7122232916265641,
      "grad_norm": 1.9299801588058472,
      "learning_rate": 0.00015253983531173136,
      "loss": 4.1683,
      "step": 2220
    },
    {
      "epoch": 0.7154315046519089,
      "grad_norm": 2.3217341899871826,
      "learning_rate": 0.00015232595444337506,
      "loss": 3.9257,
      "step": 2230
    },
    {
      "epoch": 0.7186397176772538,
      "grad_norm": 1.6795283555984497,
      "learning_rate": 0.00015211207357501873,
      "loss": 3.9216,
      "step": 2240
    },
    {
      "epoch": 0.7218479307025987,
      "grad_norm": 2.299752950668335,
      "learning_rate": 0.0001518981927066624,
      "loss": 4.036,
      "step": 2250
    },
    {
      "epoch": 0.7250561437279436,
      "grad_norm": 2.168635845184326,
      "learning_rate": 0.00015168431183830607,
      "loss": 4.1398,
      "step": 2260
    },
    {
      "epoch": 0.7282643567532884,
      "grad_norm": 2.7265212535858154,
      "learning_rate": 0.00015147043096994976,
      "loss": 3.6874,
      "step": 2270
    },
    {
      "epoch": 0.7314725697786333,
      "grad_norm": 2.038642406463623,
      "learning_rate": 0.00015125655010159343,
      "loss": 4.0312,
      "step": 2280
    },
    {
      "epoch": 0.7346807828039782,
      "grad_norm": 2.150573253631592,
      "learning_rate": 0.0001510426692332371,
      "loss": 3.947,
      "step": 2290
    },
    {
      "epoch": 0.7378889958293231,
      "grad_norm": 1.7935576438903809,
      "learning_rate": 0.00015082878836488077,
      "loss": 3.9574,
      "step": 2300
    },
    {
      "epoch": 0.7410972088546679,
      "grad_norm": 2.3070485591888428,
      "learning_rate": 0.00015061490749652444,
      "loss": 3.9745,
      "step": 2310
    },
    {
      "epoch": 0.7443054218800128,
      "grad_norm": 3.014315366744995,
      "learning_rate": 0.0001504010266281681,
      "loss": 4.0126,
      "step": 2320
    },
    {
      "epoch": 0.7475136349053577,
      "grad_norm": 1.7987183332443237,
      "learning_rate": 0.00015018714575981178,
      "loss": 3.9854,
      "step": 2330
    },
    {
      "epoch": 0.7507218479307026,
      "grad_norm": 2.4941742420196533,
      "learning_rate": 0.00014997326489145547,
      "loss": 3.9909,
      "step": 2340
    },
    {
      "epoch": 0.7539300609560475,
      "grad_norm": 1.9753049612045288,
      "learning_rate": 0.00014975938402309914,
      "loss": 3.9946,
      "step": 2350
    },
    {
      "epoch": 0.7571382739813923,
      "grad_norm": 1.7457146644592285,
      "learning_rate": 0.0001495455031547428,
      "loss": 4.0189,
      "step": 2360
    },
    {
      "epoch": 0.7603464870067372,
      "grad_norm": 2.5149712562561035,
      "learning_rate": 0.00014933162228638648,
      "loss": 3.9668,
      "step": 2370
    },
    {
      "epoch": 0.7635547000320821,
      "grad_norm": 1.795072078704834,
      "learning_rate": 0.00014911774141803018,
      "loss": 4.1523,
      "step": 2380
    },
    {
      "epoch": 0.766762913057427,
      "grad_norm": 2.7472569942474365,
      "learning_rate": 0.00014890386054967385,
      "loss": 3.9339,
      "step": 2390
    },
    {
      "epoch": 0.7699711260827719,
      "grad_norm": 3.31738018989563,
      "learning_rate": 0.00014868997968131751,
      "loss": 4.0458,
      "step": 2400
    },
    {
      "epoch": 0.7731793391081168,
      "grad_norm": 2.591106414794922,
      "learning_rate": 0.00014847609881296118,
      "loss": 3.7805,
      "step": 2410
    },
    {
      "epoch": 0.7763875521334617,
      "grad_norm": 3.2873291969299316,
      "learning_rate": 0.00014826221794460485,
      "loss": 4.0533,
      "step": 2420
    },
    {
      "epoch": 0.7795957651588066,
      "grad_norm": 2.17500376701355,
      "learning_rate": 0.00014804833707624855,
      "loss": 3.8955,
      "step": 2430
    },
    {
      "epoch": 0.7828039781841514,
      "grad_norm": 1.5964462757110596,
      "learning_rate": 0.0001478344562078922,
      "loss": 3.9066,
      "step": 2440
    },
    {
      "epoch": 0.7860121912094963,
      "grad_norm": 1.6264476776123047,
      "learning_rate": 0.0001476205753395359,
      "loss": 3.9326,
      "step": 2450
    },
    {
      "epoch": 0.7892204042348412,
      "grad_norm": 1.9662729501724243,
      "learning_rate": 0.00014740669447117956,
      "loss": 4.0061,
      "step": 2460
    },
    {
      "epoch": 0.7924286172601861,
      "grad_norm": 2.291187047958374,
      "learning_rate": 0.00014719281360282325,
      "loss": 3.9532,
      "step": 2470
    },
    {
      "epoch": 0.795636830285531,
      "grad_norm": 2.01975679397583,
      "learning_rate": 0.0001469789327344669,
      "loss": 3.8722,
      "step": 2480
    },
    {
      "epoch": 0.7988450433108758,
      "grad_norm": 2.08661150932312,
      "learning_rate": 0.0001467650518661106,
      "loss": 3.9521,
      "step": 2490
    },
    {
      "epoch": 0.8020532563362207,
      "grad_norm": 1.7681858539581299,
      "learning_rate": 0.00014655117099775426,
      "loss": 3.9023,
      "step": 2500
    },
    {
      "epoch": 0.8052614693615656,
      "grad_norm": 4.5421223640441895,
      "learning_rate": 0.00014633729012939793,
      "loss": 4.1736,
      "step": 2510
    },
    {
      "epoch": 0.8084696823869105,
      "grad_norm": 2.577997922897339,
      "learning_rate": 0.0001461234092610416,
      "loss": 3.8761,
      "step": 2520
    },
    {
      "epoch": 0.8116778954122553,
      "grad_norm": 4.018702030181885,
      "learning_rate": 0.00014590952839268527,
      "loss": 3.7657,
      "step": 2530
    },
    {
      "epoch": 0.8148861084376002,
      "grad_norm": 2.598421573638916,
      "learning_rate": 0.00014569564752432896,
      "loss": 3.9408,
      "step": 2540
    },
    {
      "epoch": 0.8180943214629451,
      "grad_norm": 4.238223075866699,
      "learning_rate": 0.00014548176665597263,
      "loss": 3.9372,
      "step": 2550
    },
    {
      "epoch": 0.82130253448829,
      "grad_norm": 3.053446054458618,
      "learning_rate": 0.0001452678857876163,
      "loss": 3.9174,
      "step": 2560
    },
    {
      "epoch": 0.824510747513635,
      "grad_norm": 1.4329825639724731,
      "learning_rate": 0.00014505400491925997,
      "loss": 4.0261,
      "step": 2570
    },
    {
      "epoch": 0.8277189605389798,
      "grad_norm": 1.9860457181930542,
      "learning_rate": 0.00014484012405090367,
      "loss": 3.7858,
      "step": 2580
    },
    {
      "epoch": 0.8309271735643247,
      "grad_norm": 2.638535976409912,
      "learning_rate": 0.00014462624318254734,
      "loss": 3.9149,
      "step": 2590
    },
    {
      "epoch": 0.8341353865896696,
      "grad_norm": 2.102673053741455,
      "learning_rate": 0.000144412362314191,
      "loss": 4.1525,
      "step": 2600
    },
    {
      "epoch": 0.8373435996150145,
      "grad_norm": 2.0812244415283203,
      "learning_rate": 0.00014419848144583467,
      "loss": 3.842,
      "step": 2610
    },
    {
      "epoch": 0.8405518126403593,
      "grad_norm": 2.727604389190674,
      "learning_rate": 0.00014398460057747837,
      "loss": 4.2656,
      "step": 2620
    },
    {
      "epoch": 0.8437600256657042,
      "grad_norm": 1.897367000579834,
      "learning_rate": 0.000143770719709122,
      "loss": 3.9903,
      "step": 2630
    },
    {
      "epoch": 0.8469682386910491,
      "grad_norm": 2.44559383392334,
      "learning_rate": 0.0001435568388407657,
      "loss": 4.0395,
      "step": 2640
    },
    {
      "epoch": 0.850176451716394,
      "grad_norm": 2.1061086654663086,
      "learning_rate": 0.00014334295797240938,
      "loss": 3.8798,
      "step": 2650
    },
    {
      "epoch": 0.8533846647417388,
      "grad_norm": 3.1863203048706055,
      "learning_rate": 0.00014312907710405305,
      "loss": 3.8736,
      "step": 2660
    },
    {
      "epoch": 0.8565928777670837,
      "grad_norm": 2.047208547592163,
      "learning_rate": 0.00014291519623569672,
      "loss": 3.9846,
      "step": 2670
    },
    {
      "epoch": 0.8598010907924286,
      "grad_norm": 1.7556374073028564,
      "learning_rate": 0.00014270131536734038,
      "loss": 4.0636,
      "step": 2680
    },
    {
      "epoch": 0.8630093038177735,
      "grad_norm": 2.713083028793335,
      "learning_rate": 0.00014248743449898408,
      "loss": 3.9777,
      "step": 2690
    },
    {
      "epoch": 0.8662175168431184,
      "grad_norm": 2.3725686073303223,
      "learning_rate": 0.00014227355363062775,
      "loss": 3.9645,
      "step": 2700
    },
    {
      "epoch": 0.8694257298684632,
      "grad_norm": 1.7237383127212524,
      "learning_rate": 0.00014205967276227142,
      "loss": 3.9207,
      "step": 2710
    },
    {
      "epoch": 0.8726339428938081,
      "grad_norm": 2.5982556343078613,
      "learning_rate": 0.0001418457918939151,
      "loss": 4.0717,
      "step": 2720
    },
    {
      "epoch": 0.875842155919153,
      "grad_norm": 2.5663814544677734,
      "learning_rate": 0.00014163191102555878,
      "loss": 3.913,
      "step": 2730
    },
    {
      "epoch": 0.879050368944498,
      "grad_norm": 1.5334782600402832,
      "learning_rate": 0.00014141803015720245,
      "loss": 3.9786,
      "step": 2740
    },
    {
      "epoch": 0.8822585819698427,
      "grad_norm": 1.9064080715179443,
      "learning_rate": 0.00014120414928884612,
      "loss": 3.8793,
      "step": 2750
    },
    {
      "epoch": 0.8854667949951877,
      "grad_norm": 1.6761411428451538,
      "learning_rate": 0.0001409902684204898,
      "loss": 3.8029,
      "step": 2760
    },
    {
      "epoch": 0.8886750080205326,
      "grad_norm": 2.9898319244384766,
      "learning_rate": 0.00014077638755213346,
      "loss": 3.8813,
      "step": 2770
    },
    {
      "epoch": 0.8918832210458775,
      "grad_norm": 2.3758385181427,
      "learning_rate": 0.00014056250668377716,
      "loss": 3.5688,
      "step": 2780
    },
    {
      "epoch": 0.8950914340712224,
      "grad_norm": 1.6554383039474487,
      "learning_rate": 0.0001403486258154208,
      "loss": 3.9659,
      "step": 2790
    },
    {
      "epoch": 0.8982996470965672,
      "grad_norm": 2.0385661125183105,
      "learning_rate": 0.0001401347449470645,
      "loss": 3.7587,
      "step": 2800
    },
    {
      "epoch": 0.9015078601219121,
      "grad_norm": 2.7332775592803955,
      "learning_rate": 0.00013992086407870816,
      "loss": 3.9954,
      "step": 2810
    },
    {
      "epoch": 0.904716073147257,
      "grad_norm": 2.5674102306365967,
      "learning_rate": 0.00013970698321035186,
      "loss": 3.953,
      "step": 2820
    },
    {
      "epoch": 0.9079242861726019,
      "grad_norm": 2.1665849685668945,
      "learning_rate": 0.0001394931023419955,
      "loss": 3.9543,
      "step": 2830
    },
    {
      "epoch": 0.9111324991979467,
      "grad_norm": 2.0977799892425537,
      "learning_rate": 0.0001392792214736392,
      "loss": 3.7345,
      "step": 2840
    },
    {
      "epoch": 0.9143407122232916,
      "grad_norm": 2.4375555515289307,
      "learning_rate": 0.00013906534060528287,
      "loss": 3.8363,
      "step": 2850
    },
    {
      "epoch": 0.9175489252486365,
      "grad_norm": 2.7153594493865967,
      "learning_rate": 0.00013885145973692654,
      "loss": 3.947,
      "step": 2860
    },
    {
      "epoch": 0.9207571382739814,
      "grad_norm": 2.5794453620910645,
      "learning_rate": 0.0001386375788685702,
      "loss": 3.8237,
      "step": 2870
    },
    {
      "epoch": 0.9239653512993262,
      "grad_norm": 3.0685548782348633,
      "learning_rate": 0.00013842369800021387,
      "loss": 3.8364,
      "step": 2880
    },
    {
      "epoch": 0.9271735643246711,
      "grad_norm": 2.2057650089263916,
      "learning_rate": 0.00013820981713185757,
      "loss": 3.6449,
      "step": 2890
    },
    {
      "epoch": 0.930381777350016,
      "grad_norm": 2.531128406524658,
      "learning_rate": 0.00013799593626350124,
      "loss": 4.0186,
      "step": 2900
    },
    {
      "epoch": 0.933589990375361,
      "grad_norm": 1.6140120029449463,
      "learning_rate": 0.0001377820553951449,
      "loss": 4.0693,
      "step": 2910
    },
    {
      "epoch": 0.9367982034007059,
      "grad_norm": 2.4779610633850098,
      "learning_rate": 0.00013756817452678858,
      "loss": 4.1598,
      "step": 2920
    },
    {
      "epoch": 0.9400064164260507,
      "grad_norm": 3.8837928771972656,
      "learning_rate": 0.00013735429365843227,
      "loss": 3.9707,
      "step": 2930
    },
    {
      "epoch": 0.9432146294513956,
      "grad_norm": 2.1497795581817627,
      "learning_rate": 0.00013714041279007594,
      "loss": 3.9874,
      "step": 2940
    },
    {
      "epoch": 0.9464228424767405,
      "grad_norm": 2.151076316833496,
      "learning_rate": 0.0001369265319217196,
      "loss": 4.0312,
      "step": 2950
    },
    {
      "epoch": 0.9496310555020854,
      "grad_norm": 2.04116153717041,
      "learning_rate": 0.00013671265105336328,
      "loss": 3.8117,
      "step": 2960
    },
    {
      "epoch": 0.9528392685274302,
      "grad_norm": 2.0266478061676025,
      "learning_rate": 0.00013649877018500695,
      "loss": 4.0052,
      "step": 2970
    },
    {
      "epoch": 0.9560474815527751,
      "grad_norm": 1.8620800971984863,
      "learning_rate": 0.00013628488931665062,
      "loss": 3.7706,
      "step": 2980
    },
    {
      "epoch": 0.95925569457812,
      "grad_norm": 2.0280282497406006,
      "learning_rate": 0.0001360710084482943,
      "loss": 3.8398,
      "step": 2990
    },
    {
      "epoch": 0.9624639076034649,
      "grad_norm": 1.760537028312683,
      "learning_rate": 0.00013585712757993798,
      "loss": 3.9363,
      "step": 3000
    },
    {
      "epoch": 0.9656721206288098,
      "grad_norm": 2.2017576694488525,
      "learning_rate": 0.00013564324671158165,
      "loss": 3.8538,
      "step": 3010
    },
    {
      "epoch": 0.9688803336541546,
      "grad_norm": 2.048632860183716,
      "learning_rate": 0.00013542936584322532,
      "loss": 3.924,
      "step": 3020
    },
    {
      "epoch": 0.9720885466794995,
      "grad_norm": 2.154470443725586,
      "learning_rate": 0.000135215484974869,
      "loss": 4.0776,
      "step": 3030
    },
    {
      "epoch": 0.9752967597048444,
      "grad_norm": 2.091087579727173,
      "learning_rate": 0.0001350016041065127,
      "loss": 4.0132,
      "step": 3040
    },
    {
      "epoch": 0.9785049727301893,
      "grad_norm": 2.8166537284851074,
      "learning_rate": 0.00013478772323815636,
      "loss": 3.6905,
      "step": 3050
    },
    {
      "epoch": 0.9817131857555341,
      "grad_norm": 1.775668740272522,
      "learning_rate": 0.00013457384236980003,
      "loss": 4.1485,
      "step": 3060
    },
    {
      "epoch": 0.984921398780879,
      "grad_norm": 2.372047185897827,
      "learning_rate": 0.0001343599615014437,
      "loss": 3.9823,
      "step": 3070
    },
    {
      "epoch": 0.9881296118062239,
      "grad_norm": 2.5656871795654297,
      "learning_rate": 0.0001341460806330874,
      "loss": 4.1929,
      "step": 3080
    },
    {
      "epoch": 0.9913378248315688,
      "grad_norm": 2.276310682296753,
      "learning_rate": 0.00013393219976473106,
      "loss": 3.5942,
      "step": 3090
    },
    {
      "epoch": 0.9945460378569136,
      "grad_norm": 2.0566937923431396,
      "learning_rate": 0.00013371831889637473,
      "loss": 3.7608,
      "step": 3100
    },
    {
      "epoch": 0.9977542508822586,
      "grad_norm": 2.408677339553833,
      "learning_rate": 0.0001335044380280184,
      "loss": 3.8344,
      "step": 3110
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.7668333053588867,
      "eval_runtime": 4.2431,
      "eval_samples_per_second": 326.649,
      "eval_steps_per_second": 41.008,
      "step": 3117
    },
    {
      "epoch": 1.0009624639076036,
      "grad_norm": 2.01812481880188,
      "learning_rate": 0.00013329055715966207,
      "loss": 3.8504,
      "step": 3120
    },
    {
      "epoch": 1.0041706769329484,
      "grad_norm": 3.328529119491577,
      "learning_rate": 0.00013307667629130576,
      "loss": 4.1452,
      "step": 3130
    },
    {
      "epoch": 1.0073788899582932,
      "grad_norm": 2.035022497177124,
      "learning_rate": 0.0001328627954229494,
      "loss": 3.9111,
      "step": 3140
    },
    {
      "epoch": 1.0105871029836382,
      "grad_norm": 2.1510112285614014,
      "learning_rate": 0.0001326489145545931,
      "loss": 3.8702,
      "step": 3150
    },
    {
      "epoch": 1.013795316008983,
      "grad_norm": 1.6478052139282227,
      "learning_rate": 0.00013243503368623677,
      "loss": 3.9777,
      "step": 3160
    },
    {
      "epoch": 1.0170035290343278,
      "grad_norm": 2.32491397857666,
      "learning_rate": 0.00013222115281788047,
      "loss": 3.8448,
      "step": 3170
    },
    {
      "epoch": 1.0202117420596728,
      "grad_norm": 1.5753332376480103,
      "learning_rate": 0.0001320072719495241,
      "loss": 3.8432,
      "step": 3180
    },
    {
      "epoch": 1.0234199550850176,
      "grad_norm": 1.654861330986023,
      "learning_rate": 0.0001317933910811678,
      "loss": 3.7847,
      "step": 3190
    },
    {
      "epoch": 1.0266281681103626,
      "grad_norm": 1.9489670991897583,
      "learning_rate": 0.00013157951021281148,
      "loss": 3.9014,
      "step": 3200
    },
    {
      "epoch": 1.0298363811357074,
      "grad_norm": 2.8407301902770996,
      "learning_rate": 0.00013136562934445514,
      "loss": 3.9082,
      "step": 3210
    },
    {
      "epoch": 1.0330445941610522,
      "grad_norm": 3.790144681930542,
      "learning_rate": 0.0001311517484760988,
      "loss": 3.8193,
      "step": 3220
    },
    {
      "epoch": 1.0362528071863972,
      "grad_norm": 3.058791160583496,
      "learning_rate": 0.00013093786760774248,
      "loss": 3.68,
      "step": 3230
    },
    {
      "epoch": 1.039461020211742,
      "grad_norm": 1.613163709640503,
      "learning_rate": 0.00013072398673938618,
      "loss": 3.9856,
      "step": 3240
    },
    {
      "epoch": 1.042669233237087,
      "grad_norm": 2.497279644012451,
      "learning_rate": 0.00013051010587102985,
      "loss": 3.8825,
      "step": 3250
    },
    {
      "epoch": 1.0458774462624318,
      "grad_norm": 2.3578033447265625,
      "learning_rate": 0.00013029622500267352,
      "loss": 3.7492,
      "step": 3260
    },
    {
      "epoch": 1.0490856592877766,
      "grad_norm": 2.2154171466827393,
      "learning_rate": 0.00013008234413431719,
      "loss": 4.1336,
      "step": 3270
    },
    {
      "epoch": 1.0522938723131217,
      "grad_norm": 3.061270236968994,
      "learning_rate": 0.00012986846326596088,
      "loss": 4.0111,
      "step": 3280
    },
    {
      "epoch": 1.0555020853384665,
      "grad_norm": 2.1023433208465576,
      "learning_rate": 0.00012965458239760455,
      "loss": 4.0185,
      "step": 3290
    },
    {
      "epoch": 1.0587102983638113,
      "grad_norm": 2.9358606338500977,
      "learning_rate": 0.00012944070152924822,
      "loss": 4.1021,
      "step": 3300
    },
    {
      "epoch": 1.0619185113891563,
      "grad_norm": 3.346384048461914,
      "learning_rate": 0.0001292268206608919,
      "loss": 4.0188,
      "step": 3310
    },
    {
      "epoch": 1.065126724414501,
      "grad_norm": 2.2383861541748047,
      "learning_rate": 0.00012901293979253556,
      "loss": 3.7257,
      "step": 3320
    },
    {
      "epoch": 1.068334937439846,
      "grad_norm": 2.1567440032958984,
      "learning_rate": 0.00012879905892417923,
      "loss": 3.992,
      "step": 3330
    },
    {
      "epoch": 1.0715431504651909,
      "grad_norm": 1.6299084424972534,
      "learning_rate": 0.0001285851780558229,
      "loss": 3.6875,
      "step": 3340
    },
    {
      "epoch": 1.0747513634905357,
      "grad_norm": 2.295968532562256,
      "learning_rate": 0.0001283712971874666,
      "loss": 3.8566,
      "step": 3350
    },
    {
      "epoch": 1.0779595765158807,
      "grad_norm": 2.258836030960083,
      "learning_rate": 0.00012815741631911026,
      "loss": 3.9331,
      "step": 3360
    },
    {
      "epoch": 1.0811677895412255,
      "grad_norm": 2.121201753616333,
      "learning_rate": 0.00012794353545075393,
      "loss": 3.8656,
      "step": 3370
    },
    {
      "epoch": 1.0843760025665705,
      "grad_norm": 2.7034974098205566,
      "learning_rate": 0.0001277296545823976,
      "loss": 3.6733,
      "step": 3380
    },
    {
      "epoch": 1.0875842155919153,
      "grad_norm": 1.9414643049240112,
      "learning_rate": 0.0001275157737140413,
      "loss": 4.0621,
      "step": 3390
    },
    {
      "epoch": 1.0907924286172601,
      "grad_norm": 2.700204610824585,
      "learning_rate": 0.00012730189284568497,
      "loss": 3.8156,
      "step": 3400
    },
    {
      "epoch": 1.0940006416426051,
      "grad_norm": 2.0397000312805176,
      "learning_rate": 0.00012708801197732863,
      "loss": 3.9975,
      "step": 3410
    },
    {
      "epoch": 1.09720885466795,
      "grad_norm": 1.6680048704147339,
      "learning_rate": 0.0001268741311089723,
      "loss": 4.0791,
      "step": 3420
    },
    {
      "epoch": 1.1004170676932947,
      "grad_norm": 2.024756908416748,
      "learning_rate": 0.00012666025024061597,
      "loss": 3.9387,
      "step": 3430
    },
    {
      "epoch": 1.1036252807186397,
      "grad_norm": 1.9360295534133911,
      "learning_rate": 0.00012644636937225967,
      "loss": 3.7569,
      "step": 3440
    },
    {
      "epoch": 1.1068334937439845,
      "grad_norm": 2.1407740116119385,
      "learning_rate": 0.0001262324885039033,
      "loss": 3.86,
      "step": 3450
    },
    {
      "epoch": 1.1100417067693296,
      "grad_norm": 1.9134365320205688,
      "learning_rate": 0.000126018607635547,
      "loss": 3.8645,
      "step": 3460
    },
    {
      "epoch": 1.1132499197946744,
      "grad_norm": 2.0077290534973145,
      "learning_rate": 0.00012580472676719068,
      "loss": 3.6079,
      "step": 3470
    },
    {
      "epoch": 1.1164581328200192,
      "grad_norm": 1.9841097593307495,
      "learning_rate": 0.00012559084589883437,
      "loss": 3.7533,
      "step": 3480
    },
    {
      "epoch": 1.1196663458453642,
      "grad_norm": 2.1564581394195557,
      "learning_rate": 0.00012537696503047801,
      "loss": 3.87,
      "step": 3490
    },
    {
      "epoch": 1.122874558870709,
      "grad_norm": 3.026742935180664,
      "learning_rate": 0.0001251630841621217,
      "loss": 3.9717,
      "step": 3500
    }
  ],
  "logging_steps": 10,
  "max_steps": 9351,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 917693005824000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
