{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.48123195380173245,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003208213025344883,
      "grad_norm": 1.1983283758163452,
      "learning_rate": 0.00019980750721847933,
      "loss": 6.5784,
      "step": 10
    },
    {
      "epoch": 0.006416426050689766,
      "grad_norm": 2.383748769760132,
      "learning_rate": 0.000199593626350123,
      "loss": 6.5244,
      "step": 20
    },
    {
      "epoch": 0.009624639076034648,
      "grad_norm": 1.4066181182861328,
      "learning_rate": 0.00019937974548176667,
      "loss": 6.1228,
      "step": 30
    },
    {
      "epoch": 0.012832852101379532,
      "grad_norm": 1.604810118675232,
      "learning_rate": 0.00019916586461341034,
      "loss": 5.4314,
      "step": 40
    },
    {
      "epoch": 0.016041065126724416,
      "grad_norm": 1.9869211912155151,
      "learning_rate": 0.000198951983745054,
      "loss": 5.0872,
      "step": 50
    },
    {
      "epoch": 0.019249278152069296,
      "grad_norm": 2.957611083984375,
      "learning_rate": 0.0001987381028766977,
      "loss": 4.9137,
      "step": 60
    },
    {
      "epoch": 0.02245749117741418,
      "grad_norm": 1.4865000247955322,
      "learning_rate": 0.00019852422200834134,
      "loss": 5.0434,
      "step": 70
    },
    {
      "epoch": 0.025665704202759064,
      "grad_norm": 2.3109586238861084,
      "learning_rate": 0.00019831034113998504,
      "loss": 4.7802,
      "step": 80
    },
    {
      "epoch": 0.028873917228103944,
      "grad_norm": 1.243062138557434,
      "learning_rate": 0.0001980964602716287,
      "loss": 4.663,
      "step": 90
    },
    {
      "epoch": 0.03208213025344883,
      "grad_norm": 1.3511428833007812,
      "learning_rate": 0.0001978825794032724,
      "loss": 4.6094,
      "step": 100
    },
    {
      "epoch": 0.03529034327879371,
      "grad_norm": 1.9429733753204346,
      "learning_rate": 0.00019766869853491605,
      "loss": 4.5744,
      "step": 110
    },
    {
      "epoch": 0.03849855630413859,
      "grad_norm": 1.392471432685852,
      "learning_rate": 0.00019745481766655974,
      "loss": 4.3532,
      "step": 120
    },
    {
      "epoch": 0.04170676932948348,
      "grad_norm": 1.6916451454162598,
      "learning_rate": 0.0001972409367982034,
      "loss": 4.4809,
      "step": 130
    },
    {
      "epoch": 0.04491498235482836,
      "grad_norm": 1.2407877445220947,
      "learning_rate": 0.00019702705592984708,
      "loss": 4.2253,
      "step": 140
    },
    {
      "epoch": 0.04812319538017324,
      "grad_norm": 1.8410860300064087,
      "learning_rate": 0.00019681317506149075,
      "loss": 4.466,
      "step": 150
    },
    {
      "epoch": 0.05133140840551813,
      "grad_norm": 1.4824504852294922,
      "learning_rate": 0.00019659929419313442,
      "loss": 4.556,
      "step": 160
    },
    {
      "epoch": 0.05453962143086301,
      "grad_norm": 1.5274494886398315,
      "learning_rate": 0.00019638541332477811,
      "loss": 4.3291,
      "step": 170
    },
    {
      "epoch": 0.05774783445620789,
      "grad_norm": 1.708851933479309,
      "learning_rate": 0.00019617153245642178,
      "loss": 4.2644,
      "step": 180
    },
    {
      "epoch": 0.060956047481552776,
      "grad_norm": 1.5556572675704956,
      "learning_rate": 0.00019595765158806545,
      "loss": 4.2656,
      "step": 190
    },
    {
      "epoch": 0.06416426050689766,
      "grad_norm": 1.3052595853805542,
      "learning_rate": 0.00019574377071970912,
      "loss": 4.515,
      "step": 200
    },
    {
      "epoch": 0.06737247353224254,
      "grad_norm": 2.4464001655578613,
      "learning_rate": 0.00019552988985135282,
      "loss": 4.4356,
      "step": 210
    },
    {
      "epoch": 0.07058068655758742,
      "grad_norm": 1.5678684711456299,
      "learning_rate": 0.0001953160089829965,
      "loss": 4.4698,
      "step": 220
    },
    {
      "epoch": 0.0737888995829323,
      "grad_norm": 1.6955628395080566,
      "learning_rate": 0.00019510212811464016,
      "loss": 4.3303,
      "step": 230
    },
    {
      "epoch": 0.07699711260827719,
      "grad_norm": 3.168471336364746,
      "learning_rate": 0.00019488824724628383,
      "loss": 4.3839,
      "step": 240
    },
    {
      "epoch": 0.08020532563362208,
      "grad_norm": 2.1055126190185547,
      "learning_rate": 0.00019467436637792752,
      "loss": 4.1951,
      "step": 250
    },
    {
      "epoch": 0.08341353865896696,
      "grad_norm": 2.154214859008789,
      "learning_rate": 0.0001944604855095712,
      "loss": 4.2911,
      "step": 260
    },
    {
      "epoch": 0.08662175168431184,
      "grad_norm": 2.1030399799346924,
      "learning_rate": 0.00019424660464121486,
      "loss": 4.1543,
      "step": 270
    },
    {
      "epoch": 0.08982996470965672,
      "grad_norm": 2.408141851425171,
      "learning_rate": 0.00019403272377285853,
      "loss": 4.1835,
      "step": 280
    },
    {
      "epoch": 0.0930381777350016,
      "grad_norm": 1.7365314960479736,
      "learning_rate": 0.0001938188429045022,
      "loss": 4.356,
      "step": 290
    },
    {
      "epoch": 0.09624639076034648,
      "grad_norm": 2.310044765472412,
      "learning_rate": 0.0001936049620361459,
      "loss": 4.3094,
      "step": 300
    },
    {
      "epoch": 0.09945460378569138,
      "grad_norm": 2.2792177200317383,
      "learning_rate": 0.00019339108116778954,
      "loss": 4.344,
      "step": 310
    },
    {
      "epoch": 0.10266281681103626,
      "grad_norm": 2.3968708515167236,
      "learning_rate": 0.00019317720029943323,
      "loss": 4.1977,
      "step": 320
    },
    {
      "epoch": 0.10587102983638114,
      "grad_norm": 1.9320766925811768,
      "learning_rate": 0.0001929633194310769,
      "loss": 4.228,
      "step": 330
    },
    {
      "epoch": 0.10907924286172602,
      "grad_norm": 1.6803889274597168,
      "learning_rate": 0.00019274943856272057,
      "loss": 4.05,
      "step": 340
    },
    {
      "epoch": 0.1122874558870709,
      "grad_norm": 1.8266921043395996,
      "learning_rate": 0.00019253555769436424,
      "loss": 4.1899,
      "step": 350
    },
    {
      "epoch": 0.11549566891241578,
      "grad_norm": 1.4962135553359985,
      "learning_rate": 0.00019232167682600794,
      "loss": 4.3269,
      "step": 360
    },
    {
      "epoch": 0.11870388193776067,
      "grad_norm": 2.7197048664093018,
      "learning_rate": 0.0001921077959576516,
      "loss": 4.1859,
      "step": 370
    },
    {
      "epoch": 0.12191209496310555,
      "grad_norm": 3.336015462875366,
      "learning_rate": 0.00019189391508929527,
      "loss": 4.1157,
      "step": 380
    },
    {
      "epoch": 0.12512030798845045,
      "grad_norm": 2.937793254852295,
      "learning_rate": 0.00019168003422093894,
      "loss": 4.2637,
      "step": 390
    },
    {
      "epoch": 0.12832852101379533,
      "grad_norm": 1.4884883165359497,
      "learning_rate": 0.0001914661533525826,
      "loss": 4.1511,
      "step": 400
    },
    {
      "epoch": 0.1315367340391402,
      "grad_norm": 1.9539333581924438,
      "learning_rate": 0.0001912522724842263,
      "loss": 3.9707,
      "step": 410
    },
    {
      "epoch": 0.1347449470644851,
      "grad_norm": 1.5094072818756104,
      "learning_rate": 0.00019103839161586995,
      "loss": 4.145,
      "step": 420
    },
    {
      "epoch": 0.13795316008982997,
      "grad_norm": 2.652632236480713,
      "learning_rate": 0.00019082451074751365,
      "loss": 4.1095,
      "step": 430
    },
    {
      "epoch": 0.14116137311517485,
      "grad_norm": 2.1157455444335938,
      "learning_rate": 0.00019061062987915732,
      "loss": 4.0097,
      "step": 440
    },
    {
      "epoch": 0.14436958614051973,
      "grad_norm": 2.671804189682007,
      "learning_rate": 0.000190396749010801,
      "loss": 4.2516,
      "step": 450
    },
    {
      "epoch": 0.1475777991658646,
      "grad_norm": 2.2349512577056885,
      "learning_rate": 0.00019018286814244465,
      "loss": 4.1692,
      "step": 460
    },
    {
      "epoch": 0.1507860121912095,
      "grad_norm": 1.4282678365707397,
      "learning_rate": 0.00018996898727408835,
      "loss": 4.1818,
      "step": 470
    },
    {
      "epoch": 0.15399422521655437,
      "grad_norm": 2.2927565574645996,
      "learning_rate": 0.00018975510640573202,
      "loss": 4.1962,
      "step": 480
    },
    {
      "epoch": 0.15720243824189925,
      "grad_norm": 2.240100622177124,
      "learning_rate": 0.0001895412255373757,
      "loss": 4.1023,
      "step": 490
    },
    {
      "epoch": 0.16041065126724416,
      "grad_norm": 1.7559913396835327,
      "learning_rate": 0.00018932734466901936,
      "loss": 4.2296,
      "step": 500
    },
    {
      "epoch": 0.16361886429258904,
      "grad_norm": 2.80961275100708,
      "learning_rate": 0.00018911346380066303,
      "loss": 4.2073,
      "step": 510
    },
    {
      "epoch": 0.16682707731793392,
      "grad_norm": 1.9017000198364258,
      "learning_rate": 0.00018889958293230672,
      "loss": 3.943,
      "step": 520
    },
    {
      "epoch": 0.1700352903432788,
      "grad_norm": 3.002676010131836,
      "learning_rate": 0.0001886857020639504,
      "loss": 4.1292,
      "step": 530
    },
    {
      "epoch": 0.17324350336862368,
      "grad_norm": 1.7197189331054688,
      "learning_rate": 0.00018847182119559406,
      "loss": 4.1951,
      "step": 540
    },
    {
      "epoch": 0.17645171639396856,
      "grad_norm": 1.8173481225967407,
      "learning_rate": 0.00018825794032723773,
      "loss": 4.2885,
      "step": 550
    },
    {
      "epoch": 0.17965992941931344,
      "grad_norm": 2.5451200008392334,
      "learning_rate": 0.00018804405945888143,
      "loss": 3.8283,
      "step": 560
    },
    {
      "epoch": 0.18286814244465832,
      "grad_norm": 4.116666316986084,
      "learning_rate": 0.0001878301785905251,
      "loss": 4.3159,
      "step": 570
    },
    {
      "epoch": 0.1860763554700032,
      "grad_norm": 2.4113287925720215,
      "learning_rate": 0.00018761629772216876,
      "loss": 4.1235,
      "step": 580
    },
    {
      "epoch": 0.18928456849534808,
      "grad_norm": 1.7203558683395386,
      "learning_rate": 0.00018740241685381243,
      "loss": 4.209,
      "step": 590
    },
    {
      "epoch": 0.19249278152069296,
      "grad_norm": 2.0295886993408203,
      "learning_rate": 0.0001871885359854561,
      "loss": 4.1689,
      "step": 600
    },
    {
      "epoch": 0.19570099454603784,
      "grad_norm": 2.5288290977478027,
      "learning_rate": 0.0001869746551170998,
      "loss": 3.7552,
      "step": 610
    },
    {
      "epoch": 0.19890920757138275,
      "grad_norm": 1.8706116676330566,
      "learning_rate": 0.00018676077424874344,
      "loss": 4.1645,
      "step": 620
    },
    {
      "epoch": 0.20211742059672763,
      "grad_norm": 2.057385206222534,
      "learning_rate": 0.00018654689338038714,
      "loss": 4.2275,
      "step": 630
    },
    {
      "epoch": 0.2053256336220725,
      "grad_norm": 2.0618972778320312,
      "learning_rate": 0.0001863330125120308,
      "loss": 4.325,
      "step": 640
    },
    {
      "epoch": 0.2085338466474174,
      "grad_norm": 1.9901732206344604,
      "learning_rate": 0.00018611913164367447,
      "loss": 3.9644,
      "step": 650
    },
    {
      "epoch": 0.21174205967276227,
      "grad_norm": 2.8751800060272217,
      "learning_rate": 0.00018590525077531814,
      "loss": 4.0262,
      "step": 660
    },
    {
      "epoch": 0.21495027269810715,
      "grad_norm": 2.0578691959381104,
      "learning_rate": 0.00018569136990696184,
      "loss": 4.1838,
      "step": 670
    },
    {
      "epoch": 0.21815848572345203,
      "grad_norm": 1.968585729598999,
      "learning_rate": 0.0001854774890386055,
      "loss": 3.9027,
      "step": 680
    },
    {
      "epoch": 0.22136669874879691,
      "grad_norm": 2.03605318069458,
      "learning_rate": 0.00018526360817024918,
      "loss": 4.0758,
      "step": 690
    },
    {
      "epoch": 0.2245749117741418,
      "grad_norm": 2.7659213542938232,
      "learning_rate": 0.00018504972730189285,
      "loss": 4.0117,
      "step": 700
    },
    {
      "epoch": 0.22778312479948667,
      "grad_norm": 1.7381958961486816,
      "learning_rate": 0.00018483584643353654,
      "loss": 3.7897,
      "step": 710
    },
    {
      "epoch": 0.23099133782483156,
      "grad_norm": 1.9161226749420166,
      "learning_rate": 0.0001846219655651802,
      "loss": 4.1692,
      "step": 720
    },
    {
      "epoch": 0.23419955085017646,
      "grad_norm": 1.9479641914367676,
      "learning_rate": 0.00018440808469682388,
      "loss": 3.9752,
      "step": 730
    },
    {
      "epoch": 0.23740776387552134,
      "grad_norm": 2.3413400650024414,
      "learning_rate": 0.00018419420382846755,
      "loss": 4.2622,
      "step": 740
    },
    {
      "epoch": 0.24061597690086622,
      "grad_norm": 1.9952948093414307,
      "learning_rate": 0.00018398032296011122,
      "loss": 4.1897,
      "step": 750
    },
    {
      "epoch": 0.2438241899262111,
      "grad_norm": 1.6951522827148438,
      "learning_rate": 0.00018376644209175492,
      "loss": 3.965,
      "step": 760
    },
    {
      "epoch": 0.24703240295155598,
      "grad_norm": 2.83673357963562,
      "learning_rate": 0.00018355256122339856,
      "loss": 4.2604,
      "step": 770
    },
    {
      "epoch": 0.2502406159769009,
      "grad_norm": 1.7921984195709229,
      "learning_rate": 0.00018333868035504225,
      "loss": 4.0102,
      "step": 780
    },
    {
      "epoch": 0.25344882900224575,
      "grad_norm": 3.1436421871185303,
      "learning_rate": 0.00018312479948668592,
      "loss": 4.1704,
      "step": 790
    },
    {
      "epoch": 0.25665704202759065,
      "grad_norm": 2.363903045654297,
      "learning_rate": 0.00018291091861832962,
      "loss": 4.0511,
      "step": 800
    },
    {
      "epoch": 0.2598652550529355,
      "grad_norm": 1.81502366065979,
      "learning_rate": 0.00018269703774997326,
      "loss": 3.9104,
      "step": 810
    },
    {
      "epoch": 0.2630734680782804,
      "grad_norm": 1.801508903503418,
      "learning_rate": 0.00018248315688161696,
      "loss": 3.996,
      "step": 820
    },
    {
      "epoch": 0.26628168110362527,
      "grad_norm": 2.337092399597168,
      "learning_rate": 0.00018226927601326063,
      "loss": 3.7473,
      "step": 830
    },
    {
      "epoch": 0.2694898941289702,
      "grad_norm": 1.9983611106872559,
      "learning_rate": 0.0001820553951449043,
      "loss": 4.2882,
      "step": 840
    },
    {
      "epoch": 0.27269810715431503,
      "grad_norm": 2.027909278869629,
      "learning_rate": 0.00018184151427654797,
      "loss": 4.2433,
      "step": 850
    },
    {
      "epoch": 0.27590632017965994,
      "grad_norm": 1.712869644165039,
      "learning_rate": 0.00018162763340819163,
      "loss": 3.8283,
      "step": 860
    },
    {
      "epoch": 0.2791145332050048,
      "grad_norm": 1.7736915349960327,
      "learning_rate": 0.00018141375253983533,
      "loss": 4.0457,
      "step": 870
    },
    {
      "epoch": 0.2823227462303497,
      "grad_norm": 2.2658841609954834,
      "learning_rate": 0.000181199871671479,
      "loss": 4.1728,
      "step": 880
    },
    {
      "epoch": 0.2855309592556946,
      "grad_norm": 2.3572957515716553,
      "learning_rate": 0.00018098599080312267,
      "loss": 4.005,
      "step": 890
    },
    {
      "epoch": 0.28873917228103946,
      "grad_norm": 4.014227390289307,
      "learning_rate": 0.00018077210993476634,
      "loss": 4.2235,
      "step": 900
    },
    {
      "epoch": 0.29194738530638437,
      "grad_norm": 2.143670082092285,
      "learning_rate": 0.00018055822906641003,
      "loss": 3.9217,
      "step": 910
    },
    {
      "epoch": 0.2951555983317292,
      "grad_norm": 2.538867950439453,
      "learning_rate": 0.0001803443481980537,
      "loss": 4.1954,
      "step": 920
    },
    {
      "epoch": 0.2983638113570741,
      "grad_norm": 1.6266535520553589,
      "learning_rate": 0.00018013046732969737,
      "loss": 4.242,
      "step": 930
    },
    {
      "epoch": 0.301572024382419,
      "grad_norm": 2.407832622528076,
      "learning_rate": 0.00017991658646134104,
      "loss": 3.8215,
      "step": 940
    },
    {
      "epoch": 0.3047802374077639,
      "grad_norm": 2.9120304584503174,
      "learning_rate": 0.0001797027055929847,
      "loss": 3.9595,
      "step": 950
    },
    {
      "epoch": 0.30798845043310874,
      "grad_norm": 2.097067356109619,
      "learning_rate": 0.0001794888247246284,
      "loss": 3.6518,
      "step": 960
    },
    {
      "epoch": 0.31119666345845365,
      "grad_norm": 1.8193588256835938,
      "learning_rate": 0.00017927494385627205,
      "loss": 3.8854,
      "step": 970
    },
    {
      "epoch": 0.3144048764837985,
      "grad_norm": 2.4384777545928955,
      "learning_rate": 0.00017906106298791574,
      "loss": 3.9523,
      "step": 980
    },
    {
      "epoch": 0.3176130895091434,
      "grad_norm": 2.7034900188446045,
      "learning_rate": 0.00017884718211955941,
      "loss": 4.1155,
      "step": 990
    },
    {
      "epoch": 0.3208213025344883,
      "grad_norm": 1.479897379875183,
      "learning_rate": 0.00017863330125120308,
      "loss": 4.0588,
      "step": 1000
    },
    {
      "epoch": 0.32402951555983317,
      "grad_norm": 1.9492558240890503,
      "learning_rate": 0.00017841942038284675,
      "loss": 4.0175,
      "step": 1010
    },
    {
      "epoch": 0.3272377285851781,
      "grad_norm": 1.9467204809188843,
      "learning_rate": 0.00017820553951449045,
      "loss": 4.1938,
      "step": 1020
    },
    {
      "epoch": 0.33044594161052293,
      "grad_norm": 2.906378984451294,
      "learning_rate": 0.00017799165864613412,
      "loss": 4.0278,
      "step": 1030
    },
    {
      "epoch": 0.33365415463586784,
      "grad_norm": 2.1511802673339844,
      "learning_rate": 0.00017777777777777779,
      "loss": 4.0039,
      "step": 1040
    },
    {
      "epoch": 0.3368623676612127,
      "grad_norm": 1.5978970527648926,
      "learning_rate": 0.00017756389690942146,
      "loss": 3.98,
      "step": 1050
    },
    {
      "epoch": 0.3400705806865576,
      "grad_norm": 1.5500731468200684,
      "learning_rate": 0.00017735001604106512,
      "loss": 4.0384,
      "step": 1060
    },
    {
      "epoch": 0.34327879371190245,
      "grad_norm": 2.749234676361084,
      "learning_rate": 0.00017713613517270882,
      "loss": 3.9761,
      "step": 1070
    },
    {
      "epoch": 0.34648700673724736,
      "grad_norm": 1.9572138786315918,
      "learning_rate": 0.00017692225430435246,
      "loss": 3.8539,
      "step": 1080
    },
    {
      "epoch": 0.3496952197625922,
      "grad_norm": 2.164808750152588,
      "learning_rate": 0.00017670837343599616,
      "loss": 4.0017,
      "step": 1090
    },
    {
      "epoch": 0.3529034327879371,
      "grad_norm": 1.5234178304672241,
      "learning_rate": 0.00017649449256763983,
      "loss": 4.0751,
      "step": 1100
    },
    {
      "epoch": 0.35611164581328203,
      "grad_norm": 2.439108371734619,
      "learning_rate": 0.00017628061169928352,
      "loss": 3.9935,
      "step": 1110
    },
    {
      "epoch": 0.3593198588386269,
      "grad_norm": 1.7081377506256104,
      "learning_rate": 0.00017606673083092717,
      "loss": 3.9497,
      "step": 1120
    },
    {
      "epoch": 0.3625280718639718,
      "grad_norm": 2.918872356414795,
      "learning_rate": 0.00017585284996257086,
      "loss": 3.8335,
      "step": 1130
    },
    {
      "epoch": 0.36573628488931664,
      "grad_norm": 2.6193735599517822,
      "learning_rate": 0.00017563896909421453,
      "loss": 3.9514,
      "step": 1140
    },
    {
      "epoch": 0.36894449791466155,
      "grad_norm": 2.0638182163238525,
      "learning_rate": 0.0001754250882258582,
      "loss": 4.0007,
      "step": 1150
    },
    {
      "epoch": 0.3721527109400064,
      "grad_norm": 2.0979418754577637,
      "learning_rate": 0.00017521120735750187,
      "loss": 4.2207,
      "step": 1160
    },
    {
      "epoch": 0.3753609239653513,
      "grad_norm": 1.6236650943756104,
      "learning_rate": 0.00017499732648914554,
      "loss": 4.1278,
      "step": 1170
    },
    {
      "epoch": 0.37856913699069616,
      "grad_norm": 1.4541728496551514,
      "learning_rate": 0.00017478344562078923,
      "loss": 4.0222,
      "step": 1180
    },
    {
      "epoch": 0.3817773500160411,
      "grad_norm": 2.231041193008423,
      "learning_rate": 0.0001745695647524329,
      "loss": 3.7468,
      "step": 1190
    },
    {
      "epoch": 0.3849855630413859,
      "grad_norm": 2.645289659500122,
      "learning_rate": 0.00017435568388407657,
      "loss": 3.7373,
      "step": 1200
    },
    {
      "epoch": 0.38819377606673083,
      "grad_norm": 2.5603702068328857,
      "learning_rate": 0.00017414180301572024,
      "loss": 3.8212,
      "step": 1210
    },
    {
      "epoch": 0.3914019890920757,
      "grad_norm": 2.0443761348724365,
      "learning_rate": 0.00017392792214736394,
      "loss": 4.0386,
      "step": 1220
    },
    {
      "epoch": 0.3946102021174206,
      "grad_norm": 1.8223211765289307,
      "learning_rate": 0.0001737140412790076,
      "loss": 4.0416,
      "step": 1230
    },
    {
      "epoch": 0.3978184151427655,
      "grad_norm": 3.0214056968688965,
      "learning_rate": 0.00017350016041065128,
      "loss": 3.937,
      "step": 1240
    },
    {
      "epoch": 0.40102662816811036,
      "grad_norm": 1.8569966554641724,
      "learning_rate": 0.00017328627954229495,
      "loss": 4.1451,
      "step": 1250
    },
    {
      "epoch": 0.40423484119345526,
      "grad_norm": 2.539517879486084,
      "learning_rate": 0.00017307239867393864,
      "loss": 4.0165,
      "step": 1260
    },
    {
      "epoch": 0.4074430542188001,
      "grad_norm": 2.017457962036133,
      "learning_rate": 0.0001728585178055823,
      "loss": 3.8963,
      "step": 1270
    },
    {
      "epoch": 0.410651267244145,
      "grad_norm": 2.2230031490325928,
      "learning_rate": 0.00017264463693722598,
      "loss": 3.8815,
      "step": 1280
    },
    {
      "epoch": 0.4138594802694899,
      "grad_norm": 1.9695563316345215,
      "learning_rate": 0.00017243075606886965,
      "loss": 3.8648,
      "step": 1290
    },
    {
      "epoch": 0.4170676932948348,
      "grad_norm": 1.8194799423217773,
      "learning_rate": 0.00017221687520051332,
      "loss": 4.145,
      "step": 1300
    },
    {
      "epoch": 0.42027590632017964,
      "grad_norm": 2.072035074234009,
      "learning_rate": 0.000172002994332157,
      "loss": 3.8141,
      "step": 1310
    },
    {
      "epoch": 0.42348411934552455,
      "grad_norm": 1.9647324085235596,
      "learning_rate": 0.00017178911346380066,
      "loss": 4.1425,
      "step": 1320
    },
    {
      "epoch": 0.4266923323708694,
      "grad_norm": 2.0801777839660645,
      "learning_rate": 0.00017157523259544435,
      "loss": 4.0883,
      "step": 1330
    },
    {
      "epoch": 0.4299005453962143,
      "grad_norm": 1.7603702545166016,
      "learning_rate": 0.00017136135172708802,
      "loss": 3.8815,
      "step": 1340
    },
    {
      "epoch": 0.4331087584215592,
      "grad_norm": 2.200935125350952,
      "learning_rate": 0.0001711474708587317,
      "loss": 3.7951,
      "step": 1350
    },
    {
      "epoch": 0.43631697144690407,
      "grad_norm": 2.3795740604400635,
      "learning_rate": 0.00017093358999037536,
      "loss": 4.031,
      "step": 1360
    },
    {
      "epoch": 0.439525184472249,
      "grad_norm": 3.0328521728515625,
      "learning_rate": 0.00017071970912201906,
      "loss": 3.9536,
      "step": 1370
    },
    {
      "epoch": 0.44273339749759383,
      "grad_norm": 1.859718680381775,
      "learning_rate": 0.00017050582825366273,
      "loss": 4.1404,
      "step": 1380
    },
    {
      "epoch": 0.44594161052293874,
      "grad_norm": 1.8813097476959229,
      "learning_rate": 0.0001702919473853064,
      "loss": 4.068,
      "step": 1390
    },
    {
      "epoch": 0.4491498235482836,
      "grad_norm": 1.587493658065796,
      "learning_rate": 0.00017007806651695006,
      "loss": 3.7291,
      "step": 1400
    },
    {
      "epoch": 0.4523580365736285,
      "grad_norm": 2.6667652130126953,
      "learning_rate": 0.00016986418564859373,
      "loss": 4.1571,
      "step": 1410
    },
    {
      "epoch": 0.45556624959897335,
      "grad_norm": 2.054722785949707,
      "learning_rate": 0.00016965030478023743,
      "loss": 3.5774,
      "step": 1420
    },
    {
      "epoch": 0.45877446262431826,
      "grad_norm": 2.0262043476104736,
      "learning_rate": 0.00016943642391188107,
      "loss": 3.9615,
      "step": 1430
    },
    {
      "epoch": 0.4619826756496631,
      "grad_norm": 2.4865529537200928,
      "learning_rate": 0.00016922254304352477,
      "loss": 3.8782,
      "step": 1440
    },
    {
      "epoch": 0.465190888675008,
      "grad_norm": 1.8818535804748535,
      "learning_rate": 0.00016900866217516844,
      "loss": 4.1086,
      "step": 1450
    },
    {
      "epoch": 0.4683991017003529,
      "grad_norm": 2.109370470046997,
      "learning_rate": 0.00016879478130681213,
      "loss": 3.9845,
      "step": 1460
    },
    {
      "epoch": 0.4716073147256978,
      "grad_norm": 1.8926252126693726,
      "learning_rate": 0.00016858090043845577,
      "loss": 4.1483,
      "step": 1470
    },
    {
      "epoch": 0.4748155277510427,
      "grad_norm": 1.878896951675415,
      "learning_rate": 0.00016836701957009947,
      "loss": 4.0156,
      "step": 1480
    },
    {
      "epoch": 0.47802374077638754,
      "grad_norm": 1.9727569818496704,
      "learning_rate": 0.00016815313870174314,
      "loss": 4.0899,
      "step": 1490
    },
    {
      "epoch": 0.48123195380173245,
      "grad_norm": 2.6462152004241943,
      "learning_rate": 0.0001679392578333868,
      "loss": 4.0509,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 9351,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 393297002496000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
