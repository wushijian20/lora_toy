{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.085338466474174,
  "eval_steps": 500,
  "global_step": 6500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003208213025344883,
      "grad_norm": 0.9929807782173157,
      "learning_rate": 0.00019980750721847933,
      "loss": 6.6175,
      "step": 10
    },
    {
      "epoch": 0.006416426050689766,
      "grad_norm": 1.3569743633270264,
      "learning_rate": 0.000199593626350123,
      "loss": 6.4576,
      "step": 20
    },
    {
      "epoch": 0.009624639076034648,
      "grad_norm": 1.9002569913864136,
      "learning_rate": 0.00019937974548176667,
      "loss": 5.9434,
      "step": 30
    },
    {
      "epoch": 0.012832852101379532,
      "grad_norm": 2.187473773956299,
      "learning_rate": 0.00019916586461341034,
      "loss": 4.9691,
      "step": 40
    },
    {
      "epoch": 0.016041065126724416,
      "grad_norm": 1.5395301580429077,
      "learning_rate": 0.000198951983745054,
      "loss": 5.1283,
      "step": 50
    },
    {
      "epoch": 0.019249278152069296,
      "grad_norm": 1.0944130420684814,
      "learning_rate": 0.0001987381028766977,
      "loss": 5.014,
      "step": 60
    },
    {
      "epoch": 0.02245749117741418,
      "grad_norm": 1.463235855102539,
      "learning_rate": 0.00019852422200834134,
      "loss": 4.8141,
      "step": 70
    },
    {
      "epoch": 0.025665704202759064,
      "grad_norm": 1.5792930126190186,
      "learning_rate": 0.00019831034113998504,
      "loss": 4.8278,
      "step": 80
    },
    {
      "epoch": 0.028873917228103944,
      "grad_norm": 1.4631986618041992,
      "learning_rate": 0.0001980964602716287,
      "loss": 4.6946,
      "step": 90
    },
    {
      "epoch": 0.03208213025344883,
      "grad_norm": 1.7708677053451538,
      "learning_rate": 0.0001978825794032724,
      "loss": 4.5593,
      "step": 100
    },
    {
      "epoch": 0.03529034327879371,
      "grad_norm": 1.3442966938018799,
      "learning_rate": 0.00019766869853491605,
      "loss": 4.5715,
      "step": 110
    },
    {
      "epoch": 0.03849855630413859,
      "grad_norm": 3.7381999492645264,
      "learning_rate": 0.00019745481766655974,
      "loss": 4.361,
      "step": 120
    },
    {
      "epoch": 0.04170676932948348,
      "grad_norm": 1.6169660091400146,
      "learning_rate": 0.0001972409367982034,
      "loss": 4.3992,
      "step": 130
    },
    {
      "epoch": 0.04491498235482836,
      "grad_norm": 1.7813392877578735,
      "learning_rate": 0.00019702705592984708,
      "loss": 4.5034,
      "step": 140
    },
    {
      "epoch": 0.04812319538017324,
      "grad_norm": 1.304078221321106,
      "learning_rate": 0.00019681317506149075,
      "loss": 4.6195,
      "step": 150
    },
    {
      "epoch": 0.05133140840551813,
      "grad_norm": 1.8750945329666138,
      "learning_rate": 0.00019659929419313442,
      "loss": 4.2862,
      "step": 160
    },
    {
      "epoch": 0.05453962143086301,
      "grad_norm": 1.4983075857162476,
      "learning_rate": 0.00019638541332477811,
      "loss": 4.3601,
      "step": 170
    },
    {
      "epoch": 0.05774783445620789,
      "grad_norm": 1.5268137454986572,
      "learning_rate": 0.00019617153245642178,
      "loss": 4.443,
      "step": 180
    },
    {
      "epoch": 0.060956047481552776,
      "grad_norm": 1.523414134979248,
      "learning_rate": 0.00019595765158806545,
      "loss": 4.4724,
      "step": 190
    },
    {
      "epoch": 0.06416426050689766,
      "grad_norm": 1.7114750146865845,
      "learning_rate": 0.00019574377071970912,
      "loss": 4.4758,
      "step": 200
    },
    {
      "epoch": 0.06737247353224254,
      "grad_norm": 2.3549723625183105,
      "learning_rate": 0.00019552988985135282,
      "loss": 4.4372,
      "step": 210
    },
    {
      "epoch": 0.07058068655758742,
      "grad_norm": 2.8332765102386475,
      "learning_rate": 0.0001953160089829965,
      "loss": 4.4495,
      "step": 220
    },
    {
      "epoch": 0.0737888995829323,
      "grad_norm": 2.200261354446411,
      "learning_rate": 0.00019510212811464016,
      "loss": 4.3283,
      "step": 230
    },
    {
      "epoch": 0.07699711260827719,
      "grad_norm": 1.7542457580566406,
      "learning_rate": 0.00019488824724628383,
      "loss": 4.3585,
      "step": 240
    },
    {
      "epoch": 0.08020532563362208,
      "grad_norm": 2.2906546592712402,
      "learning_rate": 0.00019467436637792752,
      "loss": 4.3102,
      "step": 250
    },
    {
      "epoch": 0.08341353865896696,
      "grad_norm": 3.6942086219787598,
      "learning_rate": 0.0001944604855095712,
      "loss": 4.515,
      "step": 260
    },
    {
      "epoch": 0.08662175168431184,
      "grad_norm": 1.9311494827270508,
      "learning_rate": 0.00019424660464121486,
      "loss": 4.0416,
      "step": 270
    },
    {
      "epoch": 0.08982996470965672,
      "grad_norm": 3.2551963329315186,
      "learning_rate": 0.00019403272377285853,
      "loss": 4.2073,
      "step": 280
    },
    {
      "epoch": 0.0930381777350016,
      "grad_norm": 2.0594606399536133,
      "learning_rate": 0.0001938188429045022,
      "loss": 4.0984,
      "step": 290
    },
    {
      "epoch": 0.09624639076034648,
      "grad_norm": 2.2760660648345947,
      "learning_rate": 0.0001936049620361459,
      "loss": 4.291,
      "step": 300
    },
    {
      "epoch": 0.09945460378569138,
      "grad_norm": 1.9606152772903442,
      "learning_rate": 0.00019339108116778954,
      "loss": 4.0656,
      "step": 310
    },
    {
      "epoch": 0.10266281681103626,
      "grad_norm": 1.33408522605896,
      "learning_rate": 0.00019317720029943323,
      "loss": 4.2605,
      "step": 320
    },
    {
      "epoch": 0.10587102983638114,
      "grad_norm": 3.5272436141967773,
      "learning_rate": 0.0001929633194310769,
      "loss": 4.0691,
      "step": 330
    },
    {
      "epoch": 0.10907924286172602,
      "grad_norm": 3.050316333770752,
      "learning_rate": 0.00019274943856272057,
      "loss": 4.1776,
      "step": 340
    },
    {
      "epoch": 0.1122874558870709,
      "grad_norm": 1.6468826532363892,
      "learning_rate": 0.00019253555769436424,
      "loss": 4.405,
      "step": 350
    },
    {
      "epoch": 0.11549566891241578,
      "grad_norm": 1.9157220125198364,
      "learning_rate": 0.00019232167682600794,
      "loss": 4.1456,
      "step": 360
    },
    {
      "epoch": 0.11870388193776067,
      "grad_norm": 3.1583054065704346,
      "learning_rate": 0.0001921077959576516,
      "loss": 4.1256,
      "step": 370
    },
    {
      "epoch": 0.12191209496310555,
      "grad_norm": 1.8012769222259521,
      "learning_rate": 0.00019189391508929527,
      "loss": 4.156,
      "step": 380
    },
    {
      "epoch": 0.12512030798845045,
      "grad_norm": 2.0717804431915283,
      "learning_rate": 0.00019168003422093894,
      "loss": 4.397,
      "step": 390
    },
    {
      "epoch": 0.12832852101379533,
      "grad_norm": 2.4557945728302,
      "learning_rate": 0.0001914661533525826,
      "loss": 4.2566,
      "step": 400
    },
    {
      "epoch": 0.1315367340391402,
      "grad_norm": 1.787262201309204,
      "learning_rate": 0.0001912522724842263,
      "loss": 4.38,
      "step": 410
    },
    {
      "epoch": 0.1347449470644851,
      "grad_norm": 1.9285707473754883,
      "learning_rate": 0.00019103839161586995,
      "loss": 4.0746,
      "step": 420
    },
    {
      "epoch": 0.13795316008982997,
      "grad_norm": 2.7452640533447266,
      "learning_rate": 0.00019082451074751365,
      "loss": 4.2652,
      "step": 430
    },
    {
      "epoch": 0.14116137311517485,
      "grad_norm": 2.027345895767212,
      "learning_rate": 0.00019061062987915732,
      "loss": 4.1917,
      "step": 440
    },
    {
      "epoch": 0.14436958614051973,
      "grad_norm": 1.647667407989502,
      "learning_rate": 0.000190396749010801,
      "loss": 4.2686,
      "step": 450
    },
    {
      "epoch": 0.1475777991658646,
      "grad_norm": 2.308013916015625,
      "learning_rate": 0.00019018286814244465,
      "loss": 4.2013,
      "step": 460
    },
    {
      "epoch": 0.1507860121912095,
      "grad_norm": 2.484921932220459,
      "learning_rate": 0.00018996898727408835,
      "loss": 4.2281,
      "step": 470
    },
    {
      "epoch": 0.15399422521655437,
      "grad_norm": 1.358372688293457,
      "learning_rate": 0.00018975510640573202,
      "loss": 3.7731,
      "step": 480
    },
    {
      "epoch": 0.15720243824189925,
      "grad_norm": 1.484513521194458,
      "learning_rate": 0.0001895412255373757,
      "loss": 4.2264,
      "step": 490
    },
    {
      "epoch": 0.16041065126724416,
      "grad_norm": 1.9827213287353516,
      "learning_rate": 0.00018932734466901936,
      "loss": 4.2347,
      "step": 500
    },
    {
      "epoch": 0.16361886429258904,
      "grad_norm": 1.3327556848526,
      "learning_rate": 0.00018911346380066303,
      "loss": 4.0443,
      "step": 510
    },
    {
      "epoch": 0.16682707731793392,
      "grad_norm": 1.8973983526229858,
      "learning_rate": 0.00018889958293230672,
      "loss": 4.2888,
      "step": 520
    },
    {
      "epoch": 0.1700352903432788,
      "grad_norm": 1.7959173917770386,
      "learning_rate": 0.0001886857020639504,
      "loss": 4.1918,
      "step": 530
    },
    {
      "epoch": 0.17324350336862368,
      "grad_norm": 1.7852979898452759,
      "learning_rate": 0.00018847182119559406,
      "loss": 4.0564,
      "step": 540
    },
    {
      "epoch": 0.17645171639396856,
      "grad_norm": 1.680551528930664,
      "learning_rate": 0.00018825794032723773,
      "loss": 4.3001,
      "step": 550
    },
    {
      "epoch": 0.17965992941931344,
      "grad_norm": 2.1219851970672607,
      "learning_rate": 0.00018804405945888143,
      "loss": 4.013,
      "step": 560
    },
    {
      "epoch": 0.18286814244465832,
      "grad_norm": 2.999102830886841,
      "learning_rate": 0.0001878301785905251,
      "loss": 3.9916,
      "step": 570
    },
    {
      "epoch": 0.1860763554700032,
      "grad_norm": 3.4120051860809326,
      "learning_rate": 0.00018761629772216876,
      "loss": 3.9925,
      "step": 580
    },
    {
      "epoch": 0.18928456849534808,
      "grad_norm": 3.4790403842926025,
      "learning_rate": 0.00018740241685381243,
      "loss": 4.0963,
      "step": 590
    },
    {
      "epoch": 0.19249278152069296,
      "grad_norm": 2.6449272632598877,
      "learning_rate": 0.0001871885359854561,
      "loss": 3.8605,
      "step": 600
    },
    {
      "epoch": 0.19570099454603784,
      "grad_norm": 1.7391000986099243,
      "learning_rate": 0.0001869746551170998,
      "loss": 4.0297,
      "step": 610
    },
    {
      "epoch": 0.19890920757138275,
      "grad_norm": 2.405965566635132,
      "learning_rate": 0.00018676077424874344,
      "loss": 4.1504,
      "step": 620
    },
    {
      "epoch": 0.20211742059672763,
      "grad_norm": 2.2231500148773193,
      "learning_rate": 0.00018654689338038714,
      "loss": 4.1188,
      "step": 630
    },
    {
      "epoch": 0.2053256336220725,
      "grad_norm": 2.632821798324585,
      "learning_rate": 0.0001863330125120308,
      "loss": 4.0629,
      "step": 640
    },
    {
      "epoch": 0.2085338466474174,
      "grad_norm": 2.695380449295044,
      "learning_rate": 0.00018611913164367447,
      "loss": 4.0123,
      "step": 650
    },
    {
      "epoch": 0.21174205967276227,
      "grad_norm": 2.8645012378692627,
      "learning_rate": 0.00018590525077531814,
      "loss": 3.9132,
      "step": 660
    },
    {
      "epoch": 0.21495027269810715,
      "grad_norm": 1.7544795274734497,
      "learning_rate": 0.00018569136990696184,
      "loss": 3.9614,
      "step": 670
    },
    {
      "epoch": 0.21815848572345203,
      "grad_norm": 1.7987943887710571,
      "learning_rate": 0.0001854774890386055,
      "loss": 4.0961,
      "step": 680
    },
    {
      "epoch": 0.22136669874879691,
      "grad_norm": 2.089115619659424,
      "learning_rate": 0.00018526360817024918,
      "loss": 4.1038,
      "step": 690
    },
    {
      "epoch": 0.2245749117741418,
      "grad_norm": 2.333409309387207,
      "learning_rate": 0.00018504972730189285,
      "loss": 4.2228,
      "step": 700
    },
    {
      "epoch": 0.22778312479948667,
      "grad_norm": 2.236550807952881,
      "learning_rate": 0.00018483584643353654,
      "loss": 4.2367,
      "step": 710
    },
    {
      "epoch": 0.23099133782483156,
      "grad_norm": 2.5641939640045166,
      "learning_rate": 0.0001846219655651802,
      "loss": 3.8369,
      "step": 720
    },
    {
      "epoch": 0.23419955085017646,
      "grad_norm": 1.9960426092147827,
      "learning_rate": 0.00018440808469682388,
      "loss": 3.979,
      "step": 730
    },
    {
      "epoch": 0.23740776387552134,
      "grad_norm": 1.7743498086929321,
      "learning_rate": 0.00018419420382846755,
      "loss": 3.8836,
      "step": 740
    },
    {
      "epoch": 0.24061597690086622,
      "grad_norm": 1.9773372411727905,
      "learning_rate": 0.00018398032296011122,
      "loss": 4.2858,
      "step": 750
    },
    {
      "epoch": 0.2438241899262111,
      "grad_norm": 2.5570192337036133,
      "learning_rate": 0.00018376644209175492,
      "loss": 4.1066,
      "step": 760
    },
    {
      "epoch": 0.24703240295155598,
      "grad_norm": 3.1494789123535156,
      "learning_rate": 0.00018355256122339856,
      "loss": 3.9116,
      "step": 770
    },
    {
      "epoch": 0.2502406159769009,
      "grad_norm": 2.2536778450012207,
      "learning_rate": 0.00018333868035504225,
      "loss": 4.0007,
      "step": 780
    },
    {
      "epoch": 0.25344882900224575,
      "grad_norm": 2.004897356033325,
      "learning_rate": 0.00018312479948668592,
      "loss": 4.0483,
      "step": 790
    },
    {
      "epoch": 0.25665704202759065,
      "grad_norm": 1.5317282676696777,
      "learning_rate": 0.00018291091861832962,
      "loss": 4.1112,
      "step": 800
    },
    {
      "epoch": 0.2598652550529355,
      "grad_norm": 1.8230574131011963,
      "learning_rate": 0.00018269703774997326,
      "loss": 4.295,
      "step": 810
    },
    {
      "epoch": 0.2630734680782804,
      "grad_norm": 1.616417646408081,
      "learning_rate": 0.00018248315688161696,
      "loss": 4.0235,
      "step": 820
    },
    {
      "epoch": 0.26628168110362527,
      "grad_norm": 1.9636763334274292,
      "learning_rate": 0.00018226927601326063,
      "loss": 4.2052,
      "step": 830
    },
    {
      "epoch": 0.2694898941289702,
      "grad_norm": 1.9898827075958252,
      "learning_rate": 0.0001820553951449043,
      "loss": 3.8299,
      "step": 840
    },
    {
      "epoch": 0.27269810715431503,
      "grad_norm": 2.130124807357788,
      "learning_rate": 0.00018184151427654797,
      "loss": 4.2331,
      "step": 850
    },
    {
      "epoch": 0.27590632017965994,
      "grad_norm": 1.859649896621704,
      "learning_rate": 0.00018162763340819163,
      "loss": 4.0141,
      "step": 860
    },
    {
      "epoch": 0.2791145332050048,
      "grad_norm": 2.182663917541504,
      "learning_rate": 0.00018141375253983533,
      "loss": 3.878,
      "step": 870
    },
    {
      "epoch": 0.2823227462303497,
      "grad_norm": 2.1161508560180664,
      "learning_rate": 0.000181199871671479,
      "loss": 4.1835,
      "step": 880
    },
    {
      "epoch": 0.2855309592556946,
      "grad_norm": 2.5770223140716553,
      "learning_rate": 0.00018098599080312267,
      "loss": 3.8088,
      "step": 890
    },
    {
      "epoch": 0.28873917228103946,
      "grad_norm": 2.4349141120910645,
      "learning_rate": 0.00018077210993476634,
      "loss": 3.9945,
      "step": 900
    },
    {
      "epoch": 0.29194738530638437,
      "grad_norm": 1.6255314350128174,
      "learning_rate": 0.00018055822906641003,
      "loss": 3.8995,
      "step": 910
    },
    {
      "epoch": 0.2951555983317292,
      "grad_norm": 3.37870192527771,
      "learning_rate": 0.0001803443481980537,
      "loss": 4.0921,
      "step": 920
    },
    {
      "epoch": 0.2983638113570741,
      "grad_norm": 2.148959159851074,
      "learning_rate": 0.00018013046732969737,
      "loss": 3.8036,
      "step": 930
    },
    {
      "epoch": 0.301572024382419,
      "grad_norm": 2.0911977291107178,
      "learning_rate": 0.00017991658646134104,
      "loss": 4.0936,
      "step": 940
    },
    {
      "epoch": 0.3047802374077639,
      "grad_norm": 1.8594186305999756,
      "learning_rate": 0.0001797027055929847,
      "loss": 4.1379,
      "step": 950
    },
    {
      "epoch": 0.30798845043310874,
      "grad_norm": 2.280639886856079,
      "learning_rate": 0.0001794888247246284,
      "loss": 4.1401,
      "step": 960
    },
    {
      "epoch": 0.31119666345845365,
      "grad_norm": 2.317460775375366,
      "learning_rate": 0.00017927494385627205,
      "loss": 3.874,
      "step": 970
    },
    {
      "epoch": 0.3144048764837985,
      "grad_norm": 1.4703642129898071,
      "learning_rate": 0.00017906106298791574,
      "loss": 4.2674,
      "step": 980
    },
    {
      "epoch": 0.3176130895091434,
      "grad_norm": 1.6096246242523193,
      "learning_rate": 0.00017884718211955941,
      "loss": 4.159,
      "step": 990
    },
    {
      "epoch": 0.3208213025344883,
      "grad_norm": 1.5120058059692383,
      "learning_rate": 0.00017863330125120308,
      "loss": 3.9474,
      "step": 1000
    },
    {
      "epoch": 0.32402951555983317,
      "grad_norm": 1.886333703994751,
      "learning_rate": 0.00017841942038284675,
      "loss": 3.9808,
      "step": 1010
    },
    {
      "epoch": 0.3272377285851781,
      "grad_norm": 2.2596702575683594,
      "learning_rate": 0.00017820553951449045,
      "loss": 3.9915,
      "step": 1020
    },
    {
      "epoch": 0.33044594161052293,
      "grad_norm": 2.04266619682312,
      "learning_rate": 0.00017799165864613412,
      "loss": 4.0146,
      "step": 1030
    },
    {
      "epoch": 0.33365415463586784,
      "grad_norm": 1.6656841039657593,
      "learning_rate": 0.00017777777777777779,
      "loss": 4.0215,
      "step": 1040
    },
    {
      "epoch": 0.3368623676612127,
      "grad_norm": 2.7082571983337402,
      "learning_rate": 0.00017756389690942146,
      "loss": 3.9371,
      "step": 1050
    },
    {
      "epoch": 0.3400705806865576,
      "grad_norm": 1.9280714988708496,
      "learning_rate": 0.00017735001604106512,
      "loss": 4.0654,
      "step": 1060
    },
    {
      "epoch": 0.34327879371190245,
      "grad_norm": 1.6905943155288696,
      "learning_rate": 0.00017713613517270882,
      "loss": 4.061,
      "step": 1070
    },
    {
      "epoch": 0.34648700673724736,
      "grad_norm": 2.4728879928588867,
      "learning_rate": 0.00017692225430435246,
      "loss": 3.8914,
      "step": 1080
    },
    {
      "epoch": 0.3496952197625922,
      "grad_norm": 1.6264522075653076,
      "learning_rate": 0.00017670837343599616,
      "loss": 3.9604,
      "step": 1090
    },
    {
      "epoch": 0.3529034327879371,
      "grad_norm": 1.7835686206817627,
      "learning_rate": 0.00017649449256763983,
      "loss": 4.0851,
      "step": 1100
    },
    {
      "epoch": 0.35611164581328203,
      "grad_norm": 1.9933665990829468,
      "learning_rate": 0.00017628061169928352,
      "loss": 3.9079,
      "step": 1110
    },
    {
      "epoch": 0.3593198588386269,
      "grad_norm": 4.560632228851318,
      "learning_rate": 0.00017606673083092717,
      "loss": 4.0444,
      "step": 1120
    },
    {
      "epoch": 0.3625280718639718,
      "grad_norm": 2.053833246231079,
      "learning_rate": 0.00017585284996257086,
      "loss": 4.1077,
      "step": 1130
    },
    {
      "epoch": 0.36573628488931664,
      "grad_norm": 2.3089566230773926,
      "learning_rate": 0.00017563896909421453,
      "loss": 3.8746,
      "step": 1140
    },
    {
      "epoch": 0.36894449791466155,
      "grad_norm": 2.273993492126465,
      "learning_rate": 0.0001754250882258582,
      "loss": 3.9762,
      "step": 1150
    },
    {
      "epoch": 0.3721527109400064,
      "grad_norm": 2.6280791759490967,
      "learning_rate": 0.00017521120735750187,
      "loss": 4.1242,
      "step": 1160
    },
    {
      "epoch": 0.3753609239653513,
      "grad_norm": 3.4105989933013916,
      "learning_rate": 0.00017499732648914554,
      "loss": 3.9282,
      "step": 1170
    },
    {
      "epoch": 0.37856913699069616,
      "grad_norm": 1.7122840881347656,
      "learning_rate": 0.00017478344562078923,
      "loss": 3.9074,
      "step": 1180
    },
    {
      "epoch": 0.3817773500160411,
      "grad_norm": 1.8528074026107788,
      "learning_rate": 0.0001745695647524329,
      "loss": 4.1092,
      "step": 1190
    },
    {
      "epoch": 0.3849855630413859,
      "grad_norm": 1.9337636232376099,
      "learning_rate": 0.00017435568388407657,
      "loss": 4.0473,
      "step": 1200
    },
    {
      "epoch": 0.38819377606673083,
      "grad_norm": 1.9283006191253662,
      "learning_rate": 0.00017414180301572024,
      "loss": 4.1426,
      "step": 1210
    },
    {
      "epoch": 0.3914019890920757,
      "grad_norm": 2.136915445327759,
      "learning_rate": 0.00017392792214736394,
      "loss": 4.118,
      "step": 1220
    },
    {
      "epoch": 0.3946102021174206,
      "grad_norm": 3.090832233428955,
      "learning_rate": 0.0001737140412790076,
      "loss": 4.0838,
      "step": 1230
    },
    {
      "epoch": 0.3978184151427655,
      "grad_norm": 1.7367016077041626,
      "learning_rate": 0.00017350016041065128,
      "loss": 4.0498,
      "step": 1240
    },
    {
      "epoch": 0.40102662816811036,
      "grad_norm": 2.549715280532837,
      "learning_rate": 0.00017328627954229495,
      "loss": 4.0492,
      "step": 1250
    },
    {
      "epoch": 0.40423484119345526,
      "grad_norm": 1.7982631921768188,
      "learning_rate": 0.00017307239867393864,
      "loss": 4.0505,
      "step": 1260
    },
    {
      "epoch": 0.4074430542188001,
      "grad_norm": 2.527186632156372,
      "learning_rate": 0.0001728585178055823,
      "loss": 3.9963,
      "step": 1270
    },
    {
      "epoch": 0.410651267244145,
      "grad_norm": 2.139296293258667,
      "learning_rate": 0.00017264463693722598,
      "loss": 4.026,
      "step": 1280
    },
    {
      "epoch": 0.4138594802694899,
      "grad_norm": 3.607391119003296,
      "learning_rate": 0.00017243075606886965,
      "loss": 3.716,
      "step": 1290
    },
    {
      "epoch": 0.4170676932948348,
      "grad_norm": 2.2469518184661865,
      "learning_rate": 0.00017221687520051332,
      "loss": 3.9968,
      "step": 1300
    },
    {
      "epoch": 0.42027590632017964,
      "grad_norm": 2.578824043273926,
      "learning_rate": 0.000172002994332157,
      "loss": 3.8834,
      "step": 1310
    },
    {
      "epoch": 0.42348411934552455,
      "grad_norm": 1.9329564571380615,
      "learning_rate": 0.00017178911346380066,
      "loss": 4.0908,
      "step": 1320
    },
    {
      "epoch": 0.4266923323708694,
      "grad_norm": 3.4855501651763916,
      "learning_rate": 0.00017157523259544435,
      "loss": 4.0754,
      "step": 1330
    },
    {
      "epoch": 0.4299005453962143,
      "grad_norm": 1.592391848564148,
      "learning_rate": 0.00017136135172708802,
      "loss": 4.0788,
      "step": 1340
    },
    {
      "epoch": 0.4331087584215592,
      "grad_norm": 1.7768322229385376,
      "learning_rate": 0.0001711474708587317,
      "loss": 4.167,
      "step": 1350
    },
    {
      "epoch": 0.43631697144690407,
      "grad_norm": 1.798975944519043,
      "learning_rate": 0.00017093358999037536,
      "loss": 3.9586,
      "step": 1360
    },
    {
      "epoch": 0.439525184472249,
      "grad_norm": 2.1416187286376953,
      "learning_rate": 0.00017071970912201906,
      "loss": 3.855,
      "step": 1370
    },
    {
      "epoch": 0.44273339749759383,
      "grad_norm": 2.7391726970672607,
      "learning_rate": 0.00017050582825366273,
      "loss": 3.8697,
      "step": 1380
    },
    {
      "epoch": 0.44594161052293874,
      "grad_norm": 1.9695415496826172,
      "learning_rate": 0.0001702919473853064,
      "loss": 3.6625,
      "step": 1390
    },
    {
      "epoch": 0.4491498235482836,
      "grad_norm": 2.2329742908477783,
      "learning_rate": 0.00017007806651695006,
      "loss": 4.2323,
      "step": 1400
    },
    {
      "epoch": 0.4523580365736285,
      "grad_norm": 2.7690107822418213,
      "learning_rate": 0.00016986418564859373,
      "loss": 3.8557,
      "step": 1410
    },
    {
      "epoch": 0.45556624959897335,
      "grad_norm": 2.4744131565093994,
      "learning_rate": 0.00016965030478023743,
      "loss": 3.9899,
      "step": 1420
    },
    {
      "epoch": 0.45877446262431826,
      "grad_norm": 2.181079149246216,
      "learning_rate": 0.00016943642391188107,
      "loss": 4.1196,
      "step": 1430
    },
    {
      "epoch": 0.4619826756496631,
      "grad_norm": 3.137943983078003,
      "learning_rate": 0.00016922254304352477,
      "loss": 4.1276,
      "step": 1440
    },
    {
      "epoch": 0.465190888675008,
      "grad_norm": 1.8179845809936523,
      "learning_rate": 0.00016900866217516844,
      "loss": 3.888,
      "step": 1450
    },
    {
      "epoch": 0.4683991017003529,
      "grad_norm": 2.0989930629730225,
      "learning_rate": 0.00016879478130681213,
      "loss": 3.8297,
      "step": 1460
    },
    {
      "epoch": 0.4716073147256978,
      "grad_norm": 2.2679786682128906,
      "learning_rate": 0.00016858090043845577,
      "loss": 3.7198,
      "step": 1470
    },
    {
      "epoch": 0.4748155277510427,
      "grad_norm": 2.0550575256347656,
      "learning_rate": 0.00016836701957009947,
      "loss": 4.1815,
      "step": 1480
    },
    {
      "epoch": 0.47802374077638754,
      "grad_norm": 3.2452001571655273,
      "learning_rate": 0.00016815313870174314,
      "loss": 4.0687,
      "step": 1490
    },
    {
      "epoch": 0.48123195380173245,
      "grad_norm": 2.349343776702881,
      "learning_rate": 0.0001679392578333868,
      "loss": 3.8802,
      "step": 1500
    },
    {
      "epoch": 0.4844401668270773,
      "grad_norm": 3.36200213432312,
      "learning_rate": 0.00016772537696503048,
      "loss": 4.1034,
      "step": 1510
    },
    {
      "epoch": 0.4876483798524222,
      "grad_norm": 2.2511489391326904,
      "learning_rate": 0.00016751149609667415,
      "loss": 3.9717,
      "step": 1520
    },
    {
      "epoch": 0.49085659287776706,
      "grad_norm": 2.5120437145233154,
      "learning_rate": 0.00016729761522831784,
      "loss": 4.1176,
      "step": 1530
    },
    {
      "epoch": 0.49406480590311197,
      "grad_norm": 2.5072154998779297,
      "learning_rate": 0.0001670837343599615,
      "loss": 3.8791,
      "step": 1540
    },
    {
      "epoch": 0.4972730189284568,
      "grad_norm": 1.8302147388458252,
      "learning_rate": 0.00016686985349160518,
      "loss": 4.0217,
      "step": 1550
    },
    {
      "epoch": 0.5004812319538018,
      "grad_norm": 1.7831693887710571,
      "learning_rate": 0.00016665597262324885,
      "loss": 3.6792,
      "step": 1560
    },
    {
      "epoch": 0.5036894449791466,
      "grad_norm": 2.4997243881225586,
      "learning_rate": 0.00016644209175489255,
      "loss": 4.2341,
      "step": 1570
    },
    {
      "epoch": 0.5068976580044915,
      "grad_norm": 2.099461793899536,
      "learning_rate": 0.00016622821088653622,
      "loss": 3.9337,
      "step": 1580
    },
    {
      "epoch": 0.5101058710298364,
      "grad_norm": 2.021148920059204,
      "learning_rate": 0.00016601433001817988,
      "loss": 4.0557,
      "step": 1590
    },
    {
      "epoch": 0.5133140840551813,
      "grad_norm": 2.2476346492767334,
      "learning_rate": 0.00016580044914982355,
      "loss": 3.923,
      "step": 1600
    },
    {
      "epoch": 0.5165222970805261,
      "grad_norm": 1.8008980751037598,
      "learning_rate": 0.00016558656828146722,
      "loss": 4.0472,
      "step": 1610
    },
    {
      "epoch": 0.519730510105871,
      "grad_norm": 1.378929615020752,
      "learning_rate": 0.00016537268741311092,
      "loss": 3.8261,
      "step": 1620
    },
    {
      "epoch": 0.5229387231312159,
      "grad_norm": 3.3628547191619873,
      "learning_rate": 0.00016515880654475456,
      "loss": 3.8221,
      "step": 1630
    },
    {
      "epoch": 0.5261469361565608,
      "grad_norm": 1.7510969638824463,
      "learning_rate": 0.00016494492567639826,
      "loss": 3.7721,
      "step": 1640
    },
    {
      "epoch": 0.5293551491819056,
      "grad_norm": 2.7384185791015625,
      "learning_rate": 0.00016473104480804193,
      "loss": 4.1257,
      "step": 1650
    },
    {
      "epoch": 0.5325633622072505,
      "grad_norm": 2.424494743347168,
      "learning_rate": 0.0001645171639396856,
      "loss": 3.8065,
      "step": 1660
    },
    {
      "epoch": 0.5357715752325954,
      "grad_norm": 2.2401938438415527,
      "learning_rate": 0.00016430328307132926,
      "loss": 3.8567,
      "step": 1670
    },
    {
      "epoch": 0.5389797882579404,
      "grad_norm": 3.1622910499572754,
      "learning_rate": 0.00016408940220297296,
      "loss": 3.6804,
      "step": 1680
    },
    {
      "epoch": 0.5421880012832853,
      "grad_norm": 2.1364176273345947,
      "learning_rate": 0.00016387552133461663,
      "loss": 3.9392,
      "step": 1690
    },
    {
      "epoch": 0.5453962143086301,
      "grad_norm": 1.8318675756454468,
      "learning_rate": 0.0001636616404662603,
      "loss": 3.7562,
      "step": 1700
    },
    {
      "epoch": 0.548604427333975,
      "grad_norm": 2.373115062713623,
      "learning_rate": 0.00016344775959790397,
      "loss": 3.781,
      "step": 1710
    },
    {
      "epoch": 0.5518126403593199,
      "grad_norm": 2.475308418273926,
      "learning_rate": 0.00016323387872954766,
      "loss": 4.0115,
      "step": 1720
    },
    {
      "epoch": 0.5550208533846648,
      "grad_norm": 5.762182712554932,
      "learning_rate": 0.00016301999786119133,
      "loss": 3.9147,
      "step": 1730
    },
    {
      "epoch": 0.5582290664100096,
      "grad_norm": 1.8071191310882568,
      "learning_rate": 0.000162806116992835,
      "loss": 4.1507,
      "step": 1740
    },
    {
      "epoch": 0.5614372794353545,
      "grad_norm": 1.9019110202789307,
      "learning_rate": 0.00016259223612447867,
      "loss": 4.1418,
      "step": 1750
    },
    {
      "epoch": 0.5646454924606994,
      "grad_norm": 1.995972752571106,
      "learning_rate": 0.00016237835525612234,
      "loss": 3.8719,
      "step": 1760
    },
    {
      "epoch": 0.5678537054860443,
      "grad_norm": 2.027113437652588,
      "learning_rate": 0.00016216447438776604,
      "loss": 3.9047,
      "step": 1770
    },
    {
      "epoch": 0.5710619185113892,
      "grad_norm": 1.954819679260254,
      "learning_rate": 0.00016195059351940968,
      "loss": 4.1468,
      "step": 1780
    },
    {
      "epoch": 0.574270131536734,
      "grad_norm": 2.091275930404663,
      "learning_rate": 0.00016173671265105337,
      "loss": 3.9813,
      "step": 1790
    },
    {
      "epoch": 0.5774783445620789,
      "grad_norm": 1.6633645296096802,
      "learning_rate": 0.00016152283178269704,
      "loss": 3.777,
      "step": 1800
    },
    {
      "epoch": 0.5806865575874238,
      "grad_norm": 2.522005796432495,
      "learning_rate": 0.00016130895091434074,
      "loss": 3.9359,
      "step": 1810
    },
    {
      "epoch": 0.5838947706127687,
      "grad_norm": 2.0835964679718018,
      "learning_rate": 0.00016109507004598438,
      "loss": 3.9168,
      "step": 1820
    },
    {
      "epoch": 0.5871029836381135,
      "grad_norm": 2.7929747104644775,
      "learning_rate": 0.00016088118917762808,
      "loss": 4.0131,
      "step": 1830
    },
    {
      "epoch": 0.5903111966634584,
      "grad_norm": 1.7257986068725586,
      "learning_rate": 0.00016066730830927175,
      "loss": 3.9439,
      "step": 1840
    },
    {
      "epoch": 0.5935194096888033,
      "grad_norm": 2.0654265880584717,
      "learning_rate": 0.00016045342744091542,
      "loss": 4.1118,
      "step": 1850
    },
    {
      "epoch": 0.5967276227141483,
      "grad_norm": 2.8161866664886475,
      "learning_rate": 0.00016023954657255909,
      "loss": 3.9887,
      "step": 1860
    },
    {
      "epoch": 0.599935835739493,
      "grad_norm": 2.5455033779144287,
      "learning_rate": 0.00016002566570420275,
      "loss": 4.0024,
      "step": 1870
    },
    {
      "epoch": 0.603144048764838,
      "grad_norm": 1.7840386629104614,
      "learning_rate": 0.00015981178483584645,
      "loss": 4.0116,
      "step": 1880
    },
    {
      "epoch": 0.6063522617901829,
      "grad_norm": 3.0625340938568115,
      "learning_rate": 0.00015959790396749012,
      "loss": 3.964,
      "step": 1890
    },
    {
      "epoch": 0.6095604748155278,
      "grad_norm": 2.045647144317627,
      "learning_rate": 0.0001593840230991338,
      "loss": 3.8874,
      "step": 1900
    },
    {
      "epoch": 0.6127686878408727,
      "grad_norm": 1.9196821451187134,
      "learning_rate": 0.00015917014223077746,
      "loss": 3.7512,
      "step": 1910
    },
    {
      "epoch": 0.6159769008662175,
      "grad_norm": 3.1822710037231445,
      "learning_rate": 0.00015895626136242115,
      "loss": 3.853,
      "step": 1920
    },
    {
      "epoch": 0.6191851138915624,
      "grad_norm": 2.67498779296875,
      "learning_rate": 0.00015874238049406482,
      "loss": 3.8842,
      "step": 1930
    },
    {
      "epoch": 0.6223933269169073,
      "grad_norm": 1.5589439868927002,
      "learning_rate": 0.0001585284996257085,
      "loss": 4.1475,
      "step": 1940
    },
    {
      "epoch": 0.6256015399422522,
      "grad_norm": 1.8868211507797241,
      "learning_rate": 0.00015831461875735216,
      "loss": 4.0337,
      "step": 1950
    },
    {
      "epoch": 0.628809752967597,
      "grad_norm": 2.9136695861816406,
      "learning_rate": 0.00015810073788899583,
      "loss": 3.7854,
      "step": 1960
    },
    {
      "epoch": 0.6320179659929419,
      "grad_norm": 2.1526665687561035,
      "learning_rate": 0.0001578868570206395,
      "loss": 4.027,
      "step": 1970
    },
    {
      "epoch": 0.6352261790182868,
      "grad_norm": 2.4602677822113037,
      "learning_rate": 0.00015767297615228317,
      "loss": 3.9006,
      "step": 1980
    },
    {
      "epoch": 0.6384343920436317,
      "grad_norm": 2.325958728790283,
      "learning_rate": 0.00015745909528392686,
      "loss": 3.9056,
      "step": 1990
    },
    {
      "epoch": 0.6416426050689766,
      "grad_norm": 1.8653922080993652,
      "learning_rate": 0.00015724521441557053,
      "loss": 4.0351,
      "step": 2000
    },
    {
      "epoch": 0.6448508180943214,
      "grad_norm": 2.088768243789673,
      "learning_rate": 0.0001570313335472142,
      "loss": 3.7137,
      "step": 2010
    },
    {
      "epoch": 0.6480590311196663,
      "grad_norm": 1.9593974351882935,
      "learning_rate": 0.00015681745267885787,
      "loss": 3.7914,
      "step": 2020
    },
    {
      "epoch": 0.6512672441450112,
      "grad_norm": 2.5103952884674072,
      "learning_rate": 0.00015660357181050157,
      "loss": 3.891,
      "step": 2030
    },
    {
      "epoch": 0.6544754571703562,
      "grad_norm": 1.492457628250122,
      "learning_rate": 0.00015638969094214524,
      "loss": 3.9842,
      "step": 2040
    },
    {
      "epoch": 0.657683670195701,
      "grad_norm": 2.515587091445923,
      "learning_rate": 0.0001561758100737889,
      "loss": 4.0309,
      "step": 2050
    },
    {
      "epoch": 0.6608918832210459,
      "grad_norm": 2.2142090797424316,
      "learning_rate": 0.00015596192920543258,
      "loss": 4.1022,
      "step": 2060
    },
    {
      "epoch": 0.6641000962463908,
      "grad_norm": 2.2114717960357666,
      "learning_rate": 0.00015574804833707624,
      "loss": 3.9963,
      "step": 2070
    },
    {
      "epoch": 0.6673083092717357,
      "grad_norm": 2.033418655395508,
      "learning_rate": 0.00015553416746871994,
      "loss": 4.1751,
      "step": 2080
    },
    {
      "epoch": 0.6705165222970805,
      "grad_norm": 1.9952499866485596,
      "learning_rate": 0.00015532028660036358,
      "loss": 3.8765,
      "step": 2090
    },
    {
      "epoch": 0.6737247353224254,
      "grad_norm": 3.195073366165161,
      "learning_rate": 0.00015510640573200728,
      "loss": 3.7897,
      "step": 2100
    },
    {
      "epoch": 0.6769329483477703,
      "grad_norm": 1.7830086946487427,
      "learning_rate": 0.00015489252486365095,
      "loss": 3.8283,
      "step": 2110
    },
    {
      "epoch": 0.6801411613731152,
      "grad_norm": 1.6678766012191772,
      "learning_rate": 0.00015467864399529464,
      "loss": 4.0029,
      "step": 2120
    },
    {
      "epoch": 0.6833493743984601,
      "grad_norm": 3.0071027278900146,
      "learning_rate": 0.00015446476312693829,
      "loss": 3.8203,
      "step": 2130
    },
    {
      "epoch": 0.6865575874238049,
      "grad_norm": 1.9064921140670776,
      "learning_rate": 0.00015425088225858198,
      "loss": 3.7773,
      "step": 2140
    },
    {
      "epoch": 0.6897658004491498,
      "grad_norm": 1.9289793968200684,
      "learning_rate": 0.00015403700139022565,
      "loss": 3.7336,
      "step": 2150
    },
    {
      "epoch": 0.6929740134744947,
      "grad_norm": 2.277463436126709,
      "learning_rate": 0.00015382312052186935,
      "loss": 3.9426,
      "step": 2160
    },
    {
      "epoch": 0.6961822264998396,
      "grad_norm": 2.0057101249694824,
      "learning_rate": 0.000153609239653513,
      "loss": 4.0449,
      "step": 2170
    },
    {
      "epoch": 0.6993904395251844,
      "grad_norm": 2.138139009475708,
      "learning_rate": 0.00015339535878515669,
      "loss": 3.973,
      "step": 2180
    },
    {
      "epoch": 0.7025986525505293,
      "grad_norm": 2.481306791305542,
      "learning_rate": 0.00015318147791680035,
      "loss": 3.8689,
      "step": 2190
    },
    {
      "epoch": 0.7058068655758742,
      "grad_norm": 1.8604938983917236,
      "learning_rate": 0.00015296759704844402,
      "loss": 3.8348,
      "step": 2200
    },
    {
      "epoch": 0.7090150786012192,
      "grad_norm": 3.08248233795166,
      "learning_rate": 0.0001527537161800877,
      "loss": 3.9962,
      "step": 2210
    },
    {
      "epoch": 0.7122232916265641,
      "grad_norm": 3.3053228855133057,
      "learning_rate": 0.00015253983531173136,
      "loss": 4.1355,
      "step": 2220
    },
    {
      "epoch": 0.7154315046519089,
      "grad_norm": 1.9488270282745361,
      "learning_rate": 0.00015232595444337506,
      "loss": 4.2517,
      "step": 2230
    },
    {
      "epoch": 0.7186397176772538,
      "grad_norm": 1.8827457427978516,
      "learning_rate": 0.00015211207357501873,
      "loss": 3.9881,
      "step": 2240
    },
    {
      "epoch": 0.7218479307025987,
      "grad_norm": 1.871275782585144,
      "learning_rate": 0.0001518981927066624,
      "loss": 3.9882,
      "step": 2250
    },
    {
      "epoch": 0.7250561437279436,
      "grad_norm": 3.001356840133667,
      "learning_rate": 0.00015168431183830607,
      "loss": 3.9887,
      "step": 2260
    },
    {
      "epoch": 0.7282643567532884,
      "grad_norm": 2.8097801208496094,
      "learning_rate": 0.00015147043096994976,
      "loss": 3.9143,
      "step": 2270
    },
    {
      "epoch": 0.7314725697786333,
      "grad_norm": 1.9800089597702026,
      "learning_rate": 0.00015125655010159343,
      "loss": 4.1458,
      "step": 2280
    },
    {
      "epoch": 0.7346807828039782,
      "grad_norm": 1.96299409866333,
      "learning_rate": 0.0001510426692332371,
      "loss": 4.0853,
      "step": 2290
    },
    {
      "epoch": 0.7378889958293231,
      "grad_norm": 2.354498863220215,
      "learning_rate": 0.00015082878836488077,
      "loss": 3.8417,
      "step": 2300
    },
    {
      "epoch": 0.7410972088546679,
      "grad_norm": 2.176910877227783,
      "learning_rate": 0.00015061490749652444,
      "loss": 3.8521,
      "step": 2310
    },
    {
      "epoch": 0.7443054218800128,
      "grad_norm": 1.8055734634399414,
      "learning_rate": 0.0001504010266281681,
      "loss": 3.9303,
      "step": 2320
    },
    {
      "epoch": 0.7475136349053577,
      "grad_norm": 1.7917792797088623,
      "learning_rate": 0.00015018714575981178,
      "loss": 3.738,
      "step": 2330
    },
    {
      "epoch": 0.7507218479307026,
      "grad_norm": 2.117816686630249,
      "learning_rate": 0.00014997326489145547,
      "loss": 4.048,
      "step": 2340
    },
    {
      "epoch": 0.7539300609560475,
      "grad_norm": 1.8879636526107788,
      "learning_rate": 0.00014975938402309914,
      "loss": 3.6788,
      "step": 2350
    },
    {
      "epoch": 0.7571382739813923,
      "grad_norm": 3.0844225883483887,
      "learning_rate": 0.0001495455031547428,
      "loss": 3.7891,
      "step": 2360
    },
    {
      "epoch": 0.7603464870067372,
      "grad_norm": 2.077103853225708,
      "learning_rate": 0.00014933162228638648,
      "loss": 3.9988,
      "step": 2370
    },
    {
      "epoch": 0.7635547000320821,
      "grad_norm": 2.6136064529418945,
      "learning_rate": 0.00014911774141803018,
      "loss": 3.7669,
      "step": 2380
    },
    {
      "epoch": 0.766762913057427,
      "grad_norm": 1.6826047897338867,
      "learning_rate": 0.00014890386054967385,
      "loss": 3.8013,
      "step": 2390
    },
    {
      "epoch": 0.7699711260827719,
      "grad_norm": 2.2695109844207764,
      "learning_rate": 0.00014868997968131751,
      "loss": 4.1352,
      "step": 2400
    },
    {
      "epoch": 0.7731793391081168,
      "grad_norm": 1.9605375528335571,
      "learning_rate": 0.00014847609881296118,
      "loss": 3.945,
      "step": 2410
    },
    {
      "epoch": 0.7763875521334617,
      "grad_norm": 1.8378612995147705,
      "learning_rate": 0.00014826221794460485,
      "loss": 4.0383,
      "step": 2420
    },
    {
      "epoch": 0.7795957651588066,
      "grad_norm": 2.031574010848999,
      "learning_rate": 0.00014804833707624855,
      "loss": 3.8376,
      "step": 2430
    },
    {
      "epoch": 0.7828039781841514,
      "grad_norm": 1.9898104667663574,
      "learning_rate": 0.0001478344562078922,
      "loss": 3.8931,
      "step": 2440
    },
    {
      "epoch": 0.7860121912094963,
      "grad_norm": 2.859266996383667,
      "learning_rate": 0.0001476205753395359,
      "loss": 4.0024,
      "step": 2450
    },
    {
      "epoch": 0.7892204042348412,
      "grad_norm": 2.4702200889587402,
      "learning_rate": 0.00014740669447117956,
      "loss": 3.7418,
      "step": 2460
    },
    {
      "epoch": 0.7924286172601861,
      "grad_norm": 1.877955675125122,
      "learning_rate": 0.00014719281360282325,
      "loss": 4.0296,
      "step": 2470
    },
    {
      "epoch": 0.795636830285531,
      "grad_norm": 2.0829124450683594,
      "learning_rate": 0.0001469789327344669,
      "loss": 3.919,
      "step": 2480
    },
    {
      "epoch": 0.7988450433108758,
      "grad_norm": 3.3244564533233643,
      "learning_rate": 0.0001467650518661106,
      "loss": 4.045,
      "step": 2490
    },
    {
      "epoch": 0.8020532563362207,
      "grad_norm": 2.064141035079956,
      "learning_rate": 0.00014655117099775426,
      "loss": 3.9802,
      "step": 2500
    },
    {
      "epoch": 0.8052614693615656,
      "grad_norm": 2.8309741020202637,
      "learning_rate": 0.00014633729012939793,
      "loss": 3.7764,
      "step": 2510
    },
    {
      "epoch": 0.8084696823869105,
      "grad_norm": 2.4796295166015625,
      "learning_rate": 0.0001461234092610416,
      "loss": 3.8847,
      "step": 2520
    },
    {
      "epoch": 0.8116778954122553,
      "grad_norm": 1.698612093925476,
      "learning_rate": 0.00014590952839268527,
      "loss": 3.8307,
      "step": 2530
    },
    {
      "epoch": 0.8148861084376002,
      "grad_norm": 2.738569736480713,
      "learning_rate": 0.00014569564752432896,
      "loss": 3.9198,
      "step": 2540
    },
    {
      "epoch": 0.8180943214629451,
      "grad_norm": 3.0707762241363525,
      "learning_rate": 0.00014548176665597263,
      "loss": 3.768,
      "step": 2550
    },
    {
      "epoch": 0.82130253448829,
      "grad_norm": 2.2247698307037354,
      "learning_rate": 0.0001452678857876163,
      "loss": 3.9061,
      "step": 2560
    },
    {
      "epoch": 0.824510747513635,
      "grad_norm": 3.0362846851348877,
      "learning_rate": 0.00014505400491925997,
      "loss": 3.8026,
      "step": 2570
    },
    {
      "epoch": 0.8277189605389798,
      "grad_norm": 1.9672211408615112,
      "learning_rate": 0.00014484012405090367,
      "loss": 3.9032,
      "step": 2580
    },
    {
      "epoch": 0.8309271735643247,
      "grad_norm": 2.3331706523895264,
      "learning_rate": 0.00014462624318254734,
      "loss": 3.9796,
      "step": 2590
    },
    {
      "epoch": 0.8341353865896696,
      "grad_norm": 1.7021349668502808,
      "learning_rate": 0.000144412362314191,
      "loss": 4.0833,
      "step": 2600
    },
    {
      "epoch": 0.8373435996150145,
      "grad_norm": 1.694108486175537,
      "learning_rate": 0.00014419848144583467,
      "loss": 3.9671,
      "step": 2610
    },
    {
      "epoch": 0.8405518126403593,
      "grad_norm": 2.5554816722869873,
      "learning_rate": 0.00014398460057747837,
      "loss": 3.9753,
      "step": 2620
    },
    {
      "epoch": 0.8437600256657042,
      "grad_norm": 2.163881778717041,
      "learning_rate": 0.000143770719709122,
      "loss": 4.022,
      "step": 2630
    },
    {
      "epoch": 0.8469682386910491,
      "grad_norm": 2.019744873046875,
      "learning_rate": 0.0001435568388407657,
      "loss": 3.9977,
      "step": 2640
    },
    {
      "epoch": 0.850176451716394,
      "grad_norm": 2.6273162364959717,
      "learning_rate": 0.00014334295797240938,
      "loss": 3.978,
      "step": 2650
    },
    {
      "epoch": 0.8533846647417388,
      "grad_norm": 2.14687180519104,
      "learning_rate": 0.00014312907710405305,
      "loss": 4.2031,
      "step": 2660
    },
    {
      "epoch": 0.8565928777670837,
      "grad_norm": 2.8306446075439453,
      "learning_rate": 0.00014291519623569672,
      "loss": 3.8411,
      "step": 2670
    },
    {
      "epoch": 0.8598010907924286,
      "grad_norm": 2.250063180923462,
      "learning_rate": 0.00014270131536734038,
      "loss": 4.0618,
      "step": 2680
    },
    {
      "epoch": 0.8630093038177735,
      "grad_norm": 2.203354597091675,
      "learning_rate": 0.00014248743449898408,
      "loss": 3.8217,
      "step": 2690
    },
    {
      "epoch": 0.8662175168431184,
      "grad_norm": 2.717074155807495,
      "learning_rate": 0.00014227355363062775,
      "loss": 3.8408,
      "step": 2700
    },
    {
      "epoch": 0.8694257298684632,
      "grad_norm": 2.20389986038208,
      "learning_rate": 0.00014205967276227142,
      "loss": 4.2645,
      "step": 2710
    },
    {
      "epoch": 0.8726339428938081,
      "grad_norm": 1.997582197189331,
      "learning_rate": 0.0001418457918939151,
      "loss": 4.0416,
      "step": 2720
    },
    {
      "epoch": 0.875842155919153,
      "grad_norm": 2.1643781661987305,
      "learning_rate": 0.00014163191102555878,
      "loss": 3.9677,
      "step": 2730
    },
    {
      "epoch": 0.879050368944498,
      "grad_norm": 2.6494596004486084,
      "learning_rate": 0.00014141803015720245,
      "loss": 3.837,
      "step": 2740
    },
    {
      "epoch": 0.8822585819698427,
      "grad_norm": 2.8622848987579346,
      "learning_rate": 0.00014120414928884612,
      "loss": 3.961,
      "step": 2750
    },
    {
      "epoch": 0.8854667949951877,
      "grad_norm": 2.805772542953491,
      "learning_rate": 0.0001409902684204898,
      "loss": 4.1464,
      "step": 2760
    },
    {
      "epoch": 0.8886750080205326,
      "grad_norm": 2.3361117839813232,
      "learning_rate": 0.00014077638755213346,
      "loss": 4.0967,
      "step": 2770
    },
    {
      "epoch": 0.8918832210458775,
      "grad_norm": 3.002622365951538,
      "learning_rate": 0.00014056250668377716,
      "loss": 3.8562,
      "step": 2780
    },
    {
      "epoch": 0.8950914340712224,
      "grad_norm": 2.404017925262451,
      "learning_rate": 0.0001403486258154208,
      "loss": 3.8703,
      "step": 2790
    },
    {
      "epoch": 0.8982996470965672,
      "grad_norm": 3.2577126026153564,
      "learning_rate": 0.0001401347449470645,
      "loss": 3.8915,
      "step": 2800
    },
    {
      "epoch": 0.9015078601219121,
      "grad_norm": 1.698504090309143,
      "learning_rate": 0.00013992086407870816,
      "loss": 4.0433,
      "step": 2810
    },
    {
      "epoch": 0.904716073147257,
      "grad_norm": 1.7847799062728882,
      "learning_rate": 0.00013970698321035186,
      "loss": 3.8729,
      "step": 2820
    },
    {
      "epoch": 0.9079242861726019,
      "grad_norm": 2.3853254318237305,
      "learning_rate": 0.0001394931023419955,
      "loss": 4.0199,
      "step": 2830
    },
    {
      "epoch": 0.9111324991979467,
      "grad_norm": 1.9074283838272095,
      "learning_rate": 0.0001392792214736392,
      "loss": 4.0776,
      "step": 2840
    },
    {
      "epoch": 0.9143407122232916,
      "grad_norm": 2.5169012546539307,
      "learning_rate": 0.00013906534060528287,
      "loss": 4.0815,
      "step": 2850
    },
    {
      "epoch": 0.9175489252486365,
      "grad_norm": 1.6002197265625,
      "learning_rate": 0.00013885145973692654,
      "loss": 4.1745,
      "step": 2860
    },
    {
      "epoch": 0.9207571382739814,
      "grad_norm": 3.0542430877685547,
      "learning_rate": 0.0001386375788685702,
      "loss": 3.9334,
      "step": 2870
    },
    {
      "epoch": 0.9239653512993262,
      "grad_norm": 2.8249781131744385,
      "learning_rate": 0.00013842369800021387,
      "loss": 4.0022,
      "step": 2880
    },
    {
      "epoch": 0.9271735643246711,
      "grad_norm": 2.581197738647461,
      "learning_rate": 0.00013820981713185757,
      "loss": 3.5955,
      "step": 2890
    },
    {
      "epoch": 0.930381777350016,
      "grad_norm": 2.2718803882598877,
      "learning_rate": 0.00013799593626350124,
      "loss": 4.0766,
      "step": 2900
    },
    {
      "epoch": 0.933589990375361,
      "grad_norm": 1.8439786434173584,
      "learning_rate": 0.0001377820553951449,
      "loss": 4.0461,
      "step": 2910
    },
    {
      "epoch": 0.9367982034007059,
      "grad_norm": 2.307706117630005,
      "learning_rate": 0.00013756817452678858,
      "loss": 3.8564,
      "step": 2920
    },
    {
      "epoch": 0.9400064164260507,
      "grad_norm": 1.9784843921661377,
      "learning_rate": 0.00013735429365843227,
      "loss": 3.8205,
      "step": 2930
    },
    {
      "epoch": 0.9432146294513956,
      "grad_norm": 3.4192357063293457,
      "learning_rate": 0.00013714041279007594,
      "loss": 3.736,
      "step": 2940
    },
    {
      "epoch": 0.9464228424767405,
      "grad_norm": 2.3413097858428955,
      "learning_rate": 0.0001369265319217196,
      "loss": 3.9269,
      "step": 2950
    },
    {
      "epoch": 0.9496310555020854,
      "grad_norm": 2.2233259677886963,
      "learning_rate": 0.00013671265105336328,
      "loss": 3.8232,
      "step": 2960
    },
    {
      "epoch": 0.9528392685274302,
      "grad_norm": 1.924659252166748,
      "learning_rate": 0.00013649877018500695,
      "loss": 3.9582,
      "step": 2970
    },
    {
      "epoch": 0.9560474815527751,
      "grad_norm": 2.416752576828003,
      "learning_rate": 0.00013628488931665062,
      "loss": 3.7752,
      "step": 2980
    },
    {
      "epoch": 0.95925569457812,
      "grad_norm": 2.156132459640503,
      "learning_rate": 0.0001360710084482943,
      "loss": 3.7071,
      "step": 2990
    },
    {
      "epoch": 0.9624639076034649,
      "grad_norm": 1.9862070083618164,
      "learning_rate": 0.00013585712757993798,
      "loss": 4.0175,
      "step": 3000
    },
    {
      "epoch": 0.9656721206288098,
      "grad_norm": 1.8460052013397217,
      "learning_rate": 0.00013564324671158165,
      "loss": 3.6651,
      "step": 3010
    },
    {
      "epoch": 0.9688803336541546,
      "grad_norm": 1.9900583028793335,
      "learning_rate": 0.00013542936584322532,
      "loss": 3.839,
      "step": 3020
    },
    {
      "epoch": 0.9720885466794995,
      "grad_norm": 2.117030143737793,
      "learning_rate": 0.000135215484974869,
      "loss": 3.8861,
      "step": 3030
    },
    {
      "epoch": 0.9752967597048444,
      "grad_norm": 2.250786304473877,
      "learning_rate": 0.0001350016041065127,
      "loss": 4.0564,
      "step": 3040
    },
    {
      "epoch": 0.9785049727301893,
      "grad_norm": 2.1853291988372803,
      "learning_rate": 0.00013478772323815636,
      "loss": 3.9234,
      "step": 3050
    },
    {
      "epoch": 0.9817131857555341,
      "grad_norm": 2.110940933227539,
      "learning_rate": 0.00013457384236980003,
      "loss": 3.7883,
      "step": 3060
    },
    {
      "epoch": 0.984921398780879,
      "grad_norm": 1.717597246170044,
      "learning_rate": 0.0001343599615014437,
      "loss": 3.8724,
      "step": 3070
    },
    {
      "epoch": 0.9881296118062239,
      "grad_norm": 2.250906229019165,
      "learning_rate": 0.0001341460806330874,
      "loss": 3.7084,
      "step": 3080
    },
    {
      "epoch": 0.9913378248315688,
      "grad_norm": 2.6862199306488037,
      "learning_rate": 0.00013393219976473106,
      "loss": 3.9365,
      "step": 3090
    },
    {
      "epoch": 0.9945460378569136,
      "grad_norm": 2.5358142852783203,
      "learning_rate": 0.00013371831889637473,
      "loss": 3.7613,
      "step": 3100
    },
    {
      "epoch": 0.9977542508822586,
      "grad_norm": 2.1256699562072754,
      "learning_rate": 0.0001335044380280184,
      "loss": 4.139,
      "step": 3110
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.7357161045074463,
      "eval_runtime": 4.2019,
      "eval_samples_per_second": 329.852,
      "eval_steps_per_second": 41.41,
      "step": 3117
    },
    {
      "epoch": 1.0009624639076036,
      "grad_norm": 2.5277538299560547,
      "learning_rate": 0.00013329055715966207,
      "loss": 3.769,
      "step": 3120
    },
    {
      "epoch": 1.0041706769329484,
      "grad_norm": 2.4966907501220703,
      "learning_rate": 0.00013307667629130576,
      "loss": 3.9815,
      "step": 3130
    },
    {
      "epoch": 1.0073788899582932,
      "grad_norm": 2.6184725761413574,
      "learning_rate": 0.0001328627954229494,
      "loss": 4.0449,
      "step": 3140
    },
    {
      "epoch": 1.0105871029836382,
      "grad_norm": 2.469510078430176,
      "learning_rate": 0.0001326489145545931,
      "loss": 3.7072,
      "step": 3150
    },
    {
      "epoch": 1.013795316008983,
      "grad_norm": 1.4724143743515015,
      "learning_rate": 0.00013243503368623677,
      "loss": 3.6915,
      "step": 3160
    },
    {
      "epoch": 1.0170035290343278,
      "grad_norm": 1.8580434322357178,
      "learning_rate": 0.00013222115281788047,
      "loss": 3.803,
      "step": 3170
    },
    {
      "epoch": 1.0202117420596728,
      "grad_norm": 1.885103464126587,
      "learning_rate": 0.0001320072719495241,
      "loss": 4.1736,
      "step": 3180
    },
    {
      "epoch": 1.0234199550850176,
      "grad_norm": 2.024955987930298,
      "learning_rate": 0.0001317933910811678,
      "loss": 3.9003,
      "step": 3190
    },
    {
      "epoch": 1.0266281681103626,
      "grad_norm": 2.0031466484069824,
      "learning_rate": 0.00013157951021281148,
      "loss": 3.9676,
      "step": 3200
    },
    {
      "epoch": 1.0298363811357074,
      "grad_norm": 2.7822930812835693,
      "learning_rate": 0.00013136562934445514,
      "loss": 4.1752,
      "step": 3210
    },
    {
      "epoch": 1.0330445941610522,
      "grad_norm": 1.8134351968765259,
      "learning_rate": 0.0001311517484760988,
      "loss": 3.7861,
      "step": 3220
    },
    {
      "epoch": 1.0362528071863972,
      "grad_norm": 3.3063178062438965,
      "learning_rate": 0.00013093786760774248,
      "loss": 3.7511,
      "step": 3230
    },
    {
      "epoch": 1.039461020211742,
      "grad_norm": 2.2332093715667725,
      "learning_rate": 0.00013072398673938618,
      "loss": 3.7694,
      "step": 3240
    },
    {
      "epoch": 1.042669233237087,
      "grad_norm": 1.5620228052139282,
      "learning_rate": 0.00013051010587102985,
      "loss": 3.8284,
      "step": 3250
    },
    {
      "epoch": 1.0458774462624318,
      "grad_norm": 2.0223007202148438,
      "learning_rate": 0.00013029622500267352,
      "loss": 3.7565,
      "step": 3260
    },
    {
      "epoch": 1.0490856592877766,
      "grad_norm": 2.456294536590576,
      "learning_rate": 0.00013008234413431719,
      "loss": 3.6208,
      "step": 3270
    },
    {
      "epoch": 1.0522938723131217,
      "grad_norm": 2.459709644317627,
      "learning_rate": 0.00012986846326596088,
      "loss": 3.9007,
      "step": 3280
    },
    {
      "epoch": 1.0555020853384665,
      "grad_norm": 2.373814582824707,
      "learning_rate": 0.00012965458239760455,
      "loss": 4.0542,
      "step": 3290
    },
    {
      "epoch": 1.0587102983638113,
      "grad_norm": 3.388913869857788,
      "learning_rate": 0.00012944070152924822,
      "loss": 3.981,
      "step": 3300
    },
    {
      "epoch": 1.0619185113891563,
      "grad_norm": 2.128830671310425,
      "learning_rate": 0.0001292268206608919,
      "loss": 3.9406,
      "step": 3310
    },
    {
      "epoch": 1.065126724414501,
      "grad_norm": 2.081047296524048,
      "learning_rate": 0.00012901293979253556,
      "loss": 3.8588,
      "step": 3320
    },
    {
      "epoch": 1.068334937439846,
      "grad_norm": 1.8594791889190674,
      "learning_rate": 0.00012879905892417923,
      "loss": 3.7883,
      "step": 3330
    },
    {
      "epoch": 1.0715431504651909,
      "grad_norm": 1.617692232131958,
      "learning_rate": 0.0001285851780558229,
      "loss": 3.6826,
      "step": 3340
    },
    {
      "epoch": 1.0747513634905357,
      "grad_norm": 2.6649258136749268,
      "learning_rate": 0.0001283712971874666,
      "loss": 3.8722,
      "step": 3350
    },
    {
      "epoch": 1.0779595765158807,
      "grad_norm": 2.9866840839385986,
      "learning_rate": 0.00012815741631911026,
      "loss": 3.8081,
      "step": 3360
    },
    {
      "epoch": 1.0811677895412255,
      "grad_norm": 2.114790678024292,
      "learning_rate": 0.00012794353545075393,
      "loss": 3.8291,
      "step": 3370
    },
    {
      "epoch": 1.0843760025665705,
      "grad_norm": 1.53213369846344,
      "learning_rate": 0.0001277296545823976,
      "loss": 3.6484,
      "step": 3380
    },
    {
      "epoch": 1.0875842155919153,
      "grad_norm": 3.6127355098724365,
      "learning_rate": 0.0001275157737140413,
      "loss": 3.8762,
      "step": 3390
    },
    {
      "epoch": 1.0907924286172601,
      "grad_norm": 1.918333649635315,
      "learning_rate": 0.00012730189284568497,
      "loss": 3.7519,
      "step": 3400
    },
    {
      "epoch": 1.0940006416426051,
      "grad_norm": 2.2503585815429688,
      "learning_rate": 0.00012708801197732863,
      "loss": 3.6611,
      "step": 3410
    },
    {
      "epoch": 1.09720885466795,
      "grad_norm": 2.078467607498169,
      "learning_rate": 0.0001268741311089723,
      "loss": 3.7592,
      "step": 3420
    },
    {
      "epoch": 1.1004170676932947,
      "grad_norm": 2.5111985206604004,
      "learning_rate": 0.00012666025024061597,
      "loss": 3.6898,
      "step": 3430
    },
    {
      "epoch": 1.1036252807186397,
      "grad_norm": 1.8444814682006836,
      "learning_rate": 0.00012644636937225967,
      "loss": 4.0338,
      "step": 3440
    },
    {
      "epoch": 1.1068334937439845,
      "grad_norm": 2.058281421661377,
      "learning_rate": 0.0001262324885039033,
      "loss": 4.0054,
      "step": 3450
    },
    {
      "epoch": 1.1100417067693296,
      "grad_norm": 2.896561622619629,
      "learning_rate": 0.000126018607635547,
      "loss": 4.0157,
      "step": 3460
    },
    {
      "epoch": 1.1132499197946744,
      "grad_norm": 2.5261828899383545,
      "learning_rate": 0.00012580472676719068,
      "loss": 3.9905,
      "step": 3470
    },
    {
      "epoch": 1.1164581328200192,
      "grad_norm": 2.7520763874053955,
      "learning_rate": 0.00012559084589883437,
      "loss": 3.7549,
      "step": 3480
    },
    {
      "epoch": 1.1196663458453642,
      "grad_norm": 4.368017673492432,
      "learning_rate": 0.00012537696503047801,
      "loss": 3.8794,
      "step": 3490
    },
    {
      "epoch": 1.122874558870709,
      "grad_norm": 2.0545270442962646,
      "learning_rate": 0.0001251630841621217,
      "loss": 4.1365,
      "step": 3500
    },
    {
      "epoch": 1.126082771896054,
      "grad_norm": 2.2790985107421875,
      "learning_rate": 0.00012494920329376538,
      "loss": 4.0111,
      "step": 3510
    },
    {
      "epoch": 1.1292909849213988,
      "grad_norm": 2.70198130607605,
      "learning_rate": 0.00012473532242540908,
      "loss": 3.9383,
      "step": 3520
    },
    {
      "epoch": 1.1324991979467436,
      "grad_norm": 1.5335408449172974,
      "learning_rate": 0.00012452144155705272,
      "loss": 3.8296,
      "step": 3530
    },
    {
      "epoch": 1.1357074109720886,
      "grad_norm": 1.821384072303772,
      "learning_rate": 0.00012430756068869641,
      "loss": 3.7409,
      "step": 3540
    },
    {
      "epoch": 1.1389156239974334,
      "grad_norm": 2.4324121475219727,
      "learning_rate": 0.00012409367982034008,
      "loss": 3.7691,
      "step": 3550
    },
    {
      "epoch": 1.1421238370227784,
      "grad_norm": 2.3252503871917725,
      "learning_rate": 0.00012387979895198375,
      "loss": 3.9253,
      "step": 3560
    },
    {
      "epoch": 1.1453320500481232,
      "grad_norm": 2.6269001960754395,
      "learning_rate": 0.00012366591808362742,
      "loss": 3.7463,
      "step": 3570
    },
    {
      "epoch": 1.148540263073468,
      "grad_norm": 2.6321897506713867,
      "learning_rate": 0.0001234520372152711,
      "loss": 3.7483,
      "step": 3580
    },
    {
      "epoch": 1.151748476098813,
      "grad_norm": 2.07837176322937,
      "learning_rate": 0.00012323815634691479,
      "loss": 3.9143,
      "step": 3590
    },
    {
      "epoch": 1.1549566891241578,
      "grad_norm": 2.1116318702697754,
      "learning_rate": 0.00012302427547855846,
      "loss": 4.0845,
      "step": 3600
    },
    {
      "epoch": 1.1581649021495029,
      "grad_norm": 2.0086257457733154,
      "learning_rate": 0.00012281039461020212,
      "loss": 3.764,
      "step": 3610
    },
    {
      "epoch": 1.1613731151748476,
      "grad_norm": 1.8477798700332642,
      "learning_rate": 0.0001225965137418458,
      "loss": 3.8544,
      "step": 3620
    },
    {
      "epoch": 1.1645813282001924,
      "grad_norm": 2.260687828063965,
      "learning_rate": 0.0001223826328734895,
      "loss": 3.8869,
      "step": 3630
    },
    {
      "epoch": 1.1677895412255375,
      "grad_norm": 1.9386069774627686,
      "learning_rate": 0.00012216875200513313,
      "loss": 3.8924,
      "step": 3640
    },
    {
      "epoch": 1.1709977542508823,
      "grad_norm": 2.8590829372406006,
      "learning_rate": 0.00012195487113677681,
      "loss": 3.9739,
      "step": 3650
    },
    {
      "epoch": 1.174205967276227,
      "grad_norm": 2.784174919128418,
      "learning_rate": 0.0001217409902684205,
      "loss": 3.9907,
      "step": 3660
    },
    {
      "epoch": 1.177414180301572,
      "grad_norm": 2.4956438541412354,
      "learning_rate": 0.00012152710940006418,
      "loss": 3.8589,
      "step": 3670
    },
    {
      "epoch": 1.1806223933269169,
      "grad_norm": 3.0058813095092773,
      "learning_rate": 0.00012131322853170784,
      "loss": 4.0011,
      "step": 3680
    },
    {
      "epoch": 1.1838306063522617,
      "grad_norm": 2.2067880630493164,
      "learning_rate": 0.00012109934766335152,
      "loss": 4.0829,
      "step": 3690
    },
    {
      "epoch": 1.1870388193776067,
      "grad_norm": 2.131589651107788,
      "learning_rate": 0.0001208854667949952,
      "loss": 4.0656,
      "step": 3700
    },
    {
      "epoch": 1.1902470324029515,
      "grad_norm": 2.479835271835327,
      "learning_rate": 0.00012067158592663887,
      "loss": 4.2204,
      "step": 3710
    },
    {
      "epoch": 1.1934552454282965,
      "grad_norm": 1.7011319398880005,
      "learning_rate": 0.00012045770505828254,
      "loss": 3.9749,
      "step": 3720
    },
    {
      "epoch": 1.1966634584536413,
      "grad_norm": 2.0629823207855225,
      "learning_rate": 0.00012024382418992621,
      "loss": 3.6804,
      "step": 3730
    },
    {
      "epoch": 1.199871671478986,
      "grad_norm": 1.9897804260253906,
      "learning_rate": 0.00012002994332156989,
      "loss": 3.822,
      "step": 3740
    },
    {
      "epoch": 1.2030798845043311,
      "grad_norm": 1.874730110168457,
      "learning_rate": 0.00011981606245321357,
      "loss": 3.9348,
      "step": 3750
    },
    {
      "epoch": 1.206288097529676,
      "grad_norm": 1.9632515907287598,
      "learning_rate": 0.00011960218158485723,
      "loss": 3.9405,
      "step": 3760
    },
    {
      "epoch": 1.209496310555021,
      "grad_norm": 2.400195837020874,
      "learning_rate": 0.00011938830071650091,
      "loss": 3.9325,
      "step": 3770
    },
    {
      "epoch": 1.2127045235803657,
      "grad_norm": 2.0695960521698,
      "learning_rate": 0.0001191744198481446,
      "loss": 3.9483,
      "step": 3780
    },
    {
      "epoch": 1.2159127366057105,
      "grad_norm": 2.1255664825439453,
      "learning_rate": 0.00011896053897978828,
      "loss": 3.7742,
      "step": 3790
    },
    {
      "epoch": 1.2191209496310556,
      "grad_norm": 1.8633310794830322,
      "learning_rate": 0.00011874665811143193,
      "loss": 4.0028,
      "step": 3800
    },
    {
      "epoch": 1.2223291626564003,
      "grad_norm": 1.819897174835205,
      "learning_rate": 0.00011853277724307561,
      "loss": 3.8492,
      "step": 3810
    },
    {
      "epoch": 1.2255373756817454,
      "grad_norm": 2.49935245513916,
      "learning_rate": 0.00011831889637471928,
      "loss": 3.997,
      "step": 3820
    },
    {
      "epoch": 1.2287455887070902,
      "grad_norm": 1.6448605060577393,
      "learning_rate": 0.00011810501550636297,
      "loss": 3.7217,
      "step": 3830
    },
    {
      "epoch": 1.231953801732435,
      "grad_norm": 2.5618231296539307,
      "learning_rate": 0.00011789113463800662,
      "loss": 3.8553,
      "step": 3840
    },
    {
      "epoch": 1.23516201475778,
      "grad_norm": 2.5541839599609375,
      "learning_rate": 0.0001176772537696503,
      "loss": 3.8396,
      "step": 3850
    },
    {
      "epoch": 1.2383702277831248,
      "grad_norm": 2.467637300491333,
      "learning_rate": 0.00011746337290129399,
      "loss": 3.883,
      "step": 3860
    },
    {
      "epoch": 1.2415784408084698,
      "grad_norm": 2.584395170211792,
      "learning_rate": 0.00011724949203293767,
      "loss": 3.9439,
      "step": 3870
    },
    {
      "epoch": 1.2447866538338146,
      "grad_norm": 1.95113205909729,
      "learning_rate": 0.00011703561116458133,
      "loss": 3.8846,
      "step": 3880
    },
    {
      "epoch": 1.2479948668591594,
      "grad_norm": 1.71753990650177,
      "learning_rate": 0.00011682173029622501,
      "loss": 4.1437,
      "step": 3890
    },
    {
      "epoch": 1.2512030798845044,
      "grad_norm": 2.3722901344299316,
      "learning_rate": 0.00011660784942786869,
      "loss": 3.9933,
      "step": 3900
    },
    {
      "epoch": 1.2544112929098492,
      "grad_norm": 1.946497917175293,
      "learning_rate": 0.00011639396855951237,
      "loss": 4.0233,
      "step": 3910
    },
    {
      "epoch": 1.2576195059351942,
      "grad_norm": 1.9337002038955688,
      "learning_rate": 0.00011618008769115603,
      "loss": 3.9048,
      "step": 3920
    },
    {
      "epoch": 1.260827718960539,
      "grad_norm": 1.8286411762237549,
      "learning_rate": 0.00011596620682279971,
      "loss": 4.0921,
      "step": 3930
    },
    {
      "epoch": 1.2640359319858838,
      "grad_norm": 1.7877334356307983,
      "learning_rate": 0.00011575232595444338,
      "loss": 3.7522,
      "step": 3940
    },
    {
      "epoch": 1.2672441450112286,
      "grad_norm": 2.2582004070281982,
      "learning_rate": 0.00011553844508608706,
      "loss": 3.6721,
      "step": 3950
    },
    {
      "epoch": 1.2704523580365736,
      "grad_norm": 3.3662354946136475,
      "learning_rate": 0.00011532456421773072,
      "loss": 3.9646,
      "step": 3960
    },
    {
      "epoch": 1.2736605710619184,
      "grad_norm": 2.6846656799316406,
      "learning_rate": 0.0001151106833493744,
      "loss": 3.9083,
      "step": 3970
    },
    {
      "epoch": 1.2768687840872635,
      "grad_norm": 1.6540049314498901,
      "learning_rate": 0.00011489680248101808,
      "loss": 3.9278,
      "step": 3980
    },
    {
      "epoch": 1.2800769971126083,
      "grad_norm": 1.744443416595459,
      "learning_rate": 0.00011468292161266174,
      "loss": 4.1074,
      "step": 3990
    },
    {
      "epoch": 1.283285210137953,
      "grad_norm": 1.7124364376068115,
      "learning_rate": 0.00011446904074430542,
      "loss": 3.7179,
      "step": 4000
    },
    {
      "epoch": 1.286493423163298,
      "grad_norm": 2.269200325012207,
      "learning_rate": 0.0001142551598759491,
      "loss": 4.0944,
      "step": 4010
    },
    {
      "epoch": 1.2897016361886429,
      "grad_norm": 2.685533046722412,
      "learning_rate": 0.00011404127900759279,
      "loss": 3.899,
      "step": 4020
    },
    {
      "epoch": 1.2929098492139879,
      "grad_norm": 1.7395490407943726,
      "learning_rate": 0.00011382739813923644,
      "loss": 4.2181,
      "step": 4030
    },
    {
      "epoch": 1.2961180622393327,
      "grad_norm": 2.8593764305114746,
      "learning_rate": 0.00011361351727088013,
      "loss": 3.9439,
      "step": 4040
    },
    {
      "epoch": 1.2993262752646775,
      "grad_norm": 1.915686011314392,
      "learning_rate": 0.0001133996364025238,
      "loss": 3.9519,
      "step": 4050
    },
    {
      "epoch": 1.3025344882900225,
      "grad_norm": 1.7587790489196777,
      "learning_rate": 0.00011318575553416748,
      "loss": 3.8251,
      "step": 4060
    },
    {
      "epoch": 1.3057427013153673,
      "grad_norm": 3.38811993598938,
      "learning_rate": 0.00011297187466581113,
      "loss": 4.0079,
      "step": 4070
    },
    {
      "epoch": 1.3089509143407123,
      "grad_norm": 2.1314918994903564,
      "learning_rate": 0.00011275799379745482,
      "loss": 3.9442,
      "step": 4080
    },
    {
      "epoch": 1.312159127366057,
      "grad_norm": 3.0086982250213623,
      "learning_rate": 0.0001125441129290985,
      "loss": 3.67,
      "step": 4090
    },
    {
      "epoch": 1.315367340391402,
      "grad_norm": 2.2147653102874756,
      "learning_rate": 0.00011233023206074218,
      "loss": 3.9227,
      "step": 4100
    },
    {
      "epoch": 1.318575553416747,
      "grad_norm": 2.038381576538086,
      "learning_rate": 0.00011211635119238584,
      "loss": 3.9095,
      "step": 4110
    },
    {
      "epoch": 1.3217837664420917,
      "grad_norm": 3.036738395690918,
      "learning_rate": 0.00011190247032402952,
      "loss": 4.0001,
      "step": 4120
    },
    {
      "epoch": 1.3249919794674367,
      "grad_norm": 1.7403532266616821,
      "learning_rate": 0.0001116885894556732,
      "loss": 4.0021,
      "step": 4130
    },
    {
      "epoch": 1.3282001924927815,
      "grad_norm": 2.286850690841675,
      "learning_rate": 0.00011147470858731688,
      "loss": 3.7575,
      "step": 4140
    },
    {
      "epoch": 1.3314084055181263,
      "grad_norm": 2.975905418395996,
      "learning_rate": 0.00011126082771896054,
      "loss": 3.8303,
      "step": 4150
    },
    {
      "epoch": 1.3346166185434714,
      "grad_norm": 2.3235034942626953,
      "learning_rate": 0.00011104694685060422,
      "loss": 3.9514,
      "step": 4160
    },
    {
      "epoch": 1.3378248315688162,
      "grad_norm": 2.1184675693511963,
      "learning_rate": 0.00011083306598224789,
      "loss": 3.8788,
      "step": 4170
    },
    {
      "epoch": 1.3410330445941612,
      "grad_norm": 1.9071680307388306,
      "learning_rate": 0.00011061918511389157,
      "loss": 3.6091,
      "step": 4180
    },
    {
      "epoch": 1.344241257619506,
      "grad_norm": 2.179407835006714,
      "learning_rate": 0.00011040530424553523,
      "loss": 3.6386,
      "step": 4190
    },
    {
      "epoch": 1.3474494706448508,
      "grad_norm": 2.514840841293335,
      "learning_rate": 0.00011019142337717891,
      "loss": 3.6199,
      "step": 4200
    },
    {
      "epoch": 1.3506576836701956,
      "grad_norm": 2.2863821983337402,
      "learning_rate": 0.0001099775425088226,
      "loss": 3.8432,
      "step": 4210
    },
    {
      "epoch": 1.3538658966955406,
      "grad_norm": 3.6600635051727295,
      "learning_rate": 0.00010976366164046628,
      "loss": 3.7375,
      "step": 4220
    },
    {
      "epoch": 1.3570741097208856,
      "grad_norm": 2.218935489654541,
      "learning_rate": 0.00010954978077210993,
      "loss": 4.0576,
      "step": 4230
    },
    {
      "epoch": 1.3602823227462304,
      "grad_norm": 2.2586474418640137,
      "learning_rate": 0.00010933589990375362,
      "loss": 3.7961,
      "step": 4240
    },
    {
      "epoch": 1.3634905357715752,
      "grad_norm": 2.0086123943328857,
      "learning_rate": 0.0001091220190353973,
      "loss": 3.7752,
      "step": 4250
    },
    {
      "epoch": 1.36669874879692,
      "grad_norm": 1.9906080961227417,
      "learning_rate": 0.00010890813816704097,
      "loss": 3.9417,
      "step": 4260
    },
    {
      "epoch": 1.369906961822265,
      "grad_norm": 3.1049745082855225,
      "learning_rate": 0.00010869425729868464,
      "loss": 3.7867,
      "step": 4270
    },
    {
      "epoch": 1.3731151748476098,
      "grad_norm": 2.3650219440460205,
      "learning_rate": 0.0001084803764303283,
      "loss": 3.8474,
      "step": 4280
    },
    {
      "epoch": 1.3763233878729548,
      "grad_norm": 2.005283832550049,
      "learning_rate": 0.00010826649556197199,
      "loss": 4.0468,
      "step": 4290
    },
    {
      "epoch": 1.3795316008982996,
      "grad_norm": 1.5594613552093506,
      "learning_rate": 0.00010805261469361564,
      "loss": 3.7718,
      "step": 4300
    },
    {
      "epoch": 1.3827398139236444,
      "grad_norm": 2.7248551845550537,
      "learning_rate": 0.00010783873382525933,
      "loss": 3.8162,
      "step": 4310
    },
    {
      "epoch": 1.3859480269489894,
      "grad_norm": 3.0193867683410645,
      "learning_rate": 0.00010762485295690301,
      "loss": 3.8694,
      "step": 4320
    },
    {
      "epoch": 1.3891562399743342,
      "grad_norm": 1.8908042907714844,
      "learning_rate": 0.00010741097208854669,
      "loss": 3.7549,
      "step": 4330
    },
    {
      "epoch": 1.3923644529996793,
      "grad_norm": 2.220299482345581,
      "learning_rate": 0.00010719709122019035,
      "loss": 3.8117,
      "step": 4340
    },
    {
      "epoch": 1.395572666025024,
      "grad_norm": 3.2471354007720947,
      "learning_rate": 0.00010698321035183403,
      "loss": 3.7388,
      "step": 4350
    },
    {
      "epoch": 1.3987808790503689,
      "grad_norm": 2.1324098110198975,
      "learning_rate": 0.00010676932948347771,
      "loss": 4.077,
      "step": 4360
    },
    {
      "epoch": 1.4019890920757139,
      "grad_norm": 1.6986231803894043,
      "learning_rate": 0.0001065554486151214,
      "loss": 4.0996,
      "step": 4370
    },
    {
      "epoch": 1.4051973051010587,
      "grad_norm": 2.0201711654663086,
      "learning_rate": 0.00010634156774676505,
      "loss": 3.9124,
      "step": 4380
    },
    {
      "epoch": 1.4084055181264037,
      "grad_norm": 1.9027854204177856,
      "learning_rate": 0.00010612768687840873,
      "loss": 3.8554,
      "step": 4390
    },
    {
      "epoch": 1.4116137311517485,
      "grad_norm": 3.3794405460357666,
      "learning_rate": 0.0001059138060100524,
      "loss": 3.7293,
      "step": 4400
    },
    {
      "epoch": 1.4148219441770933,
      "grad_norm": 1.92641019821167,
      "learning_rate": 0.00010569992514169609,
      "loss": 3.9239,
      "step": 4410
    },
    {
      "epoch": 1.4180301572024383,
      "grad_norm": 2.077824115753174,
      "learning_rate": 0.00010548604427333974,
      "loss": 3.9685,
      "step": 4420
    },
    {
      "epoch": 1.421238370227783,
      "grad_norm": 2.1304283142089844,
      "learning_rate": 0.00010527216340498342,
      "loss": 3.9875,
      "step": 4430
    },
    {
      "epoch": 1.4244465832531281,
      "grad_norm": 3.416429281234741,
      "learning_rate": 0.0001050582825366271,
      "loss": 3.9812,
      "step": 4440
    },
    {
      "epoch": 1.427654796278473,
      "grad_norm": 2.3426477909088135,
      "learning_rate": 0.00010484440166827079,
      "loss": 3.5223,
      "step": 4450
    },
    {
      "epoch": 1.4308630093038177,
      "grad_norm": 1.949764370918274,
      "learning_rate": 0.00010463052079991444,
      "loss": 4.0059,
      "step": 4460
    },
    {
      "epoch": 1.4340712223291627,
      "grad_norm": 2.247694253921509,
      "learning_rate": 0.00010441663993155813,
      "loss": 3.8965,
      "step": 4470
    },
    {
      "epoch": 1.4372794353545075,
      "grad_norm": 2.254763603210449,
      "learning_rate": 0.00010420275906320181,
      "loss": 3.9097,
      "step": 4480
    },
    {
      "epoch": 1.4404876483798525,
      "grad_norm": 3.036403179168701,
      "learning_rate": 0.00010398887819484548,
      "loss": 3.7789,
      "step": 4490
    },
    {
      "epoch": 1.4436958614051973,
      "grad_norm": 2.503572940826416,
      "learning_rate": 0.00010377499732648915,
      "loss": 3.7352,
      "step": 4500
    },
    {
      "epoch": 1.4469040744305421,
      "grad_norm": 2.469165563583374,
      "learning_rate": 0.00010356111645813282,
      "loss": 3.7634,
      "step": 4510
    },
    {
      "epoch": 1.450112287455887,
      "grad_norm": 1.665696382522583,
      "learning_rate": 0.0001033472355897765,
      "loss": 3.867,
      "step": 4520
    },
    {
      "epoch": 1.453320500481232,
      "grad_norm": 1.8552824258804321,
      "learning_rate": 0.00010313335472142018,
      "loss": 3.8674,
      "step": 4530
    },
    {
      "epoch": 1.4565287135065768,
      "grad_norm": 1.931757926940918,
      "learning_rate": 0.00010291947385306384,
      "loss": 4.0106,
      "step": 4540
    },
    {
      "epoch": 1.4597369265319218,
      "grad_norm": 1.9189153909683228,
      "learning_rate": 0.00010270559298470752,
      "loss": 3.8753,
      "step": 4550
    },
    {
      "epoch": 1.4629451395572666,
      "grad_norm": 2.882816791534424,
      "learning_rate": 0.0001024917121163512,
      "loss": 4.0063,
      "step": 4560
    },
    {
      "epoch": 1.4661533525826114,
      "grad_norm": 1.766337513923645,
      "learning_rate": 0.00010227783124799489,
      "loss": 3.85,
      "step": 4570
    },
    {
      "epoch": 1.4693615656079564,
      "grad_norm": 2.566344976425171,
      "learning_rate": 0.00010206395037963854,
      "loss": 3.8035,
      "step": 4580
    },
    {
      "epoch": 1.4725697786333012,
      "grad_norm": 2.49685001373291,
      "learning_rate": 0.00010185006951128222,
      "loss": 3.7223,
      "step": 4590
    },
    {
      "epoch": 1.4757779916586462,
      "grad_norm": 2.062095880508423,
      "learning_rate": 0.0001016361886429259,
      "loss": 3.8688,
      "step": 4600
    },
    {
      "epoch": 1.478986204683991,
      "grad_norm": 4.10341215133667,
      "learning_rate": 0.00010142230777456958,
      "loss": 4.023,
      "step": 4610
    },
    {
      "epoch": 1.4821944177093358,
      "grad_norm": 2.150958776473999,
      "learning_rate": 0.00010120842690621324,
      "loss": 3.9158,
      "step": 4620
    },
    {
      "epoch": 1.4854026307346808,
      "grad_norm": 2.188572645187378,
      "learning_rate": 0.00010099454603785691,
      "loss": 3.8365,
      "step": 4630
    },
    {
      "epoch": 1.4886108437600256,
      "grad_norm": 1.9555333852767944,
      "learning_rate": 0.0001007806651695006,
      "loss": 3.8912,
      "step": 4640
    },
    {
      "epoch": 1.4918190567853706,
      "grad_norm": 3.662189483642578,
      "learning_rate": 0.00010056678430114425,
      "loss": 3.7656,
      "step": 4650
    },
    {
      "epoch": 1.4950272698107154,
      "grad_norm": 1.444885492324829,
      "learning_rate": 0.00010035290343278793,
      "loss": 3.977,
      "step": 4660
    },
    {
      "epoch": 1.4982354828360602,
      "grad_norm": 2.8366849422454834,
      "learning_rate": 0.00010013902256443162,
      "loss": 3.6944,
      "step": 4670
    },
    {
      "epoch": 1.5014436958614052,
      "grad_norm": 1.9880322217941284,
      "learning_rate": 9.992514169607529e-05,
      "loss": 3.8754,
      "step": 4680
    },
    {
      "epoch": 1.50465190888675,
      "grad_norm": 1.9110606908798218,
      "learning_rate": 9.971126082771897e-05,
      "loss": 3.5793,
      "step": 4690
    },
    {
      "epoch": 1.507860121912095,
      "grad_norm": 1.9218523502349854,
      "learning_rate": 9.949737995936264e-05,
      "loss": 4.2185,
      "step": 4700
    },
    {
      "epoch": 1.5110683349374399,
      "grad_norm": 2.8961398601531982,
      "learning_rate": 9.928349909100632e-05,
      "loss": 3.7572,
      "step": 4710
    },
    {
      "epoch": 1.5142765479627847,
      "grad_norm": 1.8908733129501343,
      "learning_rate": 9.906961822264999e-05,
      "loss": 3.9773,
      "step": 4720
    },
    {
      "epoch": 1.5174847609881295,
      "grad_norm": 2.191157579421997,
      "learning_rate": 9.885573735429366e-05,
      "loss": 3.8816,
      "step": 4730
    },
    {
      "epoch": 1.5206929740134745,
      "grad_norm": 2.055190086364746,
      "learning_rate": 9.864185648593733e-05,
      "loss": 3.9908,
      "step": 4740
    },
    {
      "epoch": 1.5239011870388195,
      "grad_norm": 1.8049125671386719,
      "learning_rate": 9.842797561758101e-05,
      "loss": 4.0505,
      "step": 4750
    },
    {
      "epoch": 1.5271094000641643,
      "grad_norm": 1.9739993810653687,
      "learning_rate": 9.821409474922468e-05,
      "loss": 3.9248,
      "step": 4760
    },
    {
      "epoch": 1.530317613089509,
      "grad_norm": 1.913437843322754,
      "learning_rate": 9.800021388086836e-05,
      "loss": 3.6526,
      "step": 4770
    },
    {
      "epoch": 1.5335258261148539,
      "grad_norm": 2.479522943496704,
      "learning_rate": 9.778633301251203e-05,
      "loss": 3.8732,
      "step": 4780
    },
    {
      "epoch": 1.536734039140199,
      "grad_norm": 1.988147497177124,
      "learning_rate": 9.757245214415571e-05,
      "loss": 3.9255,
      "step": 4790
    },
    {
      "epoch": 1.539942252165544,
      "grad_norm": 2.149273633956909,
      "learning_rate": 9.735857127579938e-05,
      "loss": 3.8738,
      "step": 4800
    },
    {
      "epoch": 1.5431504651908887,
      "grad_norm": 3.420203447341919,
      "learning_rate": 9.714469040744307e-05,
      "loss": 3.872,
      "step": 4810
    },
    {
      "epoch": 1.5463586782162335,
      "grad_norm": 2.6136417388916016,
      "learning_rate": 9.693080953908673e-05,
      "loss": 3.8711,
      "step": 4820
    },
    {
      "epoch": 1.5495668912415783,
      "grad_norm": 3.0763792991638184,
      "learning_rate": 9.671692867073042e-05,
      "loss": 4.1872,
      "step": 4830
    },
    {
      "epoch": 1.5527751042669233,
      "grad_norm": 2.2595131397247314,
      "learning_rate": 9.650304780237409e-05,
      "loss": 3.8221,
      "step": 4840
    },
    {
      "epoch": 1.5559833172922684,
      "grad_norm": 2.090500593185425,
      "learning_rate": 9.628916693401776e-05,
      "loss": 3.8757,
      "step": 4850
    },
    {
      "epoch": 1.5591915303176132,
      "grad_norm": 2.1271305084228516,
      "learning_rate": 9.607528606566142e-05,
      "loss": 3.8903,
      "step": 4860
    },
    {
      "epoch": 1.562399743342958,
      "grad_norm": 2.194540023803711,
      "learning_rate": 9.586140519730511e-05,
      "loss": 3.6685,
      "step": 4870
    },
    {
      "epoch": 1.5656079563683027,
      "grad_norm": 2.030151128768921,
      "learning_rate": 9.564752432894878e-05,
      "loss": 4.0168,
      "step": 4880
    },
    {
      "epoch": 1.5688161693936478,
      "grad_norm": 2.4368276596069336,
      "learning_rate": 9.543364346059246e-05,
      "loss": 3.9353,
      "step": 4890
    },
    {
      "epoch": 1.5720243824189928,
      "grad_norm": 2.5183804035186768,
      "learning_rate": 9.521976259223613e-05,
      "loss": 3.764,
      "step": 4900
    },
    {
      "epoch": 1.5752325954443376,
      "grad_norm": 2.5962090492248535,
      "learning_rate": 9.500588172387981e-05,
      "loss": 3.8344,
      "step": 4910
    },
    {
      "epoch": 1.5784408084696824,
      "grad_norm": 1.946359634399414,
      "learning_rate": 9.479200085552348e-05,
      "loss": 3.7549,
      "step": 4920
    },
    {
      "epoch": 1.5816490214950272,
      "grad_norm": 2.0060875415802,
      "learning_rate": 9.457811998716716e-05,
      "loss": 3.7475,
      "step": 4930
    },
    {
      "epoch": 1.5848572345203722,
      "grad_norm": 2.2024648189544678,
      "learning_rate": 9.436423911881083e-05,
      "loss": 3.9809,
      "step": 4940
    },
    {
      "epoch": 1.588065447545717,
      "grad_norm": 3.645437002182007,
      "learning_rate": 9.41503582504545e-05,
      "loss": 3.6416,
      "step": 4950
    },
    {
      "epoch": 1.591273660571062,
      "grad_norm": 3.843630313873291,
      "learning_rate": 9.393647738209817e-05,
      "loss": 3.868,
      "step": 4960
    },
    {
      "epoch": 1.5944818735964068,
      "grad_norm": 2.1674301624298096,
      "learning_rate": 9.372259651374184e-05,
      "loss": 3.8365,
      "step": 4970
    },
    {
      "epoch": 1.5976900866217516,
      "grad_norm": 2.590350866317749,
      "learning_rate": 9.350871564538552e-05,
      "loss": 3.5688,
      "step": 4980
    },
    {
      "epoch": 1.6008982996470964,
      "grad_norm": 2.3278422355651855,
      "learning_rate": 9.329483477702919e-05,
      "loss": 4.0662,
      "step": 4990
    },
    {
      "epoch": 1.6041065126724414,
      "grad_norm": 2.2468090057373047,
      "learning_rate": 9.308095390867287e-05,
      "loss": 3.8731,
      "step": 5000
    },
    {
      "epoch": 1.6073147256977864,
      "grad_norm": 1.979140281677246,
      "learning_rate": 9.286707304031654e-05,
      "loss": 4.0643,
      "step": 5010
    },
    {
      "epoch": 1.6105229387231312,
      "grad_norm": 2.662753105163574,
      "learning_rate": 9.265319217196023e-05,
      "loss": 3.6945,
      "step": 5020
    },
    {
      "epoch": 1.613731151748476,
      "grad_norm": 1.5366263389587402,
      "learning_rate": 9.24393113036039e-05,
      "loss": 3.8904,
      "step": 5030
    },
    {
      "epoch": 1.6169393647738208,
      "grad_norm": 2.127300977706909,
      "learning_rate": 9.222543043524758e-05,
      "loss": 3.9667,
      "step": 5040
    },
    {
      "epoch": 1.6201475777991659,
      "grad_norm": 1.7043899297714233,
      "learning_rate": 9.201154956689125e-05,
      "loss": 4.2207,
      "step": 5050
    },
    {
      "epoch": 1.6233557908245109,
      "grad_norm": 2.115654230117798,
      "learning_rate": 9.179766869853493e-05,
      "loss": 3.8072,
      "step": 5060
    },
    {
      "epoch": 1.6265640038498557,
      "grad_norm": 1.9632436037063599,
      "learning_rate": 9.15837878301786e-05,
      "loss": 3.9243,
      "step": 5070
    },
    {
      "epoch": 1.6297722168752005,
      "grad_norm": 3.260100841522217,
      "learning_rate": 9.136990696182227e-05,
      "loss": 3.8483,
      "step": 5080
    },
    {
      "epoch": 1.6329804299005453,
      "grad_norm": 2.7711448669433594,
      "learning_rate": 9.115602609346594e-05,
      "loss": 3.7479,
      "step": 5090
    },
    {
      "epoch": 1.6361886429258903,
      "grad_norm": 1.7984685897827148,
      "learning_rate": 9.094214522510962e-05,
      "loss": 3.8822,
      "step": 5100
    },
    {
      "epoch": 1.6393968559512353,
      "grad_norm": 2.101609706878662,
      "learning_rate": 9.072826435675329e-05,
      "loss": 3.9201,
      "step": 5110
    },
    {
      "epoch": 1.64260506897658,
      "grad_norm": 1.8932240009307861,
      "learning_rate": 9.051438348839697e-05,
      "loss": 3.706,
      "step": 5120
    },
    {
      "epoch": 1.645813282001925,
      "grad_norm": 1.7366766929626465,
      "learning_rate": 9.030050262004064e-05,
      "loss": 3.8774,
      "step": 5130
    },
    {
      "epoch": 1.6490214950272697,
      "grad_norm": 2.2233188152313232,
      "learning_rate": 9.008662175168432e-05,
      "loss": 3.8312,
      "step": 5140
    },
    {
      "epoch": 1.6522297080526147,
      "grad_norm": 1.9589146375656128,
      "learning_rate": 8.987274088332799e-05,
      "loss": 3.8294,
      "step": 5150
    },
    {
      "epoch": 1.6554379210779597,
      "grad_norm": 1.9862587451934814,
      "learning_rate": 8.965886001497167e-05,
      "loss": 3.6585,
      "step": 5160
    },
    {
      "epoch": 1.6586461341033045,
      "grad_norm": 1.9593522548675537,
      "learning_rate": 8.944497914661534e-05,
      "loss": 3.5882,
      "step": 5170
    },
    {
      "epoch": 1.6618543471286493,
      "grad_norm": 1.4548676013946533,
      "learning_rate": 8.923109827825901e-05,
      "loss": 3.9278,
      "step": 5180
    },
    {
      "epoch": 1.6650625601539941,
      "grad_norm": 1.6684269905090332,
      "learning_rate": 8.901721740990268e-05,
      "loss": 3.991,
      "step": 5190
    },
    {
      "epoch": 1.6682707731793391,
      "grad_norm": 2.2125754356384277,
      "learning_rate": 8.880333654154636e-05,
      "loss": 3.8756,
      "step": 5200
    },
    {
      "epoch": 1.671478986204684,
      "grad_norm": 2.1753153800964355,
      "learning_rate": 8.858945567319003e-05,
      "loss": 3.8101,
      "step": 5210
    },
    {
      "epoch": 1.674687199230029,
      "grad_norm": 2.7153522968292236,
      "learning_rate": 8.837557480483372e-05,
      "loss": 3.915,
      "step": 5220
    },
    {
      "epoch": 1.6778954122553738,
      "grad_norm": 3.4180312156677246,
      "learning_rate": 8.816169393647738e-05,
      "loss": 4.0613,
      "step": 5230
    },
    {
      "epoch": 1.6811036252807185,
      "grad_norm": 2.832322120666504,
      "learning_rate": 8.794781306812107e-05,
      "loss": 3.835,
      "step": 5240
    },
    {
      "epoch": 1.6843118383060636,
      "grad_norm": 1.9358329772949219,
      "learning_rate": 8.773393219976474e-05,
      "loss": 3.9406,
      "step": 5250
    },
    {
      "epoch": 1.6875200513314084,
      "grad_norm": 2.0025501251220703,
      "learning_rate": 8.752005133140842e-05,
      "loss": 3.9138,
      "step": 5260
    },
    {
      "epoch": 1.6907282643567534,
      "grad_norm": 1.9723072052001953,
      "learning_rate": 8.730617046305209e-05,
      "loss": 3.8647,
      "step": 5270
    },
    {
      "epoch": 1.6939364773820982,
      "grad_norm": 4.37200927734375,
      "learning_rate": 8.709228959469577e-05,
      "loss": 3.7977,
      "step": 5280
    },
    {
      "epoch": 1.697144690407443,
      "grad_norm": 1.9854202270507812,
      "learning_rate": 8.687840872633944e-05,
      "loss": 3.6119,
      "step": 5290
    },
    {
      "epoch": 1.7003529034327878,
      "grad_norm": 2.235375165939331,
      "learning_rate": 8.666452785798311e-05,
      "loss": 3.8434,
      "step": 5300
    },
    {
      "epoch": 1.7035611164581328,
      "grad_norm": 1.6759752035140991,
      "learning_rate": 8.645064698962678e-05,
      "loss": 3.8298,
      "step": 5310
    },
    {
      "epoch": 1.7067693294834778,
      "grad_norm": 2.5071170330047607,
      "learning_rate": 8.623676612127045e-05,
      "loss": 3.7354,
      "step": 5320
    },
    {
      "epoch": 1.7099775425088226,
      "grad_norm": 2.2193922996520996,
      "learning_rate": 8.602288525291413e-05,
      "loss": 3.8381,
      "step": 5330
    },
    {
      "epoch": 1.7131857555341674,
      "grad_norm": 1.9466344118118286,
      "learning_rate": 8.58090043845578e-05,
      "loss": 4.0151,
      "step": 5340
    },
    {
      "epoch": 1.7163939685595122,
      "grad_norm": 1.7891851663589478,
      "learning_rate": 8.559512351620148e-05,
      "loss": 3.9787,
      "step": 5350
    },
    {
      "epoch": 1.7196021815848572,
      "grad_norm": 2.439317464828491,
      "learning_rate": 8.538124264784515e-05,
      "loss": 3.9355,
      "step": 5360
    },
    {
      "epoch": 1.7228103946102022,
      "grad_norm": 2.396314859390259,
      "learning_rate": 8.516736177948883e-05,
      "loss": 4.0888,
      "step": 5370
    },
    {
      "epoch": 1.726018607635547,
      "grad_norm": 1.6744966506958008,
      "learning_rate": 8.49534809111325e-05,
      "loss": 3.8937,
      "step": 5380
    },
    {
      "epoch": 1.7292268206608918,
      "grad_norm": 2.004966974258423,
      "learning_rate": 8.473960004277618e-05,
      "loss": 3.718,
      "step": 5390
    },
    {
      "epoch": 1.7324350336862366,
      "grad_norm": 2.1781907081604004,
      "learning_rate": 8.452571917441985e-05,
      "loss": 3.6857,
      "step": 5400
    },
    {
      "epoch": 1.7356432467115817,
      "grad_norm": 2.51503324508667,
      "learning_rate": 8.431183830606352e-05,
      "loss": 4.0581,
      "step": 5410
    },
    {
      "epoch": 1.7388514597369267,
      "grad_norm": 1.9835304021835327,
      "learning_rate": 8.409795743770719e-05,
      "loss": 3.8633,
      "step": 5420
    },
    {
      "epoch": 1.7420596727622715,
      "grad_norm": 2.4317357540130615,
      "learning_rate": 8.388407656935087e-05,
      "loss": 3.5622,
      "step": 5430
    },
    {
      "epoch": 1.7452678857876163,
      "grad_norm": 2.726515531539917,
      "learning_rate": 8.367019570099454e-05,
      "loss": 3.7702,
      "step": 5440
    },
    {
      "epoch": 1.748476098812961,
      "grad_norm": 3.2529053688049316,
      "learning_rate": 8.345631483263823e-05,
      "loss": 3.7036,
      "step": 5450
    },
    {
      "epoch": 1.751684311838306,
      "grad_norm": 2.1373300552368164,
      "learning_rate": 8.32424339642819e-05,
      "loss": 4.0717,
      "step": 5460
    },
    {
      "epoch": 1.754892524863651,
      "grad_norm": 2.4759552478790283,
      "learning_rate": 8.302855309592558e-05,
      "loss": 4.0168,
      "step": 5470
    },
    {
      "epoch": 1.758100737888996,
      "grad_norm": 1.840488314628601,
      "learning_rate": 8.281467222756925e-05,
      "loss": 3.9697,
      "step": 5480
    },
    {
      "epoch": 1.7613089509143407,
      "grad_norm": 2.1608316898345947,
      "learning_rate": 8.260079135921293e-05,
      "loss": 3.7395,
      "step": 5490
    },
    {
      "epoch": 1.7645171639396855,
      "grad_norm": 2.130770206451416,
      "learning_rate": 8.23869104908566e-05,
      "loss": 3.8173,
      "step": 5500
    },
    {
      "epoch": 1.7677253769650305,
      "grad_norm": 1.9758003950119019,
      "learning_rate": 8.217302962250028e-05,
      "loss": 4.0764,
      "step": 5510
    },
    {
      "epoch": 1.7709335899903753,
      "grad_norm": 3.404630422592163,
      "learning_rate": 8.195914875414395e-05,
      "loss": 3.7058,
      "step": 5520
    },
    {
      "epoch": 1.7741418030157203,
      "grad_norm": 3.1179051399230957,
      "learning_rate": 8.174526788578762e-05,
      "loss": 3.8744,
      "step": 5530
    },
    {
      "epoch": 1.7773500160410651,
      "grad_norm": 2.5145974159240723,
      "learning_rate": 8.153138701743129e-05,
      "loss": 4.1034,
      "step": 5540
    },
    {
      "epoch": 1.78055822906641,
      "grad_norm": 2.1649603843688965,
      "learning_rate": 8.131750614907497e-05,
      "loss": 3.8598,
      "step": 5550
    },
    {
      "epoch": 1.783766442091755,
      "grad_norm": 2.1873788833618164,
      "learning_rate": 8.110362528071864e-05,
      "loss": 3.907,
      "step": 5560
    },
    {
      "epoch": 1.7869746551170997,
      "grad_norm": 2.586534023284912,
      "learning_rate": 8.088974441236232e-05,
      "loss": 3.879,
      "step": 5570
    },
    {
      "epoch": 1.7901828681424448,
      "grad_norm": 1.9595320224761963,
      "learning_rate": 8.067586354400599e-05,
      "loss": 3.6576,
      "step": 5580
    },
    {
      "epoch": 1.7933910811677896,
      "grad_norm": 1.889378547668457,
      "learning_rate": 8.046198267564967e-05,
      "loss": 3.6444,
      "step": 5590
    },
    {
      "epoch": 1.7965992941931344,
      "grad_norm": 2.947279214859009,
      "learning_rate": 8.024810180729334e-05,
      "loss": 4.1237,
      "step": 5600
    },
    {
      "epoch": 1.7998075072184792,
      "grad_norm": 1.724255919456482,
      "learning_rate": 8.003422093893703e-05,
      "loss": 3.841,
      "step": 5610
    },
    {
      "epoch": 1.8030157202438242,
      "grad_norm": 3.8523592948913574,
      "learning_rate": 7.98203400705807e-05,
      "loss": 3.6492,
      "step": 5620
    },
    {
      "epoch": 1.8062239332691692,
      "grad_norm": 2.1152570247650146,
      "learning_rate": 7.960645920222436e-05,
      "loss": 4.0659,
      "step": 5630
    },
    {
      "epoch": 1.809432146294514,
      "grad_norm": 3.992964267730713,
      "learning_rate": 7.939257833386803e-05,
      "loss": 3.6581,
      "step": 5640
    },
    {
      "epoch": 1.8126403593198588,
      "grad_norm": 2.420858144760132,
      "learning_rate": 7.91786974655117e-05,
      "loss": 3.9633,
      "step": 5650
    },
    {
      "epoch": 1.8158485723452036,
      "grad_norm": 2.205268144607544,
      "learning_rate": 7.896481659715539e-05,
      "loss": 4.2237,
      "step": 5660
    },
    {
      "epoch": 1.8190567853705486,
      "grad_norm": 3.0390148162841797,
      "learning_rate": 7.875093572879905e-05,
      "loss": 3.6755,
      "step": 5670
    },
    {
      "epoch": 1.8222649983958936,
      "grad_norm": 1.7843643426895142,
      "learning_rate": 7.853705486044274e-05,
      "loss": 3.8437,
      "step": 5680
    },
    {
      "epoch": 1.8254732114212384,
      "grad_norm": 2.0913503170013428,
      "learning_rate": 7.83231739920864e-05,
      "loss": 3.9743,
      "step": 5690
    },
    {
      "epoch": 1.8286814244465832,
      "grad_norm": 1.9339656829833984,
      "learning_rate": 7.810929312373009e-05,
      "loss": 3.8953,
      "step": 5700
    },
    {
      "epoch": 1.831889637471928,
      "grad_norm": 2.1359927654266357,
      "learning_rate": 7.789541225537376e-05,
      "loss": 3.7624,
      "step": 5710
    },
    {
      "epoch": 1.835097850497273,
      "grad_norm": 2.043696165084839,
      "learning_rate": 7.768153138701744e-05,
      "loss": 4.0363,
      "step": 5720
    },
    {
      "epoch": 1.838306063522618,
      "grad_norm": 3.1259987354278564,
      "learning_rate": 7.746765051866111e-05,
      "loss": 3.9388,
      "step": 5730
    },
    {
      "epoch": 1.8415142765479628,
      "grad_norm": 2.5812289714813232,
      "learning_rate": 7.725376965030479e-05,
      "loss": 3.777,
      "step": 5740
    },
    {
      "epoch": 1.8447224895733076,
      "grad_norm": 2.2320191860198975,
      "learning_rate": 7.703988878194846e-05,
      "loss": 3.949,
      "step": 5750
    },
    {
      "epoch": 1.8479307025986524,
      "grad_norm": 2.0422184467315674,
      "learning_rate": 7.682600791359213e-05,
      "loss": 3.7515,
      "step": 5760
    },
    {
      "epoch": 1.8511389156239975,
      "grad_norm": 2.393946409225464,
      "learning_rate": 7.66121270452358e-05,
      "loss": 3.8027,
      "step": 5770
    },
    {
      "epoch": 1.8543471286493425,
      "grad_norm": 2.268441677093506,
      "learning_rate": 7.639824617687948e-05,
      "loss": 3.79,
      "step": 5780
    },
    {
      "epoch": 1.8575553416746873,
      "grad_norm": 3.2238974571228027,
      "learning_rate": 7.618436530852315e-05,
      "loss": 3.8161,
      "step": 5790
    },
    {
      "epoch": 1.860763554700032,
      "grad_norm": 2.5628864765167236,
      "learning_rate": 7.597048444016683e-05,
      "loss": 3.9726,
      "step": 5800
    },
    {
      "epoch": 1.8639717677253769,
      "grad_norm": 1.968786358833313,
      "learning_rate": 7.57566035718105e-05,
      "loss": 3.9081,
      "step": 5810
    },
    {
      "epoch": 1.867179980750722,
      "grad_norm": 2.2866218090057373,
      "learning_rate": 7.554272270345419e-05,
      "loss": 3.8581,
      "step": 5820
    },
    {
      "epoch": 1.8703881937760667,
      "grad_norm": 2.2269999980926514,
      "learning_rate": 7.532884183509785e-05,
      "loss": 3.9282,
      "step": 5830
    },
    {
      "epoch": 1.8735964068014117,
      "grad_norm": 2.9844579696655273,
      "learning_rate": 7.511496096674154e-05,
      "loss": 3.719,
      "step": 5840
    },
    {
      "epoch": 1.8768046198267565,
      "grad_norm": 1.6712861061096191,
      "learning_rate": 7.49010800983852e-05,
      "loss": 3.698,
      "step": 5850
    },
    {
      "epoch": 1.8800128328521013,
      "grad_norm": 2.892049789428711,
      "learning_rate": 7.468719923002888e-05,
      "loss": 4.0408,
      "step": 5860
    },
    {
      "epoch": 1.883221045877446,
      "grad_norm": 1.9607446193695068,
      "learning_rate": 7.447331836167254e-05,
      "loss": 3.8413,
      "step": 5870
    },
    {
      "epoch": 1.8864292589027911,
      "grad_norm": 2.3757266998291016,
      "learning_rate": 7.425943749331623e-05,
      "loss": 3.6356,
      "step": 5880
    },
    {
      "epoch": 1.8896374719281361,
      "grad_norm": 1.7743455171585083,
      "learning_rate": 7.40455566249599e-05,
      "loss": 3.8262,
      "step": 5890
    },
    {
      "epoch": 1.892845684953481,
      "grad_norm": 2.5823593139648438,
      "learning_rate": 7.383167575660358e-05,
      "loss": 3.782,
      "step": 5900
    },
    {
      "epoch": 1.8960538979788257,
      "grad_norm": 1.8660993576049805,
      "learning_rate": 7.361779488824725e-05,
      "loss": 3.8806,
      "step": 5910
    },
    {
      "epoch": 1.8992621110041705,
      "grad_norm": 1.7500735521316528,
      "learning_rate": 7.340391401989093e-05,
      "loss": 3.9413,
      "step": 5920
    },
    {
      "epoch": 1.9024703240295155,
      "grad_norm": 1.8012959957122803,
      "learning_rate": 7.31900331515346e-05,
      "loss": 3.831,
      "step": 5930
    },
    {
      "epoch": 1.9056785370548606,
      "grad_norm": 2.121507167816162,
      "learning_rate": 7.297615228317828e-05,
      "loss": 3.967,
      "step": 5940
    },
    {
      "epoch": 1.9088867500802054,
      "grad_norm": 2.1953203678131104,
      "learning_rate": 7.276227141482195e-05,
      "loss": 3.9205,
      "step": 5950
    },
    {
      "epoch": 1.9120949631055502,
      "grad_norm": 2.119879961013794,
      "learning_rate": 7.254839054646562e-05,
      "loss": 3.7503,
      "step": 5960
    },
    {
      "epoch": 1.915303176130895,
      "grad_norm": 2.4336776733398438,
      "learning_rate": 7.23345096781093e-05,
      "loss": 4.0268,
      "step": 5970
    },
    {
      "epoch": 1.91851138915624,
      "grad_norm": 2.331946611404419,
      "learning_rate": 7.212062880975297e-05,
      "loss": 3.8841,
      "step": 5980
    },
    {
      "epoch": 1.921719602181585,
      "grad_norm": 2.2881247997283936,
      "learning_rate": 7.190674794139664e-05,
      "loss": 3.9919,
      "step": 5990
    },
    {
      "epoch": 1.9249278152069298,
      "grad_norm": 2.9937267303466797,
      "learning_rate": 7.169286707304031e-05,
      "loss": 3.5521,
      "step": 6000
    },
    {
      "epoch": 1.9281360282322746,
      "grad_norm": 1.940887212753296,
      "learning_rate": 7.1478986204684e-05,
      "loss": 3.8624,
      "step": 6010
    },
    {
      "epoch": 1.9313442412576194,
      "grad_norm": 1.9826207160949707,
      "learning_rate": 7.126510533632766e-05,
      "loss": 3.7752,
      "step": 6020
    },
    {
      "epoch": 1.9345524542829644,
      "grad_norm": 2.5633461475372314,
      "learning_rate": 7.105122446797135e-05,
      "loss": 3.7207,
      "step": 6030
    },
    {
      "epoch": 1.9377606673083094,
      "grad_norm": 1.962955117225647,
      "learning_rate": 7.083734359961501e-05,
      "loss": 3.6826,
      "step": 6040
    },
    {
      "epoch": 1.9409688803336542,
      "grad_norm": 2.3063435554504395,
      "learning_rate": 7.06234627312587e-05,
      "loss": 3.7154,
      "step": 6050
    },
    {
      "epoch": 1.944177093358999,
      "grad_norm": 1.9532605409622192,
      "learning_rate": 7.040958186290237e-05,
      "loss": 3.9013,
      "step": 6060
    },
    {
      "epoch": 1.9473853063843438,
      "grad_norm": 3.4191248416900635,
      "learning_rate": 7.019570099454605e-05,
      "loss": 3.8076,
      "step": 6070
    },
    {
      "epoch": 1.9505935194096888,
      "grad_norm": 2.2814388275146484,
      "learning_rate": 6.998182012618972e-05,
      "loss": 3.7739,
      "step": 6080
    },
    {
      "epoch": 1.9538017324350336,
      "grad_norm": 3.149883985519409,
      "learning_rate": 6.976793925783339e-05,
      "loss": 3.8021,
      "step": 6090
    },
    {
      "epoch": 1.9570099454603787,
      "grad_norm": 1.8258243799209595,
      "learning_rate": 6.955405838947706e-05,
      "loss": 3.6453,
      "step": 6100
    },
    {
      "epoch": 1.9602181584857234,
      "grad_norm": 1.5753703117370605,
      "learning_rate": 6.934017752112074e-05,
      "loss": 3.9546,
      "step": 6110
    },
    {
      "epoch": 1.9634263715110682,
      "grad_norm": 2.9321956634521484,
      "learning_rate": 6.912629665276441e-05,
      "loss": 3.7632,
      "step": 6120
    },
    {
      "epoch": 1.9666345845364133,
      "grad_norm": 2.0775794982910156,
      "learning_rate": 6.891241578440809e-05,
      "loss": 3.6231,
      "step": 6130
    },
    {
      "epoch": 1.969842797561758,
      "grad_norm": 2.87746262550354,
      "learning_rate": 6.869853491605176e-05,
      "loss": 3.7856,
      "step": 6140
    },
    {
      "epoch": 1.973051010587103,
      "grad_norm": 1.981567144393921,
      "learning_rate": 6.848465404769544e-05,
      "loss": 3.6371,
      "step": 6150
    },
    {
      "epoch": 1.9762592236124479,
      "grad_norm": 2.3796095848083496,
      "learning_rate": 6.827077317933911e-05,
      "loss": 3.8155,
      "step": 6160
    },
    {
      "epoch": 1.9794674366377927,
      "grad_norm": 2.02878475189209,
      "learning_rate": 6.80568923109828e-05,
      "loss": 3.9665,
      "step": 6170
    },
    {
      "epoch": 1.9826756496631375,
      "grad_norm": 1.8706822395324707,
      "learning_rate": 6.784301144262646e-05,
      "loss": 3.7484,
      "step": 6180
    },
    {
      "epoch": 1.9858838626884825,
      "grad_norm": 2.448698043823242,
      "learning_rate": 6.762913057427015e-05,
      "loss": 3.645,
      "step": 6190
    },
    {
      "epoch": 1.9890920757138275,
      "grad_norm": 2.5770819187164307,
      "learning_rate": 6.741524970591381e-05,
      "loss": 3.9788,
      "step": 6200
    },
    {
      "epoch": 1.9923002887391723,
      "grad_norm": 1.744452953338623,
      "learning_rate": 6.720136883755748e-05,
      "loss": 3.9666,
      "step": 6210
    },
    {
      "epoch": 1.995508501764517,
      "grad_norm": 2.6971821784973145,
      "learning_rate": 6.698748796920115e-05,
      "loss": 3.9415,
      "step": 6220
    },
    {
      "epoch": 1.998716714789862,
      "grad_norm": 1.816607117652893,
      "learning_rate": 6.677360710084484e-05,
      "loss": 3.7364,
      "step": 6230
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.689649820327759,
      "eval_runtime": 4.2969,
      "eval_samples_per_second": 322.555,
      "eval_steps_per_second": 40.494,
      "step": 6234
    },
    {
      "epoch": 2.001924927815207,
      "grad_norm": 1.9239943027496338,
      "learning_rate": 6.65597262324885e-05,
      "loss": 3.846,
      "step": 6240
    },
    {
      "epoch": 2.005133140840552,
      "grad_norm": 2.648892402648926,
      "learning_rate": 6.634584536413219e-05,
      "loss": 4.0868,
      "step": 6250
    },
    {
      "epoch": 2.0083413538658967,
      "grad_norm": 1.979353904724121,
      "learning_rate": 6.613196449577586e-05,
      "loss": 3.7695,
      "step": 6260
    },
    {
      "epoch": 2.0115495668912415,
      "grad_norm": 2.5212345123291016,
      "learning_rate": 6.591808362741954e-05,
      "loss": 3.8332,
      "step": 6270
    },
    {
      "epoch": 2.0147577799165863,
      "grad_norm": 3.3330397605895996,
      "learning_rate": 6.570420275906321e-05,
      "loss": 3.7802,
      "step": 6280
    },
    {
      "epoch": 2.017965992941931,
      "grad_norm": 2.0124900341033936,
      "learning_rate": 6.549032189070689e-05,
      "loss": 3.9047,
      "step": 6290
    },
    {
      "epoch": 2.0211742059672764,
      "grad_norm": 2.0812013149261475,
      "learning_rate": 6.527644102235056e-05,
      "loss": 3.8205,
      "step": 6300
    },
    {
      "epoch": 2.024382418992621,
      "grad_norm": 2.151435613632202,
      "learning_rate": 6.506256015399423e-05,
      "loss": 3.8401,
      "step": 6310
    },
    {
      "epoch": 2.027590632017966,
      "grad_norm": 2.3001785278320312,
      "learning_rate": 6.48486792856379e-05,
      "loss": 3.6041,
      "step": 6320
    },
    {
      "epoch": 2.0307988450433108,
      "grad_norm": 2.5784029960632324,
      "learning_rate": 6.463479841728157e-05,
      "loss": 4.0232,
      "step": 6330
    },
    {
      "epoch": 2.0340070580686556,
      "grad_norm": 2.8760838508605957,
      "learning_rate": 6.442091754892525e-05,
      "loss": 3.7254,
      "step": 6340
    },
    {
      "epoch": 2.037215271094001,
      "grad_norm": 1.8910409212112427,
      "learning_rate": 6.420703668056892e-05,
      "loss": 3.6862,
      "step": 6350
    },
    {
      "epoch": 2.0404234841193456,
      "grad_norm": 2.006903886795044,
      "learning_rate": 6.39931558122126e-05,
      "loss": 3.7746,
      "step": 6360
    },
    {
      "epoch": 2.0436316971446904,
      "grad_norm": 1.948969841003418,
      "learning_rate": 6.377927494385627e-05,
      "loss": 3.9618,
      "step": 6370
    },
    {
      "epoch": 2.046839910170035,
      "grad_norm": 1.8536639213562012,
      "learning_rate": 6.356539407549995e-05,
      "loss": 3.9554,
      "step": 6380
    },
    {
      "epoch": 2.05004812319538,
      "grad_norm": 2.6327993869781494,
      "learning_rate": 6.335151320714362e-05,
      "loss": 3.9015,
      "step": 6390
    },
    {
      "epoch": 2.0532563362207252,
      "grad_norm": 2.118114709854126,
      "learning_rate": 6.31376323387873e-05,
      "loss": 3.9235,
      "step": 6400
    },
    {
      "epoch": 2.05646454924607,
      "grad_norm": 2.166095733642578,
      "learning_rate": 6.292375147043097e-05,
      "loss": 3.8212,
      "step": 6410
    },
    {
      "epoch": 2.059672762271415,
      "grad_norm": 2.448218584060669,
      "learning_rate": 6.270987060207464e-05,
      "loss": 3.8721,
      "step": 6420
    },
    {
      "epoch": 2.0628809752967596,
      "grad_norm": 2.997519016265869,
      "learning_rate": 6.249598973371831e-05,
      "loss": 3.7703,
      "step": 6430
    },
    {
      "epoch": 2.0660891883221044,
      "grad_norm": 2.0549559593200684,
      "learning_rate": 6.2282108865362e-05,
      "loss": 3.9158,
      "step": 6440
    },
    {
      "epoch": 2.0692974013474497,
      "grad_norm": 1.8234206438064575,
      "learning_rate": 6.206822799700566e-05,
      "loss": 3.9096,
      "step": 6450
    },
    {
      "epoch": 2.0725056143727945,
      "grad_norm": 1.9059096574783325,
      "learning_rate": 6.185434712864935e-05,
      "loss": 3.6501,
      "step": 6460
    },
    {
      "epoch": 2.0757138273981393,
      "grad_norm": 1.7030199766159058,
      "learning_rate": 6.164046626029302e-05,
      "loss": 3.6931,
      "step": 6470
    },
    {
      "epoch": 2.078922040423484,
      "grad_norm": 2.2955710887908936,
      "learning_rate": 6.14265853919367e-05,
      "loss": 3.7289,
      "step": 6480
    },
    {
      "epoch": 2.082130253448829,
      "grad_norm": 1.7562758922576904,
      "learning_rate": 6.121270452358037e-05,
      "loss": 3.7998,
      "step": 6490
    },
    {
      "epoch": 2.085338466474174,
      "grad_norm": 5.040389537811279,
      "learning_rate": 6.099882365522405e-05,
      "loss": 3.8833,
      "step": 6500
    }
  ],
  "logging_steps": 10,
  "max_steps": 9351,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1704287010816000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
