{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1350,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.022222222222222223,
      "grad_norm": NaN,
      "learning_rate": 0.00019866666666666668,
      "loss": 7.5592,
      "step": 10
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 1.0002001523971558,
      "learning_rate": 0.0001971851851851852,
      "loss": 6.8608,
      "step": 20
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.514258861541748,
      "learning_rate": 0.0001957037037037037,
      "loss": 5.9519,
      "step": 30
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 1.6999998092651367,
      "learning_rate": 0.00019422222222222223,
      "loss": 5.4073,
      "step": 40
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.3975061178207397,
      "learning_rate": 0.00019274074074074075,
      "loss": 5.1726,
      "step": 50
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.734574794769287,
      "learning_rate": 0.00019125925925925926,
      "loss": 4.6245,
      "step": 60
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 1.7296136617660522,
      "learning_rate": 0.00018977777777777778,
      "loss": 4.6302,
      "step": 70
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.841040849685669,
      "learning_rate": 0.0001882962962962963,
      "loss": 4.5581,
      "step": 80
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.489641547203064,
      "learning_rate": 0.0001868148148148148,
      "loss": 4.5659,
      "step": 90
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 1.3450934886932373,
      "learning_rate": 0.00018533333333333333,
      "loss": 4.2786,
      "step": 100
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 2.5396065711975098,
      "learning_rate": 0.00018385185185185185,
      "loss": 4.4905,
      "step": 110
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.617583990097046,
      "learning_rate": 0.00018237037037037036,
      "loss": 4.3143,
      "step": 120
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 1.9875487089157104,
      "learning_rate": 0.0001808888888888889,
      "loss": 4.4093,
      "step": 130
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 1.5278558731079102,
      "learning_rate": 0.00017940740740740742,
      "loss": 4.2885,
      "step": 140
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.847956657409668,
      "learning_rate": 0.00017792592592592594,
      "loss": 4.3652,
      "step": 150
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 2.0871803760528564,
      "learning_rate": 0.00017644444444444446,
      "loss": 4.1588,
      "step": 160
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 1.9803929328918457,
      "learning_rate": 0.00017496296296296297,
      "loss": 4.3061,
      "step": 170
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.88179612159729,
      "learning_rate": 0.0001734814814814815,
      "loss": 4.3017,
      "step": 180
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 1.423699140548706,
      "learning_rate": 0.000172,
      "loss": 4.2925,
      "step": 190
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 5.172730445861816,
      "learning_rate": 0.00017051851851851852,
      "loss": 4.3831,
      "step": 200
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 2.0437779426574707,
      "learning_rate": 0.00016903703703703704,
      "loss": 4.3322,
      "step": 210
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 2.517566680908203,
      "learning_rate": 0.00016755555555555556,
      "loss": 4.2288,
      "step": 220
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 4.185309410095215,
      "learning_rate": 0.0001660740740740741,
      "loss": 4.0353,
      "step": 230
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.2583043575286865,
      "learning_rate": 0.00016459259259259262,
      "loss": 3.8267,
      "step": 240
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 2.780539035797119,
      "learning_rate": 0.00016311111111111113,
      "loss": 4.1367,
      "step": 250
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 1.787148356437683,
      "learning_rate": 0.00016162962962962965,
      "loss": 4.1095,
      "step": 260
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.991499900817871,
      "learning_rate": 0.00016014814814814817,
      "loss": 4.049,
      "step": 270
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 3.301140308380127,
      "learning_rate": 0.00015866666666666668,
      "loss": 3.8845,
      "step": 280
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 2.12703013420105,
      "learning_rate": 0.0001571851851851852,
      "loss": 3.9915,
      "step": 290
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.9960891008377075,
      "learning_rate": 0.00015570370370370372,
      "loss": 4.0721,
      "step": 300
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 2.377140998840332,
      "learning_rate": 0.00015422222222222223,
      "loss": 3.9122,
      "step": 310
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 2.413872480392456,
      "learning_rate": 0.00015274074074074075,
      "loss": 4.0409,
      "step": 320
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 3.0002167224884033,
      "learning_rate": 0.00015125925925925927,
      "loss": 3.8099,
      "step": 330
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 2.2120206356048584,
      "learning_rate": 0.00014977777777777778,
      "loss": 3.8407,
      "step": 340
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 2.7954413890838623,
      "learning_rate": 0.0001482962962962963,
      "loss": 3.7917,
      "step": 350
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.978381633758545,
      "learning_rate": 0.00014681481481481482,
      "loss": 3.9913,
      "step": 360
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 2.130051851272583,
      "learning_rate": 0.00014533333333333333,
      "loss": 3.7574,
      "step": 370
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 2.883937358856201,
      "learning_rate": 0.00014385185185185185,
      "loss": 3.7893,
      "step": 380
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 2.3012855052948,
      "learning_rate": 0.00014237037037037037,
      "loss": 3.9839,
      "step": 390
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.663818597793579,
      "learning_rate": 0.00014088888888888888,
      "loss": 3.9698,
      "step": 400
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 2.5065982341766357,
      "learning_rate": 0.0001394074074074074,
      "loss": 4.107,
      "step": 410
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.8524818420410156,
      "learning_rate": 0.00013792592592592592,
      "loss": 3.7921,
      "step": 420
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 1.8428270816802979,
      "learning_rate": 0.00013644444444444443,
      "loss": 4.0242,
      "step": 430
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.7257611751556396,
      "learning_rate": 0.00013496296296296298,
      "loss": 3.8031,
      "step": 440
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.7180742025375366,
      "learning_rate": 0.0001334814814814815,
      "loss": 3.7366,
      "step": 450
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.6097497940063477,
      "eval_runtime": 0.6219,
      "eval_samples_per_second": 321.594,
      "eval_steps_per_second": 40.199,
      "step": 450
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 2.1147327423095703,
      "learning_rate": 0.000132,
      "loss": 3.7672,
      "step": 460
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 2.496960401535034,
      "learning_rate": 0.00013051851851851853,
      "loss": 3.8518,
      "step": 470
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 2.196284055709839,
      "learning_rate": 0.00012903703703703704,
      "loss": 4.0501,
      "step": 480
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 1.8659701347351074,
      "learning_rate": 0.00012755555555555556,
      "loss": 3.8568,
      "step": 490
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.7193256616592407,
      "learning_rate": 0.00012607407407407408,
      "loss": 3.9082,
      "step": 500
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 2.1795363426208496,
      "learning_rate": 0.0001245925925925926,
      "loss": 3.7217,
      "step": 510
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 2.652407646179199,
      "learning_rate": 0.0001231111111111111,
      "loss": 3.8765,
      "step": 520
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 2.290348529815674,
      "learning_rate": 0.00012162962962962963,
      "loss": 3.5641,
      "step": 530
    },
    {
      "epoch": 1.2,
      "grad_norm": 3.1338629722595215,
      "learning_rate": 0.00012014814814814814,
      "loss": 3.5841,
      "step": 540
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 2.8144638538360596,
      "learning_rate": 0.00011866666666666669,
      "loss": 3.8368,
      "step": 550
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 2.4091644287109375,
      "learning_rate": 0.0001171851851851852,
      "loss": 3.8079,
      "step": 560
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 2.694390296936035,
      "learning_rate": 0.00011570370370370372,
      "loss": 3.8535,
      "step": 570
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 2.308865785598755,
      "learning_rate": 0.00011422222222222224,
      "loss": 3.9216,
      "step": 580
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 2.8598666191101074,
      "learning_rate": 0.00011274074074074074,
      "loss": 3.8672,
      "step": 590
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.9498353004455566,
      "learning_rate": 0.00011125925925925926,
      "loss": 3.7669,
      "step": 600
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 2.3273770809173584,
      "learning_rate": 0.00010977777777777777,
      "loss": 3.7323,
      "step": 610
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 2.4256484508514404,
      "learning_rate": 0.00010829629629629629,
      "loss": 3.84,
      "step": 620
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.0698726177215576,
      "learning_rate": 0.0001068148148148148,
      "loss": 3.7189,
      "step": 630
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 2.123128890991211,
      "learning_rate": 0.00010533333333333332,
      "loss": 3.7347,
      "step": 640
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 2.0175423622131348,
      "learning_rate": 0.00010385185185185187,
      "loss": 3.7795,
      "step": 650
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 4.026335716247559,
      "learning_rate": 0.00010237037037037038,
      "loss": 3.757,
      "step": 660
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 2.277179479598999,
      "learning_rate": 0.0001008888888888889,
      "loss": 3.6564,
      "step": 670
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 2.883333921432495,
      "learning_rate": 9.940740740740742e-05,
      "loss": 4.0001,
      "step": 680
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 2.189471483230591,
      "learning_rate": 9.792592592592593e-05,
      "loss": 3.891,
      "step": 690
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 3.200540781021118,
      "learning_rate": 9.644444444444445e-05,
      "loss": 3.7006,
      "step": 700
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 2.250440835952759,
      "learning_rate": 9.496296296296297e-05,
      "loss": 3.6219,
      "step": 710
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.0120677947998047,
      "learning_rate": 9.348148148148148e-05,
      "loss": 3.6266,
      "step": 720
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 2.4617323875427246,
      "learning_rate": 9.200000000000001e-05,
      "loss": 3.8216,
      "step": 730
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 2.6916046142578125,
      "learning_rate": 9.051851851851853e-05,
      "loss": 3.744,
      "step": 740
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.1065917015075684,
      "learning_rate": 8.903703703703705e-05,
      "loss": 4.2216,
      "step": 750
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 1.8737945556640625,
      "learning_rate": 8.755555555555556e-05,
      "loss": 3.8029,
      "step": 760
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 2.420947790145874,
      "learning_rate": 8.607407407407408e-05,
      "loss": 3.8652,
      "step": 770
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 2.318789482116699,
      "learning_rate": 8.45925925925926e-05,
      "loss": 3.9287,
      "step": 780
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 1.9455351829528809,
      "learning_rate": 8.311111111111111e-05,
      "loss": 3.6461,
      "step": 790
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 2.844564199447632,
      "learning_rate": 8.162962962962963e-05,
      "loss": 3.7703,
      "step": 800
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.104227066040039,
      "learning_rate": 8.014814814814815e-05,
      "loss": 3.6831,
      "step": 810
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 2.5832314491271973,
      "learning_rate": 7.866666666666666e-05,
      "loss": 3.7588,
      "step": 820
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 1.9490848779678345,
      "learning_rate": 7.71851851851852e-05,
      "loss": 3.8527,
      "step": 830
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 2.68280291557312,
      "learning_rate": 7.570370370370371e-05,
      "loss": 3.7489,
      "step": 840
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 2.020634174346924,
      "learning_rate": 7.422222222222223e-05,
      "loss": 3.941,
      "step": 850
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 2.441362142562866,
      "learning_rate": 7.274074074074074e-05,
      "loss": 3.9807,
      "step": 860
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 2.256321668624878,
      "learning_rate": 7.125925925925926e-05,
      "loss": 3.8309,
      "step": 870
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 2.9416089057922363,
      "learning_rate": 6.977777777777779e-05,
      "loss": 3.726,
      "step": 880
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 2.576784372329712,
      "learning_rate": 6.829629629629631e-05,
      "loss": 3.705,
      "step": 890
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.223825454711914,
      "learning_rate": 6.681481481481482e-05,
      "loss": 3.8524,
      "step": 900
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.538109064102173,
      "eval_runtime": 0.6325,
      "eval_samples_per_second": 316.202,
      "eval_steps_per_second": 39.525,
      "step": 900
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 2.8309035301208496,
      "learning_rate": 6.533333333333334e-05,
      "loss": 3.8016,
      "step": 910
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 2.3316872119903564,
      "learning_rate": 6.385185185185186e-05,
      "loss": 3.8946,
      "step": 920
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 2.114025592803955,
      "learning_rate": 6.237037037037037e-05,
      "loss": 3.7442,
      "step": 930
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 2.3024239540100098,
      "learning_rate": 6.08888888888889e-05,
      "loss": 3.8717,
      "step": 940
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 2.6676251888275146,
      "learning_rate": 5.9407407407407414e-05,
      "loss": 3.8042,
      "step": 950
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 2.380887269973755,
      "learning_rate": 5.792592592592593e-05,
      "loss": 3.78,
      "step": 960
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 2.0837390422821045,
      "learning_rate": 5.644444444444445e-05,
      "loss": 3.8538,
      "step": 970
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 2.025076389312744,
      "learning_rate": 5.4962962962962964e-05,
      "loss": 3.4989,
      "step": 980
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.254840612411499,
      "learning_rate": 5.348148148148149e-05,
      "loss": 3.6157,
      "step": 990
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 2.927389144897461,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 3.8169,
      "step": 1000
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 2.5632290840148926,
      "learning_rate": 5.051851851851852e-05,
      "loss": 3.9246,
      "step": 1010
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 2.234510660171509,
      "learning_rate": 4.903703703703704e-05,
      "loss": 3.6572,
      "step": 1020
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 3.1348462104797363,
      "learning_rate": 4.755555555555556e-05,
      "loss": 3.7249,
      "step": 1030
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 2.770350456237793,
      "learning_rate": 4.607407407407408e-05,
      "loss": 3.7475,
      "step": 1040
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 3.6149744987487793,
      "learning_rate": 4.4592592592592594e-05,
      "loss": 3.8773,
      "step": 1050
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 2.472212314605713,
      "learning_rate": 4.311111111111111e-05,
      "loss": 3.7225,
      "step": 1060
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 2.0862998962402344,
      "learning_rate": 4.162962962962963e-05,
      "loss": 3.8364,
      "step": 1070
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.901318073272705,
      "learning_rate": 4.014814814814815e-05,
      "loss": 3.75,
      "step": 1080
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 2.7660810947418213,
      "learning_rate": 3.866666666666667e-05,
      "loss": 3.6492,
      "step": 1090
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.8520081043243408,
      "learning_rate": 3.718518518518519e-05,
      "loss": 3.7441,
      "step": 1100
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 1.9760524034500122,
      "learning_rate": 3.570370370370371e-05,
      "loss": 3.8335,
      "step": 1110
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 1.8779224157333374,
      "learning_rate": 3.4222222222222224e-05,
      "loss": 3.5251,
      "step": 1120
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 4.109439849853516,
      "learning_rate": 3.274074074074075e-05,
      "loss": 3.7687,
      "step": 1130
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 2.6588313579559326,
      "learning_rate": 3.1259259259259264e-05,
      "loss": 3.7828,
      "step": 1140
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 2.660327196121216,
      "learning_rate": 2.9777777777777777e-05,
      "loss": 3.8205,
      "step": 1150
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 2.3896591663360596,
      "learning_rate": 2.8296296296296297e-05,
      "loss": 3.5764,
      "step": 1160
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.657138466835022,
      "learning_rate": 2.6814814814814814e-05,
      "loss": 3.7142,
      "step": 1170
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 1.9370330572128296,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 3.8141,
      "step": 1180
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 2.0583457946777344,
      "learning_rate": 2.3851851851851854e-05,
      "loss": 3.7217,
      "step": 1190
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 2.6509149074554443,
      "learning_rate": 2.2370370370370374e-05,
      "loss": 3.6664,
      "step": 1200
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 2.285348653793335,
      "learning_rate": 2.088888888888889e-05,
      "loss": 3.9148,
      "step": 1210
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 1.8224443197250366,
      "learning_rate": 1.9407407407407407e-05,
      "loss": 3.5817,
      "step": 1220
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 2.4862213134765625,
      "learning_rate": 1.7925925925925927e-05,
      "loss": 3.8855,
      "step": 1230
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 2.3791821002960205,
      "learning_rate": 1.6444444444444447e-05,
      "loss": 3.7002,
      "step": 1240
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 2.293961524963379,
      "learning_rate": 1.4962962962962965e-05,
      "loss": 3.6284,
      "step": 1250
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.875502586364746,
      "learning_rate": 1.348148148148148e-05,
      "loss": 3.6528,
      "step": 1260
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 2.289391040802002,
      "learning_rate": 1.2e-05,
      "loss": 3.7714,
      "step": 1270
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 1.9243581295013428,
      "learning_rate": 1.0518518518518519e-05,
      "loss": 3.6028,
      "step": 1280
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 2.0860655307769775,
      "learning_rate": 9.037037037037037e-06,
      "loss": 3.715,
      "step": 1290
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 2.909653663635254,
      "learning_rate": 7.555555555555556e-06,
      "loss": 3.7157,
      "step": 1300
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 2.6621031761169434,
      "learning_rate": 6.0740740740740745e-06,
      "loss": 3.5197,
      "step": 1310
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 1.702470302581787,
      "learning_rate": 4.592592592592593e-06,
      "loss": 3.8834,
      "step": 1320
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 2.928288459777832,
      "learning_rate": 3.111111111111111e-06,
      "loss": 3.5084,
      "step": 1330
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 3.4078519344329834,
      "learning_rate": 1.6296296296296295e-06,
      "loss": 3.9043,
      "step": 1340
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.3840742111206055,
      "learning_rate": 1.4814814814814815e-07,
      "loss": 3.7358,
      "step": 1350
    }
  ],
  "logging_steps": 10,
  "max_steps": 1350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 353967302246400.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
